"""
Empire v7.3 - Document Processing Tasks
Celery tasks for document parsing, validation, and metadata extraction

Production Readiness: All TODOs implemented with real service integrations
"""

from app.celery_app import celery_app, PRIORITY_URGENT, PRIORITY_HIGH, PRIORITY_NORMAL, PRIORITY_LOW, PRIORITY_BACKGROUND
from app.services.supabase_storage import get_supabase_storage
from app.services.notification_dispatcher import get_notification_dispatcher
from app.services.b2_workflow import get_workflow_manager
from app.services.b2_storage import B2StorageService
from app.services.metadata_extractor import get_metadata_extractor
from app.services.llama_index_service import get_llama_index_service
from app.services.department_classifier_agent import get_department_classifier_service
from typing import Dict, Any, Optional
import os
import asyncio
import logging
import tempfile
import hashlib
from pathlib import Path

logger = logging.getLogger(__name__)

# Get notification dispatcher (lightweight, safe at import)
dispatcher = get_notification_dispatcher()

# Lazy workflow manager initialization (requires B2 credentials)
_workflow_manager = None


def _get_workflow_manager():
    """Lazy initialize workflow manager to avoid B2 import errors in tests"""
    global _workflow_manager
    if _workflow_manager is None:
        _workflow_manager = get_workflow_manager()
    return _workflow_manager


@celery_app.task(name='app.tasks.document_processing.process_document', bind=True)
def process_document(self, file_id: str, filename: str, b2_path: str) -> Dict[str, Any]:
    """
    Process a document: parse, extract metadata, validate

    Args:
        file_id: B2 file ID
        filename: Original filename
        b2_path: Full path in B2 storage

    Returns:
        Processing result with metadata
    """
    supabase_storage = get_supabase_storage()

    try:
        logger.info(f"üìÑ Processing document: {filename} (file_id: {file_id})")

        # Notify task started
        dispatcher.notify_task_started(
            task_id=self.request.id,
            task_type="document_processing",
            filename=filename
        )

        # Move file from PENDING ‚Üí PROCESSING in B2
        asyncio.run(_get_workflow_manager().start_processing(
            file_id=file_id,
            processor_id=self.request.id
        ))

        # Update status to "processing" at task start in Supabase
        asyncio.run(supabase_storage.update_document_status(
            b2_file_id=file_id,
            status="processing"
        ))

        # Extract source metadata
        temp_file_path = None
        try:
            # Download file from B2 to temp location
            b2_storage = B2StorageService()
            temp_dir = tempfile.gettempdir()
            temp_file_path = os.path.join(temp_dir, filename)

            logger.info(f"üì• Downloading {filename} from B2 for metadata extraction")
            asyncio.run(b2_storage.download_file(
                file_id=file_id,
                file_name=b2_path,
                destination_path=temp_file_path
            ))

            # Extract source metadata using MetadataExtractor
            logger.info(f"üìä Extracting source metadata from {filename}")
            metadata_extractor = get_metadata_extractor()
            source_metadata = metadata_extractor.extract_source_metadata(temp_file_path)

            logger.info(f"‚úÖ Extracted source metadata: {source_metadata}")

            # Store source metadata in Supabase
            asyncio.run(supabase_storage.update_source_metadata(
                b2_file_id=file_id,
                source_metadata=source_metadata
            ))

            logger.info(f"üíæ Stored source metadata for {filename}")

            # Parse document using LlamaIndex service
            try:
                logger.info(f"üìñ Parsing document with LlamaIndex: {filename}")
                llama_service = get_llama_index_service()

                # Read file content for parsing
                with open(temp_file_path, 'rb') as f:
                    file_content = f.read()

                # Determine content type from extension
                ext = os.path.splitext(filename)[1].lower()
                content_type_map = {
                    '.pdf': 'application/pdf',
                    '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                    '.doc': 'application/msword',
                    '.txt': 'text/plain',
                    '.md': 'text/markdown',
                }
                content_type = content_type_map.get(ext, 'application/octet-stream')

                # Parse document
                parse_result = asyncio.run(llama_service.parse_document(
                    file_content=file_content,
                    filename=filename,
                    content_type=content_type,
                    parsing_instructions="Extract text content, maintain structure, identify sections and headings"
                ))

                logger.info(f"‚úÖ LlamaIndex parsing complete for {filename}")

                # Store parsed content in Supabase
                if parse_result and parse_result.get('content'):
                    asyncio.run(supabase_storage.update_parsed_content(
                        b2_file_id=file_id,
                        parsed_content=parse_result.get('content', ''),
                        parse_metadata={
                            'parse_status': 'success',
                            'sections_count': parse_result.get('sections_count', 0),
                            'word_count': len(parse_result.get('content', '').split()),
                        }
                    ))
                    logger.info(f"üíæ Stored parsed content for {filename}")

            except Exception as parse_error:
                logger.warning(f"‚ö†Ô∏è LlamaIndex parsing failed for {filename}: {parse_error}")
                # Continue processing - parsing failure is non-fatal

        except Exception as metadata_error:
            logger.error(f"‚ö†Ô∏è Metadata extraction failed for {filename}: {metadata_error}")
            # Continue processing even if metadata extraction fails
            # This is non-critical, document can still be processed

        finally:
            # Clean up temporary file
            if temp_file_path and os.path.exists(temp_file_path):
                os.remove(temp_file_path)
                logger.info(f"üóëÔ∏è Cleaned up temporary file: {temp_file_path}")

        logger.info(f"‚úÖ Document processed successfully: {filename}")

        # Move file from PROCESSING ‚Üí PROCESSED in B2
        asyncio.run(_get_workflow_manager().complete_processing(
            file_id=file_id,
            result_data={
                "task_id": self.request.id,
                "filename": filename,
                "status": "success"
            }
        ))

        # Update status to "processed" on success in Supabase
        asyncio.run(supabase_storage.update_document_status(
            b2_file_id=file_id,
            status="processed"
        ))

        # Notify task completed
        dispatcher.notify_task_completed(
            task_id=self.request.id,
            task_type="document_processing",
            filename=filename,
            result={
                "status": "success",
                "file_id": file_id,
                "filename": filename
            }
        )

        return {
            "status": "success",
            "file_id": file_id,
            "filename": filename,
            "message": "Document processing completed successfully"
        }

    except Exception as e:
        logger.error(f"‚ùå Document processing failed for {filename}: {e}")

        # Calculate exponential backoff: 60s, 120s, 240s
        retry_count = self.request.retries
        base_delay = 60  # Base delay in seconds
        countdown = base_delay * (2 ** retry_count)  # Exponential backoff

        logger.info(f"Retry attempt {retry_count + 1}/3 for {filename} - waiting {countdown}s")

        # Notify about retry
        dispatcher.notify_task_retry(
            task_id=self.request.id,
            task_type="document_processing",
            retry_count=retry_count,
            max_retries=3,
            filename=filename
        )

        # Update status to "failed" only if we've exhausted all retries
        if retry_count >= 2:  # Max 3 attempts (0, 1, 2)
            logger.error(f"All retry attempts exhausted for {filename}")

            # Move file from PROCESSING ‚Üí FAILED in B2
            asyncio.run(_get_workflow_manager().fail_processing(
                file_id=file_id,
                error=str(e),
                retry_count=retry_count
            ))

            # Update status to "failed" in Supabase
            asyncio.run(supabase_storage.update_document_status(
                b2_file_id=file_id,
                status="failed",
                processing_error=str(e)
            ))

            # Notify task failed
            dispatcher.notify_task_failed(
                task_id=self.request.id,
                task_type="document_processing",
                error=str(e),
                filename=filename
            )
        else:
            # Move file from FAILED ‚Üí PROCESSING for retry (if this is a retry attempt)
            if retry_count > 0:
                asyncio.run(_get_workflow_manager().retry_processing(
                    file_id=file_id,
                    retry_count=retry_count + 1
                ))

        # Retry with exponential backoff
        self.retry(exc=e, countdown=countdown, max_retries=3)


@celery_app.task(name='app.tasks.document_processing.extract_metadata', bind=True)
def extract_metadata(self, document_id: str) -> Dict[str, Any]:
    """
    Extract and classify document metadata using AGENT-008 Department Classifier

    Args:
        document_id: Unique document identifier

    Returns:
        Extracted metadata including department classification
    """
    supabase_storage = get_supabase_storage()

    try:
        logger.info(f"üìã Extracting metadata for: {document_id}")

        # Fetch document content from Supabase
        document = asyncio.run(supabase_storage.get_document_by_id(document_id))
        if not document:
            raise ValueError(f"Document {document_id} not found")

        # Get content for classification - use parsed_content if available, else filename
        content_to_classify = document.get('parsed_content', '') or document.get('filename', '')
        filename = document.get('filename', 'unknown')

        # Use Department Classifier Agent (AGENT-008) for classification
        classifier_service = get_department_classifier_service()
        classification_result = asyncio.run(classifier_service.classify_content(
            content=content_to_classify[:10000],  # Limit content size
            filename=filename,
            include_all_scores=False
        ))

        logger.info(
            f"üìä Classification result for {filename}: "
            f"{classification_result.department.value} ({classification_result.confidence:.2%})"
        )

        # Prepare metadata for storage
        metadata = {
            "department": classification_result.department.value,
            "department_confidence": classification_result.confidence,
            "classification_reasoning": classification_result.reasoning,
            "keywords_matched": classification_result.keywords_matched[:10],  # Top 10 keywords
            "secondary_department": classification_result.secondary_department.value if classification_result.secondary_department else None,
            "secondary_confidence": classification_result.secondary_confidence,
            "llm_enhanced": classification_result.llm_enhanced,
            "processing_time_ms": classification_result.processing_time_ms,
        }

        # Store metadata in document_metadata table via Supabase
        asyncio.run(supabase_storage.update_document_metadata(
            document_id=document_id,
            metadata=metadata
        ))

        logger.info(f"üíæ Stored document metadata for {document_id}")

        return {
            "status": "success",
            "document_id": document_id,
            "department": classification_result.department.value,
            "confidence": classification_result.confidence,
            "message": "Metadata extraction and classification completed successfully"
        }

    except Exception as e:
        # Calculate exponential backoff: 60s, 120s, 240s
        retry_count = self.request.retries
        base_delay = 60
        countdown = base_delay * (2 ** retry_count)

        logger.error(f"‚ùå Metadata extraction failed: {e}")
        logger.info(f"Retry attempt {retry_count + 1}/3 - waiting {countdown}s")

        self.retry(exc=e, countdown=countdown, max_retries=3)


@celery_app.task(name='app.tasks.document_processing.validate_document', bind=True)
def validate_document(self, document_id: str, file_hash: str) -> Dict[str, Any]:
    """
    Validate document for duplicates and integrity

    Args:
        document_id: Unique document identifier
        file_hash: SHA-256 hash of file content

    Returns:
        Validation result with duplicate detection and integrity status
    """
    supabase_storage = get_supabase_storage()

    try:
        logger.info(f"üîç Validating document: {document_id}")

        # 1. Check for duplicates by file_hash
        duplicate_doc = asyncio.run(supabase_storage.check_duplicate_by_hash(file_hash))
        is_duplicate = duplicate_doc is not None and duplicate_doc.get('document_id') != document_id

        if is_duplicate:
            logger.warning(
                f"‚ö†Ô∏è Duplicate detected: {document_id} matches {duplicate_doc.get('document_id')}"
            )

        # 2. Verify file integrity - check if the stored hash matches
        document = asyncio.run(supabase_storage.get_document_by_id(document_id))
        stored_hash = document.get('file_hash') if document else None
        hash_verified = stored_hash == file_hash if stored_hash else True

        if not hash_verified:
            logger.error(f"‚ùå Hash mismatch for {document_id}: stored={stored_hash}, provided={file_hash}")

        # 3. Check file size and format constraints
        file_size = document.get('file_size', 0) if document else 0
        filename = document.get('filename', '') if document else ''

        # Define constraints
        MAX_FILE_SIZE = 100 * 1024 * 1024  # 100 MB
        ALLOWED_EXTENSIONS = {'.pdf', '.doc', '.docx', '.txt', '.md', '.csv', '.xls', '.xlsx', '.ppt', '.pptx'}

        file_ext = os.path.splitext(filename)[1].lower() if filename else ''
        size_valid = file_size <= MAX_FILE_SIZE
        format_valid = file_ext in ALLOWED_EXTENSIONS if file_ext else True

        validation_passed = hash_verified and size_valid and format_valid

        # Build validation warnings
        warnings = []
        if is_duplicate:
            warnings.append(f"Duplicate of document {duplicate_doc.get('document_id')}")
        if not hash_verified:
            warnings.append("File hash verification failed")
        if not size_valid:
            warnings.append(f"File size ({file_size} bytes) exceeds limit ({MAX_FILE_SIZE} bytes)")
        if not format_valid:
            warnings.append(f"File format '{file_ext}' not in allowed formats")

        # Update validation status in Supabase
        asyncio.run(supabase_storage.update_document_validation_status(
            document_id=document_id,
            validation_status='valid' if validation_passed else 'invalid',
            is_duplicate=is_duplicate,
            validation_warnings=warnings
        ))

        logger.info(f"‚úÖ Validation complete for {document_id}: {'PASSED' if validation_passed else 'FAILED'}")

        return {
            "status": "success" if validation_passed else "warning",
            "document_id": document_id,
            "is_duplicate": is_duplicate,
            "duplicate_of": duplicate_doc.get('document_id') if is_duplicate else None,
            "hash_verified": hash_verified,
            "size_valid": size_valid,
            "format_valid": format_valid,
            "validation_passed": validation_passed,
            "warnings": warnings,
            "message": "Document validation completed"
        }

    except Exception as e:
        # Calculate exponential backoff: 60s, 120s, 240s
        retry_count = self.request.retries
        base_delay = 60
        countdown = base_delay * (2 ** retry_count)

        logger.error(f"‚ùå Document validation failed: {e}")
        logger.info(f"Retry attempt {retry_count + 1}/3 - waiting {countdown}s")

        self.retry(exc=e, countdown=countdown, max_retries=3)


# Helper functions for submitting tasks with priority
def submit_document_processing(
    file_id: str,
    filename: str,
    b2_path: str,
    priority: int = PRIORITY_NORMAL
) -> Any:
    """
    Submit a document processing task with specified priority

    Args:
        file_id: B2 file ID
        filename: Original filename
        b2_path: Full path in B2 storage
        priority: Task priority (0-9, 9 is highest) - use PRIORITY_* constants

    Returns:
        AsyncResult object

    Example:
        # High priority for user uploads
        submit_document_processing(file_id, filename, b2_path, priority=PRIORITY_HIGH)

        # Background processing for batch imports
        submit_document_processing(file_id, filename, b2_path, priority=PRIORITY_BACKGROUND)
    """
    return process_document.apply_async(
        args=[file_id, filename, b2_path],
        priority=priority
    )


def submit_metadata_extraction(
    document_id: str,
    priority: int = PRIORITY_NORMAL
) -> Any:
    """
    Submit a metadata extraction task with specified priority

    Args:
        document_id: Unique document identifier
        priority: Task priority (0-9, 9 is highest)

    Returns:
        AsyncResult object
    """
    return extract_metadata.apply_async(
        args=[document_id],
        priority=priority
    )


def submit_document_validation(
    document_id: str,
    file_hash: str,
    priority: int = PRIORITY_NORMAL
) -> Any:
    """
    Submit a document validation task with specified priority

    Args:
        document_id: Unique document identifier
        file_hash: SHA-256 hash of file content
        priority: Task priority (0-9, 9 is highest)

    Returns:
        AsyncResult object
    """
    return validate_document.apply_async(
        args=[document_id, file_hash],
        priority=priority
    )
