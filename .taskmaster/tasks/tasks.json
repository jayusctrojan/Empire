{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Backend Environment Setup (FastAPI, Celery, Supabase, Redis, Neo4j)",
        "description": "Establish the production backend infrastructure using FastAPI, Celery, Supabase PostgreSQL (with pgvector), Redis (Upstash), and Neo4j Community (Docker).",
        "details": "Provision Render services for FastAPI and Celery. Configure Supabase PostgreSQL with pgvector and graph tables. Set up Redis (Upstash) for caching and Celery broker. Deploy Neo4j Community via Docker on Mac Studio for knowledge graph storage. Ensure all services use TLS 1.3 and encrypted environment variables. Recommended versions: FastAPI >=0.110, Celery >=5.3, supabase-py >=2.0, redis-py >=5.0, Neo4j Community 5.x.",
        "testStrategy": "Validate service connectivity, health endpoints, and database schema migrations. Run integration tests for API endpoints and Celery task execution. Confirm Redis and Neo4j connectivity.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Provision FastAPI and Celery Services on Render",
            "description": "Deploy FastAPI and Celery worker services using Render, ensuring production-grade configuration and separation.",
            "dependencies": [],
            "details": "Set up two separate Render services: one for FastAPI (API server) and one for Celery (background worker). Use recommended versions (FastAPI >=0.110, Celery >=5.3). Configure environment variables securely and ensure both services are reachable over the network.",
            "status": "done",
            "testStrategy": "Verify service deployment via Render dashboards. Access FastAPI health endpoint and confirm Celery worker logs show successful startup.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Configure Supabase PostgreSQL with pgvector Extension and Graph Tables",
            "description": "Set up Supabase PostgreSQL instance, enable pgvector, and create required tables for embeddings and graph data.",
            "dependencies": [],
            "details": "In Supabase dashboard, enable the 'vector' extension (pgvector) via Extensions panel or SQL command. Create tables for storing embeddings (e.g., documents with embedding vector columns) and graph structures as needed. Use recommended supabase-py >=2.0 for client access.\n<info added on 2025-11-03T04:12:52.520Z>\nSupabase PostgreSQL is already provisioned at qohsmuevxuetjpuherzo.supabase.co with credentials stored in the .env file. The database is accessible via Supabase Management Console Panel (MCP). \n\nTo complete this subtask:\n\n1. Connect to the Supabase PostgreSQL instance using the MCP or SQL editor.\n\n2. Enable the pgvector extension by executing:\n   ```sql\n   CREATE EXTENSION IF NOT EXISTS vector;\n   ```\n\n3. Create all 37+ required tables from /workflows/database_setup.md, including:\n   - documents\n   - document_chunks\n   - chat_sessions\n   - user_memory_nodes\n   - crewai_agents\n   - crewai_crews\n   - vector tables with embedding columns\n   - graph structure tables\n   - and all other tables specified in the database setup file\n\n4. Verify table creation and ensure proper relationships and constraints are established according to the schema definitions.\n\n5. Test database connectivity using supabase-py >=2.0 client from the application.\n</info added on 2025-11-03T04:12:52.520Z>",
            "status": "done",
            "testStrategy": "Run SQL queries to confirm pgvector is enabled and tables exist. Insert and retrieve sample data, including vector columns.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Set Up Redis (Upstash) for Caching and Celery Broker",
            "description": "Provision a Redis instance on Upstash and configure it for both caching and as the Celery message broker.",
            "dependencies": [],
            "details": "Create a new Redis database on Upstash. Obtain connection URL and credentials. Configure FastAPI and Celery to use this Redis instance for caching and as the Celery broker. Use redis-py >=5.0 for integration.",
            "status": "done",
            "testStrategy": "Connect to Redis from both FastAPI and Celery. Set and retrieve cache keys. Confirm Celery can enqueue and process tasks using Redis broker.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Deploy Neo4j Community Edition via Docker on Mac Studio",
            "description": "Install and run Neo4j Community Edition (5.x) using Docker on the Mac Studio for knowledge graph storage.",
            "dependencies": [],
            "details": "Pull the official Neo4j Community Docker image (version 5.x). Configure Docker container with appropriate ports, volumes for data persistence, and secure environment variables. Ensure Neo4j is accessible from the local network.",
            "status": "done",
            "testStrategy": "Access Neo4j Browser UI, run basic Cypher queries, and verify data persistence after container restart.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Configure TLS 1.3 and Encrypted Environment Variables for All Services",
            "description": "Ensure all backend services (FastAPI, Celery, Supabase, Redis, Neo4j) use TLS 1.3 for secure communication and store environment variables encrypted.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Update service configurations to enforce TLS 1.3 (e.g., Render custom domains with TLS, Upstash Redis with TLS, Supabase with SSL, Neo4j Docker with TLS certificates). Store all secrets and environment variables using encrypted storage mechanisms provided by each platform.",
            "status": "done",
            "testStrategy": "Attempt connections using only TLS 1.3. Inspect certificates and verify environment variables are not exposed in logs or process listings.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Validate Integration and Connectivity Across All Services",
            "description": "Test and confirm that FastAPI, Celery, Supabase, Redis, and Neo4j are correctly integrated and can communicate securely.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Implement health checks and integration tests: FastAPI connects to Supabase and Neo4j, Celery tasks use Redis broker and access Supabase, all over TLS. Run end-to-end tests for API endpoints and background tasks.",
            "status": "done",
            "testStrategy": "Run automated integration tests. Check logs for successful connections. Use tools like curl or Postman to verify TLS and endpoint health.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down the backend environment setup into subtasks for provisioning each service (FastAPI, Celery, Supabase PostgreSQL with pgvector, Redis, Neo4j), configuring secure communication (TLS 1.3), and validating integration and connectivity."
      },
      {
        "id": 2,
        "title": "File Upload Interface & Backblaze B2 Integration",
        "description": "Implement multi-file upload (up to 10 files, 100MB each) with drag-and-drop UI, progress indicators, and direct upload to Backblaze B2 pending/courses/ folder.",
        "details": "Use Gradio or Streamlit for the web UI. Integrate Backblaze B2 via b2sdk (Python >=1.20). Support Mountain Duck polling (30s) and immediate processing for web UI uploads. Enforce file size/type limits and progress feedback. Organize files per B2 folder structure.",
        "testStrategy": "Upload various file types and sizes, verify progress indicators, and confirm files appear in B2 pending/courses/. Test both Mountain Duck and web UI flows.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Implement Multi-File Upload UI with Drag-and-Drop",
            "description": "Create a user interface using Streamlit or Gradio that supports uploading up to 10 files (max 100MB each) via drag-and-drop, with progress indicators and file type/size validation.",
            "dependencies": [],
            "details": "Use Streamlit's st.file_uploader with accept_multiple_files=True or Gradio's file upload component. Implement drag-and-drop functionality, enforce file type and size limits, and display progress indicators for each file. Ensure the UI is intuitive and provides feedback on upload status and errors.",
            "status": "done",
            "testStrategy": "Upload various file types and sizes, verify drag-and-drop works, progress indicators display correctly, and validation prevents unsupported files.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Backblaze B2 Direct Upload via b2sdk",
            "description": "Connect the file upload UI to Backblaze B2 using b2sdk (Python >=1.20), enabling direct upload of files to the pending/courses/ folder and organizing files per B2 folder structure.",
            "dependencies": [
              1
            ],
            "details": "Configure b2sdk for authentication and folder management. Implement logic to upload files directly from the UI to the pending/courses/ folder, ensuring files are organized according to the required B2 structure. Handle upload errors and provide feedback to the user.",
            "status": "done",
            "testStrategy": "Upload files through the UI and confirm they appear in the correct B2 folder. Test error handling and folder organization.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Mountain Duck Polling and Immediate Processing Logic",
            "description": "Support file uploads via Mountain Duck by polling the local folder every 30 seconds and trigger immediate processing for files uploaded via the web UI.",
            "dependencies": [
              2
            ],
            "details": "Set up a polling mechanism to detect new files in the local folder synced by Mountain Duck every 30 seconds. For files uploaded via the web UI, initiate processing immediately after upload. Ensure both flows enforce file limits and integrate with the B2 upload logic.",
            "status": "done",
            "testStrategy": "Simulate uploads via Mountain Duck and web UI, verify polling detects new files, immediate processing works, and all files are uploaded to B2 with correct feedback.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on file upload interface & backblaze b2 integration."
      },
      {
        "id": 3,
        "title": "File Format Validation & Security Scanning",
        "description": "Validate file formats, check integrity, scan for malware, and enforce MIME/extension rules before upload.",
        "details": "Use python-magic for MIME detection, validate extensions, and run integrity checks (e.g., PDF header validation). Integrate ClamAV (clamd) for malware scanning. Reject unsupported formats with clear error messages.",
        "testStrategy": "Attempt uploads of valid, corrupted, and malicious files. Confirm correct rejection and error messaging. Validate security scan logs.",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement File Format and MIME Type Validation",
            "description": "Detect and validate the file's MIME type and extension before upload using python-magic and extension checks.",
            "dependencies": [],
            "details": "Use the python-magic library to inspect the file's magic number and determine its true MIME type. Cross-check this with the file extension to ensure consistency. Reject files with mismatched or unsupported MIME types/extensions, and provide clear error messages. Consider using additional libraries like file-validator for comprehensive checks if needed.",
            "status": "done",
            "testStrategy": "Attempt uploads with valid and invalid file types and extensions. Confirm correct acceptance or rejection and error messaging.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Perform File Integrity and Header Validation",
            "description": "Check file integrity and validate headers for supported formats (e.g., PDF, images) to ensure files are not corrupted or malformed.",
            "dependencies": [
              1
            ],
            "details": "For each supported file type, implement header validation (e.g., check PDF header for '%PDF', image headers for magic numbers). Reject files that fail integrity or header checks. Ensure that only structurally valid files proceed to the next stage.",
            "status": "done",
            "testStrategy": "Upload corrupted or partially valid files and verify that they are rejected with appropriate error messages.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Malware Scanning with ClamAV",
            "description": "Scan validated files for malware using ClamAV (clamd) before final acceptance.",
            "dependencies": [
              2
            ],
            "details": "After passing format and integrity checks, submit files to ClamAV for malware scanning. Reject any files flagged as malicious and log the incident. Ensure the scanning process is efficient and does not introduce significant upload latency.",
            "status": "done",
            "testStrategy": "Upload files containing known malware signatures and verify detection, rejection, and logging. Confirm clean files are accepted.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on file format validation & security scanning."
      },
      {
        "id": 4,
        "title": "Metadata Extraction & Supabase Storage",
        "description": "Extract basic and advanced metadata (filename, size, type, timestamps, EXIF, audio/video info) and store in Supabase documents table.",
        "details": "Use Python libraries: exifread for images, mutagen for audio/video, python-docx for DOCX metadata. Store extracted metadata in Supabase documents table as per schema. Ensure upload triggers metadata extraction.",
        "testStrategy": "Upload files of each supported type, verify metadata extraction accuracy, and confirm correct Supabase storage.",
        "priority": "high",
        "dependencies": [
          "3"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Metadata Extraction for Supported File Types",
            "description": "Develop Python functions to extract basic and advanced metadata from images, audio/video, DOCX, and PDF files using appropriate libraries.",
            "dependencies": [],
            "details": "Use exifread for image EXIF data, mutagen for audio/video metadata, python-docx for DOCX files, and PyPDF2 or pdfminer.six for PDF metadata extraction. Ensure extraction covers filename, size, type, timestamps, and relevant advanced fields (EXIF, audio/video info, document properties). Structure output as per Supabase schema requirements.",
            "status": "done",
            "testStrategy": "Unit test each extractor with sample files of each type. Validate that all required metadata fields are present and accurate.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Metadata Extraction with File Upload Workflow",
            "description": "Ensure that metadata extraction is automatically triggered upon file upload and that extracted data is prepared for storage.",
            "dependencies": [
              1
            ],
            "details": "Modify the upload handler to invoke the correct extraction function based on file type immediately after upload. Collect and format extracted metadata into a dictionary/object matching the Supabase documents table schema.\n<info added on 2025-11-05T22:11:51.333Z>\nImplementation completed for metadata extraction integration with upload workflow:\n\n1. Added metadata_extractor import to upload.py\n2. Modified upload flow to:\n   - Create temp file if not already created (for virus scanning)\n   - Extract metadata from temp file using MetadataExtractor\n   - Include extracted metadata in upload results\n3. Installed required libraries: exifread 3.5.1 and mutagen 1.47.0\n4. Metadata extraction happens after validation and virus scanning, before B2 upload\n5. Graceful error handling - if extraction fails, error is logged but upload continues\n6. Metadata is included in JSON response under \"metadata\" key for each uploaded file\n</info added on 2025-11-05T22:11:51.333Z>",
            "status": "done",
            "testStrategy": "Simulate file uploads via the interface and verify that metadata extraction is triggered and output is correctly formatted for storage.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Store Extracted Metadata in Supabase Documents Table",
            "description": "Insert the extracted metadata into the Supabase documents table, ensuring schema compliance and error handling.",
            "dependencies": [
              2
            ],
            "details": "Use the Supabase Python client to insert metadata records into the documents table. Implement error handling for failed inserts and log issues for debugging. Confirm that all required fields are populated and that the data matches the schema.\n<info added on 2025-11-05T22:21:11.569Z>\nSuccessfully implemented the SupabaseStorage class in app/services/supabase_storage.py with methods for managing document metadata: store_document_metadata(), get_document_by_file_id(), update_document_status(), and list_documents(). The implementation has been integrated into the upload workflow immediately after the B2 upload process. The API response now includes a \"supabase_stored\" boolean flag to indicate successful metadata storage. The system gracefully degrades if Supabase is not configured, allowing the application to function without interruption. Testing confirms that the complete upload workflow functions as expected - metadata extraction works perfectly and Supabase storage attempts are handled gracefully, returning false if not configured.\n</info added on 2025-11-05T22:21:11.569Z>",
            "status": "done",
            "testStrategy": "Upload files of each supported type, then query the Supabase documents table to verify that metadata is stored correctly and completely.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on metadata extraction & supabase storage."
      },
      {
        "id": 5,
        "title": "Duplicate Detection (SHA-256 & Fuzzy Matching)",
        "description": "Detect duplicate and near-duplicate files using SHA-256 hashes and optional fuzzy matching.",
        "details": "Compute SHA-256 hash for each file and check against Supabase documents table. Implement fuzzy matching using Levenshtein distance for filenames and content (rapidfuzz >=2.0). Provide skip/overwrite options.",
        "testStrategy": "Upload duplicate and near-duplicate files, verify detection and user options. Confirm deduplication accuracy.",
        "priority": "high",
        "dependencies": [
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement SHA-256 Hash-Based Duplicate Detection",
            "description": "Compute SHA-256 hashes for each file and compare against existing hashes in the Supabase documents table to identify exact duplicates.",
            "dependencies": [],
            "details": "Use a reliable hashing library to generate SHA-256 hashes for all files. Query the Supabase documents table for existing hashes and flag files with matching hashes as duplicates. Ensure efficient scanning and parallel processing for large file sets.",
            "status": "done",
            "testStrategy": "Upload files with identical content and verify that duplicates are detected solely by hash comparison. Confirm that files with different content are not flagged as duplicates.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Fuzzy Matching for Near-Duplicate Detection",
            "description": "Apply fuzzy matching algorithms (Levenshtein distance via rapidfuzz >=2.0) to filenames and file content to identify near-duplicate files.",
            "dependencies": [
              1
            ],
            "details": "After hash-based filtering, use rapidfuzz to compute similarity scores for filenames and optionally file contents. Set configurable thresholds for similarity to flag near-duplicates. Optimize for performance when comparing large numbers of files.",
            "status": "done",
            "testStrategy": "Upload files with similar but not identical names and/or content. Verify that near-duplicates are detected according to the configured similarity threshold.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement User Options for Duplicate Handling (Skip/Overwrite)",
            "description": "Provide user interface and backend logic for skip or overwrite actions when duplicates or near-duplicates are detected.",
            "dependencies": [
              1,
              2
            ],
            "details": "Design UI prompts and backend logic to allow users to choose whether to skip uploading duplicates, overwrite existing files, or take other actions. Ensure options are clearly presented and actions are reliably executed.",
            "status": "done",
            "testStrategy": "Simulate duplicate and near-duplicate uploads, test all user options (skip, overwrite), and verify correct file handling and user feedback.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on duplicate detection (sha-256 & fuzzy matching)."
      },
      {
        "id": 6,
        "title": "Celery Task Queue Management",
        "description": "Implement priority-based Celery task queue for async document processing, with status tracking, retries, and dead letter queue.",
        "details": "Configure Celery with Redis broker. Use priority queues (urgent, normal, low). Implement status tracking in Supabase file_uploads table. Add retry logic (3 attempts, exponential backoff) and dead letter queue for failed tasks.",
        "testStrategy": "Submit tasks with varying priorities, simulate failures, and verify retry and dead letter queue behavior.",
        "priority": "high",
        "dependencies": [
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Celery with Redis for Priority Queues",
            "description": "Set up Celery to use Redis as the broker and implement priority-based task queues (urgent, normal, low).",
            "dependencies": [],
            "details": "Update Celery configuration to use Redis as the broker. Define separate queues for each priority level (e.g., urgent, normal, low) and configure the broker_transport_options with 'queue_order_strategy': 'priority'. Adjust worker_prefetch_multiplier to 1 for effective prioritization. Ensure workers are started with the correct queue order.",
            "status": "done",
            "testStrategy": "Submit tasks with different priorities and verify that urgent tasks are processed before normal and low priority tasks.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Status Tracking with Supabase",
            "description": "Implement status updates for each task in the Supabase file_uploads table.",
            "dependencies": [
              1
            ],
            "details": "Modify Celery tasks to update the status field in the Supabase file_uploads table at key stages (queued, started, succeeded, failed). Ensure atomic updates and handle race conditions. Use Supabase client libraries for database operations.",
            "status": "done",
            "testStrategy": "Trigger tasks and verify that status changes are accurately reflected in the Supabase file_uploads table throughout the task lifecycle.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Retry Logic with Exponential Backoff",
            "description": "Add retry logic to Celery tasks with up to 3 attempts and exponential backoff on failure.",
            "dependencies": [
              1
            ],
            "details": "Configure Celery task decorators to include retry parameters: max_retries=3 and a backoff strategy (e.g., exponential). Ensure that exceptions trigger retries and that retry attempts are logged or tracked for observability.",
            "status": "done",
            "testStrategy": "Simulate task failures and verify that tasks are retried up to 3 times with increasing delays between attempts.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Set Up Dead Letter Queue for Failed Tasks",
            "description": "Configure a dead letter queue to capture tasks that fail after all retry attempts.",
            "dependencies": [
              3
            ],
            "details": "Create a dedicated dead letter queue in Celery/Redis. Update task failure handlers to route tasks to this queue after exhausting retries. Optionally, log or notify on dead letter events for monitoring.",
            "status": "done",
            "testStrategy": "Force tasks to fail beyond retry limits and verify their presence in the dead letter queue.",
            "updatedAt": "2025-11-05T23:00:18.337Z",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "End-to-End Testing of Priority Queue Management",
            "description": "Test the complete priority queue system, including status tracking, retries, and dead letter handling.",
            "dependencies": [
              2,
              4
            ],
            "details": "Design and execute test cases covering all priority levels, status transitions, retry scenarios, and dead letter queue routing. Validate system behavior under normal and failure conditions.",
            "status": "done",
            "testStrategy": "Run integration tests that submit tasks with various priorities, induce failures, and confirm correct processing, status updates, retries, and dead letter handling.",
            "parentId": "undefined",
            "updatedAt": "2025-11-05T23:00:42.292Z"
          }
        ],
        "complexity": 6.5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Decompose Celery task queue management into subtasks for priority queue configuration, status tracking integration, retry logic implementation, dead letter queue setup, and end-to-end testing.",
        "updatedAt": "2025-11-05T23:00:42.292Z"
      },
      {
        "id": 7,
        "title": "User Notification System (WebSocket & Email)",
        "description": "Provide real-time upload and processing notifications via WebSocket, with optional email alerts for long-running tasks.",
        "details": "Implement FastAPI WebSocket endpoints for progress and completion notifications. Use SMTP or SendGrid for email alerts. Integrate with frontend for actionable error messages.",
        "testStrategy": "Trigger uploads and processing, verify real-time notifications and email delivery for long tasks.",
        "priority": "medium",
        "dependencies": [
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement FastAPI WebSocket Endpoints for Real-Time Notifications",
            "description": "Develop FastAPI WebSocket endpoints to deliver real-time upload and processing progress and completion notifications to connected clients.",
            "dependencies": [],
            "details": "Set up FastAPI WebSocket routes (e.g., /ws/notifications). Manage client connections and broadcast progress/completion events. Ensure endpoints can handle multiple simultaneous connections and send actionable error messages. Integrate with backend processing logic to emit updates as tasks progress or complete.",
            "status": "done",
            "testStrategy": "Simulate uploads and processing tasks; verify clients receive real-time progress and completion notifications via WebSocket.",
            "parentId": "undefined",
            "updatedAt": "2025-11-05T23:01:15.700Z"
          },
          {
            "id": 2,
            "title": "Integrate Email Alert System for Long-Running Tasks",
            "description": "Add optional email notifications for users when uploads or processing tasks exceed a defined duration threshold.",
            "dependencies": [
              1
            ],
            "details": "Configure SMTP or SendGrid integration for sending emails. Implement logic to detect long-running tasks and trigger email alerts with relevant status and error details. Ensure emails are sent only when user opts in or when thresholds are exceeded. Handle email delivery failures gracefully.",
            "status": "done",
            "testStrategy": "Trigger long-running tasks and confirm that email alerts are sent to the correct recipients with accurate information.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Frontend Integration for Real-Time and Email Notifications",
            "description": "Connect frontend application to WebSocket endpoints and display real-time notifications, including actionable error messages. Provide UI for email alert preferences.",
            "dependencies": [
              1,
              2
            ],
            "details": "Update frontend to establish and manage WebSocket connections, display progress/completion notifications, and show errors in a user-friendly manner. Add UI controls for users to opt in/out of email alerts. Ensure seamless user experience for both notification channels.",
            "status": "done",
            "testStrategy": "Test frontend by uploading files and processing tasks; verify real-time updates and error messages appear, and email preferences are respected.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on user notification system (websocket & email).",
        "updatedAt": "2025-11-05T23:01:15.700Z"
      },
      {
        "id": 8,
        "title": "Backblaze B2 Folder Management & Encryption",
        "description": "Automate file movement across B2 folders (pending → processing → processed/failed) and support zero-knowledge encryption for sensitive files.",
        "details": "Use b2sdk for folder operations. Implement file movement logic based on processing status. Integrate PyCryptodome for optional AES encryption before upload.",
        "testStrategy": "Process files through all folder stages, verify correct organization and encryption for flagged files.",
        "priority": "high",
        "dependencies": [
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate b2sdk and Set Up B2 Folder Interfaces",
            "description": "Initialize b2sdk, authenticate, and set up interfaces for pending, processing, processed, and failed folders in the B2 bucket.",
            "dependencies": [],
            "details": "Use b2sdk's AccountInfo and B2Api to authenticate and connect to the B2 bucket. Instantiate B2Folder objects for each logical folder (pending, processing, processed, failed) to enable file operations between them.",
            "status": "done",
            "testStrategy": "Verify connection and folder listing for each B2 folder using b2sdk methods.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement File Movement Logic Based on Processing Status",
            "description": "Develop logic to move files between B2 folders according to their processing status (pending → processing → processed/failed).",
            "dependencies": [
              1
            ],
            "details": "Create functions to list files in each folder and move them to the next stage based on status. Ensure atomicity and handle errors during move operations using b2sdk's file copy and delete methods.",
            "status": "done",
            "testStrategy": "Simulate status changes and verify files are moved to the correct folders without duplication or loss.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate PyCryptodome for Optional AES Encryption",
            "description": "Add support for zero-knowledge AES encryption of sensitive files before upload to B2.",
            "dependencies": [
              1
            ],
            "details": "Use PyCryptodome to encrypt files with a user-supplied key before uploading to B2. Ensure encryption is optional and only applied to flagged files. Store encrypted files in the appropriate B2 folder.",
            "status": "done",
            "testStrategy": "Upload both encrypted and unencrypted files, then download and verify decryption for encrypted files.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Handle Status-Based Transitions and Error Recovery",
            "description": "Implement robust handling for file status transitions, including retries and error recovery for failed moves or uploads.",
            "dependencies": [
              2,
              3
            ],
            "details": "Add logic to detect and recover from failed moves or uploads. Implement retry mechanisms and ensure files are not lost or duplicated during transitions. Log all status changes and errors for auditability.",
            "status": "done",
            "testStrategy": "Intentionally trigger errors (e.g., network failures) and verify that files are correctly retried or moved to the failed folder.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Develop Comprehensive Tests for Folder Organization and Encryption",
            "description": "Create automated tests to verify correct file organization across all folder stages and validate encryption/decryption for sensitive files.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Write tests that process files through all folder stages, check their presence in the correct folders, and confirm that encryption is correctly applied and reversible for flagged files.",
            "status": "done",
            "testStrategy": "Run end-to-end tests covering all transitions and encryption scenarios, ensuring files are organized and protected as specified.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Divide B2 folder management and encryption into subtasks for implementing folder movement logic, integrating b2sdk, supporting AES encryption with PyCryptodome, handling status-based transitions, and verifying organization/encryption through tests."
      },
      {
        "id": 9,
        "title": "AI Department Classification Workflow (Claude Haiku)",
        "description": "Classify uploaded documents into 10 departments using Claude Haiku API, storing results in Supabase.",
        "details": "Integrate anthropic-py SDK. Implement async auto_classify_course function as per PRD. Store department, confidence, and subdepartment in documents and courses tables.",
        "testStrategy": "Upload sample documents for each department, verify classification accuracy and Supabase updates.",
        "priority": "high",
        "dependencies": [
          "8"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Claude Haiku API and anthropic-py SDK for Document Classification",
            "description": "Set up the Claude Haiku API and anthropic-py SDK to enable classification of uploaded documents into 10 departments.",
            "dependencies": [],
            "details": "Install and configure the anthropic-py SDK. Implement API authentication and error handling (e.g., retries, rate limits). Ensure the async auto_classify_course function is ready to send document content to Claude Haiku and receive department predictions. Tune parameters such as temperature and max_tokens for optimal classification accuracy.",
            "status": "done",
            "testStrategy": "Send sample documents to the API and verify department predictions are returned correctly. Test error handling by simulating API failures.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Async auto_classify_course Function and PRD Logic",
            "description": "Develop the async auto_classify_course function according to the Product Requirements Document (PRD), ensuring it processes documents and extracts department, confidence, and subdepartment.",
            "dependencies": [
              1
            ],
            "details": "Write the async function to handle document input, call the Claude Haiku API, and parse the response for department, confidence score, and subdepartment. Ensure the function supports batch processing and handles edge cases (e.g., ambiguous classifications). Document the function and its parameters for maintainability.",
            "status": "done",
            "testStrategy": "Unit test the function with mock API responses. Validate output structure and accuracy against expected department labels.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Store Classification Results in Supabase Documents and Courses Tables",
            "description": "Persist the classification results (department, confidence, subdepartment) in the Supabase documents and courses tables.",
            "dependencies": [
              2
            ],
            "details": "Map the classification output to the correct schema fields in Supabase. Implement transactional writes to ensure data consistency. Add logging for successful and failed writes. Verify that updates are reflected in both documents and courses tables as required.",
            "status": "done",
            "testStrategy": "Upload test documents, run classification, and confirm Supabase tables are updated with correct department, confidence, and subdepartment values. Check for data integrity and error handling.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on ai department classification workflow (claude haiku)."
      },
      {
        "id": 10,
        "title": "Universal Document Processing Pipeline",
        "description": "Extract text and structured data from all supported document types using specialized services and fallback methods.",
        "details": "Integrate LlamaIndex (REST API) for clean PDFs, Mistral OCR for scanned PDFs, Tesseract OCR for fallback. Use python-docx for DOCX, mutagen for audio/video, Claude Vision API for images. Implement table/image extraction and maintain page/section info.",
        "testStrategy": "Process each file type, verify extraction accuracy, structure preservation, and fallback logic.",
        "priority": "high",
        "dependencies": [
          "9"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Modular Document Ingestion and Classification",
            "description": "Design and build the pipeline's ingestion layer to accept documents from various sources and classify them by type (PDF, DOCX, image, audio/video).",
            "dependencies": [],
            "details": "Set up connectors for file sources (e.g., S3 buckets, local uploads). Integrate document type detection logic to route files to appropriate extraction modules. Log ingestion events and maintain audit trails for each document.",
            "status": "done",
            "testStrategy": "Submit sample files of each supported type, verify correct classification and routing, and check ingestion logs for completeness.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Specialized Extraction Services and Fallbacks",
            "description": "Connect and orchestrate specialized extraction services for each document type, with fallback logic for unsupported or failed cases.",
            "dependencies": [
              1
            ],
            "details": "Integrate LlamaIndex REST API for clean PDFs, Mistral OCR for scanned PDFs, Tesseract OCR as fallback, python-docx for DOCX, mutagen for audio/video, Claude Vision API for images. Implement logic to select extraction method based on classification and handle failures by cascading to fallback services.",
            "status": "done",
            "testStrategy": "Process a diverse set of documents, intentionally trigger extraction failures, and verify fallback mechanisms activate and extract data as expected.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Extract Structured Data and Metadata with Section/Page Tracking",
            "description": "Develop logic to extract tables, images, and maintain page/section metadata for all processed documents, ensuring structured outputs.",
            "dependencies": [
              2
            ],
            "details": "Implement table and image extraction for supported formats. Track and store page/section information alongside extracted text and structured data. Ensure outputs are normalized for downstream consumption.",
            "status": "done",
            "testStrategy": "Validate extracted outputs for structure, completeness, and correct association of metadata (page/section info) using test documents with known layouts.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on universal document processing pipeline."
      },
      {
        "id": 11,
        "title": "Audio & Video Processing (Soniox, Claude Vision)",
        "description": "Transcribe audio, extract speakers/timestamps, and analyze video frames using Soniox and Claude Vision APIs.",
        "details": "Integrate Soniox REST API for transcription and diarization. Use ffmpeg-python for frame/audio extraction from video. Analyze frames with Claude Vision API. Store transcripts and timeline metadata.",
        "testStrategy": "Process audio and video files, verify transcript accuracy, speaker identification, and frame analysis.",
        "priority": "high",
        "dependencies": [
          "10"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Extract Audio and Video Frames from Input Files",
            "description": "Use ffmpeg-python to extract audio tracks and video frames from input video files for downstream processing.",
            "dependencies": [],
            "details": "Implement a Python module using ffmpeg-python to separate audio from video files and extract video frames at configurable intervals. Ensure extracted audio is in a Soniox-compatible format (e.g., 16kHz mono WAV). Store extracted frames and audio in a structured directory or object storage for later processing.",
            "status": "done",
            "testStrategy": "Run extraction on sample video files, verify correct number and quality of frames, and check audio format compatibility with Soniox.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Transcribe Audio and Extract Speaker/Timestamps with Soniox API",
            "description": "Integrate Soniox REST API to transcribe extracted audio, enabling speaker diarization and timestamp extraction.",
            "dependencies": [
              1
            ],
            "details": "Authenticate with Soniox API using a project API key. Send extracted audio files for transcription using the async or streaming endpoints. Enable speaker diarization and timestamp options in the API request. Parse and store the returned transcript, speaker labels, and word-level timestamps in the database or metadata files.",
            "status": "done",
            "testStrategy": "Submit test audio files, verify transcript accuracy, correct speaker segmentation, and presence of timestamps. Compare results with ground truth if available.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Analyze Video Frames with Claude Vision API and Store Metadata",
            "description": "Send extracted video frames to Claude Vision API for analysis and store the resulting metadata alongside transcripts and timeline data.",
            "dependencies": [
              1
            ],
            "details": "Batch or stream video frames to the Claude Vision API, handling authentication and rate limits. Parse the returned analysis (e.g., scene description, object detection) and associate results with corresponding timestamps. Store all metadata in a structured format, linking frame analysis to transcript timeline.",
            "status": "done",
            "testStrategy": "Process sample frames, verify that analysis results are received and correctly mapped to frame timestamps. Check integration with transcript timeline and metadata storage.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on audio & video processing (soniox, claude vision)."
      },
      {
        "id": 12,
        "title": "Structured Data Extraction (LangExtract)",
        "description": "Extract entities, key-value pairs, and course metadata using LangExtract API.",
        "details": "Integrate LangExtract REST API for field/entity extraction. Store results in Supabase courses and document_chunks tables. Implement intelligent filename generation (M01-L02 format).",
        "testStrategy": "Process documents with structured fields, verify entity extraction and metadata accuracy.",
        "priority": "high",
        "dependencies": [
          "11"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Extraction Schema and Example Prompts for LangExtract",
            "description": "Specify the entity types, key-value pairs, and course metadata fields to be extracted. Create example prompts and sample extractions to guide the LangExtract API.",
            "dependencies": [],
            "details": "List all required fields (e.g., course title, module number, lesson number, instructor, date) and define their expected formats. Write natural language prompts and provide high-quality example extractions using LangExtract's ExampleData objects to ensure consistent output schema and accurate extraction.",
            "status": "done",
            "testStrategy": "Review extracted fields from test documents to confirm schema coverage and prompt effectiveness.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate LangExtract REST API for Automated Entity and Metadata Extraction",
            "description": "Connect to the LangExtract REST API and implement logic to process course documents, extracting entities, key-value pairs, and metadata as defined in the schema.",
            "dependencies": [
              1
            ],
            "details": "Set up API authentication and request handling. For each uploaded course document, send the text and extraction instructions/examples to LangExtract. Parse the returned structured data, ensuring source grounding and attribute mapping. Handle errors and edge cases (e.g., missing fields, ambiguous extractions).",
            "status": "done",
            "testStrategy": "Process a variety of course documents and verify that all required entities and metadata are extracted with correct attributes and source positions.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Store Extracted Data in Supabase and Implement Intelligent Filename Generation",
            "description": "Save the extracted entities and metadata into Supabase courses and document_chunks tables. Generate filenames using the M01-L02 format based on extracted module and lesson numbers.",
            "dependencies": [
              2
            ],
            "details": "Map extracted fields to Supabase table schemas, ensuring correct data types and relationships. Implement logic to generate filenames (e.g., M01-L02) from extracted metadata and associate them with stored records. Validate data integrity and handle duplicate or conflicting entries.",
            "status": "done",
            "testStrategy": "Insert extracted data from sample documents into Supabase, verify correct mapping and filename generation, and check for consistency across multiple uploads.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on structured data extraction (langextract)."
      },
      {
        "id": 13,
        "title": "Adaptive Chunking Strategy Implementation",
        "description": "Implement semantic, code, and transcript chunking with configurable size and overlap, preserving context.",
        "details": "Use LlamaIndex chunking for documents, custom logic for code (AST parsing), and time/topic-based chunking for transcripts. Store chunks in document_chunks table with metadata and overlap.",
        "testStrategy": "Chunk various document types, verify chunk boundaries, overlap, and context preservation.",
        "priority": "high",
        "dependencies": [
          "12"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Adaptive Semantic Chunking for Documents",
            "description": "Develop and configure semantic chunking for text documents using LlamaIndex, supporting adjustable chunk size and overlap to preserve context.",
            "dependencies": [],
            "details": "Use LlamaIndex's semantic chunker to split documents into contextually coherent chunks. Expose configuration for chunk size and overlap (e.g., via parameters or settings). Ensure chunk metadata (source_doc_id, chunk boundaries, overlap) is captured for each chunk and stored in the document_chunks table. Validate that semantic boundaries are respected and context is preserved across chunks.",
            "status": "done",
            "testStrategy": "Chunk a variety of document types, verify chunk boundaries align with semantic units, check overlap, and confirm metadata is correctly stored.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop Custom Code Chunking Using AST Parsing",
            "description": "Create a chunking mechanism for code files that leverages AST parsing to split code into logical units with configurable size and overlap.",
            "dependencies": [
              1
            ],
            "details": "Implement code chunking logic that parses source code into AST nodes (e.g., functions, classes) and groups them into chunks based on configurable parameters (lines per chunk, overlap). Support multiple programming languages if required. Store resulting code chunks with relevant metadata (e.g., language, function/class names, overlap) in the document_chunks table.",
            "status": "done",
            "testStrategy": "Process code files in different languages, verify chunking aligns with logical code units, check overlap, and ensure metadata accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Time/Topic-Based Chunking for Transcripts",
            "description": "Design and implement a chunking strategy for transcripts that splits content based on time intervals or topic shifts, with configurable overlap.",
            "dependencies": [
              1
            ],
            "details": "Develop logic to segment transcripts using either fixed time windows or detected topic boundaries. Allow configuration of chunk duration or topic sensitivity, as well as overlap between chunks. Store transcript chunks with metadata (e.g., start/end time, topic label, overlap) in the document_chunks table. Ensure context is preserved across chunk boundaries.",
            "status": "done",
            "testStrategy": "Chunk transcripts with varying lengths and topics, verify chunk boundaries match time/topic criteria, check overlap, and validate metadata storage.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on adaptive chunking strategy implementation."
      },
      {
        "id": 14,
        "title": "Error Handling & Graceful Degradation",
        "description": "Implement robust error handling, retry logic, partial processing, and detailed logging for all pipeline stages.",
        "details": "Use Python exception handling, Celery retry policies, and fallback to simpler methods. Log errors with stack traces in processing_logs table. Move failed files to B2 failed/ folder.",
        "testStrategy": "Simulate service failures, verify retries, partial saves, and error logs.",
        "priority": "high",
        "dependencies": [
          "13"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Robust Exception Handling and Retry Logic in Pipeline Tasks",
            "description": "Integrate structured Python exception handling and Celery retry policies for all pipeline stages to ensure resilience against transient and expected failures.",
            "dependencies": [],
            "details": "Wrap all critical pipeline operations in try/except blocks. Use Celery's retry mechanisms (e.g., autoretry_for, max_retries, retry_backoff) to handle transient errors such as network or service outages. Configure per-task retry parameters and ensure idempotency to avoid side effects on repeated execution. Avoid retrying on non-transient exceptions.",
            "status": "done",
            "testStrategy": "Simulate transient and permanent failures in pipeline tasks. Verify that retries occur as configured, and that non-retriable errors do not trigger retries.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Enable Graceful Degradation and Partial Processing with Fallbacks",
            "description": "Design pipeline stages to degrade gracefully by falling back to simpler or partial processing methods when primary logic fails.",
            "dependencies": [
              1
            ],
            "details": "For each pipeline stage, define fallback logic (e.g., simplified processing, skipping non-critical steps) to be invoked when primary processing fails after retries. Ensure that partial results are saved where possible, and that the system continues processing unaffected files or stages. Move unrecoverable files to the B2 failed/ folder for later inspection.",
            "status": "done",
            "testStrategy": "Force failures in primary processing logic and verify that fallback methods are invoked, partial results are saved, and failed files are moved appropriately.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Detailed Error Logging and Monitoring",
            "description": "Log all errors, stack traces, and processing outcomes in the processing_logs table to support debugging and monitoring.",
            "dependencies": [
              1,
              2
            ],
            "details": "On every exception or failure, capture the full stack trace and relevant context. Insert detailed error records into the processing_logs table, including task identifiers, error types, messages, and timestamps. Ensure logs are structured for easy querying and monitoring. Integrate with monitoring tools if available.",
            "status": "done",
            "testStrategy": "Trigger various error scenarios and verify that all relevant details are logged in the processing_logs table, including stack traces and context.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on error handling & graceful degradation."
      },
      {
        "id": 15,
        "title": "Processing Monitoring & Metrics Collection",
        "description": "Track real-time processing progress, resource usage, and cost per document using Prometheus metrics.",
        "details": "Integrate prometheus_client for FastAPI, Celery, and custom business metrics. Track processing time, resource usage, and cost. Store stage-wise metrics in processing_logs table.",
        "testStrategy": "Process documents, verify Prometheus metrics, Grafana dashboard updates, and Supabase logs.",
        "priority": "high",
        "dependencies": [
          "14"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Prometheus Metrics Collection in FastAPI and Celery",
            "description": "Instrument FastAPI and Celery services to expose Prometheus-compatible metrics endpoints for processing progress, resource usage, and cost tracking.",
            "dependencies": [],
            "details": "Install prometheus_client in both FastAPI and Celery environments. For FastAPI, mount the /metrics endpoint using make_asgi_app and add counters, histograms, and gauges for request counts, processing time, and resource usage. For Celery, use available Prometheus exporters or integrate prometheus_client to expose worker and task metrics. Ensure all relevant business and custom metrics are included.",
            "status": "done",
            "testStrategy": "Verify /metrics endpoints in FastAPI and Celery return expected metrics. Use Prometheus to scrape these endpoints and confirm metrics are ingested.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Track and Store Stage-wise Processing Metrics in Database",
            "description": "Capture and persist detailed stage-wise metrics (processing time, resource usage, cost per document) in the processing_logs table for audit and analysis.",
            "dependencies": [
              1
            ],
            "details": "Extend processing logic to record metrics at each pipeline stage. Store metrics such as start/end timestamps, CPU/memory usage, and cost estimates in the processing_logs table. Ensure schema supports all required fields and that writes are efficient and reliable.",
            "status": "done",
            "testStrategy": "Process sample documents and verify that processing_logs table contains accurate, stage-wise metrics matching Prometheus data.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Validate Metrics Collection and Visualization End-to-End",
            "description": "Test the full monitoring pipeline from metrics emission to visualization and logging, ensuring real-time and historical data is accurate and actionable.",
            "dependencies": [
              1,
              2
            ],
            "details": "Simulate document processing and monitor Prometheus for real-time metrics updates. Confirm that Grafana dashboards reflect current and historical metrics. Cross-check database logs with Prometheus data for consistency. Validate cost calculations and resource usage reporting.",
            "status": "done",
            "testStrategy": "Run end-to-end tests: process documents, check Prometheus and Grafana for live metrics, and verify processing_logs entries. Ensure all metrics are accurate and actionable.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on processing monitoring & metrics collection."
      },
      {
        "id": 16,
        "title": "Embedding Generation Pipeline (BGE-M3, Claude API)",
        "description": "Generate and cache embeddings for document chunks using BGE-M3 (Ollama for dev, Claude API for prod).",
        "details": "Integrate langchain.embeddings.OllamaEmbeddings for dev, Claude API for prod. Batch process 100 chunks, cache embeddings in Supabase pgvector. Regenerate on content updates.",
        "testStrategy": "Generate embeddings for sample chunks, verify latency, caching, and Supabase storage.",
        "priority": "high",
        "dependencies": [
          "15"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate BGE-M3 Embedding Generation for Development (Ollama)",
            "description": "Set up and integrate the BGE-M3 embedding model using Ollama for local development, enabling batch processing of document chunks.",
            "dependencies": [],
            "details": "Install and configure langchain_ollama and OllamaEmbeddings with the BGE-M3 model. Implement batch processing for 100 document chunks at a time. Ensure the pipeline can handle content updates by triggering re-embedding as needed. Optimize for local inference speed and resource usage.",
            "status": "done",
            "testStrategy": "Generate embeddings for a sample batch of 100 chunks, verify output shape and latency, and confirm embeddings are regenerated on content updates.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Claude API for Production Embedding Generation",
            "description": "Implement embedding generation using the Claude API for production, supporting batch processing and seamless switching from development to production.",
            "dependencies": [
              1
            ],
            "details": "Configure the Claude API integration within the embedding pipeline. Ensure batch processing of 100 chunks per request, with error handling and retry logic. Provide a configuration switch to toggle between Ollama (dev) and Claude API (prod). Ensure compatibility of embedding formats and dimensions.",
            "status": "done",
            "testStrategy": "Run embedding generation for a sample batch via Claude API, verify output consistency with dev pipeline, and test failover and error handling.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Embedding Caching and Regeneration Logic in Supabase pgvector",
            "description": "Design and implement caching of generated embeddings in Supabase pgvector, including logic to detect content updates and trigger regeneration.",
            "dependencies": [
              2
            ],
            "details": "Integrate with Supabase pgvector to store and retrieve embeddings. Implement logic to check for content changes and invalidate or update cached embeddings as needed. Ensure efficient batch inserts and retrievals. Maintain metadata for tracking embedding versions and update timestamps.",
            "status": "done",
            "testStrategy": "Insert, retrieve, and update embeddings in Supabase for sample documents. Simulate content updates and verify that embeddings are correctly regenerated and cached.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on embedding generation pipeline (bge-m3, claude api)."
      },
      {
        "id": 17,
        "title": "Vector Storage & Indexing (Supabase pgvector)",
        "description": "Store embeddings in Supabase pgvector, create HNSW index for fast similarity search, and optimize batch inserts.",
        "details": "Enable pgvector extension, create HNSW index, and optimize batch inserts using supabase-py bulk operations. Organize by namespace and support metadata filtering.",
        "testStrategy": "Insert and search embeddings, verify index performance and metadata filtering.",
        "priority": "high",
        "dependencies": [
          "16"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Enable pgvector Extension in Supabase",
            "description": "Activate the pgvector extension in the Supabase PostgreSQL database to support vector data types and similarity search operations.",
            "dependencies": [],
            "details": "Access the Supabase dashboard, navigate to the Extensions section, and enable the 'vector' extension. This step is required before creating tables with vector columns and using vector search features.",
            "status": "done",
            "testStrategy": "Verify that the 'vector' extension is listed as enabled in the Supabase dashboard and that SQL commands using the 'vector' data type execute without errors.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create Embeddings Table and HNSW Index",
            "description": "Design and create a table for storing embeddings, including metadata and namespace columns, and add an HNSW index for fast similarity search.",
            "dependencies": [
              1
            ],
            "details": "Define a table schema with columns for id, embedding (vector), metadata (JSONB), and namespace (text or UUID). Use SQL to create the table and then create an HNSW index on the embedding column for efficient ANN search.",
            "status": "done",
            "testStrategy": "Insert sample embeddings and confirm that the HNSW index exists and is used in EXPLAIN query plans for similarity searches.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Optimize Batch Inserts Using supabase-py Bulk Operations",
            "description": "Implement efficient batch insertion of embeddings and metadata using supabase-py or equivalent bulk insert methods.",
            "dependencies": [
              2
            ],
            "details": "Use supabase-py or another supported client to insert multiple embeddings in a single operation, minimizing transaction overhead and maximizing throughput. Ensure the code handles large batches and error cases.",
            "status": "done",
            "testStrategy": "Benchmark batch insert performance with varying batch sizes and verify that all records are correctly stored in the table.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Organize Embeddings by Namespace",
            "description": "Implement logic to assign and query embeddings by namespace to support multi-tenant or segmented storage.",
            "dependencies": [
              2
            ],
            "details": "Add a namespace column to the embeddings table if not already present. Ensure all insert and query operations include namespace filtering to logically separate data for different use cases or clients.",
            "status": "done",
            "testStrategy": "Insert embeddings with different namespaces and verify that queries scoped to a namespace only return relevant records.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Metadata Filtering in Similarity Search",
            "description": "Enable filtering of similarity search results based on metadata fields stored with each embedding.",
            "dependencies": [
              2
            ],
            "details": "Use PostgreSQL's JSONB operators to filter embeddings by metadata fields in combination with vector similarity queries. Update search queries to support metadata-based filtering (e.g., by document type, tags, or timestamps).",
            "status": "done",
            "testStrategy": "Run similarity searches with and without metadata filters, confirming that results are correctly filtered and performance remains acceptable.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7.5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Expand vector storage and indexing into subtasks for enabling pgvector, creating HNSW index, optimizing batch inserts, organizing by namespace, and implementing metadata filtering."
      },
      {
        "id": 18,
        "title": "Hybrid Search Implementation (Dense, Sparse, Fuzzy, RRF)",
        "description": "Implement hybrid search combining vector similarity, BM25, ILIKE, fuzzy matching, and reciprocal rank fusion.",
        "details": "Use pgvector for dense search, PostgreSQL full-text search for BM25, ILIKE for pattern matching, rapidfuzz for fuzzy search. Implement RRF for result fusion with configurable weights.",
        "testStrategy": "Run hybrid searches, verify result fusion, relevance, and latency targets.",
        "priority": "high",
        "dependencies": [
          "17"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Dense, Sparse, and Fuzzy Search Pipelines",
            "description": "Develop individual search pipelines for dense (vector), sparse (BM25), and fuzzy (ILIKE, rapidfuzz) retrieval methods.",
            "dependencies": [],
            "details": "Set up pgvector for dense search, configure PostgreSQL full-text search for BM25, implement ILIKE for pattern matching, and integrate rapidfuzz for fuzzy matching. Ensure each pipeline can independently retrieve and score results for a given query.",
            "status": "done",
            "testStrategy": "Run isolated queries for each pipeline and verify result relevance, accuracy, and latency.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Design and Implement Reciprocal Rank Fusion (RRF) Algorithm",
            "description": "Create a fusion algorithm to combine ranked results from dense, sparse, and fuzzy pipelines using reciprocal rank fusion.",
            "dependencies": [
              1
            ],
            "details": "Develop RRF logic to merge result lists from all pipelines, applying configurable weights. Ensure the algorithm penalizes lower-ranked results and boosts consensus across methods. Validate with sample queries and edge cases.",
            "status": "done",
            "testStrategy": "Test fusion with controlled input lists, verify ranking consistency, and check that top results reflect combined relevance.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Hybrid Search and Expose Unified API Endpoint",
            "description": "Combine all search pipelines and RRF fusion into a single hybrid search workflow, exposing it via an API endpoint.",
            "dependencies": [
              2
            ],
            "details": "Orchestrate parallel execution of all search methods, collect results, apply RRF fusion, and return unified ranked results. Implement API endpoint with configurable fusion weights and query parameters. Ensure robust error handling and logging.",
            "status": "done",
            "testStrategy": "Run end-to-end hybrid search queries through the API, validate result quality, latency, and error handling.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on hybrid search implementation (dense, sparse, fuzzy, rrf)."
      },
      {
        "id": 19,
        "title": "Query Expansion & Reranking (Claude Haiku, BGE-Reranker-v2)",
        "description": "Expand queries using Claude Haiku, execute parallel searches, and rerank results with BGE-Reranker-v2.",
        "details": "Integrate anthropic-py for query expansion, run parallel searches, and rerank top 20-30 results using Ollama BGE-Reranker-v2 (dev) or Claude API (prod).",
        "testStrategy": "Test query expansion and reranking, verify improved recall and precision.",
        "priority": "high",
        "dependencies": [
          "18"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Claude Haiku for Query Expansion",
            "description": "Implement query expansion using Claude Haiku via anthropic-py, ensuring queries are enriched for improved recall.",
            "dependencies": [],
            "details": "Set up anthropic-py client and configure Claude Haiku 4.5 API parameters (e.g., max_tokens, temperature, top_p). Design prompt templates to expand user queries, leveraging advanced prompt engineering techniques for optimal output. Handle API errors and retries for reliability.",
            "status": "done",
            "testStrategy": "Send sample queries and verify that expanded queries are generated as expected. Compare recall and diversity of results before and after expansion.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Execute Parallel Searches with Expanded Queries",
            "description": "Run parallel searches using the expanded queries to retrieve a broad set of relevant results.",
            "dependencies": [
              1
            ],
            "details": "Implement asynchronous or concurrent search logic to execute multiple queries in parallel. Aggregate results from all searches, ensuring deduplication and efficient handling of large result sets. Optimize for latency and throughput.",
            "status": "done",
            "testStrategy": "Test with multiple expanded queries and measure search latency. Confirm that all relevant results are retrieved and aggregated correctly.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Rerank Search Results Using BGE-Reranker-v2",
            "description": "Apply BGE-Reranker-v2 to rerank the top 20-30 search results for improved relevance and precision.",
            "dependencies": [
              2
            ],
            "details": "Integrate Ollama BGE-Reranker-v2 (dev) or Claude API (prod) to rerank aggregated results. Configure reranker model and permissions, and tune reranking parameters. Validate reranked output for relevance and consistency.",
            "status": "done",
            "testStrategy": "Compare original and reranked result sets using relevance metrics (precision, recall, NDCG). Conduct manual review of top results for quality assurance.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on query expansion & reranking (claude haiku, bge-reranker-v2)."
      },
      {
        "id": 20,
        "title": "Neo4j Graph Integration & Entity Storage",
        "description": "Store entities and relationships in Neo4j, enable graph-based queries and context retrieval.",
        "details": "Use neo4j Python driver (neo4j >=5.10) to create document and entity nodes, relationships, and vector index. Implement Cypher queries for entity-centric and relationship traversal.",
        "testStrategy": "Insert entities/relationships, run Cypher queries, verify graph traversal and context retrieval.",
        "priority": "high",
        "dependencies": [
          "19"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Set Up Neo4j Python Driver and Database Connection",
            "description": "Install the Neo4j Python driver and establish a secure connection to the Neo4j database instance.",
            "dependencies": [],
            "details": "Use pip to install the neo4j Python driver (neo4j >=5.10). Configure connection parameters (URI, username, password) and verify connectivity using GraphDatabase.driver and driver.verify_connectivity(). Ensure the database instance is running and accessible.",
            "status": "done",
            "testStrategy": "Attempt connection and run a simple Cypher query to confirm connectivity.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Entity and Relationship Node Creation",
            "description": "Create Cypher queries and Python functions to insert document and entity nodes, and define relationships between them in Neo4j.",
            "dependencies": [
              1
            ],
            "details": "Define node labels (e.g., Document, Entity) and relationship types. Use MERGE or CREATE Cypher statements to add nodes and relationships. Implement Python functions to batch insert entities and relationships, ensuring idempotency and data integrity.",
            "status": "done",
            "testStrategy": "Insert sample entities and relationships, then query the graph to verify correct node and relationship creation.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Enable Graph-Based Queries and Context Retrieval",
            "description": "Develop Cypher queries and Python interfaces for entity-centric and relationship traversal, including context retrieval and vector index integration.",
            "dependencies": [
              2
            ],
            "details": "Implement Cypher queries for traversing relationships (e.g., MATCH, OPTIONAL MATCH). Integrate vector index for similarity search if required. Provide Python functions to retrieve context around entities and relationships, supporting advanced graph queries.",
            "status": "done",
            "testStrategy": "Run entity-centric and relationship traversal queries, validate context retrieval, and test vector index search if applicable.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on neo4j graph integration & entity storage."
      },
      {
        "id": 21,
        "title": "Caching Strategy (Redis, Tiered Cache)",
        "description": "Implement Redis caching for frequent queries, embeddings, and search results with semantic thresholds and tiered cache.",
        "details": "Use redis-py for L1 cache (Redis), fallback to L2 (PostgreSQL). Implement semantic cache thresholds and 5-minute TTL. Track cache hit rate.",
        "testStrategy": "Run repeated queries, verify cache hits/misses, and cache update logic.",
        "priority": "high",
        "dependencies": [
          "20"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Redis L1 Cache with Semantic Thresholds and TTL",
            "description": "Set up Redis as the primary (L1) cache for frequent queries, embeddings, and search results, applying semantic thresholds and a 5-minute TTL.",
            "dependencies": [],
            "details": "Use redis-py to connect to Redis. Define cache keys for queries, embeddings, and search results. Implement logic to only cache results that meet semantic similarity thresholds. Set a 5-minute expiration (TTL) for all cache entries to ensure freshness. Ensure cache-aside pattern is used for read-heavy workloads, checking Redis first and falling back to the database on cache miss.",
            "status": "done",
            "testStrategy": "Run repeated queries and verify that results are cached in Redis, TTL is respected, and only semantically relevant results are cached.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Tiered Cache Fallback to PostgreSQL (L2)",
            "description": "Implement fallback logic to query PostgreSQL (L2) when Redis (L1) cache misses occur, and repopulate Redis cache as needed.",
            "dependencies": [
              1
            ],
            "details": "On cache miss in Redis, query PostgreSQL for the required data. If found, repopulate Redis with the result, applying the same semantic threshold and TTL logic. Ensure the fallback mechanism is robust and does not introduce significant latency. Use efficient serialization for storing and retrieving data between Redis and PostgreSQL.",
            "status": "done",
            "testStrategy": "Simulate cache misses and verify that data is correctly fetched from PostgreSQL, then cached in Redis for subsequent requests.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Monitor and Track Cache Hit Rate and Effectiveness",
            "description": "Implement monitoring to track cache hit/miss rates and overall cache effectiveness for both Redis and PostgreSQL tiers.",
            "dependencies": [
              1,
              2
            ],
            "details": "Instrument the caching logic to record cache hits, misses, and repopulation events. Aggregate metrics such as hit rate, miss rate, and average response time. Set up dashboards or logs to visualize cache performance and identify optimization opportunities. Use these metrics to tune semantic thresholds and TTL values.",
            "status": "done",
            "testStrategy": "Generate load with a mix of repeated and unique queries, then verify that hit/miss metrics are accurately tracked and reported.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on caching strategy (redis, tiered cache)."
      },
      {
        "id": 22,
        "title": "Query Type Detection & Routing",
        "description": "Classify incoming queries and route to optimal workflow framework (LangGraph, CrewAI, Simple) based on query complexity and requirements.",
        "status": "done",
        "dependencies": [
          "21"
        ],
        "priority": "high",
        "details": "Implemented WorkflowRouter class in app/workflows/workflow_router.py with Claude Haiku-powered classification. The system analyzes queries and routes them to the appropriate processing framework with confidence scoring and reasoning.",
        "testStrategy": "Submit queries of each type, verify correct classification and routing to appropriate workflow frameworks. Validate confidence scoring and fallback mechanisms.",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Query Type Taxonomy and Routing Logic",
            "description": "Define the taxonomy of query types (semantic, relational, hybrid, metadata) and specify routing logic for each type.",
            "dependencies": [],
            "details": "Analyze typical incoming queries and categorize them into clear types. Document routing rules for each category, mapping them to the appropriate search pipeline (vector, graph, metadata). Consider hierarchical classification if the taxonomy is complex, and ensure the design supports future extensibility.",
            "status": "done",
            "testStrategy": "Review taxonomy coverage against a sample set of queries. Validate routing logic with test cases for each query type.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Query Classifier Using Claude Haiku",
            "description": "Develop and deploy a query classifier leveraging Claude Haiku to assign incoming queries to the correct type.",
            "dependencies": [
              1
            ],
            "details": "Use prompt engineering and, if needed, hierarchical classification to maximize accuracy. Integrate Claude Haiku via API, ensuring the classifier outputs only the defined category names. Optimize for speed and reliability, and consider using vector similarity retrieval for highly variable queries.",
            "status": "done",
            "testStrategy": "Submit queries of each type and edge cases to the classifier. Measure classification accuracy and latency. Confirm output matches taxonomy.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Classifier with Search Pipeline Routing",
            "description": "Connect the classifier output to the routing system, ensuring queries are dispatched to the correct search pipeline.",
            "dependencies": [
              2
            ],
            "details": "Implement the routing logic that receives the classified query type and triggers the corresponding search pipeline (vector, graph, metadata, or hybrid). Ensure robust error handling and logging. Validate that each pipeline receives only relevant queries and that fallback logic is in place for unclassified or ambiguous queries.",
            "status": "done",
            "testStrategy": "End-to-end test: submit queries, verify correct classification and routing to the intended pipeline. Check logs and error handling for misrouted or unclassified queries.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Document Workflow Type Taxonomy",
            "description": "Document the implemented workflow type taxonomy (LANGGRAPH, CREWAI, SIMPLE) with detailed characteristics of each type.",
            "dependencies": [
              3
            ],
            "details": "Create comprehensive documentation of the three workflow types: LANGGRAPH for adaptive queries needing iterative refinement and external search; CREWAI for multi-agent tasks with specialized roles and sequential processing; and SIMPLE for straightforward factual lookups. Include examples and decision criteria for each type.",
            "status": "done",
            "testStrategy": "Review documentation with team members to ensure clarity and completeness. Validate with example queries for each workflow type.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Monitor and Optimize Classification Performance",
            "description": "Implement monitoring for classification decisions and optimize performance based on real-world usage patterns.",
            "dependencies": [
              3
            ],
            "details": "Set up analytics to track classification accuracy, confidence scores, and routing decisions in production. Analyze patterns of misclassification or low confidence scores. Refine the classifier based on this data to improve accuracy and reduce fallbacks to SIMPLE workflow.",
            "status": "done",
            "testStrategy": "Analyze classification logs over time. Compare predicted workflow types with actual performance. Measure improvements in classification accuracy after optimization.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on query type detection & routing."
      },
      {
        "id": 23,
        "title": "Query Processing Pipeline & Result Merging",
        "description": "Normalize, expand, execute, deduplicate, and merge query results using RRF and reranking.",
        "status": "done",
        "dependencies": [
          "22"
        ],
        "priority": "high",
        "details": "Implemented comprehensive query processing pipeline with services for query expansion, hybrid search, reranking, and parallel execution. Features include multiple expansion strategies, parallel execution across search methods, deduplication, RRF merging, BGE-Reranker-v2 reranking, logging, metrics tracking, score thresholding, and Top-K selection.",
        "testStrategy": "Process complex queries, verify result quality, deduplication, and latency. All tests passed successfully with the implemented pipeline.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Query Normalization and Expansion",
            "description": "Develop modules to normalize incoming queries and expand them for improved recall and relevance.",
            "dependencies": [],
            "details": "Created query_expansion_service.py using Claude Haiku for query expansion with multiple strategies including synonyms and reformulations. Implemented standardization of query formats (lowercasing, removing stopwords) and comprehensive logging of all normalized and expanded queries for traceability.",
            "status": "done",
            "testStrategy": "Test with diverse query inputs, verify normalization accuracy, and check that expansions improve recall without introducing irrelevant results.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Execute Queries in Parallel and Deduplicate Results",
            "description": "Design and implement parallel query execution across multiple sources, followed by deduplication of retrieved results.",
            "dependencies": [
              1
            ],
            "details": "Implemented parallel_search_service.py to run expanded queries against all relevant data sources concurrently. Applied deduplication algorithms to remove duplicate results based on content similarity and unique identifiers. Added logging for execution times and deduplication statistics.",
            "status": "done",
            "testStrategy": "Simulate concurrent queries, measure execution latency, and verify that deduplication removes all duplicates while retaining unique results.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Merge Results Using RRF and Rerank Final Output",
            "description": "Integrate Reciprocal Rank Fusion (RRF) for merging results and apply reranking models to optimize final result order.",
            "dependencies": [
              2
            ],
            "details": "Created hybrid_search_service.py for RRF merging of results from dense, sparse, and fuzzy search methods. Implemented reranking_service.py using BGE-Reranker-v2 via Ollama (dev) or Claude API (prod). Added score thresholding and Top-K selection for optimal result quality. Ensured comprehensive logging of all queries and merged results for audit and debugging.",
            "status": "done",
            "testStrategy": "Process sample queries, validate that RRF merging and reranking improve relevance, and check that final output matches expected quality benchmarks.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on query processing pipeline & result merging."
      },
      {
        "id": 24,
        "title": "Faceted Search & Result Presentation",
        "description": "Enable faceted filtering (department, type, date, entities) and present results with snippets, highlights, and relevance scores.",
        "status": "done",
        "dependencies": [
          "23"
        ],
        "priority": "medium",
        "details": "Implemented multi-select facets for department, file_type, date_range, and entity filtering. Generated snippets with keyword highlighting using HTML <mark> tags. Displayed relevance scores, source metadata, and B2 URL links. Added pagination support and SQL WHERE clause generation for filters.",
        "testStrategy": "Verified filtered searches functionality, result presentation with snippets and highlights, facet accuracy, and pagination. Confirmed authentication with Clerk JWT tokens works correctly.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Multi-Select Faceted Filtering UI",
            "description": "Develop the frontend components to support multi-select faceted filtering by department, type, date, and entities.",
            "dependencies": [],
            "details": "Design and build user interface elements for each facet (department, type, date, entities) with multi-select capability. Ensure facets are easy to find, mobile-friendly, and update results quickly when filters are applied. Facet values should be ordered logically (alphabetical, numerical, or by relevance) and selected values should be clearly indicated. Only display relevant facets for the current result set.",
            "status": "done",
            "testStrategy": "Test by applying various combinations of facet filters and verifying that the displayed results update accordingly and facet selections persist. Check usability on both desktop and mobile.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Generate and Present Search Result Snippets with Highlights",
            "description": "Create backend and frontend logic to generate result snippets, highlight matched keywords, and display relevant metadata.",
            "dependencies": [
              1
            ],
            "details": "For each search result, extract a relevant snippet containing the matched keywords. Highlight these keywords in the snippet. Display additional metadata such as relevance score, source, and department. Ensure that snippets are concise and informative, and that highlights are visually distinct.",
            "status": "done",
            "testStrategy": "Run searches with various queries and verify that snippets are generated, keywords are highlighted, and metadata is displayed correctly for each result.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Result Presentation with B2 URLs and Relevance Scores",
            "description": "Link each search result to its corresponding Backblaze B2 URL and ensure relevance scores are visible and accurate.",
            "dependencies": [
              2
            ],
            "details": "For each result, provide a clickable link to the B2 URL. Display the relevance score prominently, ensuring it is calculated and presented consistently. Confirm that the source and department fields are shown as specified. Validate that all links are functional and direct users to the correct B2 resource.",
            "status": "done",
            "testStrategy": "Click through result links to verify correct B2 URL redirection. Check that relevance scores match backend calculations and are displayed for all results.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Documentation of Faceted Search Implementation",
            "description": "Document the implementation details of the faceted search service and API endpoint.",
            "dependencies": [
              3
            ],
            "details": "Create comprehensive documentation for the faceted search implementation, including the faceted_search_service.py and the POST /api/query/search/faceted endpoint. Document the parameters accepted by the API (query, departments, file_types, date_from, date_to, entities, page, page_size), authentication requirements, and response format.",
            "status": "done",
            "testStrategy": "Review documentation for completeness and accuracy. Ensure all parameters, response formats, and authentication requirements are clearly explained.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Performance Optimization for Faceted Search",
            "description": "Analyze and optimize the performance of the faceted search implementation.",
            "dependencies": [
              3
            ],
            "details": "Profile the faceted search implementation to identify performance bottlenecks. Optimize SQL queries for facet value extraction and filtering. Implement caching strategies for frequently used facet values. Ensure pagination works efficiently with large result sets.",
            "status": "done",
            "testStrategy": "Benchmark search performance with various query combinations and result set sizes. Verify that response times remain acceptable under load and with complex facet combinations.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on faceted search & result presentation."
      },
      {
        "id": 25,
        "title": "Query Analytics & A/B Testing",
        "description": "Log queries, track latency, CTR, and support A/B testing for ranking algorithms.",
        "details": "Store query logs and result clicks in Supabase. Implement analytics dashboard and A/B test framework for ranking methods.",
        "testStrategy": "Analyze logs, verify CTR tracking, and run A/B tests.",
        "priority": "medium",
        "dependencies": [
          "24"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Query Logging and Metric Tracking in Supabase",
            "description": "Set up infrastructure to log all search queries, track latency, and record click-through rates (CTR) in Supabase.",
            "dependencies": [],
            "details": "Design Supabase tables to store query logs, including query text, timestamps, latency, and user interactions (clicks). Integrate logging into the query execution pipeline to ensure all relevant metrics are captured for each search event.",
            "status": "pending",
            "testStrategy": "Verify that queries, latency, and clicks are correctly logged in Supabase by running test queries and inspecting the stored data for completeness and accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop Analytics Dashboard for Query Metrics",
            "description": "Build a dashboard to visualize query volume, latency, and CTR using data from Supabase.",
            "dependencies": [
              1
            ],
            "details": "Use a dashboarding tool (e.g., Grafana or Streamlit) to connect to Supabase and display real-time and historical analytics for query metrics. Include filters for date ranges and ranking algorithm versions to support analysis.",
            "status": "pending",
            "testStrategy": "Check that the dashboard accurately reflects Supabase data by comparing dashboard metrics with direct database queries. Test responsiveness and filtering capabilities.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement and Run A/B Testing Framework for Ranking Algorithms",
            "description": "Create an A/B testing framework to compare different ranking algorithms by splitting user traffic and measuring impact on CTR and latency.",
            "dependencies": [
              1
            ],
            "details": "Randomly assign users or sessions to control and treatment groups, each using a different ranking algorithm. Log group assignment and outcomes in Supabase. Analyze results using statistical tests (e.g., t-test or Z-test) to determine significance of observed differences in metrics like CTR and latency[1][2][3].",
            "status": "pending",
            "testStrategy": "Simulate A/B tests with test users, verify correct group assignment and metric logging, and validate statistical analysis pipeline with sample data.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on query analytics & a/b testing.",
        "updatedAt": "2025-11-08T18:01:36.843Z"
      },
      {
        "id": 26,
        "title": "Chat UI Implementation (WebSocket, Streaming)",
        "description": "Build a mobile-responsive chat UI with real-time messaging and token-by-token streaming.",
        "status": "done",
        "dependencies": [
          "25"
        ],
        "priority": "high",
        "details": "Implemented with Gradio for frontend. Used HTTP-based streaming with async generators instead of WebSocket for simpler implementation and better reliability with Gradio. Integrated Clerk authentication, comprehensive error handling, retry logic, and mobile-responsive design.",
        "testStrategy": "Tested chat interactions, streaming, and mobile responsiveness across multiple devices. Verified error handling and retry logic.",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop Chat UI Frontend with Gradio or Streamlit",
            "description": "Create a mobile-responsive chat interface using Gradio or Streamlit, supporting user input, message display, and chat history.",
            "dependencies": [],
            "details": "Implemented the chat UI using Gradio's ChatInterface. Created chat_ui.py with mobile-responsive design and app_with_auth.py with Clerk authentication integration. Added custom CSS styling for improved mobile experience.",
            "status": "done",
            "testStrategy": "Manually tested UI on desktop and mobile browsers for responsiveness, usability, and correct message display.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Real-time Messaging for Chat Communication",
            "description": "Set up backend with HTTP-based streaming to handle real-time chat communication between frontend and backend.",
            "dependencies": [
              1
            ],
            "details": "Implemented HTTP-based streaming with async generators instead of WebSocket. This approach proved simpler to implement, more reliable with Gradio ChatInterface, and easier to deploy on Render while providing the same user experience.",
            "status": "done",
            "testStrategy": "Tested streaming implementation to verify real-time message delivery and connection stability.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Token-by-Token Streaming from Claude API",
            "description": "Enable streaming of Claude API responses token-by-token to the frontend for real-time chat experience.",
            "dependencies": [
              2
            ],
            "details": "Modified backend to call Claude API with streaming enabled. Implemented token-by-token streaming to the frontend using HTTP async generators. Updated frontend to append streamed tokens to the chat window in real time.",
            "status": "done",
            "testStrategy": "Sent prompts and verified that responses appeared incrementally in the chat UI, matching Claude API streaming output.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Loading Indicators and Error Handling",
            "description": "Add loading indicators for the assistant and robust error handling for network/API failures.",
            "dependencies": [
              3
            ],
            "details": "Added loading indicators (🔍 Processing your query...) while waiting for Claude API responses. Implemented comprehensive error handling with user-friendly messages. Added retry logic with exponential backoff for improved reliability.",
            "status": "done",
            "testStrategy": "Simulated slow responses and errors; verified loading indicator visibility and user-friendly error messages. Tested retry logic with network interruptions.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Test and Optimize Mobile Responsiveness",
            "description": "Thoroughly test the chat UI on various mobile devices and optimize for touch interaction and layout.",
            "dependencies": [
              4
            ],
            "details": "Used browser dev tools and real devices to test UI scaling, input usability, and scrolling. Applied custom CSS styling for optimal mobile experience. Deployed to production at https://jb-empire-chat.onrender.com with both authenticated (/chat) and non-authenticated versions.",
            "status": "done",
            "testStrategy": "Performed cross-device testing and collected feedback to ensure consistent, responsive behavior on phones and tablets.",
            "parentId": "undefined"
          }
        ],
        "complexity": 6.5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Expand chat UI implementation into subtasks for frontend development (Gradio/Streamlit), WebSocket endpoint implementation, streaming response handling, typing indicator/error handling, and mobile responsiveness testing."
      },
      {
        "id": 27,
        "title": "Conversation Memory System (Supabase Graph Tables)",
        "description": "Store and retrieve user conversation memory using PostgreSQL graph tables (user_memory_nodes, user_memory_edges).",
        "details": "Implement memory node/edge creation, context window management, and recency/access-weighted retrieval. Enforce RLS policies.",
        "testStrategy": "Simulate conversations, verify memory storage, retrieval, and RLS enforcement.",
        "priority": "high",
        "dependencies": [
          "26"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Implement Graph Tables for Conversation Memory",
            "description": "Create PostgreSQL tables (user_memory_nodes, user_memory_edges) to represent conversation memory as a graph structure, supporting efficient storage and retrieval.",
            "dependencies": [],
            "details": "Define schemas for user_memory_nodes and user_memory_edges, ensuring each node represents a memory item (e.g., message, context) and edges capture relationships (e.g., temporal, reference). Implement table creation scripts and indexes for efficient traversal. Ensure compatibility with Supabase and prepare for RLS enforcement.",
            "status": "done",
            "testStrategy": "Verify table creation, schema correctness, and ability to insert and query nodes/edges representing conversation history.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Memory Node/Edge Management and Context Window Logic",
            "description": "Develop logic for creating, updating, and deleting memory nodes and edges, and manage the context window for conversation retrieval.",
            "dependencies": [
              1
            ],
            "details": "Implement backend functions to add new conversation turns as nodes, link them with edges, and prune or limit history based on a context window (e.g., last N messages). Ensure recency and access-weighted retrieval logic is in place to prioritize relevant memory during retrieval. Integrate with Supabase API for transactional consistency.",
            "status": "done",
            "testStrategy": "Simulate conversations, add and remove nodes/edges, and verify that context window and recency/access-weighted retrieval return expected results.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Enforce Row-Level Security (RLS) and Validate Secure Access",
            "description": "Apply and test RLS policies to ensure users can only access their own conversation memory data in the graph tables.",
            "dependencies": [
              1,
              2
            ],
            "details": "Define and apply RLS policies on user_memory_nodes and user_memory_edges to restrict access by user identity. Test for unauthorized access attempts and verify that only the correct user's data is accessible. Document RLS configuration and integrate with Supabase authentication.",
            "status": "done",
            "testStrategy": "Attempt cross-user access, verify RLS enforcement, and run automated tests to confirm only authorized access to memory nodes and edges.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on conversation memory system (supabase graph tables)."
      },
      {
        "id": 28,
        "title": "Session & Preference Management",
        "description": "Support multiple concurrent sessions, session persistence, user preference learning, and privacy controls.",
        "details": "Implement session tracking, timeout, export, and deletion. Store preferences as memory nodes. Provide opt-out and explicit preference UI.",
        "testStrategy": "Test session persistence, preference learning, and privacy controls.",
        "priority": "medium",
        "dependencies": [
          "27"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Multi-Session Tracking and Persistence",
            "description": "Develop mechanisms to support multiple concurrent user sessions, ensure session data is persistent across server restarts, and enable session export and deletion.",
            "dependencies": [],
            "details": "Design a session management system that assigns unique, secure session IDs, supports concurrent sessions per user, and persists session data using a shared store (e.g., Redis). Implement session timeout, export, and deletion features. Ensure session data is securely stored and can be invalidated or removed on demand.",
            "status": "done",
            "testStrategy": "Simulate multiple concurrent sessions, verify session persistence after server restart, and test session export and deletion functionality.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop User Preference Learning and Storage",
            "description": "Create a system to learn, store, and update user preferences as memory nodes, ensuring preferences are associated with the correct session and user.",
            "dependencies": [
              1
            ],
            "details": "Implement logic to capture user actions and infer preferences, storing them as structured memory nodes linked to user profiles. Ensure updates are atomic and preferences persist across sessions. Provide mechanisms to retrieve and update preferences efficiently.",
            "status": "done",
            "testStrategy": "Test preference capture, retrieval, and update across multiple sessions and users. Validate that preferences persist and are correctly associated with users.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Design Privacy Controls and Explicit Preference UI",
            "description": "Provide user-facing controls for privacy, including opt-out options and an explicit UI for managing preferences and active sessions.",
            "dependencies": [
              1,
              2
            ],
            "details": "Develop UI components that allow users to view and manage their active sessions, export or delete session data, and opt out of preference learning. Ensure privacy controls are clear, accessible, and enforceable at the backend.",
            "status": "done",
            "testStrategy": "Perform UI/UX testing for privacy controls, verify backend enforcement of opt-out and deletion, and ensure users can manage sessions and preferences as intended.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on session & preference management."
      },
      {
        "id": 29,
        "title": "Monitoring & Observability (Prometheus, Grafana, Alertmanager)",
        "description": "Collect metrics, visualize in Grafana, set up alerting, and structured logging for all services.",
        "details": "Integrate prometheus_client for FastAPI, Celery, Redis, Neo4j. Build Grafana dashboards with pre-built panels. Configure Alertmanager for multi-channel alerts. Implement JSON logs and health check endpoints.",
        "testStrategy": "Simulate load, verify metrics, dashboard updates, alert triggers, and log accuracy.",
        "priority": "high",
        "dependencies": [
          "28"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Prometheus Metrics Collection for All Services",
            "description": "Set up Prometheus metrics collection for FastAPI, Celery, Redis, and Neo4j services using prometheus_client.",
            "dependencies": [],
            "details": "Install and configure prometheus_client in each service. Expose /metrics endpoints for FastAPI, Celery, Redis, and Neo4j. Ensure custom business metrics are included where relevant. Validate that metrics are accessible and correctly formatted for Prometheus scraping.",
            "status": "done",
            "testStrategy": "Simulate service activity and verify metrics are exposed and collected by Prometheus. Check for completeness and accuracy of metrics.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Build Grafana Dashboards and Panels for Metrics Visualization",
            "description": "Create Grafana dashboards with pre-built and custom panels to visualize collected metrics from all integrated services.",
            "dependencies": [
              1
            ],
            "details": "Connect Grafana to Prometheus as a data source. Design dashboards for FastAPI, Celery, Redis, and Neo4j, including panels for key metrics (e.g., request rates, error rates, resource usage). Use Grafana's dashboard editor to organize panels and set up useful visualizations for operational monitoring.",
            "status": "done",
            "testStrategy": "Verify dashboards update in real-time with incoming metrics. Confirm panels display accurate and actionable data for each service.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Configure Alertmanager for Multi-Channel Alerting and Notification Routing",
            "description": "Set up Alertmanager to handle alerts from Prometheus and Grafana, routing notifications to multiple channels (e.g., email, Slack).",
            "dependencies": [
              2
            ],
            "details": "Install and configure Alertmanager. Define alert rules in Prometheus and Grafana for critical metrics. Set up Alertmanager contact points for email, Slack, and other channels. Configure notification policies and silences as needed. Integrate Alertmanager with Grafana to manage and route alerts, ensuring unified notification handling[1][3][4][5][6].",
            "status": "done",
            "testStrategy": "Trigger test alerts and verify notifications are sent to all configured channels. Check alert deduplication, grouping, and routing logic.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Deploy Self-Hosted Langfuse on Render",
            "description": "Deploy Langfuse web service on Render using existing Supabase PostgreSQL database for LLM observability and cost tracking.",
            "details": "Deploy Langfuse Docker container to Render as a web service. Configure database connection to existing Supabase PostgreSQL (unified database architecture). Set environment variables: LANGFUSE_DATABASE_URL, NEXTAUTH_SECRET, NEXTAUTH_URL, SALT. Generate API keys after deployment and update .env file. Verify Langfuse UI is accessible and database tables are created. Cost: $7/month (Starter plan). Full deployment guide: .taskmaster/docs/LANGFUSE_INTEGRATION_PLAN.md (Phase 1: Deployment).",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 29,
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on monitoring & observability (prometheus, grafana, alertmanager)."
      },
      {
        "id": 30,
        "title": "Cost Tracking & Optimization",
        "description": "Track API, compute, and storage costs. Generate monthly reports and trigger budget alerts.",
        "details": "Integrate cost tracking for Claude, Soniox, Mistral, LangExtract, Render, Supabase, B2. Implement budget alert logic at 80% threshold.",
        "testStrategy": "Simulate usage, verify cost reports and alert triggers.",
        "priority": "medium",
        "dependencies": [
          "29"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Cost Tracking for All Services",
            "description": "Implement automated cost tracking for Claude, Soniox, Mistral, LangExtract, Render, Supabase, and B2, covering API, compute, and storage expenses.",
            "dependencies": [],
            "details": "Set up data pipelines or use APIs to collect cost and usage data from each provider. Normalize and aggregate costs by service and resource type. Ensure tracking supports multi-cloud and SaaS sources, and enables per-service breakdowns for accurate reporting.",
            "status": "done",
            "testStrategy": "Simulate usage across all services, verify that cost data is collected, normalized, and attributed correctly for each provider.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Generate Monthly Cost Reports",
            "description": "Develop automated monthly reporting that summarizes API, compute, and storage costs for all integrated services.",
            "dependencies": [
              1
            ],
            "details": "Design and implement a reporting system that compiles monthly cost data into clear, actionable reports. Include breakdowns by service, resource type, and time period. Reports should be exportable and support visualization for trend analysis.",
            "status": "done",
            "testStrategy": "Trigger monthly report generation with sample data, verify report accuracy, completeness, and clarity for all tracked services.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Budget Alert Logic at 80% Threshold",
            "description": "Set up automated alerts to notify stakeholders when spending reaches 80% of the defined monthly budget for any tracked service.",
            "dependencies": [
              1
            ],
            "details": "Configure monitoring logic to evaluate cumulative spend against budget thresholds in real time. Integrate with notification channels (e.g., email, Slack) to deliver timely alerts. Ensure alerts are actionable and include relevant cost breakdowns.",
            "status": "done",
            "testStrategy": "Simulate cost increases to exceed 80% of budget, confirm that alerts are triggered promptly and contain accurate, actionable information.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on cost tracking & optimization."
      },
      {
        "id": 31,
        "title": "Role-Based Access Control (RBAC) & API Key Management",
        "description": "Implement RBAC for users, documents, and API keys with audit logging and row-level security.",
        "details": "Use Supabase RLS policies, implement user roles (admin, editor, viewer, guest), API key creation/rotation/revocation, and audit logs. Hash API keys with bcrypt.",
        "testStrategy": "Test role permissions, API key flows, and audit log accuracy.",
        "priority": "high",
        "dependencies": [
          "30"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Implement User Roles and Row-Level Security (RLS) Policies in Supabase",
            "description": "Define user roles (admin, editor, viewer, guest) and implement row-level security (RLS) policies for users, documents, and API keys using Supabase.",
            "dependencies": [],
            "details": "Create a roles table and associate users with roles. Use Supabase's RLS policies to restrict access to tables based on user roles. Ensure that each role has clearly defined permissions for CRUD operations on users, documents, and API keys. Reference Supabase documentation and best practices for RLS and RBAC implementation.\n<info added on 2025-11-11T02:00:19.256Z>\nImplementation completed for User Roles and RLS Policies:\n\n✅ Database Schema Created:\n- Created roles table with 4 default roles (admin, editor, viewer, guest)\n- Created user_roles table for user-to-role mappings\n- Created api_keys table with bcrypt hashing\n- Created rbac_audit_logs table for immutable audit trail\n\n✅ RLS Policies Implemented:\n- Enabled RLS on all RBAC tables\n- roles table: read-only for authenticated users\n- api_keys table: users can only see/manage their own keys\n- user_roles table: users can read own roles, admins can manage all roles\n- rbac_audit_logs table: admin-only access\n\n✅ Default Roles Seeded:\n- admin: Full system access (all permissions)\n- editor: Can read/write documents\n- viewer: Can read documents only\n- guest: Limited read access\n\nFiles created:\n- app/models/rbac.py (Pydantic models)\n- app/core/supabase_client.py (Supabase helper)\n- Supabase migration applied successfully\n\nNext: Testing RLS policies and role permissions.\n</info added on 2025-11-11T02:00:19.256Z>",
            "status": "done",
            "testStrategy": "Test RLS policies by creating users with different roles and verifying access to resources. Attempt unauthorized actions to confirm enforcement of restrictions.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement API Key Lifecycle Management with Secure Storage",
            "description": "Develop endpoints and logic for API key creation, rotation, and revocation. Store API keys securely using bcrypt hashing.",
            "dependencies": [
              1
            ],
            "details": "Create endpoints for generating, rotating, and revoking API keys. Store only hashed versions of API keys using bcrypt in the database. Ensure that API keys are associated with users and roles, and that their permissions align with RBAC policies. Document the API key management process and enforce secure handling throughout the lifecycle.\n<info added on 2025-11-11T02:00:28.704Z>\nAPI Key Lifecycle Management Implementation Complete:\n\nAPI Key Generation:\n- Secure random token generation (64 hex chars)\n- Format: emp_[64-char-token]\n- Bcrypt hashing for secure storage\n- Key prefix extraction for fast lookup (emp_xxxxxxxx)\n\nAPI Key Operations Implemented:\n- create_api_key(): Generate new key with role assignment\n- validate_api_key(): Verify key with bcrypt check\n- list_api_keys(): List user's keys (prefix only, no full keys)\n- rotate_api_key(): Create new key, revoke old one atomically\n- revoke_api_key(): Permanently disable key with reason\n\nSecurity Features:\n- Full key shown ONLY once at creation\n- Automatic expiration checking\n- Usage tracking (last_used_at, usage_count)\n- Rate limiting support (rate_limit_per_hour field)\n- Ownership verification for all operations\n\nFiles Created:\n- app/services/rbac_service.py (Complete service implementation)\n- app/routes/rbac.py (FastAPI endpoints)\n- app/middleware/auth.py (Authentication middleware)\n\nIntegration:\n- RBAC router added to main.py at /api/rbac\n- Supports both API key and JWT authentication (JWT stub for future)\n</info added on 2025-11-11T02:00:28.704Z>",
            "status": "done",
            "testStrategy": "Verify API key creation, rotation, and revocation flows. Confirm that only hashed keys are stored and that revoked keys cannot be used for access.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Audit Logging for Access and Key Management Events",
            "description": "Track and log all access events, permission changes, and API key operations for auditing and compliance.",
            "dependencies": [
              1,
              2
            ],
            "details": "Set up audit logging for all RBAC-related actions, including role assignments, permission changes, and API key lifecycle events. Store logs in a dedicated audit table with relevant metadata (user, action, timestamp, resource). Ensure logs are immutable and accessible for compliance reviews.\n<info added on 2025-11-11T02:00:37.494Z>\nCompleted implementation of Audit Logging:\n\n✅ Audit Log Events Tracked:\n- api_key_created: When new key is generated\n- api_key_used: Every time key is validated/used\n- api_key_rotated: When key is rotated\n- api_key_revoked: When key is revoked\n- role_assigned: When role is granted to user\n- role_revoked: When role is removed from user\n\n✅ Audit Log Fields:\n- event_type: Type of event\n- actor_user_id: Who performed the action\n- target_user_id: Who was affected (for role operations)\n- target_resource_type: Type of resource (api_key, user_role)\n- target_resource_id: UUID of affected resource\n- action: Action performed (create, revoke, assign, etc.)\n- result: Outcome (success, failure, denied)\n- ip_address: IP of the request\n- user_agent: User agent string\n- metadata: Additional context (JSON)\n- error_message: Error details if failed\n- created_at: Immutable timestamp\n\n✅ Audit Features:\n- Immutable logs (insert-only, no updates)\n- Automatic logging in all RBAC operations\n- Admin-only access via RLS policies\n- Query filtering by event_type, user_id\n- Pagination support (limit/offset)\n\n✅ API Endpoint:\n- GET /api/rbac/audit-logs (admin only)\n- Supports filtering and pagination\n\nNext: Testing audit log accuracy and RLS enforcement.\n</info added on 2025-11-11T02:00:37.494Z>",
            "status": "done",
            "testStrategy": "Trigger various RBAC and API key events, then review audit logs to confirm accurate and complete recording of all relevant actions.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on role-based access control (rbac) & api key management."
      },
      {
        "id": 32,
        "title": "Bulk Document Management & Batch Operations",
        "description": "Enable bulk upload, delete, reprocessing, metadata update, versioning, and approval workflow for documents.",
        "details": "Implement batch endpoints for document operations. Track progress and support document versioning and approval states.",
        "testStrategy": "Perform bulk operations, verify throughput, versioning, and approval transitions.",
        "priority": "high",
        "dependencies": [
          "31"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Bulk Document Operations Endpoints",
            "description": "Develop RESTful API endpoints to support bulk upload, delete, reprocessing, and metadata update for documents.",
            "dependencies": [],
            "details": "Design and implement backend endpoints that accept batch requests for document operations. Ensure endpoints handle large payloads efficiently, support progress tracking, and provide clear error reporting for partial failures. Integrate with storage and indexing layers to maintain consistency and performance.\n<info added on 2025-11-11T21:02:25.181Z>\n## Investigation Results\n\n**Already Implemented:**\n1. ✅ All 4 bulk operation REST API endpoints in app/routes/documents.py:\n   - POST /bulk-upload\n   - POST /bulk-delete  \n   - POST /bulk-reprocess\n   - PATCH /bulk-metadata\n   - GET /batch-operations/{operation_id}\n   - GET /batch-operations\n\n2. ✅ All 4 Celery tasks in app/tasks/bulk_operations.py:\n   - bulk_upload_documents\n   - bulk_delete_documents\n   - bulk_reprocess_documents\n   - bulk_update_metadata\n   - Includes progress tracking and error handling\n\n**Missing - Need to Implement:**\nThe Celery tasks reference 4 functions from app.services.document_processor that don't exist yet:\n1. ❌ process_document_upload(file_path, filename, metadata, user_id, auto_process)\n2. ❌ delete_document(document_id, user_id, soft_delete)\n3. ❌ reprocess_document(document_id, user_id, force_reparse, update_embeddings, preserve_metadata)\n4. ❌ update_document_metadata(document_id, metadata, user_id)\n\nThe current document_processor.py only contains text extraction/parsing logic, not document management operations.\n\n**Next Steps:**\nNeed to create these 4 document management functions to complete Task 32.1.\n</info added on 2025-11-11T21:02:25.181Z>",
            "status": "done",
            "testStrategy": "Submit bulk operation requests (upload, delete, reprocess, metadata update) with varying batch sizes. Verify throughput, error handling, and data integrity for all operations.",
            "parentId": "undefined",
            "updatedAt": "2025-11-11T03:42:03.083Z"
          },
          {
            "id": 2,
            "title": "Integrate Document Versioning and Approval Workflow",
            "description": "Enable version control and approval states for documents, supporting batch transitions and rollbacks.",
            "dependencies": [
              1
            ],
            "details": "Extend the document model to support version history and approval status. Implement logic for batch versioning (e.g., uploading new versions in bulk) and batch approval/rejection. Ensure audit trails are maintained for all version and approval changes.",
            "status": "done",
            "testStrategy": "Perform bulk version uploads and approval transitions. Verify correct version history, approval state changes, and audit trail entries for all affected documents.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Progress Tracking and Operation Auditing",
            "description": "Track and expose the progress and audit logs of all batch document operations for transparency and compliance.",
            "dependencies": [
              1,
              2
            ],
            "details": "Develop mechanisms to monitor the status of ongoing batch operations, including per-document success/failure. Provide APIs or dashboards for users to query operation progress and review detailed audit logs. Ensure compliance with organizational and regulatory requirements for traceability.\n<info added on 2025-11-11T21:17:31.729Z>\n## Implementation Status: Complete\n\n**Progress Tracking (✅ Complete):**\n1. Models defined in app/models/documents.py:\n   - BatchOperationResponse (lines 92-106) - operation tracking with progress\n   - BatchOperationStatusResponse (lines 108-123) - detailed status with progress_percentage\n\n2. REST API endpoints in app/routes/documents.py:\n   - GET /api/documents/batch-operations/{operation_id} (lines 402-447) - Get specific operation status\n   - GET /api/documents/batch-operations (lines 450-505) - List all operations with filtering, pagination, and progress calculation\n\n3. Real-time progress updates in app/tasks/bulk_operations.py:\n   - _update_operation_status() helper function (lines 551-600)\n   - Called at start, during processing (per-document), and on completion\n   - Tracks: status, processed_items, successful_items, failed_items, results array\n\n**Operation Auditing (✅ Complete):**\n1. Database table: batch_operations (workflows/database_setup.md lines 609-624)\n   - Stores: operation_type, initiated_by, items counts, status, parameters, results\n   - Timestamps: started_at, completed_at, created_at\n   - JSONB fields for detailed parameters and results\n\n2. Approval workflow audit: approval_audit_log table with ApprovalAuditLogEntry model\n   - Tracks all approval state transitions\n   - Includes: event_type, status changes, user, IP address, user agent, timestamps\n\n3. Detailed result tracking:\n   - DocumentOperationResult model (lines 83-90) - per-document status with success/failure/error\n   - Stored in results JSONB array in batch_operations table\n\n**Compliance & Traceability (✅ Complete):**\n- Full audit trail for all batch operations\n- User tracking (initiated_by field)\n- Timestamp tracking (created_at, started_at, completed_at, updated_at)\n- Error message logging\n- Detailed per-document results\n</info added on 2025-11-11T21:17:31.729Z>",
            "status": "done",
            "testStrategy": "Initiate various batch operations and monitor progress tracking endpoints or dashboards. Validate that audit logs accurately reflect all actions, including errors and rollbacks.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on bulk document management & batch operations.",
        "updatedAt": "2025-11-11T03:42:03.083Z"
      },
      {
        "id": 33,
        "title": "User Management & GDPR Compliance",
        "description": "Support user creation, editing, role assignment, password reset, suspension, activity logs, and GDPR-compliant data export.",
        "details": "Implement admin endpoints for user management. Store activity logs and support data export/deletion per GDPR.",
        "testStrategy": "Test user flows, activity logging, and GDPR export/deletion.",
        "priority": "high",
        "dependencies": [
          "32"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement User Account and Role Management Endpoints",
            "description": "Develop admin endpoints to support user creation, editing, role assignment, password reset, and suspension.",
            "dependencies": [],
            "details": "Create RESTful endpoints for user CRUD operations, role assignment, and password management. Ensure endpoints allow for user suspension/reactivation and support both pre-defined and custom roles. Integrate secure authentication and authorization checks for all admin actions.\n<info added on 2025-11-11T21:36:53.612Z>\n## Implementation Details\n\n**Database Schema (Already Implemented)**\n- admin_users table (username, email, password_hash, role, is_active, etc.)\n- admin_sessions table (session tokens)\n- admin_activity_log table (action logging)\n\n**RBAC System (Already Implemented)**\n- API key lifecycle management\n- Role assignment/revocation functionality\n- Audit logging for RBAC events\n- Authentication middleware using API keys and JWT via Clerk\n- Authorization check middleware\n\n**Required User Management Endpoints**\n1. User CRUD operations:\n   - POST /api/users - Create new admin user\n   - GET /api/users - List all users with pagination/filtering\n   - GET /api/users/{user_id} - Retrieve specific user details\n   - PATCH /api/users/{user_id} - Update user information\n   - DELETE /api/users/{user_id} - Delete user account\n\n2. Password management:\n   - POST /api/users/{user_id}/reset-password - Admin-initiated reset\n   - POST /api/users/change-password - Self-service password change\n\n3. Account status management:\n   - POST /api/users/{user_id}/suspend - Suspend user account\n   - POST /api/users/{user_id}/activate - Reactivate suspended account\n\n**Implementation Plan**\n- Create app/routes/users.py with admin user management endpoints\n- Develop app/services/user_service.py for user operations\n- Define app/models/users.py for Pydantic models\n- Utilize bcrypt for password hashing\n- Integrate with admin_activity_log for comprehensive audit trail\n</info added on 2025-11-11T21:36:53.612Z>",
            "status": "done",
            "testStrategy": "Test user creation, editing, role assignment, password reset, and suspension via API and UI. Verify role-based access control and error handling.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Activity Logging for User Actions",
            "description": "Log all significant user management actions (creation, edits, role changes, suspensions, password resets) for audit and compliance.",
            "dependencies": [
              1
            ],
            "details": "Design and implement a logging mechanism to capture all admin and user actions related to user management. Store logs securely with timestamps, user IDs, action types, and relevant metadata. Ensure logs are immutable and accessible for compliance audits.\n<info added on 2025-11-11T21:42:57.585Z>\n## Investigation Status Update\n\nInitial investigation of logging mechanism reveals:\n\n1. Implementation Status:\n   - _log_activity() function is implemented in user_service.py\n   - Function is called by all user management operations\n   - Logs are written to admin_activity_log table\n\n2. Pending Verification:\n   - Database constraints and RLS policies need to be checked to ensure log immutability\n   - No endpoints currently exist for retrieving user activity logs for compliance audits\n\n3. Action Items:\n   - Implement read-only API endpoints for retrieving filtered activity logs\n   - Add database constraints to prevent modification of existing log entries\n   - Document the logging schema and retention policies\n   - Create test cases to verify logging functionality across all user management actions\n</info added on 2025-11-11T21:42:57.585Z>",
            "status": "done",
            "testStrategy": "Trigger user management actions and verify that logs are created with correct details. Test log retrieval and integrity.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Develop GDPR-Compliant Data Export and Deletion Features",
            "description": "Enable GDPR-compliant export and deletion of user data, including activity logs, upon user or admin request.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement endpoints to export all user-related data in a machine-readable format and to delete user data in accordance with GDPR requirements. Ensure deletion covers user profile, roles, and associated activity logs, and that exports are complete and secure.\n<info added on 2025-11-11T21:46:01.948Z>\n**Requirements Analysis:**\n\n1. Data Export Endpoint (GET /api/users/{user_id}/export):\n   - Export user profile data (username, email, full_name, role, etc.)\n   - Export all activity logs related to user (both as actor and subject)\n   - Export user sessions history\n   - Export API keys (without sensitive key material)\n   - Export user roles and permissions\n   - Format: JSON (machine-readable)\n   - Admin-only access\n\n2. Data Deletion Endpoint (DELETE /api/users/{user_id}/gdpr-delete):\n   - Delete user profile from admin_users table\n   - Delete/anonymize activity logs (preserve audit trail but remove PII)\n   - Delete all user sessions from admin_sessions table\n   - Revoke all user API keys\n   - Delete user role assignments\n   - Cascade deletion with proper foreign key handling\n   - Admin-only access with confirmation required\n\n**Implementation Plan:**\n- Add export_user_data() method to UserService\n- Add gdpr_delete_user() method to UserService\n- Add GDPR export/delete endpoints to users router\n- Add Pydantic models for export response\n- Consider: Activity logs should be anonymized rather than deleted for audit compliance\n</info added on 2025-11-11T21:46:01.948Z>",
            "status": "done",
            "testStrategy": "Request data export and deletion for test users. Verify completeness of exported data and confirm all user data is removed after deletion, including logs.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on user management & gdpr compliance."
      },
      {
        "id": 34,
        "title": "Analytics Dashboard Implementation",
        "description": "Build dashboard for document stats, query metrics, user activity, storage usage, and API endpoint usage.",
        "details": "Use Grafana or Streamlit for dashboard UI. Aggregate metrics from Supabase and Prometheus.",
        "testStrategy": "Verify dashboard accuracy and responsiveness under load.",
        "priority": "medium",
        "dependencies": [
          "33"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop Dashboard UI with Grafana or Streamlit",
            "description": "Set up the dashboard user interface using either Grafana or Streamlit, ensuring a logical layout for document stats, query metrics, user activity, storage usage, and API endpoint usage.",
            "dependencies": [],
            "details": "Install and configure Grafana or Streamlit. Design the dashboard structure, applying best practices such as focusing on key metrics, using consistent layouts, and providing clear panel documentation. Ensure the UI is intuitive and supports dynamic filtering or variable selection as needed.\n<info added on 2025-11-11T21:54:47.058Z>\nBased on the investigation findings, we will implement the analytics dashboard using Grafana since an existing infrastructure pattern is already established. We'll create a comprehensive dashboard with five main panel categories: document statistics, query metrics, user activity, storage usage, and API endpoint usage.\n\nThe implementation will follow this approach:\n1. Create a dedicated metrics service in app/services/metrics_service.py to collect and organize analytics data\n2. Add a Prometheus metrics endpoint in app/routes/monitoring.py to expose metrics for Grafana consumption\n3. Develop a Grafana dashboard JSON configuration at monitoring/grafana/dashboards/empire_analytics.json\n4. Follow the established pattern from the existing ragas_metrics.json dashboard for consistency\n\nThe dashboard will leverage the existing Grafana infrastructure while providing comprehensive visibility into system performance and usage patterns across all key operational areas.\n</info added on 2025-11-11T21:54:47.058Z>",
            "status": "done",
            "testStrategy": "Verify that all required metric categories are represented and the UI is navigable. Check for adherence to dashboard design best practices.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Aggregate Metrics from Supabase and Prometheus",
            "description": "Implement data aggregation logic to collect and preprocess metrics from Supabase and Prometheus for use in the dashboard.",
            "dependencies": [
              1
            ],
            "details": "Develop scripts or queries to extract relevant metrics (document stats, query metrics, user activity, storage usage, API endpoint usage) from Supabase and Prometheus. Transform and aggregate data as needed for efficient dashboard consumption. Ensure data freshness and reliability.",
            "status": "done",
            "testStrategy": "Validate that all required metrics are accurately aggregated and available for the dashboard. Test with sample data and edge cases.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Data Visualization Components",
            "description": "Create and configure visualizations for each metric category, ensuring clarity and actionable insights.",
            "dependencies": [
              2
            ],
            "details": "Select appropriate visualization types (e.g., graphs, tables, gauges) for each metric. Configure panels to highlight key signals and trends. Apply consistent color schemes and labeling. Add annotations or context where relevant to aid interpretation.",
            "status": "done",
            "testStrategy": "Review each visualization for accuracy, clarity, and alignment with dashboard goals. Solicit feedback from stakeholders and iterate as needed.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Test Dashboard Load and Responsiveness",
            "description": "Evaluate dashboard performance under expected and peak loads, optimizing for fast load times and responsive interactions.",
            "dependencies": [
              3
            ],
            "details": "Simulate concurrent users and high data volumes. Monitor dashboard load times, panel refresh rates, and responsiveness. Apply optimizations such as query aggregation, efficient variable usage, and appropriate refresh intervals. Document and address any bottlenecks.",
            "status": "done",
            "testStrategy": "Run load tests and measure key performance indicators (KPIs) such as load time and refresh latency. Confirm dashboard remains usable and responsive under stress.",
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down analytics dashboard implementation into subtasks for dashboard UI development (Grafana/Streamlit), metrics aggregation from Supabase/Prometheus, data visualization, and load/responsiveness testing."
      },
      {
        "id": 35,
        "title": "CrewAI Multi-Agent Integration & Orchestration",
        "description": "Integrate CrewAI service (REST API) for multi-agent workflows, agent management, and orchestration.",
        "details": "Connect to CrewAI REST API (srv-d2n0hh3uibrs73buafo0). Implement agent pool management, dynamic agent creation, lifecycle, and resource allocation. Support async task execution via Celery.",
        "testStrategy": "Run multi-agent workflows, verify orchestration, agent lifecycle, and result aggregation.",
        "priority": "high",
        "dependencies": [
          "34"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate CrewAI REST API for Agent Pool Management and Dynamic Agent Creation",
            "description": "Connect to the CrewAI REST API and implement logic for managing an agent pool, including dynamic creation, configuration, and lifecycle management of agents.",
            "dependencies": [],
            "details": "Establish secure connectivity to the CrewAI REST API (srv-d2n0hh3uibrs73buafo0). Implement endpoints and logic for creating, updating, and deleting agents dynamically. Support agent configuration (roles, goals, tools, memory, etc.) as per CrewAI's agent model. Ensure agents can be instantiated with custom parameters and maintain their lifecycle state.\n<info added on 2025-11-12T02:56:38.121Z>\nBased on the investigation, I'll enhance the CrewAI integration by implementing the following:\n\n1. Extend the existing crewai_service.py with comprehensive agent pool management methods:\n   - Agent CRUD operations: create_agent(), update_agent(), delete_agent(), get_agent(), get_agents()\n   - Crew management functions: create_crew(), update_crew(), delete_crew(), get_crew(), get_crews()\n   - Resource monitoring via get_agent_pool_stats() to track agent utilization and availability\n\n2. Implement Supabase database integration for the existing schema (crewai_agents, crewai_crews, crewai_task_templates, crewai_executions) to ensure persistent storage of agent configurations and execution history.\n\n3. Develop agent lifecycle management functionality including activation, deactivation, and status tracking.\n\n4. Create REST API routes in app/routes/crewai.py exposing agent and crew management endpoints.\n\n5. Connect with the existing CrewAI REST API at https://jb-crewai.onrender.com for agent execution and orchestration.\n</info added on 2025-11-12T02:56:38.121Z>",
            "status": "done",
            "testStrategy": "Create, update, and delete agents via API calls. Verify agent state transitions and configuration persistence.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Multi-Agent Workflow Orchestration and Resource Allocation",
            "description": "Develop orchestration logic to coordinate multi-agent workflows, manage task assignments, and allocate resources efficiently among agents.",
            "dependencies": [
              1
            ],
            "details": "Design and implement orchestration mechanisms using CrewAI's crew-and-flow model. Enable both sequential and parallel task execution modes. Assign tasks to agents based on their roles and goals, and manage dependencies between tasks. Implement resource allocation strategies to optimize agent utilization and prevent overload.",
            "status": "done",
            "testStrategy": "Run sample multi-agent workflows with varying complexity. Verify correct task sequencing, parallelism, and resource allocation.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Enable Asynchronous Task Execution and Monitoring via Celery",
            "description": "Integrate Celery to support asynchronous execution of agent tasks and implement monitoring for workflow progress and agent states.",
            "dependencies": [
              2
            ],
            "details": "Set up Celery workers to handle asynchronous task execution for CrewAI workflows. Ensure tasks can be queued, executed, and monitored independently. Capture logs and state changes for each agent and workflow. Implement error handling and alerting for failed tasks or agent exceptions.",
            "status": "done",
            "testStrategy": "Submit multiple concurrent workflows, monitor execution progress, and verify correct handling of asynchronous tasks and error scenarios.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on crewai multi-agent integration & orchestration."
      },
      {
        "id": 36,
        "title": "CrewAI Asset Generation Agents Implementation",
        "description": "Implement 8 asset generation agents (orchestrator, summarizer, skill, command, agent, prompt, workflow, department classifier) per PRD specs.",
        "details": "Define agent roles, goals, tools, and LLM configs in crewai_agents table. Integrate with CrewAI API for asset generation. Store outputs in B2 processed/ folders.",
        "testStrategy": "Trigger asset generation for sample documents, verify output formats and B2 storage.",
        "priority": "high",
        "dependencies": [
          "35"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define and Configure 8 Asset Generation Agents in crewai_agents Table",
            "description": "Specify roles, goals, tools, and LLM configurations for orchestrator, summarizer, skill, command, agent, prompt, workflow, and department classifier agents as per PRD specifications.",
            "dependencies": [],
            "details": "Draft detailed agent definitions in the crewai_agents table, ensuring each agent's role, goal, toolset, and LLM configuration aligns with PRD requirements. Use YAML or database schema as appropriate. Validate configuration completeness for all 8 agents.",
            "status": "done",
            "testStrategy": "Review crewai_agents table for correct entries and completeness. Validate agent configs load without errors in CrewAI.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Agents with CrewAI API for Asset Generation Workflows",
            "description": "Connect the configured agents to the CrewAI API, enabling them to generate assets according to workflow requirements.",
            "dependencies": [
              1
            ],
            "details": "Implement integration logic to instantiate and orchestrate the 8 agents using the CrewAI API. Ensure agents can receive tasks, execute asset generation, and interact as needed. Handle API authentication and error management.",
            "status": "done",
            "testStrategy": "Trigger asset generation for sample inputs via CrewAI API and verify that each agent performs its designated function.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Store Generated Assets in B2 Processed Folders",
            "description": "Implement logic to save all outputs from asset generation agents into the appropriate B2 processed/ folders.",
            "dependencies": [
              2
            ],
            "details": "Develop or update storage routines to ensure all generated assets are saved in the correct B2 processed/ directory structure. Confirm metadata and output formats match requirements. Handle storage errors and ensure data integrity.",
            "status": "done",
            "testStrategy": "Generate assets through the workflow and verify their presence, structure, and metadata in B2 processed/ folders.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on crewai asset generation agents implementation."
      },
      {
        "id": 37,
        "title": "CrewAI Document Analysis Agents Implementation",
        "description": "Implement 3 document analysis agents (research analyst, content strategist, fact checker) for structured analysis and verification.",
        "details": "Configure agents in crewai_agents table. Integrate with CrewAI API for analysis workflows. Store analysis outputs in Supabase and B2.",
        "testStrategy": "Run analysis workflows, verify structured outputs and fact verification accuracy.",
        "priority": "high",
        "dependencies": [
          "36"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Document Analysis Agents in crewai_agents Table",
            "description": "Define and register the three specialized agents (research analyst, content strategist, fact checker) in the crewai_agents table with appropriate roles, goals, and capabilities.",
            "dependencies": [],
            "details": "Specify agent roles, goals, and backstories in the crewai_agents table or agents.yaml. Ensure each agent is configured for its analysis specialization and can be referenced by workflows. Use CrewAI's agent configuration standards for compatibility.",
            "status": "done",
            "testStrategy": "Verify agents appear in the crewai_agents table and can be instantiated by CrewAI workflows.",
            "parentId": "undefined",
            "updatedAt": "2025-11-14T18:18:43.346Z"
          },
          {
            "id": 2,
            "title": "Integrate Agents with CrewAI API for Analysis Workflows",
            "description": "Connect the configured agents to the CrewAI API, enabling them to participate in document analysis workflows.",
            "dependencies": [
              1
            ],
            "details": "Implement API integration logic to allow the agents to receive tasks, process documents, and return structured outputs. Ensure agents can be triggered via the CrewAI API and handle input/output formats as required by the workflow.",
            "status": "done",
            "testStrategy": "Trigger sample analysis workflows via the API and confirm agents process and return structured results.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Document Analysis Workflow Execution",
            "description": "Design and execute workflows that coordinate the three agents for structured document analysis and verification.",
            "dependencies": [
              2
            ],
            "details": "Define workflow logic that assigns documents to the appropriate agents, sequences their tasks (e.g., research, content strategy, fact checking), and aggregates their outputs. Use CrewAI's workflow orchestration features to manage task flow.",
            "status": "done",
            "testStrategy": "Run end-to-end workflow executions and verify that each agent performs its designated analysis step.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Store Analysis Outputs in Supabase and B2",
            "description": "Persist the structured outputs from each agent in Supabase for structured data and B2 for file storage.",
            "dependencies": [
              3
            ],
            "details": "Implement logic to map agent outputs to Supabase tables for structured results and upload any relevant files or artifacts to B2. Ensure outputs are linked to the correct document and agent metadata.",
            "status": "done",
            "testStrategy": "Check Supabase and B2 for correct storage of outputs after workflow execution; verify data integrity and retrievability.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Test and Validate Agent Output Accuracy and Fact Verification",
            "description": "Systematically test the accuracy of agent outputs, with a focus on fact-checking reliability and structured result formats.",
            "dependencies": [
              4
            ],
            "details": "Develop test cases with known document inputs and expected outputs. Evaluate the correctness of research, content strategy, and fact-checking results. Measure fact-checker precision and recall, and validate output structure.",
            "status": "done",
            "testStrategy": "Run automated and manual tests comparing outputs to ground truth; review fact-checking results for accuracy and completeness.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down document analysis agent implementation into subtasks for agent configuration, CrewAI API integration, workflow execution, output storage in Supabase/B2, and accuracy testing.",
        "updatedAt": "2025-11-14T18:18:43.346Z"
      },
      {
        "id": 38,
        "title": "CrewAI Multi-Agent Orchestration Agents Implementation",
        "description": "Implement 4 orchestration agents (research, analysis, writing, review) for complex multi-document workflows.",
        "details": "Configure agents and crews in crewai_crews table. Support sequential and parallel execution modes. Integrate with CrewAI API for orchestration.",
        "testStrategy": "Run multi-agent orchestration workflows, verify execution order and output quality.",
        "priority": "high",
        "dependencies": [
          "37"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Orchestration Agents and Crews in crewai_crews Table",
            "description": "Define and register the four orchestration agents (research, analysis, writing, review) and their crew configurations in the crewai_crews table.",
            "dependencies": [],
            "details": "Specify agent roles, goals, backstories, and advanced options (e.g., LLM, delegation, tools) for each agent. Ensure each agent is correctly mapped to its crew and that the crew structure supports both sequential and parallel execution modes.",
            "status": "done",
            "testStrategy": "Verify agents and crews are correctly listed in the crewai_crews table and can be retrieved via API.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Sequential and Parallel Execution Logic for Agent Workflows",
            "description": "Develop logic to support both sequential and parallel execution of agent tasks within a crew for multi-document workflows.",
            "dependencies": [
              1
            ],
            "details": "Design execution engine to trigger agents in order (sequential) or concurrently (parallel) based on workflow configuration. Ensure correct handling of dependencies and data flow between agents.",
            "status": "done",
            "testStrategy": "Run sample workflows in both modes, confirm correct execution order and data handoff.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate CrewAI API for Orchestration and Agent Lifecycle Management",
            "description": "Connect orchestration logic to CrewAI API endpoints for agent invocation, status tracking, and result retrieval.",
            "dependencies": [
              2
            ],
            "details": "Implement API calls for agent task submission, monitor agent progress, and handle callbacks or polling for completion. Ensure robust error handling and retries.",
            "status": "done",
            "testStrategy": "Trigger agent workflows via API, verify correct agent lifecycle events and result collection.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Develop Workflow Management and State Tracking Mechanisms",
            "description": "Create workflow management logic to track the state, progress, and dependencies of multi-agent, multi-document workflows.",
            "dependencies": [
              3
            ],
            "details": "Implement state machine or workflow tracker to monitor each agent's status, handle transitions, and manage workflow metadata. Support resumption and recovery from failures.",
            "status": "done",
            "testStrategy": "Simulate workflow interruptions and restarts, verify accurate state tracking and recovery.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Output Validation and Quality Assurance for Agent Results",
            "description": "Design and apply validation checks to ensure agent outputs meet expected quality, format, and completeness standards.",
            "dependencies": [
              4
            ],
            "details": "Define validation rules for each agent type (e.g., research completeness, analysis accuracy, writing coherence, review thoroughness). Integrate automated and optional human-in-the-loop checks.",
            "status": "done",
            "testStrategy": "Run workflows with known-good and intentionally flawed inputs, verify validation catches errors and approves correct outputs.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Develop and Execute Comprehensive Orchestration Testing Suite",
            "description": "Create automated tests to validate orchestration logic, agent integration, workflow management, and output quality across various scenarios.",
            "dependencies": [
              5
            ],
            "details": "Design test cases for sequential and parallel workflows, error handling, state recovery, and output validation. Use both unit and integration tests to ensure system robustness.",
            "status": "done",
            "testStrategy": "Run full test suite, confirm all orchestration paths and edge cases are covered and pass.",
            "parentId": "undefined"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Expand orchestration agent implementation into subtasks for agent/crew configuration, sequential/parallel execution logic, CrewAI API integration, workflow management, output validation, and orchestration testing."
      },
      {
        "id": 39,
        "title": "CrewAI Inter-Agent Messaging & Collaboration",
        "description": "Enable inter-agent messaging, task delegation, result sharing, and conflict resolution within CrewAI workflows.",
        "details": "Implement agent interactions in crewai_agent_interactions table. Support direct/broadcast messaging, event publication, and state synchronization.",
        "testStrategy": "Simulate collaborative workflows, verify messaging, delegation, and result aggregation.",
        "priority": "high",
        "dependencies": [
          "38"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Inter-Agent Interaction Schema",
            "description": "Define the database schema and data model for agent interactions, supporting messaging, delegation, event publication, and state synchronization.",
            "dependencies": [],
            "details": "Create or update the crewai_agent_interactions table to capture direct/broadcast messages, event logs, delegation records, and state changes. Ensure extensibility for future collaboration features.",
            "status": "done",
            "testStrategy": "Review schema against requirements; validate with sample interaction records.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Direct and Broadcast Messaging Logic",
            "description": "Develop backend logic for agents to send direct and broadcast messages to other agents within a crew.",
            "dependencies": [
              1
            ],
            "details": "Implement API endpoints and internal functions for direct (agent-to-agent) and broadcast (agent-to-crew) messaging. Store messages in the interaction table and trigger notifications as needed.",
            "status": "done",
            "testStrategy": "Unit test message delivery, verify correct routing and storage for both direct and broadcast cases.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Develop Event Publication Mechanism",
            "description": "Enable agents to publish events (e.g., task completion, delegation, errors) for workflow coordination and monitoring.",
            "dependencies": [
              1
            ],
            "details": "Implement event publishing logic, allowing agents to emit structured events to the crewai_agent_interactions table. Support event subscription and notification for relevant agents.",
            "status": "done",
            "testStrategy": "Simulate event publication and subscription; verify event propagation and logging.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement State Synchronization Across Agents",
            "description": "Ensure agents maintain consistent shared state during collaborative workflows, including task progress and result sharing.",
            "dependencies": [
              1
            ],
            "details": "Design and implement mechanisms for agents to synchronize state changes (e.g., task status, shared data) via the interaction table or dedicated state sync service. Handle concurrent updates and conflict scenarios.",
            "status": "done",
            "testStrategy": "Test state updates under concurrent agent actions; verify consistency and conflict handling.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Integrate Conflict Resolution Logic",
            "description": "Develop logic for detecting and resolving conflicts between agents, such as task assignment disputes or inconsistent states.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Implement automated and/or human-in-the-loop conflict resolution workflows. Log conflict events, trigger resolution protocols, and update agent states accordingly.",
            "status": "done",
            "testStrategy": "Simulate conflict scenarios; verify detection, resolution, and state updates.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Simulate and Test Collaborative Workflow Scenarios",
            "description": "Create and execute end-to-end workflow simulations to validate inter-agent messaging, delegation, event handling, state sync, and conflict resolution.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "Design test scenarios covering typical and edge-case collaborative workflows. Automate simulation runs and verify expected outcomes in the interaction table and agent states.",
            "status": "done",
            "testStrategy": "Run integration tests for full workflows; check messaging, event logs, state consistency, and conflict resolution.",
            "parentId": "undefined"
          }
        ],
        "complexity": 8.5,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down inter-agent messaging and collaboration into subtasks for designing the interaction schema, implementing direct/broadcast messaging, event publication, state synchronization, conflict resolution, and workflow simulation testing."
      },
      {
        "id": 40,
        "title": "CrewAI Asset Storage & Retrieval",
        "description": "Store generated assets in crewai_generated_assets table and B2, enable retrieval by department, type, and confidence.",
        "details": "Implement asset storage logic, organize B2 folders, and support asset retrieval APIs. Track confidence scores and metadata.",
        "testStrategy": "Generate and retrieve assets, verify storage, organization, and retrieval accuracy.",
        "priority": "high",
        "dependencies": [
          "39"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Asset Storage Logic in crewai_generated_assets Table and B2",
            "description": "Design and implement the logic to store generated assets in the crewai_generated_assets database table and organize them in B2 cloud storage.",
            "dependencies": [],
            "details": "Define the schema for asset metadata, including department, type, and confidence score. Integrate asset generation outputs with the database and B2 storage. Ensure assets are stored in organized B2 folders based on department and type, and metadata is consistently tracked in the database.\n<info added on 2025-11-13T20:40:40.237Z>\nIMPLEMENTATION COMPLETE (2025-01-13):\n\n✅ Database Schema:\n- crewai_generated_assets table created in Supabase production\n- All columns implemented: id, execution_id, document_id, department, asset_type, asset_name, content, content_format, b2_path, file_size, mime_type, metadata, confidence_score, created_at\n- Foreign keys configured: execution_id → crewai_executions, document_id → documents\n\n✅ Service Implementation:\n- app/services/crewai_asset_service.py (324 lines)\n- store_asset() method handles both text-based and file-based assets\n- Text assets: stored in DB content column\n- File assets: uploaded to B2, b2_path stored in DB\n- B2 folder organization: crewai/assets/{department}/{asset_type}/{execution_id}/{filename}\n\n✅ Pydantic Models:\n- app/models/crewai_asset.py (173 lines)\n- AssetStorageRequest, AssetResponse, AssetUpdateRequest, AssetListResponse, AssetRetrievalFilters\n- Enums: AssetType, Department, ContentFormat\n\nStatus: READY FOR TESTING\n</info added on 2025-11-13T20:40:40.237Z>",
            "status": "done",
            "testStrategy": "Create sample assets, store them, and verify correct database entries and B2 folder organization.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop Asset Retrieval APIs by Department, Type, and Confidence",
            "description": "Build APIs to enable retrieval of stored assets filtered by department, asset type, and confidence score.",
            "dependencies": [
              1
            ],
            "details": "Design RESTful endpoints for asset retrieval. Implement query logic to filter assets using department, type, and confidence score from the crewai_generated_assets table and B2 storage. Ensure efficient and secure access to asset files and metadata.\n<info added on 2025-11-13T20:41:13.454Z>\nIMPLEMENTATION COMPLETE (2025-01-13):\n\n✅ API Routes Implemented:\n- app/routes/crewai_assets.py (284 lines)\n- Router prefix: /api/crewai/assets\n- Tags: [\"CrewAI Assets\"]\n\n✅ Endpoints:\n1. POST /api/crewai/assets/ - Store asset (text or file-based)\n2. GET /api/crewai/assets/ - Retrieve with filters (department, asset_type, confidence, pagination)\n3. GET /api/crewai/assets/{asset_id} - Get single asset by ID\n4. PATCH /api/crewai/assets/{asset_id} - Update confidence score and metadata\n5. GET /api/crewai/assets/execution/{execution_id} - Get all assets for execution\n\n✅ Filter Implementation:\n- execution_id (UUID)\n- department (enum: marketing, legal, hr, finance, etc.)\n- asset_type (enum: summary, analysis, report, etc.)\n- min_confidence / max_confidence (0-1)\n- limit (max 1000)\n- offset (pagination)\n\n✅ Service Integration:\n- Uses CrewAIAssetService via dependency injection\n- Full error handling (400, 404, 500)\n- Logging for all operations\n\nStatus: IMPLEMENTATION COMPLETE - Need to verify route registration in main.py\n</info added on 2025-11-13T20:41:13.454Z>",
            "status": "done",
            "testStrategy": "Test API endpoints with various filter combinations and validate that correct assets and metadata are returned.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Track and Update Asset Confidence Scores and Metadata",
            "description": "Implement mechanisms to track, update, and manage confidence scores and metadata for each asset throughout its lifecycle.",
            "dependencies": [
              1
            ],
            "details": "Add logic to update confidence scores and metadata in the crewai_generated_assets table as assets are processed or reviewed. Ensure changes are reflected in both the database and B2 storage organization if relevant. Provide audit trails for metadata updates.\n<info added on 2025-11-13T20:41:18.472Z>\nIMPLEMENTATION COMPLETE (2025-01-13):\n\n✅ Confidence Score Tracking:\n- Database column: confidence_score (float, nullable)\n- Validation: 0.0 - 1.0 range enforced in Pydantic models\n- Initial score set during asset creation\n- Update via PATCH /api/crewai/assets/{asset_id}\n\n✅ Metadata Management:\n- Database column: metadata (JSONB, default {})\n- Stored in Supabase as structured JSON\n- Full flexibility for custom metadata fields\n- MERGE behavior: new metadata merged with existing (preserves existing keys)\n- Update via AssetUpdateRequest model\n\n✅ Update Method (app/services/crewai_asset_service.py):\n- update_asset(asset_id, update_request)\n- Fetches existing asset\n- Merges metadata: {**existing.metadata, **update.metadata}\n- Updates confidence_score if provided\n- Returns updated AssetResponse\n\n✅ API Endpoint:\n- PATCH /api/crewai/assets/{asset_id}\n- Request: {confidence_score?: float, metadata?: dict}\n- Response: Updated AssetResponse\n- Errors: 404 (not found), 400 (invalid), 500 (server error)\n\nStatus: READY FOR TESTING\n</info added on 2025-11-13T20:41:18.472Z>",
            "status": "done",
            "testStrategy": "Simulate asset review and update workflows, verify that confidence scores and metadata are correctly updated and tracked.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on crewai asset storage & retrieval."
      },
      {
        "id": 41,
        "title": "Security Hardening & Compliance",
        "description": "Implement JWT authentication, RBAC, encrypted storage, input validation, and compliance features (GDPR, HIPAA, SOC 2).",
        "details": "Use PyJWT for authentication, enforce RBAC, encrypt Supabase volumes and B2 files, validate inputs, and implement audit trails. Support data export/deletion for GDPR.",
        "testStrategy": "Run security tests, penetration testing, and compliance checks.",
        "priority": "high",
        "dependencies": [
          "40"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement JWT Authentication with PyJWT",
            "description": "Set up secure JWT authentication using PyJWT, ensuring best practices for token issuance, validation, and storage.",
            "dependencies": [],
            "details": "Configure PyJWT to use strong signing algorithms (e.g., RS256), set short expiration times, validate all claims (issuer, audience, expiration), and store tokens securely (prefer HttpOnly cookies). Avoid storing sensitive data in JWTs and ensure all token transmission uses HTTPS.\n<info added on 2025-11-14T19:10:23.776Z>\n## Current Status\nJWT authentication implemented via Clerk integration (app/middleware/clerk_auth.py) with session token verification working.\n\n## Required Security Enhancements\n1. Add rate limiting to authentication endpoints using slowapi library\n2. Implement token refresh endpoint with refresh token rotation\n3. Add session timeout middleware with both idle and absolute timeout enforcement\n4. Ensure HTTPS-only transmission in production environment\n\n## Implementation Files\n- Existing: app/middleware/clerk_auth.py (JWT verification)\n- Existing: app/middleware/auth.py (JWT/API key validation)\n- New: app/middleware/rate_limit.py (for API rate limiting)\n- New: app/routes/auth.py (token refresh endpoint)\n\n## Security Assessment\nAuthentication foundation is solid. Focus should be on hardening through rate limiting and robust session management.\n</info added on 2025-11-14T19:10:23.776Z>\n<info added on 2025-11-14T19:38:41.247Z>\n## Implementation Complete\n\nSecurity hardening implementation for JWT authentication has been successfully completed with the following components:\n\n### Rate Limiting\n- Implemented using slowapi>=0.1.9\n- Created app/middleware/rate_limit.py with tiered limits:\n  - Auth endpoints: 5 login attempts/minute, 3 registrations/hour\n  - API key management: 10 creates/hour, 20 revocations/minute\n  - File uploads: 50/hour for single, 10/hour for bulk\n  - Query endpoints: 100/minute for simple, 20/minute for complex\n- Uses Redis in production, in-memory storage in development\n- Per-user and per-IP rate limiting with proper headers\n\n### Security Headers Middleware\n- Created app/middleware/security.py with SecurityHeadersMiddleware\n- Implemented headers: HSTS, X-Content-Type-Options, X-Frame-Options, X-XSS-Protection, Referrer-Policy, Permissions-Policy, and Content-Security-Policy\n- Environment-specific configurations with relaxed settings for documentation endpoints\n\n### CORS Hardening\n- Updated configuration in app/main.py with explicit HTTP methods\n- Environment-based configuration with production warnings\n\n### Testing\n- Created comprehensive test_task41_security.py (320 lines)\n- Tests for headers, rate limiting, CORS, and overall API health\n\n### Files Modified/Created\n- requirements.txt: Added slowapi and redis\n- app/middleware/security.py: NEW (180 lines)\n- app/middleware/rate_limit.py: NEW (260 lines)\n- app/main.py: MODIFIED\n- test_task41_security.py: NEW (320 lines)\n\n### Security Improvements\n- Protection against brute force, clickjacking, MIME sniffing, XSS\n- HTTPS enforcement in production\n- Information disclosure prevention\n- DoS protection through rate limiting\n</info added on 2025-11-14T19:38:41.247Z>",
            "status": "done",
            "testStrategy": "Unit test token issuance and validation, attempt token tampering, and verify rejection of invalid or expired tokens.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Enforce Role-Based Access Control (RBAC)",
            "description": "Integrate RBAC to restrict access to resources based on user roles and permissions.",
            "dependencies": [
              1
            ],
            "details": "Design a roles and permissions schema. Implement middleware to check user roles (from identity, not from JWT claims) on each protected endpoint. Ensure permissions are managed in the authorization layer, not embedded in JWTs.\n<info added on 2025-11-14T19:10:28.200Z>\n## Current Status\nRBAC fully implemented with 4 roles (admin, editor, viewer, guest). Complete lifecycle management in app/services/rbac_service.py. Database tables exist (users, roles, user_roles, api_keys).\n\n## Implementation Details\n- Role permission checking: app/middleware/auth.py:require_admin(), require_role()\n- API key lifecycle: generation, rotation, revocation, expiration\n- Bcrypt hashing for API keys (never stores plaintext)\n- Database schema ready in Supabase\n\n## Additional Work Needed\n- Row-Level Security (RLS) policies on all user-facing tables (CRITICAL)\n- API key scope validation (scopes field exists but not enforced)\n- Permission cache invalidation for role updates\n\n## Focus Areas\n1. Design and implement PostgreSQL RLS policies for data isolation\n2. Add scope validation middleware for API keys\n3. Test user data isolation at database level\n\n## Security Assessment\nAuthorization system is production-ready. Main gap is RLS enforcement.\n</info added on 2025-11-14T19:10:28.200Z>",
            "status": "done",
            "testStrategy": "Test endpoints with users of different roles, verify access is correctly granted or denied according to RBAC rules.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Set Up Encrypted Storage for Supabase and B2 Files",
            "description": "Encrypt all data at rest in Supabase volumes and Backblaze B2 file storage.",
            "dependencies": [],
            "details": "Enable encryption for Supabase storage volumes and configure server-side encryption for B2 buckets. Ensure encryption keys are securely managed and rotated according to policy.\n<info added on 2025-11-14T19:10:32.282Z>\n## Current Status\nFile encryption implementation is EXCELLENT with AES-256-GCM in app/services/encryption.py featuring:\n- 256-bit keys with PBKDF2 (100k iterations)\n- Random salts and nonces per file\n- Authenticated encryption with GCM mode\n- B2 integration ready\n- Test coverage in tests/test_encryption.py\n\n## Additional Work Needed\n- Verify Supabase encryption-at-rest is enabled\n- Confirm TLS for all database connections (Neo4j already using TLS with bolt+ssc://localhost:7687)\n- Add key rotation policies\n- Optional: Integrate with AWS KMS or HashiCorp Vault for key management\n\n## Verification Tasks\n1. Check Supabase project settings for encryption-at-rest\n2. Verify B2 server-side encryption configuration\n3. Document encryption key management procedures\n4. Test file encryption/decryption with B2 upload\n\n## Security Assessment\nEncryption implementation is production-grade. Focus should be on verification and key management procedures.\n</info added on 2025-11-14T19:10:32.282Z>",
            "status": "done",
            "testStrategy": "Verify files and database volumes are encrypted at rest, attempt unauthorized access to raw storage, and confirm data is unreadable without decryption keys.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Comprehensive Input Validation",
            "description": "Validate all user and API inputs to prevent injection and data integrity issues.",
            "dependencies": [],
            "details": "Apply strict input validation on all endpoints using whitelisting and schema validation. Sanitize inputs to prevent SQL injection, XSS, and other common attacks. Use libraries for validation where possible.\n<info added on 2025-11-14T19:10:37.398Z>\n## Current Status\nInput validation implementation is GOOD. Pydantic models are used throughout the codebase (7 model files) with:\n- Type hints and Field() constraints\n- File upload validation (whitelist, 100MB limit, 10 files max)\n- Email validation with EmailStr\n- Custom validators for key fields\n\n## Files With Validation\n- app/models/rbac.py (RBAC validation)\n- app/models/documents.py (document validation)\n- app/models/users.py (user validation)\n- app/api/upload.py (file upload validation)\n\n## Additional Work Needed\n- Request body size limits middleware (prevent DoS)\n- Custom validators for SQL injection prevention\n- Path traversal validation (no ../, null bytes)\n- XSS prevention in metadata fields\n- Rate limiting on all API endpoints\n\n## Hardening Tasks\n1. Add max_body_size middleware\n2. Create security validators for:\n   - Document paths\n   - Query parameters\n   - Metadata values\n3. Audit all database queries for parameterization\n4. Add input sanitization for user-generated content\n\n## Security Assessment\nValidation foundation is solid. Need additional hardening for edge cases.\n</info added on 2025-11-14T19:10:37.398Z>",
            "status": "done",
            "testStrategy": "Fuzz endpoints with invalid and malicious inputs, verify that invalid data is rejected and no vulnerabilities are introduced.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Audit Trail and Logging",
            "description": "Create an audit trail system to log security-relevant events and user actions for compliance and forensic analysis.",
            "dependencies": [
              1,
              2
            ],
            "details": "Log authentication events, access control decisions, data exports/deletions, and administrative actions. Ensure logs are tamper-evident and securely stored. Provide tools for querying and exporting audit logs.\n<info added on 2025-11-14T19:10:44.198Z>\n## Current Status\nAudit logging partially implemented with structlog throughout the application. AuditLogEntry model defined in app/models/rbac.py with all required fields (event_type, actor, target, IP, user_agent, metadata).\n\n## Events Currently Logged\n- Authentication attempts (success/failure)\n- API key creation/rotation/revocation\n- Role assignments\n- Access denials\n\n## Critical Gap\nLogs are only stored in application logs, not persisted to database, preventing querying for compliance or incident investigation purposes.\n\n## Implementation Plan\n1. Create audit_logs table in Supabase with AuditLogEntry schema\n2. Create app/middleware/audit.py to persist all security events\n3. Add audit log query/search endpoints in app/routes/audit.py\n4. Implement log retention policies (90 days active, 7 years archive)\n5. Extract IP address and User-Agent from requests\n6. Make logs tamper-evident (append-only, signed)\n\n## Database Schema\n- Table: audit_logs\n- Columns: id, event_type, actor_user_id, target_user_id, target_resource_type, target_resource_id, action, result, ip_address, user_agent, metadata (JSONB), error_message, created_at\n\n## Security Assessment\nFoundation exists but high priority to persist logs to database for compliance and security investigation capabilities.\n</info added on 2025-11-14T19:10:44.198Z>",
            "status": "done",
            "testStrategy": "Trigger various security events, verify logs are generated, immutable, and contain all required information.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Integrate Compliance Features (GDPR, HIPAA, SOC 2)",
            "description": "Implement features to meet GDPR, HIPAA, and SOC 2 requirements, including data export/deletion and privacy controls.",
            "dependencies": [
              3,
              4,
              5
            ],
            "details": "Support user data export and deletion (GDPR), ensure auditability and access controls (SOC 2), and implement privacy and security safeguards (HIPAA). Document compliance measures and provide user interfaces for data requests.\n<info added on 2025-11-14T19:10:53.454Z>\n## CURRENT STATUS\n- GDPR models exist in app/models/users.py (UserDataExport, GDPRDeleteResponse) but implementation needs verification.\n\n## COMPLIANCE REQUIREMENTS\n- GDPR: User data export, complete deletion, consent tracking, data retention\n- HIPAA: Encryption (✅), access controls (✅), audit trails (⚠️ needs DB persistence)\n- SOC 2: Auditability (⚠️), access controls (✅), security monitoring\n\n## VERIFICATION TASKS\n1. Test GDPR data export endpoint - verify all user PII is included\n2. Test GDPR deletion endpoint - verify complete removal from all tables\n3. Document data retention policies\n4. Add consent tracking for data processing\n5. Create user-facing data request interface\n\n## COMPLIANCE FEATURES TO IMPLEMENT\n- Data export: JSON download of all user data\n- Right to deletion: Remove all PII from documents, embeddings, graphs\n- Data portability: Export in machine-readable format\n- Privacy controls: User-configurable data retention\n- Breach notification: Automated alerts for security incidents\n\n## SOC 2 REQUIREMENTS\n- Access control documentation (✅ via RBAC)\n- Audit trail persistence (⚠️ task 41.5)\n- Security monitoring dashboards\n- Incident response procedures\n\n## HIPAA SAFEGUARDS\n- Technical safeguards: Encryption (✅), access controls (✅)\n- Physical safeguards: Document B2/Supabase security\n- Administrative safeguards: Policies and training documentation\n\n## DEPENDENCIES\nRequires audit logging (41.5) and RLS policies (41.2) to be complete first.\n</info added on 2025-11-14T19:10:53.454Z>",
            "status": "done",
            "testStrategy": "Perform compliance checks, simulate data subject requests, and verify all regulatory requirements are met.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Conduct Security and Compliance Testing",
            "description": "Perform security testing, penetration testing, and compliance verification across all implemented features.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Run automated security scans, manual penetration tests, and compliance audits. Address any vulnerabilities or compliance gaps identified. Document test results and remediation steps.\n<info added on 2025-11-14T19:11:00.118Z>\nCURRENT STATUS: No security testing suite exists yet. This is the final validation phase after all security features are implemented.\n\nTESTING PLAN:\n\n1. AUTOMATED SECURITY SCANS:\n   - OWASP ZAP for penetration testing\n   - Bandit for Python code security analysis\n   - Safety for dependency vulnerability scanning\n   - SQLMap for SQL injection testing\n\n2. MANUAL PENETRATION TESTING:\n   - Auth bypass attempts\n   - Token tampering and replay attacks\n   - RBAC privilege escalation tests\n   - Input fuzzing (SQL injection, XSS, path traversal)\n   - Rate limit bypass attempts\n   - CORS misconfiguration exploits\n\n3. COMPLIANCE VERIFICATION:\n   - GDPR data export/deletion validation\n   - HIPAA audit trail completeness\n   - SOC 2 access control verification\n   - Encryption verification (at-rest, in-transit)\n\n4. SECURITY TEST SUITE:\n   - Create tests/security/ directory\n   - Write pytest tests for:\n     - Authentication flows\n     - RBAC enforcement\n     - Input validation edge cases\n     - Audit log persistence\n     - Rate limiting\n     - Session management\n\n5. DOCUMENTATION:\n   - Security architecture document\n   - Threat model and mitigations\n   - Incident response playbook\n   - Compliance certification evidence\n\nDEPENDENCIES: All subtasks 41.1-41.6 must be complete before testing can begin.\n\nDELIVERABLES:\n- Security test report with findings\n- Remediation plan for any issues\n- Compliance certification readiness assessment\n</info added on 2025-11-14T19:11:00.118Z>",
            "status": "done",
            "testStrategy": "Review test reports, verify all critical issues are resolved, and confirm compliance with GDPR, HIPAA, and SOC 2.",
            "parentId": "undefined"
          }
        ],
        "complexity": 9,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Decompose security hardening and compliance into subtasks for JWT authentication, RBAC enforcement, encrypted storage setup, input validation, audit trail implementation, compliance feature integration (GDPR, HIPAA, SOC 2), and security/compliance testing."
      },
      {
        "id": 42,
        "title": "Reliability & Disaster Recovery Implementation",
        "description": "Set up automated backups, health checks, auto-restart, and disaster recovery procedures.",
        "details": "Configure daily B2 backups, implement health endpoints, auto-restart on failure, and document disaster recovery drills. Use Infrastructure as Code (Terraform/Ansible) for fast rebuild.",
        "testStrategy": "Simulate failures, verify backup/restore, health checks, and recovery procedures.",
        "priority": "high",
        "dependencies": [
          "41"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Automated Backups and Restore Procedures",
            "description": "Set up daily automated B2 backups and validate restore processes to ensure data durability and rapid recovery.",
            "dependencies": [],
            "details": "Configure daily automated backups to Backblaze B2 using Infrastructure as Code (Terraform/Ansible). Regularly test backup integrity and perform restore drills to verify data can be recovered quickly and accurately. Document backup schedules, retention policies, and restoration steps.",
            "status": "done",
            "testStrategy": "Simulate data loss scenarios and perform full and partial restores from backups to verify data integrity and recovery time objectives.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Deploy Health Checks and Auto-Restart Mechanisms",
            "description": "Implement health endpoints and configure automated service restarts on failure to maintain high availability.",
            "dependencies": [
              1
            ],
            "details": "Develop and expose health check endpoints for all critical services. Integrate monitoring tools to continuously check service health. Configure auto-restart policies (e.g., systemd, Kubernetes liveness probes) to automatically recover failed services. Ensure monitoring alerts are in place for failed health checks and restarts.",
            "status": "done",
            "testStrategy": "Induce service failures and verify that health checks detect issues and auto-restart mechanisms restore service availability without manual intervention.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Document and Test Disaster Recovery Procedures",
            "description": "Create, document, and regularly test disaster recovery (DR) drills to ensure readiness for major outages.",
            "dependencies": [
              1,
              2
            ],
            "details": "Develop comprehensive disaster recovery documentation covering failover, rebuild, and recovery steps using Infrastructure as Code. Schedule and execute regular DR drills simulating various failure scenarios (e.g., region outage, data corruption). Update documentation based on drill outcomes and lessons learned.",
            "status": "done",
            "testStrategy": "Conduct scheduled disaster recovery drills, measure recovery time and data loss, and review documentation for completeness and clarity after each drill.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on reliability & disaster recovery implementation."
      },
      {
        "id": 43,
        "title": "Load Testing & Performance Optimization",
        "description": "Conduct load testing for document processing, query execution, and WebSocket connections. Optimize for throughput and latency.",
        "details": "Use locust or k6 for load testing. Profile bottlenecks, optimize Celery worker scaling, database indexes, and caching. Tune API and WebSocket performance.",
        "testStrategy": "Run load tests at 2x expected traffic, verify performance metrics and optimize as needed.",
        "priority": "high",
        "dependencies": [
          "42"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Execute Load Testing Scenarios for Document Processing, Query Execution, and WebSocket Connections",
            "description": "Develop and run comprehensive load tests targeting document processing, query execution, and WebSocket endpoints using tools like Locust or k6.",
            "dependencies": [],
            "details": "Identify key user flows and endpoints for document processing, query execution, and WebSocket communication. Create load test scripts in Locust (Python) or k6 (JavaScript), simulating realistic traffic patterns and scaling up to at least 2x expected peak load. Collect baseline metrics for throughput, latency, and error rates.\n<info added on 2025-11-16T19:08:05.314Z>\nAuthentication setup for load testing completed:\n- Fixed bug in app/middleware/clerk_auth.py by replacing non-existent sessions.verify_token() with proper JWT verification\n- Implemented JWT verification using jwt.decode() with CLERK_SECRET_KEY\n- Created generate_test_token.py script to generate valid JWT tokens for load testing\n- Changes committed (6a67a3b) and pushed to main branch\n- System is now ready for authentication-enabled load testing scenarios\n</info added on 2025-11-16T19:08:05.314Z>",
            "status": "done",
            "testStrategy": "Verify that load tests execute as intended, generate reproducible results, and cover all critical workflows. Ensure metrics are collected for throughput, latency, and error rates under varying load conditions.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Profile System Performance and Identify Bottlenecks",
            "description": "Analyze system performance under load to pinpoint bottlenecks in Celery worker scaling, database indexing, caching, and API/WebSocket layers.",
            "dependencies": [
              1
            ],
            "details": "Use profiling tools and application logs to monitor CPU, memory, database query times, and network utilization during load tests. Focus on Celery worker queues, database slow queries, cache hit/miss ratios, and WebSocket throughput. Document all identified bottlenecks with supporting metrics.",
            "status": "done",
            "testStrategy": "Correlate load test results with profiling data to confirm bottleneck locations. Validate findings by reproducing issues under controlled load and measuring impact of each suspected bottleneck.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Optimize and Re-Test for Throughput and Latency Improvements",
            "description": "Implement targeted optimizations (e.g., Celery scaling, database indexes, caching strategies, API/WebSocket tuning) and validate improvements through iterative load testing.",
            "dependencies": [
              2
            ],
            "details": "Apply optimizations based on profiling results: adjust Celery worker counts, add or tune database indexes, refine caching logic, and optimize API/WebSocket configurations. Re-run load tests to measure improvements in throughput and latency. Iterate as needed until performance targets are met.\n<info added on 2025-11-17T00:16:21.618Z>\n## Progress Update (75% Complete)\n\n### Accomplishments\n1. Created comprehensive load testing framework (query_load_test.py)\n2. Identified and fixed 4 critical bugs:\n   - Langfuse decorator async bug (5e8c9c1) - adaptive endpoint 0% → 100% success\n   - Pydantic cache serialization (f2707f5) - enabled cache infrastructure\n   - LangGraph ToolNode error (7e0972f) - fixed 33% failure rate on complex queries\n   - Redis connection for Upstash (7e0972f) - SSL/TLS support for production\n3. Generated comprehensive documentation:\n   - PERFORMANCE_REPORT_TASK43_3.md (8 sections)\n   - TASK43_3_FINAL_STATUS.md (complete status)\n   - 3 JSON test result files\n4. Re-tested and validated all bug fixes in production\n5. Profiled performance bottlenecks and documented optimizations\n\n### Current Performance Metrics\n- Adaptive endpoint: 100% success (was 0%)\n- Auto-routed endpoint: 100% success\n- Cache hit rate: 0% (embedding service unavailable)\n- Adaptive P95 latency: 14.6s (target: <1s)\n- Auto-routed P95 latency: 7.1s (target: <2s)\n\n### Outstanding Issues\n1. Caching not functional - BGE-M3 via Ollama unavailable from Render\n   - Need OpenAI embeddings fallback\n   - Verify Redis connection in logs\n2. Performance too slow - sequential LLM calls taking 12-14s\n   - Need to combine analyze+plan into single call\n   - Add streaming responses\n   - Implement prompt caching\n\n### Next Steps\n- Add OpenAI embeddings fallback for production caching\n- Optimize LangGraph workflow (combine nodes, add streaming)\n- Final validation with working cache and optimized performance\n- Estimated: 2-4 hours to complete remaining 25%\n</info added on 2025-11-17T00:16:21.618Z>",
            "status": "done",
            "testStrategy": "Compare pre- and post-optimization metrics for throughput, latency, and error rates. Confirm that optimizations resolve identified bottlenecks and that the system meets or exceeds performance goals under 2x expected load.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on load testing & performance optimization."
      },
      {
        "id": 44,
        "title": "Documentation Finalization & User Onboarding",
        "description": "Prepare comprehensive documentation for developers and users. Implement onboarding flows and training materials.",
        "details": "Document API endpoints, workflows, agent configurations, and UI usage. Create onboarding guides and training videos.",
        "testStrategy": "Review documentation for completeness and clarity. Test onboarding flows with new users.",
        "priority": "medium",
        "dependencies": [
          "43"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Comprehensive API & Workflow Documentation",
            "description": "Create detailed, accurate documentation covering all API endpoints, workflows, agent configurations, and UI usage for both developers and end-users.",
            "dependencies": [
              43
            ],
            "details": "Document each API endpoint with request/response examples, authentication details, and error codes. Outline workflows with diagrams and step-by-step instructions. Describe agent configuration options and UI navigation paths. Use clear headings, code samples, and visuals to enhance readability and accessibility[1][2]. Ensure documentation is reviewed by technical stakeholders for accuracy before finalization.",
            "status": "done",
            "testStrategy": "Conduct peer reviews with developers and QA to verify completeness, clarity, and technical accuracy. Test documented workflows against the live system to ensure they match actual behavior.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Onboarding Guide & Training Material Development",
            "description": "Develop onboarding guides and training materials tailored to different user roles, including step-by-step tutorials, FAQs, and best practices.",
            "dependencies": [
              43
            ],
            "details": "Write onboarding guides for new users and developers, focusing on getting started, common tasks, and troubleshooting. Create training videos (e.g., using Loom or similar tools) demonstrating key features and workflows. Include exercises and real-world examples to reinforce learning. Structure content for easy navigation and quick reference, using consistent formatting and visual aids[1][2]. Collaborate with support and training teams to ensure materials address common user pain points.",
            "status": "done",
            "testStrategy": "Pilot onboarding materials with a group of new users and gather feedback on clarity, usefulness, and ease of understanding. Revise materials based on feedback before broad release.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Documentation Maintenance & Continuous Improvement Plan",
            "description": "Establish processes for ongoing documentation review, updates, and user feedback integration to keep materials accurate and relevant.",
            "dependencies": [
              43
            ],
            "details": "Set up a schedule for regular documentation reviews, especially after product updates or releases. Implement a feedback loop where users can report issues or suggest improvements. Use version control to track changes and ensure all stakeholders have access to the latest documentation. Standardize templates and update procedures to maintain consistency across all docs[2][4]. Assign clear ownership for documentation maintenance within the team.",
            "status": "done",
            "testStrategy": "Monitor documentation usage analytics and user feedback channels. Periodically audit docs for outdated information and verify that updates are correctly propagated. Test revised documentation with both new and experienced users to ensure continued effectiveness.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on documentation finalization & user onboarding."
      },
      {
        "id": 45,
        "title": "Integrate RAGAS Metrics Evaluation and Visualization for RAG Pipeline",
        "description": "Implement automated RAG quality evaluation using the RAGAS framework with 4 core metrics (Faithfulness, Answer Relevancy, Context Precision, Context Recall). Store results in Supabase and visualize in Grafana dashboards.",
        "details": "- Set up RAGAS framework integration for automated evaluation of RAG pipeline quality\n- Implement evaluation of 4 core metrics:\n  * Faithfulness (0-1): Measures if the generated answer is factually consistent with the retrieved context\n  * Answer Relevancy (0-1): Measures if the answer addresses the query intent\n  * Context Precision (0-1): Measures the proportion of relevant context chunks\n  * Context Recall (0-1): Measures if all necessary information is present in context\n- Create a test dataset of 30 curated samples from Empire documentation stored in .taskmaster/docs/ragas_test_dataset.json\n- Design and implement Supabase ragas_evaluations table with schema including:\n  * evaluation_id (UUID)\n  * timestamp (TIMESTAMP)\n  * query_text (TEXT)\n  * answer_text (TEXT)\n  * context_chunks (JSONB array)\n  * faithfulness_score (FLOAT)\n  * answer_relevancy_score (FLOAT)\n  * context_precision_score (FLOAT)\n  * context_recall_score (FLOAT)\n  * overall_score (FLOAT)\n  * metadata (JSONB)\n- Develop scripts/ragas_evaluation.py for batch evaluation with:\n  * Command-line interface for running evaluations\n  * Integration with existing RAG pipeline components\n  * Configurable parameters for evaluation settings\n  * Automatic storage of results in Supabase\n- Create Grafana dashboards showing:\n  * Metric trends over time\n  * Comparison between different RAG configurations\n  * Alerts when metrics fall below threshold (0.70)\n  * Drill-down capability to examine specific evaluation runs\n- Document expected baseline performance (0.70-0.85 overall scores)\n- Calculate and document cost estimates (~$0.20 per evaluation run for 30 samples)\n- Integrate with existing observability infrastructure from Task 25",
        "testStrategy": "1. Prepare test environment with sample RAG pipeline and test dataset\n2. Run baseline evaluation on the 30-sample test dataset from .taskmaster/docs/ragas_test_dataset.json\n3. Validate all 4 metrics (Faithfulness, Answer Relevancy, Context Precision, Context Recall) are calculated correctly\n4. Verify scores are within expected ranges (0-1) and reasonable for test data\n5. Confirm results are properly stored in Supabase ragas_evaluations table\n6. Check that all required fields in the schema are populated correctly\n7. Verify Grafana dashboard correctly displays:\n   - Individual metric scores\n   - Overall score trends\n   - Comparison between evaluation runs\n8. Test alert triggers by artificially setting scores below the 0.70 threshold\n9. Validate dashboard filtering and drill-down capabilities\n10. Perform a complete end-to-end test with a new document to ensure the entire evaluation pipeline works\n11. Measure performance and resource usage during evaluation runs\n12. Document baseline scores for the current RAG implementation",
        "status": "done",
        "dependencies": [
          "18",
          "25"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up RAGAS framework integration and test dataset",
            "description": "Integrate the RAGAS framework into the project and prepare the test dataset for evaluation.",
            "dependencies": [],
            "details": "Install RAGAS library and dependencies. Configure the framework to work with the existing RAG pipeline. Prepare and validate the test dataset of 30 curated samples from Empire documentation stored in .taskmaster/docs/ragas_test_dataset.json. Ensure the dataset contains appropriate query-answer-context triplets for evaluation.",
            "status": "done",
            "testStrategy": "Verify RAGAS installation and imports work correctly. Validate test dataset structure and content. Ensure sample queries cover diverse use cases from Empire documentation.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement core metrics evaluation logic",
            "description": "Develop the core functionality to evaluate the 4 RAGAS metrics: Faithfulness, Answer Relevancy, Context Precision, and Context Recall.",
            "dependencies": [
              1
            ],
            "details": "Create evaluation functions for each metric. Implement Faithfulness calculation to measure factual consistency between answers and context. Develop Answer Relevancy evaluation to assess query intent alignment. Build Context Precision measurement for relevant chunk proportion. Implement Context Recall to verify information completeness. Calculate overall combined score from individual metrics.",
            "status": "done",
            "testStrategy": "Run evaluations on sample data and verify each metric produces values between 0-1. Compare results with manual assessments of a subset of examples to validate accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create Supabase ragas_evaluations table and storage logic",
            "description": "Design and implement the Supabase database schema for storing RAGAS evaluation results and develop storage functionality.",
            "dependencies": [
              2
            ],
            "details": "Create ragas_evaluations table with schema including evaluation_id, timestamp, query_text, answer_text, context_chunks, all metric scores (faithfulness, answer_relevancy, context_precision, context_recall), overall_score, and metadata fields. Implement functions to store evaluation results in the database. Add batch processing capabilities for multiple evaluations.",
            "status": "done",
            "testStrategy": "Test database schema creation and data insertion. Verify all fields are properly stored and retrieved. Check batch processing with multiple evaluation records.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Develop scripts/ragas_evaluation.py with CLI interface",
            "description": "Create a command-line script for running RAGAS evaluations with configurable parameters and Supabase integration.",
            "dependencies": [
              3
            ],
            "details": "Develop scripts/ragas_evaluation.py with command-line arguments for evaluation settings. Integrate with existing RAG pipeline components to access retrieval and generation functions. Implement configurable parameters for batch size, metric weights, and thresholds. Add automatic storage of results in Supabase. Include logging and error handling. Document cost estimates (~$0.20 per evaluation run for 30 samples).",
            "status": "done",
            "testStrategy": "Test CLI with various parameter combinations. Verify integration with RAG pipeline components. Confirm results are properly stored in Supabase. Validate error handling for edge cases.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create Grafana dashboards for metrics visualization",
            "description": "Design and implement Grafana dashboards to visualize RAGAS metrics and integrate with existing observability infrastructure.",
            "dependencies": [
              4
            ],
            "details": "Create Grafana dashboards showing metric trends over time. Implement comparison views between different RAG configurations. Set up alerts when metrics fall below threshold (0.70). Add drill-down capability to examine specific evaluation runs. Document expected baseline performance (0.70-0.85 overall scores). Integrate with existing observability infrastructure from Task 25.",
            "status": "done",
            "testStrategy": "Verify dashboard displays all metrics correctly. Test alert functionality with below-threshold values. Confirm drill-down navigation works properly. Validate integration with existing observability infrastructure.",
            "parentId": "undefined"
          }
        ]
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-11-14T18:18:43.347Z",
      "taskCount": 45,
      "completedCount": 39,
      "tags": [
        "master"
      ],
      "created": "2025-11-14T19:10:19.748Z",
      "description": "Tasks for master context",
      "updated": "2025-11-17T18:28:30.661Z"
    }
  },
  "v7_3_features": {
    "tasks": [
      {
        "id": 1,
        "title": "Establish OpenAPI Contracts for All Features",
        "description": "Create OpenAPI 3.0 YAML specifications for each of the 9 features, defining endpoints, request/response schemas, error codes, and examples.",
        "details": "Use Swagger Editor or Stoplight Studio to author OpenAPI 3.0 YAML files for each feature. Ensure all endpoints, parameters, authentication (JWT/OAuth2), and error responses are specified. Include example payloads and document all enums and JSONB fields. Store contracts in a /openapi directory and validate with openapi-cli.",
        "testStrategy": "Validate YAML files using openapi-cli linting and ensure all endpoints are covered. Review with backend and frontend leads for completeness.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Draft OpenAPI YAML for R&D Department Feature",
            "description": "Create an OpenAPI 3.0 YAML specification for the R&D Department feature, covering endpoints, request/response schemas, authentication, error codes, and examples.",
            "dependencies": [],
            "details": "Use Swagger Editor or Stoplight Studio to author the YAML file. Define endpoints for R&D department management, specify JWT/OAuth2 authentication, enumerate all parameters, and document enums and JSONB fields. Include example payloads and error responses. Store the file in /openapi/rd_department.yaml.",
            "status": "done",
            "testStrategy": "Validate YAML with openapi-cli and review with backend lead for completeness."
          },
          {
            "id": 2,
            "title": "Draft OpenAPI YAML for Loading Status Feature",
            "description": "Create an OpenAPI 3.0 YAML specification for the Loading Status feature, detailing endpoints, schemas, authentication, error codes, and examples.",
            "dependencies": [],
            "details": "Author the YAML file using Swagger Editor. Define endpoints for querying and updating loading status, specify required parameters, authentication, and error responses. Document enums and JSONB fields. Include example payloads. Store in /openapi/loading_status.yaml.",
            "status": "done",
            "testStrategy": "Lint YAML with openapi-cli and verify endpoint coverage with frontend lead."
          },
          {
            "id": 3,
            "title": "Draft OpenAPI YAML for URL Upload Feature",
            "description": "Create an OpenAPI 3.0 YAML specification for the URL Upload feature, including endpoints, request/response schemas, authentication, error codes, and examples.",
            "dependencies": [],
            "details": "Use Stoplight Studio to define endpoints for uploading URLs, specify input validation, authentication, and error handling. Document all enums and JSONB fields. Provide example payloads. Save as /openapi/url_upload.yaml.",
            "status": "done",
            "testStrategy": "Validate with openapi-cli and confirm with backend team."
          },
          {
            "id": 4,
            "title": "Draft OpenAPI YAML for Source Attribution Feature",
            "description": "Create an OpenAPI 3.0 YAML specification for the Source Attribution feature, specifying endpoints, schemas, authentication, error codes, and examples.",
            "dependencies": [],
            "details": "Author YAML using Swagger Editor. Define endpoints for source attribution, document parameters, authentication, enums, and JSONB fields. Include example requests and responses. Store in /openapi/source_attribution.yaml.",
            "status": "done",
            "testStrategy": "Lint YAML and review with product owner for requirements coverage."
          },
          {
            "id": 5,
            "title": "Draft OpenAPI YAML for Agent Chat Feature",
            "description": "Create an OpenAPI 3.0 YAML specification for the Agent Chat feature, covering endpoints, request/response schemas, authentication, error codes, and examples.",
            "dependencies": [],
            "details": "Use Stoplight Studio to define chat endpoints, specify JWT/OAuth2 authentication, document all parameters, enums, and JSONB fields. Include example payloads and error responses. Save as /openapi/agent_chat.yaml.",
            "status": "done",
            "testStrategy": "Validate YAML with openapi-cli and confirm with chat service team."
          },
          {
            "id": 6,
            "title": "Draft OpenAPI YAML for Course Addition Feature",
            "description": "Create an OpenAPI 3.0 YAML specification for the Course Addition feature, detailing endpoints, schemas, authentication, error codes, and examples.",
            "dependencies": [],
            "details": "Author YAML using Swagger Editor. Define endpoints for adding courses, specify required parameters, authentication, enums, and JSONB fields. Include example payloads and error responses. Store in /openapi/course_addition.yaml.",
            "status": "done",
            "testStrategy": "Lint YAML and review with education product manager."
          },
          {
            "id": 7,
            "title": "Draft OpenAPI YAML for Chat File Upload Feature",
            "description": "Create an OpenAPI 3.0 YAML specification for the Chat File Upload feature, specifying endpoints, schemas, authentication, error codes, and examples.",
            "dependencies": [],
            "details": "Use Stoplight Studio to define endpoints for file upload in chat, specify multipart/form-data handling, authentication, enums, and JSONB fields. Include example payloads and error responses. Save as /openapi/chat_file_upload.yaml.",
            "status": "done",
            "testStrategy": "Validate YAML with openapi-cli and test multipart payloads."
          },
          {
            "id": 8,
            "title": "Draft OpenAPI YAML for Book Processing Feature",
            "description": "Create an OpenAPI 3.0 YAML specification for the Book Processing feature, covering endpoints, request/response schemas, authentication, error codes, and examples.",
            "dependencies": [],
            "details": "Author YAML using Swagger Editor. Define endpoints for book processing, specify required parameters, authentication, enums, and JSONB fields. Include example payloads and error responses. Store in /openapi/book_processing.yaml.",
            "status": "done",
            "testStrategy": "Lint YAML and review with backend and QA teams."
          },
          {
            "id": 9,
            "title": "Draft OpenAPI YAML for Agent Router Feature",
            "description": "Create an OpenAPI 3.0 YAML specification for the Agent Router feature, specifying endpoints, schemas, authentication, error codes, and examples.",
            "dependencies": [],
            "details": "Use Stoplight Studio to define endpoints for agent routing, specify authentication, document all parameters, enums, and JSONB fields. Include example payloads and error responses. Save as /openapi/agent_router.yaml.",
            "status": "done",
            "testStrategy": "Validate YAML with openapi-cli and confirm with integration team."
          }
        ]
      },
      {
        "id": 2,
        "title": "Design and Implement Database Migrations",
        "description": "Create SQL migration files for all schema changes required by v7.3 features.",
        "details": "Use Alembic (Python) or Prisma (TypeScript) for migration management. Write migrations to: (1) add 'R&D' to department enum, (2) add processing_status and source_metadata JSONB columns, (3) create agent_router_cache and agent_feedback tables, (4) create book metadata and course tables. Ensure migrations are idempotent and reversible.",
        "testStrategy": "Run migrations on a test database, verify schema changes, and perform rollback tests. Use dbmate or Alembic test commands.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add 'R&D' to Department Enum Migration",
            "description": "Create migration script to add 'R&D' as a new value to the existing department enum type in the database.",
            "dependencies": [],
            "details": "Use Alembic/Prisma to create a migration that alters the department enum type to include 'R&D' as a valid option. Ensure backward compatibility with existing code. Include proper up and down migration functions for reversibility.",
            "status": "done",
            "testStrategy": "Verify the enum contains the new value after migration. Test that existing records with other department values remain unchanged. Test rollback removes the new enum value."
          },
          {
            "id": 2,
            "title": "Add Processing Status Column Migration",
            "description": "Create migration script to add the processing_status JSONB column to the appropriate tables.",
            "dependencies": [
              1
            ],
            "details": "Identify tables that need processing status tracking (likely documents, uploads, or processing_jobs tables). Add JSONB column 'processing_status' with appropriate default value (empty JSON object). Include proper indexing for query performance.",
            "status": "done",
            "testStrategy": "Verify column exists with correct data type after migration. Test inserting and querying various processing status values. Confirm rollback removes the column."
          },
          {
            "id": 3,
            "title": "Add Source Metadata JSONB Column Migration",
            "description": "Create migration script to add the source_metadata JSONB column to store extracted document metadata.",
            "dependencies": [
              1
            ],
            "details": "Add 'source_metadata' JSONB column to the documents or content table. This will store extracted metadata like title, author, publication date, and page count. Set appropriate default value (empty JSON) and add GIN index for efficient querying of JSON properties.",
            "status": "done",
            "testStrategy": "Verify column exists with correct data type. Test storing and retrieving complex JSON metadata. Test querying by specific JSON fields. Confirm rollback removes the column."
          },
          {
            "id": 4,
            "title": "Create Agent Router Cache Table Migration",
            "description": "Create migration script to establish the agent_router_cache table for storing routing decisions.",
            "dependencies": [
              1
            ],
            "details": "Create new table 'agent_router_cache' with columns for query_hash, agent_id, confidence_score, timestamp, and any other required fields. Add appropriate indexes for query performance. Set up foreign key relationships to agents table if applicable. Include TTL mechanism for cache expiration.",
            "status": "done",
            "testStrategy": "Verify table creation with all columns and constraints. Test inserting, updating and querying cache entries. Confirm foreign key constraints work as expected. Test rollback drops the table."
          },
          {
            "id": 5,
            "title": "Create Agent Feedback Table Migration",
            "description": "Create migration script to establish the agent_feedback table for storing user feedback on agent responses.",
            "dependencies": [
              1
            ],
            "details": "Create new table 'agent_feedback' with columns for feedback_id, agent_id, user_id, query_id, rating, feedback_text, timestamp, and any other required fields. Add appropriate indexes and foreign key constraints. Include audit columns for tracking feedback modifications.",
            "status": "done",
            "testStrategy": "Verify table creation with all columns and constraints. Test inserting and querying feedback entries. Confirm foreign key constraints work as expected. Test rollback drops the table."
          },
          {
            "id": 6,
            "title": "Create Book Metadata Tables Migration",
            "description": "Create migration script to establish tables for storing book metadata including authors, publishers, and editions.",
            "dependencies": [
              1
            ],
            "details": "Create tables for 'books', 'book_authors', 'publishers', and join tables as needed. Include columns for ISBN, title, publication_date, edition, page_count, etc. Set up proper relationships between tables with foreign keys. Add appropriate indexes for common query patterns.",
            "status": "done",
            "testStrategy": "Verify all tables are created with correct columns and relationships. Test inserting and querying book metadata across related tables. Confirm cascading operations work as expected. Test rollback removes all tables."
          },
          {
            "id": 7,
            "title": "Create Course Structure Tables Migration",
            "description": "Create migration script to establish tables for course structure including courses, modules, and lessons.",
            "dependencies": [
              1
            ],
            "details": "Create tables for 'courses', 'modules', 'lessons', and any necessary join tables. Include columns for title, description, sequence, status, and metadata. Implement hierarchical relationships between courses, modules, and lessons. Add constraints to maintain data integrity across the course structure.",
            "status": "done",
            "testStrategy": "Verify all tables are created with correct columns and relationships. Test inserting and querying course structures. Test hierarchical queries across the course structure. Confirm rollback removes all tables."
          },
          {
            "id": 8,
            "title": "Validate and Test All Migrations",
            "description": "Perform comprehensive testing of all migrations to ensure they work correctly and can be rolled back safely.",
            "dependencies": [
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Create a test script that applies all migrations in sequence to a test database. Verify each schema change is applied correctly. Test data insertion and querying for each new table and column. Test rollback of each migration individually and in sequence. Document any issues or edge cases discovered.",
            "status": "done",
            "testStrategy": "Run full migration sequence on clean test database. Verify schema matches expected state. Run rollback sequence and verify original schema is restored. Test with sample data to ensure functionality works as expected."
          }
        ]
      },
      {
        "id": 3,
        "title": "Set Up Feature Flags and Configuration Management",
        "description": "Implement feature flag system and update environment configuration for all new features.",
        "details": "Integrate a feature flag library such as Unleash or LaunchDarkly (open-source version if cost is a concern). Store flags in a centralized config file or service. Update .env files and Render.com environment variables for new settings. Document flag usage for each feature.",
        "testStrategy": "Toggle flags in staging and verify feature enable/disable behavior. Write unit tests for flag checks in code.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Research and Select Feature Flag Library",
            "description": "Evaluate feature flag libraries like Unleash and LaunchDarkly, focusing on open-source options that integrate well with FastAPI and Celery.",
            "dependencies": [],
            "details": "Compare at least 3 feature flag libraries (Unleash, LaunchDarkly open-source, and a simple file-based system). Evaluate based on: ease of integration with FastAPI, performance impact, self-hosting options, and developer experience. Create a comparison document with recommendations and prepare installation instructions for the selected library.",
            "status": "done",
            "testStrategy": "Create a simple test FastAPI app to verify the selected library can toggle features correctly and has minimal performance impact."
          },
          {
            "id": 2,
            "title": "Implement Core Feature Flag Infrastructure",
            "description": "Set up the selected feature flag library and create the core infrastructure for managing flags within the FastAPI application.",
            "dependencies": [
              1
            ],
            "details": "Install the selected feature flag library. Create a centralized configuration module for managing flags. Implement helper functions for checking flag status. Set up initialization in the FastAPI app startup. Create a configuration file or database schema for storing flag states. Ensure the system works in both development and production environments.",
            "status": "done",
            "testStrategy": "Write unit tests to verify flag checking functions work correctly. Test initialization during app startup and proper loading of flag configurations."
          },
          {
            "id": 3,
            "title": "Configure Feature Flags for All New Features",
            "description": "Create and configure feature flags for all 9 new features, including the 6 specified priority features.",
            "dependencies": [
              2
            ],
            "details": "Define feature flags for each of the 9 new features with appropriate naming conventions. Set default values for each environment (dev, staging, prod). Create feature flag definitions for: R&D Department, Loading Status UI, Source Attribution, Course Addition, Book Processing, and Intelligent Agent Router. Implement flag checking in relevant API endpoints and services. Ensure flags can be toggled without code changes or redeployment.",
            "status": "done",
            "testStrategy": "Test each feature with flags turned on and off to verify proper enabling/disabling behavior. Verify that toggling flags doesn't require application restart."
          },
          {
            "id": 4,
            "title": "Update Environment Configuration",
            "description": "Update environment variables in .env files and Render.com deployment configuration to support feature flags.",
            "dependencies": [
              2,
              3
            ],
            "details": "Update local .env files with feature flag configuration variables. Configure Render.com environment variables for all environments (dev, staging, production). Create documentation for required environment variables. Implement validation to ensure all required configuration is present at startup. Update CI/CD pipeline to handle feature flag configuration during deployment.",
            "status": "done",
            "testStrategy": "Test application startup with various environment configurations to ensure proper validation and error handling. Verify Render.com deployments correctly apply feature flag settings."
          },
          {
            "id": 5,
            "title": "Create Admin Interface for Feature Flag Management",
            "description": "Develop an admin interface or CLI tool for managing and toggling feature flags in different environments.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Create a protected admin endpoint in FastAPI for viewing and updating feature flag states. Implement authentication and authorization for the admin interface. Develop a simple UI for toggling flags or a CLI tool for administrators. Add audit logging for flag changes. Ensure the interface works across all environments. Add ability to schedule flag activation/deactivation for controlled rollouts.",
            "status": "done",
            "testStrategy": "Test admin interface with different user roles to verify proper access controls. Verify flag changes take effect immediately. Test scheduled flag changes to ensure they activate at the correct time."
          },
          {
            "id": 6,
            "title": "Document Feature Flag System for Developers",
            "description": "Create comprehensive documentation for the feature flag system, including usage guidelines, best practices, and examples.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Create developer documentation explaining how to use feature flags in code. Document the admin interface or CLI usage. Provide examples for common use cases like A/B testing, gradual rollouts, and emergency disabling. Create a troubleshooting guide. Add documentation for how to add new feature flags. Include architecture diagrams showing how the feature flag system integrates with the application. Update project README with feature flag information.",
            "status": "done",
            "testStrategy": "Have developers review documentation and attempt to implement a new feature flag following the guidelines to verify clarity and completeness."
          }
        ]
      },
      {
        "id": 4,
        "title": "Configure Monitoring: Prometheus, Grafana, and Alertmanager",
        "description": "Set up monitoring and alerting for all new features using Prometheus, Grafana, and Alertmanager.",
        "details": "Add Prometheus exporters to FastAPI and Celery services (use prometheus_fastapi_instrumentator and celery-prometheus-exporter). Create Grafana dashboards for processing status, agent routing, and upload metrics. Define alert rules for error rates, latency, and processing failures. Integrate email alerts for critical issues.",
        "testStrategy": "Trigger test alerts and verify dashboard metrics update in real time. Simulate failures to ensure alerts fire.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Develop Data Model and API Documentation",
        "description": "Document all data models, API endpoints, and implementation guides for development teams.",
        "details": "Use tools like Sphinx (Python) or Docusaurus (JS) to generate developer documentation. Include ER diagrams, endpoint descriptions, request/response examples, and migration guides. Link OpenAPI YAML files and provide usage examples for each API.",
        "testStrategy": "Review documentation for completeness and accuracy. Have developers follow guides to implement a sample feature.",
        "priority": "medium",
        "dependencies": [
          1,
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up Sphinx or Docusaurus documentation framework",
            "description": "Install and configure the chosen documentation framework (Sphinx for Python or Docusaurus for JavaScript) and set up the project structure.",
            "dependencies": [],
            "details": "Choose between Sphinx or Docusaurus based on the project's primary language. Install dependencies, initialize the documentation project, configure theme and plugins, and set up CI/CD for automatic documentation builds. Create a basic structure with sections for data models, API endpoints, migration guides, and developer onboarding.",
            "status": "done",
            "testStrategy": "Verify documentation builds successfully without errors. Test navigation between sections and ensure proper rendering of code blocks and diagrams."
          },
          {
            "id": 2,
            "title": "Create ER diagrams for all database tables",
            "description": "Generate entity-relationship diagrams for all database tables and document their relationships and constraints.",
            "dependencies": [
              1
            ],
            "details": "Use tools like dbdiagram.io, ERDPlus, or PlantUML to create visual representations of the database schema. Include all tables from Task 2 (agent_router_cache, agent_feedback, book metadata, course tables). Document primary keys, foreign keys, indexes, and constraints. Provide detailed descriptions for each field including data types, validation rules, and business purpose.",
            "status": "done",
            "testStrategy": "Review diagrams with database administrators and developers to ensure accuracy. Verify all tables from Task 2 migrations are properly represented."
          },
          {
            "id": 3,
            "title": "Document API endpoints with request/response examples",
            "description": "Create comprehensive documentation for all API endpoints including parameters, authentication requirements, and example requests and responses.",
            "dependencies": [
              1
            ],
            "details": "Document each API endpoint with its HTTP method, URL path, query parameters, request body schema, response schema, status codes, and error handling. Include curl examples and code snippets in multiple languages (Python, JavaScript, etc.). Group endpoints by functionality and provide use cases for each. Link to relevant data models and include authentication requirements.",
            "status": "done",
            "testStrategy": "Test documentation by following examples to make actual API calls. Verify all endpoints are documented and examples work as described."
          },
          {
            "id": 4,
            "title": "Write migration guides for schema changes",
            "description": "Create detailed migration guides explaining how to handle schema changes for developers and operations teams.",
            "dependencies": [
              2
            ],
            "details": "Document the migration process for all schema changes in Task 2, including adding 'R&D' to department enum, adding processing_status and source_metadata columns, and creating new tables. Provide step-by-step instructions for applying migrations in different environments. Include rollback procedures and troubleshooting tips. Document any data transformation or backfill requirements.",
            "status": "done",
            "testStrategy": "Have a developer follow the migration guide on a test environment to verify completeness and accuracy of instructions."
          },
          {
            "id": 5,
            "title": "Integrate OpenAPI specifications and create developer onboarding guide",
            "description": "Link OpenAPI YAML files to documentation and create a comprehensive developer onboarding guide for new team members.",
            "dependencies": [
              3,
              4
            ],
            "details": "Import and render OpenAPI specifications within the documentation. Create interactive API explorers using Swagger UI or ReDoc. Develop a developer onboarding guide that includes environment setup, architecture overview, coding standards, testing procedures, and common workflows. Include a quickstart tutorial that walks through implementing a simple feature using the documented APIs and data models.",
            "status": "done",
            "testStrategy": "Have a new or junior developer follow the onboarding guide and provide feedback. Verify OpenAPI specifications match the actual API implementation."
          }
        ]
      },
      {
        "id": 6,
        "title": "Set Up CI/CD Pipeline and Test Infrastructure",
        "description": "Configure GitHub Actions for CI/CD, run migrations on staging, and set up integration test data.",
        "details": "Update GitHub Actions workflows to run tests, lint code, and deploy to Render.com. Use pytest for backend and Playwright for frontend tests. Automate database migrations on staging. Seed test data for all new features.",
        "testStrategy": "Verify CI/CD runs on PRs and pushes. Confirm deployments and migrations succeed without manual intervention.",
        "priority": "high",
        "dependencies": [
          2,
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement Rollback Plan and Performance Baselines",
        "description": "Prepare rollback scripts and establish performance baselines for new features.",
        "details": "Write SQL and infrastructure scripts to revert migrations and deployments. Use Locust or k6 to benchmark API endpoints and document baseline metrics for processing time, latency, and error rates.",
        "testStrategy": "Perform rollback drills on staging. Run load tests and compare results to success metrics.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Write SQL rollback scripts for all database migrations",
            "description": "Create comprehensive SQL scripts that can revert all database migrations related to the new features.",
            "dependencies": [],
            "details": "Identify all migration files in the project. For each migration, create a corresponding rollback script that reverts the changes. Test each rollback script in a development environment to ensure it correctly restores the database to its previous state. Document each script with comments explaining what it reverts and any dependencies.",
            "status": "done",
            "testStrategy": "Execute each rollback script in a test database after running the forward migration to verify the database returns to its previous state. Validate data integrity after rollback."
          },
          {
            "id": 2,
            "title": "Create infrastructure rollback scripts for Render deployments",
            "description": "Develop scripts to revert infrastructure changes and deployments on the Render platform.",
            "dependencies": [
              1
            ],
            "details": "Document current Render configuration settings. Create shell scripts or Render API calls that can revert to previous deployment versions. Implement configuration rollback procedures for environment variables, service settings, and networking configurations. Include procedures for reverting to previous Docker images if applicable.",
            "status": "done",
            "testStrategy": "Perform a mock deployment followed by rollback in the staging environment. Verify all services return to their previous state and function correctly."
          },
          {
            "id": 3,
            "title": "Set up Locust or k6 for load testing API endpoints",
            "description": "Configure and implement load testing framework to establish performance baselines for all API endpoints.",
            "dependencies": [],
            "details": "Choose between Locust and k6 based on project requirements. Set up the testing environment with appropriate configuration files. Create test scenarios that simulate realistic user behavior for all critical API endpoints. Configure test parameters including concurrent users, ramp-up time, and test duration. Implement custom metrics collection for detailed performance analysis.",
            "status": "done",
            "testStrategy": "Run initial tests with minimal load to verify test setup. Gradually increase load to identify performance boundaries. Verify metrics collection is working correctly."
          },
          {
            "id": 4,
            "title": "Document baseline performance metrics for all endpoints",
            "description": "Run load tests and document comprehensive baseline metrics for API performance including latency, throughput, and error rates.",
            "dependencies": [
              3
            ],
            "details": "Execute load tests on all critical endpoints using the configured testing framework. Collect and analyze metrics for response time (average, p95, p99), requests per second, error rates, and resource utilization (CPU, memory, database connections). Create visualizations of performance data. Document acceptable thresholds for each metric that will trigger alerts or rollback procedures if exceeded in production.",
            "status": "done",
            "testStrategy": "Compare metrics across multiple test runs to ensure consistency. Validate that metrics collection captures all required data points for future comparison."
          },
          {
            "id": 5,
            "title": "Create comprehensive rollback runbook with step-by-step procedures",
            "description": "Develop a detailed runbook documenting the complete rollback process for all components of the system.",
            "dependencies": [
              1,
              2,
              4
            ],
            "details": "Compile all rollback scripts and procedures into a comprehensive runbook. Include decision criteria for when to initiate rollback. Document step-by-step procedures for database rollback, infrastructure rollback, and application rollback. Include verification steps after each stage of rollback. Add troubleshooting guidance for common rollback issues. Document communication templates for stakeholder notification during rollback events. Schedule and document a rollback drill on the staging environment.",
            "status": "done",
            "testStrategy": "Conduct a full rollback drill in staging following the runbook exactly as written. Identify and address any gaps or unclear instructions. Have a team member unfamiliar with the rollback process attempt to follow the runbook to verify clarity."
          }
        ]
      },
      {
        "id": 8,
        "title": "Prepare for Security Audit and Stakeholder Sign-off",
        "description": "Conduct security review and obtain sign-off from stakeholders before feature rollout.",
        "details": "Use Bandit (Python) and OWASP ZAP for static and dynamic security scans. Review authentication, authorization, and data validation. Schedule stakeholder demo and collect sign-off.",
        "testStrategy": "Address all critical/high findings from security tools. Document sign-off in project tracker.",
        "priority": "high",
        "dependencies": [
          7
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Run Bandit Static Security Analysis on Python Code",
            "description": "Perform static code analysis using Bandit to identify security vulnerabilities in the Python codebase.",
            "dependencies": [],
            "details": "Configure and run Bandit with appropriate security plugins. Focus on identifying common vulnerabilities like SQL injection, XSS, insecure file operations, and use of deprecated/insecure functions. Generate a comprehensive report of findings categorized by severity.",
            "status": "done",
            "testStrategy": "Verify all critical and high severity issues are addressed. Document false positives with justification."
          },
          {
            "id": 2,
            "title": "Run OWASP ZAP Dynamic Security Scan on API Endpoints",
            "description": "Conduct dynamic security testing using OWASP ZAP to identify vulnerabilities in API endpoints during runtime.",
            "dependencies": [],
            "details": "Configure OWASP ZAP to scan all API endpoints in the staging environment. Test for OWASP Top 10 vulnerabilities including injection flaws, broken authentication, sensitive data exposure, and CSRF. Generate a detailed report with screenshots and reproduction steps for each finding.",
            "status": "done",
            "testStrategy": "Validate findings with manual testing. Ensure all critical and high vulnerabilities are remediated before proceeding to stakeholder sign-off."
          },
          {
            "id": 3,
            "title": "Review Authentication and Authorization Implementation",
            "description": "Conduct a comprehensive review of the authentication and authorization mechanisms to ensure secure access control.",
            "dependencies": [
              1,
              2
            ],
            "details": "Examine JWT implementation, session management, password policies, and role-based access controls. Verify proper implementation of authentication flows including login, logout, password reset, and account recovery. Check for secure token handling and proper authorization checks across all protected resources.",
            "status": "done",
            "testStrategy": "Create test cases for authentication bypass attempts and unauthorized access scenarios. Verify all security controls are functioning as expected."
          },
          {
            "id": 4,
            "title": "Review Data Validation and Input Sanitization",
            "description": "Audit all data input points to ensure proper validation and sanitization is implemented to prevent injection attacks.",
            "dependencies": [
              1,
              2
            ],
            "details": "Review all form inputs, API parameters, file uploads, and database queries for proper validation and sanitization. Check for implementation of parameterized queries, input encoding, and content security policies. Identify any instances where user input is processed without proper validation.",
            "status": "done",
            "testStrategy": "Test with malicious payloads to verify sanitization effectiveness. Ensure all input validation controls properly reject malformed data."
          },
          {
            "id": 5,
            "title": "Document Security Findings and Conduct Stakeholder Demo",
            "description": "Compile security findings, prepare remediation plan, and present to stakeholders for final sign-off.",
            "dependencies": [
              3,
              4
            ],
            "details": "Create a comprehensive security report documenting all findings from previous subtasks, including severity ratings and recommended fixes. Prepare a remediation plan with timelines. Schedule and conduct a stakeholder demo to present findings, demonstrate security measures, and obtain formal sign-off for feature rollout.",
            "status": "done",
            "testStrategy": "Collect stakeholder feedback during demo and address any concerns. Document sign-off in project tracker with approver names and dates."
          }
        ]
      },
      {
        "id": 9,
        "title": "Add R&D Department to Department Enum and Services",
        "description": "Update backend and storage logic to support 'Research & Development' as a department.",
        "details": "Update department enum in PostgreSQL and service layer constants. Update B2 storage logic to include 'rnd/' folder. Update Neo4j department nodes. Ensure all APIs and UIs reflect the new department.",
        "testStrategy": "Verify R&D appears in all selection UIs and documents are correctly classified and stored. Write integration tests for classification.",
        "priority": "high",
        "dependencies": [
          2,
          5
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Update PostgreSQL Department Enum to Include R&D",
            "description": "Modify the database schema to add 'Research & Development' as a valid department option in the department enum type.",
            "dependencies": [],
            "details": "Create an Alembic migration script to alter the department enum type in PostgreSQL. Add 'rnd' or 'research_development' (based on existing naming conventions) to the enum. Update any relevant ORM models to include the new enum value. Test the migration in development environment before applying to production.",
            "status": "done",
            "testStrategy": "Write unit tests to verify the enum contains the new value. Test database queries filtering by the new department value."
          },
          {
            "id": 2,
            "title": "Update B2 Storage Logic for R&D Department Folder",
            "description": "Extend the B2 storage service to include an 'rnd/' folder structure for R&D department documents.",
            "dependencies": [
              1
            ],
            "details": "Modify the storage service to recognize the R&D department and route files to the appropriate 'rnd/' folder in B2 storage. Update path generation logic in file upload services. Ensure proper permissions are set on the new folder. Create the folder structure if it doesn't exist automatically.",
            "status": "done",
            "testStrategy": "Test file uploads with R&D department classification to verify they're stored in the correct location. Verify folder permissions match other department folders."
          },
          {
            "id": 3,
            "title": "Create R&D Department Node in Neo4j Knowledge Graph",
            "description": "Add a new department node for 'Research & Development' in the Neo4j knowledge graph and establish proper relationships.",
            "dependencies": [
              1
            ],
            "details": "Create Cypher query to add the R&D department node to Neo4j. Establish appropriate relationships between the new department node and existing nodes (e.g., connect to relevant topics, documents, or other organizational units). Update any graph traversal queries that rely on department information.",
            "status": "done",
            "testStrategy": "Execute graph queries to verify the R&D node exists and has correct relationships. Test knowledge graph traversals that should include R&D department data."
          },
          {
            "id": 4,
            "title": "Update Department Classification Service for R&D Content",
            "description": "Modify the department classification service to recognize and properly categorize R&D-related content.",
            "dependencies": [
              1,
              3
            ],
            "details": "Update classification algorithms and rules to identify R&D content based on keywords, topics, and document metadata. Modify service layer constants and enums to include R&D department. Update any caching mechanisms that store department information. Ensure classification accuracy for R&D content through testing with sample documents.",
            "status": "done",
            "testStrategy": "Run classification tests with known R&D content to verify correct department assignment. Compare classification results before and after changes to ensure existing content classification remains accurate."
          },
          {
            "id": 5,
            "title": "Update UI Components to Include R&D Department Option",
            "description": "Modify all user interface components to include 'Research & Development' in department dropdowns, filters, and displays.",
            "dependencies": [
              1,
              4
            ],
            "details": "Identify all UI components that display or filter by department (dropdowns, filter panels, department badges, etc.). Update these components to include the R&D option. Ensure proper labeling and consistent terminology across the UI. Update any client-side validation or department-specific business logic. Verify UI changes in all affected pages and components.",
            "status": "done",
            "testStrategy": "Perform UI testing to verify R&D appears in all relevant dropdowns and filters. Test document upload and classification flows to ensure R&D appears as an option and works correctly."
          }
        ]
      },
      {
        "id": 10,
        "title": "Implement Real-Time Processing Status with WebSockets",
        "description": "Create WebSocket manager and endpoints for real-time document and query status updates.",
        "details": "Use FastAPI's WebSocket support (starlette.websockets) to implement a manager for broadcasting status. Create endpoints for bidirectional communication. Use Redis pub/sub for scaling across workers. Integrate with Celery task events.",
        "testStrategy": "Simulate concurrent uploads and queries, verify status updates within 500ms. Use WebSocket test clients.",
        "priority": "high",
        "dependencies": [
          6
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create WebSocket Connection Manager",
            "description": "Implement a WebSocket connection manager class to handle client connections and message broadcasting",
            "dependencies": [],
            "details": "Create a ConnectionManager class using starlette.websockets that maintains active connections, handles new connections, disconnections, and provides methods for broadcasting messages to all or specific clients. Implement proper error handling and connection lifecycle management.",
            "status": "done",
            "testStrategy": "Write unit tests for connection handling, message broadcasting, and error scenarios. Test with multiple simultaneous connections."
          },
          {
            "id": 2,
            "title": "Implement WebSocket Endpoints for Status Updates",
            "description": "Create FastAPI WebSocket endpoints for document processing and query status updates",
            "dependencies": [
              1
            ],
            "details": "Implement /ws/document/{document_id} and /ws/query/{query_id} endpoints that allow clients to subscribe to status updates for specific documents or queries. Include authentication and validation. Handle connection establishment, message parsing, and proper error responses.",
            "status": "done",
            "testStrategy": "Test endpoint connectivity, authentication, and message handling with WebSocket test clients. Verify proper status code responses."
          },
          {
            "id": 3,
            "title": "Integrate Redis Pub/Sub for Distributed Broadcasting",
            "description": "Set up Redis pub/sub channels for distributing WebSocket messages across multiple worker instances",
            "dependencies": [
              1,
              2
            ],
            "details": "Configure Redis pub/sub channels for document and query status updates. Implement publisher components that send messages to Redis channels. Create subscriber components that listen to Redis channels and forward messages to the WebSocket manager for broadcasting to clients. Ensure proper serialization/deserialization of messages.",
            "status": "done",
            "testStrategy": "Test message publishing and subscription across multiple worker instances. Verify messages are correctly distributed and received by all relevant clients."
          },
          {
            "id": 4,
            "title": "Connect Celery Task Events to WebSocket Updates",
            "description": "Implement event listeners for Celery task state changes to trigger WebSocket updates",
            "dependencies": [
              1,
              3
            ],
            "details": "Create Celery task event listeners that capture task state changes (pending, started, success, failure) for document processing and query tasks. Map task IDs to document/query IDs. Publish status updates to Redis channels when task states change. Include progress information when available.",
            "status": "done",
            "testStrategy": "Test with various Celery task scenarios (success, failure, retry). Verify WebSocket clients receive timely and accurate status updates."
          },
          {
            "id": 5,
            "title": "Implement Client-Side Status Display Components",
            "description": "Create frontend components to display real-time status updates from WebSocket connections",
            "dependencies": [
              2
            ],
            "details": "Develop JavaScript components that establish WebSocket connections, handle reconnection logic, parse incoming messages, and update UI elements with status information. Create progress indicators, status badges, and notification components. Implement error handling for connection issues.",
            "status": "done",
            "testStrategy": "Test components with mocked WebSocket servers. Verify UI updates correctly with different status messages. Test reconnection behavior and error handling."
          },
          {
            "id": 6,
            "title": "Implement Performance Monitoring and Load Testing",
            "description": "Set up monitoring for WebSocket connections and implement load testing for the real-time status system",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Implement metrics collection for WebSocket connections (count, message rate, latency). Configure logging for connection events. Create load testing scripts using WebSocket test clients to simulate high connection counts and message volumes. Document performance characteristics and bottlenecks.",
            "status": "done",
            "testStrategy": "Run load tests with increasing numbers of concurrent connections. Verify system maintains <500ms latency for status updates under expected load. Test recovery from connection spikes."
          }
        ]
      },
      {
        "id": 11,
        "title": "Create REST Status Endpoint and Polling Logic",
        "description": "Implement REST endpoint for polling processing/query status as a fallback to WebSockets.",
        "details": "Add /status endpoint to FastAPI, returning current processing state from database. Ensure endpoint is rate-limited and returns consistent schema.",
        "testStrategy": "Test polling from frontend, verify correct status at each stage. Write unit and integration tests.",
        "priority": "medium",
        "dependencies": [
          10
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create /status endpoint in FastAPI",
            "description": "Implement a REST endpoint at /status that returns the current processing state from the database",
            "dependencies": [],
            "details": "Create a new FastAPI route handler for GET /status that queries the database for current processing state. Include query parameter support for specific job IDs. Ensure proper error handling for non-existent jobs and database connection issues.",
            "status": "done",
            "testStrategy": "Write unit tests to verify endpoint returns correct status codes and data. Test with various job states (pending, processing, complete, error)."
          },
          {
            "id": 2,
            "title": "Design and implement consistent response schema",
            "description": "Create a standardized response schema for status queries that maintains consistency with WebSocket responses",
            "dependencies": [
              1
            ],
            "details": "Define Pydantic models for status responses that include fields for job_id, status, progress_percentage, created_at, updated_at, and error_message if applicable. Ensure schema is consistent with existing WebSocket status messages to simplify frontend integration.",
            "status": "done",
            "testStrategy": "Validate response schema against OpenAPI specification. Test serialization/deserialization with various status states."
          },
          {
            "id": 3,
            "title": "Implement rate limiting for status endpoint",
            "description": "Add rate limiting to the status endpoint to prevent abuse and ensure service stability",
            "dependencies": [
              1
            ],
            "details": "Integrate a rate limiting middleware (such as slowapi) to restrict requests per client IP or API key. Configure appropriate limits (e.g., 60 requests per minute) and ensure proper response headers (X-RateLimit-*) are included. Implement graceful handling of rate limit exceeded scenarios.",
            "status": "done",
            "testStrategy": "Test rate limiting by sending requests at various frequencies. Verify correct 429 responses when limits are exceeded and headers indicate remaining quota."
          },
          {
            "id": 4,
            "title": "Create frontend polling logic helper functions",
            "description": "Develop JavaScript utility functions to handle polling the status endpoint as a fallback when WebSockets are unavailable",
            "dependencies": [
              2
            ],
            "details": "Create a polling utility in JavaScript that implements exponential backoff strategy. Include functions for starting/stopping polling, handling connection errors, and transitioning between WebSocket and REST polling when needed. Ensure proper cleanup of polling intervals to prevent memory leaks.",
            "status": "done",
            "testStrategy": "Write Jest tests to verify polling behavior, backoff strategy, and proper state transitions. Mock API responses to test various scenarios including network failures."
          },
          {
            "id": 5,
            "title": "Add endpoint documentation and examples",
            "description": "Document the status endpoint with OpenAPI annotations and create usage examples for frontend developers",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Add comprehensive OpenAPI documentation to the status endpoint including parameter descriptions, response schemas, and status codes. Create a documentation page with example requests/responses and polling implementation patterns. Include guidance on rate limits and best practices for efficient polling.",
            "status": "done",
            "testStrategy": "Review documentation for completeness. Have another developer attempt to implement polling using only the documentation to verify clarity and accuracy."
          }
        ]
      },
      {
        "id": 12,
        "title": "Update Celery Tasks to Broadcast Status Changes",
        "description": "Modify Celery tasks to emit status updates to WebSocket manager and REST endpoint.",
        "details": "Use celery-signals to hook into task state changes. Publish updates to Redis channel for WebSocket manager. Update processing_status JSONB column in database.",
        "testStrategy": "Run end-to-end upload and query flows, verify all status transitions are broadcast and persisted.",
        "priority": "high",
        "dependencies": [
          10,
          11
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Celery Signal Hooks for Task State Changes",
            "description": "Create signal handlers for Celery task state transitions including pending, started, success, and failure states.",
            "dependencies": [],
            "details": "Define signal handlers using @task_prerun, @task_postrun, @task_success, and @task_failure decorators from celery.signals. Create a common status update function that will be called by all signal handlers. Ensure handlers capture task_id, state, runtime, and any error information.",
            "status": "done",
            "testStrategy": "Write unit tests for each signal handler to verify they correctly capture state changes and call the status update function with proper parameters."
          },
          {
            "id": 2,
            "title": "Create Status Message Format Schema",
            "description": "Define a standardized JSON schema for status update messages that will be used across Redis pub/sub and database updates.",
            "dependencies": [
              1
            ],
            "details": "Create a Pydantic model for status messages with fields for task_id, status, timestamp, progress percentage, error details (if applicable), and metadata. Include validation for required fields and data types. Document the schema for frontend developers who will consume these messages.",
            "status": "done",
            "testStrategy": "Write unit tests to validate message serialization/deserialization and schema validation for both valid and invalid message formats."
          },
          {
            "id": 3,
            "title": "Implement Redis Pub/Sub for Status Updates",
            "description": "Create a service to publish task status updates to Redis channels for consumption by the WebSocket manager.",
            "dependencies": [
              2
            ],
            "details": "Implement a StatusPublisher class that connects to Redis and publishes status messages to appropriate channels. Create channel naming convention based on task type and ID. Add configuration for Redis connection parameters. Ensure messages follow the defined schema from subtask 2.",
            "status": "done",
            "testStrategy": "Create integration tests that verify messages are correctly published to Redis channels and can be received by subscribers. Mock Redis for unit tests."
          },
          {
            "id": 4,
            "title": "Update Database with Processing Status",
            "description": "Modify database operations to store task status updates in the processing_status JSONB column.",
            "dependencies": [
              2
            ],
            "details": "Create a database service method to update the processing_status column with the latest status information. Implement transaction handling to ensure data consistency. Store the complete status history in the JSONB column, not just the current status. Add indexes if needed for query performance.",
            "status": "done",
            "testStrategy": "Write integration tests with a test database to verify status updates are correctly persisted. Test edge cases like concurrent updates and error recovery."
          },
          {
            "id": 5,
            "title": "Create Integration Tests for Status Broadcasting",
            "description": "Develop comprehensive integration tests to verify the entire status broadcasting pipeline works correctly.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Create end-to-end tests that trigger Celery tasks and verify: 1) Signal handlers are called, 2) Status messages are published to Redis, 3) Database is updated with status changes, 4) WebSocket clients receive updates. Test all state transitions including error states and retries. Create a test harness that can monitor all components simultaneously.",
            "status": "done",
            "testStrategy": "Use pytest fixtures to set up test environment with Redis, database, and mock WebSocket clients. Verify message delivery and content at each stage of the pipeline."
          }
        ]
      },
      {
        "id": 13,
        "title": "Integrate Gradio Processing Status Component",
        "description": "Add progress bars and status indicators to Gradio upload and query UI.",
        "details": "Use Gradio's custom components (Blocks API) to display real-time progress. Connect to WebSocket and REST endpoints for updates. Ensure accessibility and responsive design.",
        "testStrategy": "Upload documents and run queries in UI, verify progress bars update in real time. Test on multiple browsers.",
        "priority": "medium",
        "dependencies": [
          10,
          11,
          12
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Gradio Blocks Component for Progress Display",
            "description": "Develop a reusable Gradio Blocks component that can display processing status and progress information for various operations.",
            "dependencies": [],
            "details": "Use Gradio's Blocks API to create a custom component with progress bars, status text, and animated indicators. Implement CSS styling for different states (pending, processing, complete, error). Create a Python class that encapsulates the component for easy reuse across different parts of the UI.",
            "status": "done",
            "testStrategy": "Test component rendering with different progress values and states. Verify proper display on different screen sizes and browsers."
          },
          {
            "id": 2,
            "title": "Implement Progress Bars for Upload and Processing Stages",
            "description": "Add visual progress indicators for document upload and query processing stages to provide users with real-time feedback.",
            "dependencies": [
              1
            ],
            "details": "Extend the base progress component to handle multiple stages of processing (upload, parsing, indexing, querying). Implement percentage-based progress bars with appropriate labels for each stage. Add animations for indeterminate progress states when exact progress cannot be calculated. Ensure smooth transitions between stages.",
            "status": "done",
            "testStrategy": "Upload documents of various sizes and verify progress bars accurately reflect upload and processing progress. Test with both fast and slow network connections."
          },
          {
            "id": 3,
            "title": "Connect to WebSocket Endpoint for Real-time Updates",
            "description": "Implement WebSocket connection to receive real-time progress updates from the backend processing services.",
            "dependencies": [
              2
            ],
            "details": "Create a WebSocket client that connects to the backend progress update endpoint. Implement message handlers for different update types (progress, status change, completion, error). Add reconnection logic with exponential backoff for connection failures. Update the progress component UI in response to received messages.",
            "status": "done",
            "testStrategy": "Test WebSocket connection establishment, message handling, and reconnection behavior. Verify UI updates correctly in response to different message types."
          },
          {
            "id": 4,
            "title": "Add REST Polling Fallback When WebSocket Unavailable",
            "description": "Implement a fallback mechanism using REST API polling when WebSocket connections are not available or supported.",
            "dependencies": [
              3
            ],
            "details": "Create a polling service that fetches progress updates via REST API at regular intervals. Implement detection logic to determine when to fall back to polling (WebSocket connection failures, browser limitations). Ensure smooth transition between WebSocket and polling modes. Optimize polling frequency to balance responsiveness and server load.",
            "status": "done",
            "testStrategy": "Test fallback behavior by simulating WebSocket failures. Verify polling correctly retrieves and displays progress updates. Measure performance impact of polling vs WebSocket updates."
          },
          {
            "id": 5,
            "title": "Ensure Accessibility and Mobile Responsiveness",
            "description": "Optimize the progress component for accessibility standards and responsive design across different devices.",
            "dependencies": [
              4
            ],
            "details": "Add ARIA attributes to progress elements for screen reader support. Implement keyboard navigation for interactive elements. Test and adjust component layout for mobile devices, ensuring proper scaling and touch interactions. Add high-contrast mode support and ensure color choices meet WCAG 2.1 AA standards. Document accessibility features for future maintenance.",
            "status": "done",
            "testStrategy": "Test with screen readers (NVDA, VoiceOver) to verify accessibility. Verify responsive behavior on various device sizes. Use Lighthouse or similar tools to audit accessibility compliance."
          }
        ]
      },
      {
        "id": 14,
        "title": "Implement Source Metadata Extraction and Storage",
        "description": "Extract and store source metadata (title, author, date, page) for all uploaded documents.",
        "details": "Integrate LangExtract (latest version) for metadata extraction. Update upload pipeline to extract and store metadata in source_metadata JSONB column. Ensure support for PDFs, DOCX, and web articles.",
        "testStrategy": "Upload sample documents, verify metadata extraction accuracy (>95%). Write unit tests for extraction logic.",
        "priority": "high",
        "dependencies": [
          2,
          6
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate LangExtract Library into Project Environment",
            "description": "Install and configure the latest version of LangExtract in the project environment to enable metadata extraction capabilities.",
            "dependencies": [],
            "details": "Add LangExtract as a dependency in the project requirements. Set up API keys or credentials if required. Verify installation by running a simple extraction example using LangExtract to ensure it is operational.\n<info added on 2025-11-25T17:41:00.894Z>\nSKIPPED - Used native Python libraries (pypdf, python-docx, python-pptx) instead of LangExtract for better accuracy and simpler integration. The native library approach provides direct access to document metadata without additional API dependencies, offers reliable extraction for title, author, publication_date, and page_count, and includes built-in confidence scoring based on metadata availability. This decision was made during 14.2 implementation and validated during 14.5 testing with 100% pass rate (14/14 tests).\n</info added on 2025-11-25T17:41:00.894Z>",
            "status": "done",
            "testStrategy": "Run a basic LangExtract extraction on a sample text and confirm output structure matches expectations."
          },
          {
            "id": 2,
            "title": "Develop Metadata Extraction Logic for Supported Formats",
            "description": "Implement logic to extract source metadata (title, author, date, page) from PDFs, DOCX, and web articles using LangExtract.",
            "dependencies": [
              1
            ],
            "details": "Create format-specific handlers that preprocess documents and invoke LangExtract with tailored prompts and examples. Ensure extraction covers all required metadata fields and handles format-specific quirks (e.g., PDF metadata tags, DOCX properties, web article HTML parsing).\n<info added on 2025-11-25T04:40:27.029Z>\nImplementation completed for metadata extraction logic across PDF, DOCX, and PPTX formats. The solution uses native libraries instead of LangExtract for improved accuracy. The `extract_source_metadata()` method in `app/services/metadata_extractor.py` now extracts standard fields (title, author, publication_date, page_count) for all supported formats and returns data in standardized JSONB format matching the database schema. Format-specific extractors were implemented for each document type with tailored approaches: PDFs leverage document properties with creation date as publication proxy, DOCX extracts core properties with paragraph-based page estimation, and PPTX uses slide count for page metrics. A confidence scoring system (0.0-1.0) evaluates metadata quality based on field availability. Additional non-standard metadata (file_hash, creator, producer, keywords) is stored separately in the `additional_metadata` field for future use.\n</info added on 2025-11-25T04:40:27.029Z>",
            "status": "done",
            "testStrategy": "Unit test extraction logic with diverse sample documents for each format, verifying all required metadata fields are extracted."
          },
          {
            "id": 3,
            "title": "Update Upload Pipeline to Invoke Metadata Extraction",
            "description": "Modify the document upload pipeline to trigger metadata extraction for each uploaded document and capture the results.",
            "dependencies": [
              2
            ],
            "details": "Integrate the extraction logic into the upload workflow. Ensure that, upon upload, the document is processed and metadata is extracted before storage. Handle errors gracefully and log extraction failures for review.\n<info added on 2025-11-25T04:40:51.608Z>\nImplementation completed for integrating metadata extraction into the document upload pipeline. The workflow now includes downloading files from B2 storage, extracting metadata using the MetadataExtractor, and storing it in the documents.source_metadata JSONB column. The process follows a clear pipeline: document upload to B2, Celery task triggering, file movement to processing directory, temporary download, metadata extraction, storage in Supabase, cleanup, and continuation of processing. Error handling ensures metadata extraction failures don't halt document processing, temporary files are always removed, and the existing retry logic with exponential backoff applies to the entire task. Key files modified include document_processing.py, b2_storage.py (with new download_file method), and supabase_storage.py (with new update_source_metadata method).\n</info added on 2025-11-25T04:40:51.608Z>",
            "status": "done",
            "testStrategy": "Upload documents via the pipeline and verify that metadata extraction is triggered and results are available for subsequent storage."
          },
          {
            "id": 4,
            "title": "Store Extracted Metadata in source_metadata JSONB Column",
            "description": "Persist the extracted metadata in the source_metadata JSONB column of the database for each uploaded document.",
            "dependencies": [
              3
            ],
            "details": "Update the database interaction layer to accept and store the extracted metadata as a JSONB object. Ensure schema compatibility and handle updates or overwrites of metadata for re-uploaded documents.",
            "status": "done",
            "testStrategy": "Verify that uploaded documents have their metadata correctly stored in the JSONB column by querying the database and inspecting the stored values."
          },
          {
            "id": 5,
            "title": "Validate End-to-End Metadata Extraction and Storage",
            "description": "Test the complete workflow by uploading sample documents and verifying that metadata is accurately extracted and stored for all supported formats.",
            "dependencies": [
              4
            ],
            "details": "Prepare a diverse set of sample PDFs, DOCX files, and web articles. Upload them through the system, then retrieve and inspect the stored metadata. Confirm extraction accuracy (>95%) and completeness for all required fields.\n<info added on 2025-11-25T17:40:39.006Z>\nEnd-to-End Validation Testing Results:\n\n**Test Results (14 tests, all passing):**\n\n1. PDF Metadata Extraction:\n   - Full metadata extraction: PASSED (title, author, publication date, page count)\n   - Minimal metadata extraction: PASSED (filename fallback, page count)\n   - Timestamp validation: PASSED (ISO 8601 format)\n\n2. DOCX Metadata Extraction:\n   - Full metadata extraction: PASSED (core properties)\n   - Minimal metadata extraction: PASSED (handles auto-populated fields)\n\n3. PPTX Metadata Extraction:\n   - Full metadata extraction: PASSED (slide count as page count)\n   - Minimal metadata extraction: PASSED\n\n4. Edge Cases:\n   - File not found: PASSED (raises FileNotFoundError)\n   - Unsupported file type: PASSED (uses filename, 0.3 confidence)\n   - Corrupted PDF: PASSED (handles gracefully)\n\n5. Confidence Scoring:\n   - All fields present: PASSED (0.975 confidence)\n   - Missing author: PASSED (0.725 confidence)\n   - Filename only: PASSED (<0.5 confidence)\n\n6. Output Schema Validation:\n   - All required fields present: PASSED\n   - Type validation: PASSED\n\n**Database Verification:**\n- `source_metadata` JSONB column exists in `documents` table\n- `source_metadata` JSONB column exists in `document_chunks` table\n- `source_attribution` JSONB column exists in `chat_messages` table\n- GIN indexes created for efficient JSONB querying\n- Pipeline integration verified in document_processing.py\n\n**Test File:** tests/test_metadata_extraction_e2e.py (14 tests)\n</info added on 2025-11-25T17:40:39.006Z>",
            "status": "done",
            "testStrategy": "Automated and manual tests: Upload test documents, query stored metadata, and compare against expected values. Write assertions for extraction accuracy and field presence."
          }
        ]
      },
      {
        "id": 15,
        "title": "Update Chat Endpoint to Include Inline Citations",
        "description": "Modify chat API to return inline citations and source metadata for each AI response.",
        "details": "Update FastAPI chat endpoint to include citations in response payload. Format citations with page numbers and metadata. Ensure compatibility with Gradio citation component.",
        "testStrategy": "Send chat queries, verify citations are present and accurate. Write tests for citation formatting.",
        "priority": "high",
        "dependencies": [
          14
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Source Metadata Retrieval from RAG",
            "description": "Modify the RAG retrieval process to capture and return source metadata from document chunks used in generating responses.",
            "dependencies": [],
            "details": "Update the retrieval augmented generation (RAG) component to extract source_metadata from document chunks. Ensure the metadata includes title, author, publication_date, page_count, document_type, and confidence_score. Create a function to collect and deduplicate metadata from multiple chunks used in a single response.",
            "status": "done",
            "testStrategy": "Write unit tests to verify metadata extraction from document chunks. Test with various document types to ensure all metadata fields are properly retrieved."
          },
          {
            "id": 2,
            "title": "Create Citation Formatting Service",
            "description": "Develop a service to format citations with document title, author, page number, and other relevant metadata.",
            "dependencies": [
              1
            ],
            "details": "Create a CitationFormatter class that takes source_metadata and generates properly formatted citations. Implement methods for different citation styles (e.g., APA, MLA). Include functionality to generate inline citation markers (e.g., [1], [2]) and map them to the full citation data. Handle edge cases like missing metadata fields.",
            "status": "done",
            "testStrategy": "Unit test the CitationFormatter with various metadata inputs, including incomplete metadata. Verify correct formatting of citations according to specified styles."
          },
          {
            "id": 3,
            "title": "Integrate Inline Citations in AI Response Text",
            "description": "Modify the AI response generation to include inline citation references at appropriate points in the text.",
            "dependencies": [
              2
            ],
            "details": "Update the response generation process to insert citation markers (e.g., [1], [2]) at appropriate points in the AI-generated text. Implement logic to determine where citations should be placed based on the chunks used for each part of the response. Ensure citations are properly numbered and correspond to the correct source metadata.",
            "status": "done",
            "testStrategy": "Test with various queries to ensure citations are placed appropriately in the text. Verify that citation numbers match the correct sources in the citations array."
          },
          {
            "id": 4,
            "title": "Update FastAPI Endpoint Response Structure",
            "description": "Modify the chat endpoint response payload to include a structured citations array with full metadata for each citation.",
            "dependencies": [
              3
            ],
            "details": "Update the FastAPI chat endpoint to return a new response structure that includes both the AI response text (with inline citations) and a citations array containing full metadata for each source. Define a Pydantic model for the citation structure. Ensure the response format is consistent and well-documented. Update API documentation to reflect the new response structure.",
            "status": "done",
            "testStrategy": "Write API tests to verify the endpoint returns the expected structure. Test with various queries to ensure citations array contains all referenced sources with complete metadata."
          },
          {
            "id": 5,
            "title": "Ensure Compatibility with Gradio Citation UI Component",
            "description": "Verify and update the response format to ensure compatibility with the Gradio citation UI component.",
            "dependencies": [
              4
            ],
            "details": "Review the Gradio citation component requirements and ensure the API response structure is compatible. Update the response format if necessary to match Gradio's expected input structure. Test the integration between the API and the Gradio UI to ensure citations are properly displayed. Document the integration requirements for frontend developers.",
            "status": "done",
            "testStrategy": "Perform end-to-end testing with the Gradio UI to verify citations are correctly displayed. Test various response scenarios including multiple citations, missing metadata fields, and different citation formats."
          }
        ]
      },
      {
        "id": 16,
        "title": "Create Gradio Citation Component with Expandable Cards",
        "description": "Develop a Gradio UI component to display inline citations and allow users to expand for full source context.",
        "details": "Use Gradio's Blocks API to create clickable citation cards. On click, show full metadata and excerpt. Ensure mobile responsiveness and accessibility.",
        "testStrategy": "Test citation display and expansion in chat UI. Verify usability and correct linkage to sources.",
        "priority": "medium",
        "dependencies": [
          15
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Implement Agent Router Data Models and Service",
        "description": "Create data models and service logic for intelligent agent routing (LangGraph, CrewAI, Simple RAG).",
        "details": "Define SQLAlchemy models for agent_router_cache and decision history. Implement routing logic in a dedicated FastAPI service. Use ML-based or rule-based approach for workflow selection. Cache recent decisions for performance.",
        "testStrategy": "Unit test routing logic with diverse queries. Verify cache hits and correct workflow selection.",
        "priority": "high",
        "dependencies": [
          2,
          6
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Pydantic models for agent router requests and responses",
            "description": "Define the data structures for agent router API communication using Pydantic models",
            "dependencies": [],
            "details": "Create AgentRouterRequest model with fields for query text, user context, and optional parameters. Create AgentRouterResponse model with fields for selected agent type, confidence score, and routing metadata. Include validation logic for required fields and proper typing.",
            "status": "done",
            "testStrategy": "Write unit tests to verify model validation, serialization/deserialization, and edge cases like empty queries or missing context."
          },
          {
            "id": 2,
            "title": "Implement SQLAlchemy models for agent router cache and decision history",
            "description": "Create database models to store routing decisions and enable caching of previous results",
            "dependencies": [
              1
            ],
            "details": "Define AgentRouterCache model with fields for query hash, selected agent, timestamp, and expiration. Create DecisionHistory model to track all routing decisions with query text, selected agent, timestamp, and outcome metrics. Add indexes for efficient querying and implement TTL logic for cache entries.",
            "status": "done",
            "testStrategy": "Test database operations including insertion, retrieval, and cache expiration. Verify query hashing consistency and proper indexing performance."
          },
          {
            "id": 3,
            "title": "Implement query classification logic for document vs conversational queries",
            "description": "Build the core logic to analyze and classify incoming queries to determine the appropriate agent type",
            "dependencies": [
              1,
              2
            ],
            "details": "Develop a classifier using rule-based patterns and ML techniques to distinguish between document-focused queries (e.g., 'summarize this document') and conversational queries (e.g., 'what is machine learning?'). Implement feature extraction from queries, classification logic, and confidence scoring. Support both rule-based fallbacks and ML-based primary classification.",
            "status": "done",
            "testStrategy": "Create a test suite with diverse query examples for each category. Measure classification accuracy against a labeled dataset. Test edge cases and ambiguous queries."
          },
          {
            "id": 4,
            "title": "Build agent selection algorithm based on query type and context",
            "description": "Develop the decision-making logic to route queries to LangGraph, CrewAI, or Simple RAG based on classification results",
            "dependencies": [
              3
            ],
            "details": "Implement the core routing algorithm that maps query classifications to specific agent types. Create decision trees or scoring mechanisms to select between LangGraph (for complex multi-step reasoning), CrewAI (for collaborative tasks), and Simple RAG (for direct document retrieval). Include logic to consider user context, query history, and performance metrics in the decision process.",
            "status": "done",
            "testStrategy": "Test routing decisions against predefined scenarios. Verify that similar queries consistently route to the same agent type. Simulate various user contexts to test contextual routing."
          },
          {
            "id": 5,
            "title": "Create FastAPI endpoint with metrics and logging for agent routing",
            "description": "Implement the REST API endpoint for agent routing with comprehensive logging and performance metrics",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Develop a FastAPI endpoint at /api/v1/agent-router that accepts POST requests with AgentRouterRequest payloads. Implement request validation, error handling, and response formatting. Add detailed logging for each routing decision, including query, selected agent, confidence score, and processing time. Implement metrics collection for routing performance, cache hit rates, and agent success rates.",
            "status": "done",
            "testStrategy": "Perform integration testing with the full API stack. Test error handling, rate limiting, and concurrent requests. Verify metrics collection and log output format."
          }
        ]
      },
      {
        "id": 18,
        "title": "Create Agent Router API Endpoint",
        "description": "Expose agent router logic via a REST endpoint for frontend and service integration.",
        "details": "Add /agent-router endpoint to FastAPI. Accept query payload, return selected workflow and decision metadata. Enforce authentication and rate limiting.",
        "testStrategy": "Send test queries, verify correct workflow is selected and response is within 100ms.",
        "priority": "high",
        "dependencies": [
          17
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Integrate and Monitor Agent Routing Decisions",
        "description": "Log and monitor agent routing decisions for accuracy and performance.",
        "details": "Log all routing decisions to agent_router_cache. Add Prometheus metrics for routing accuracy and latency. Create Grafana dashboard for monitoring.",
        "testStrategy": "Review logs and metrics, verify >90% accuracy and <100ms latency under load.",
        "priority": "medium",
        "dependencies": [
          18,
          4
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Implement URL/Link Upload and Processing",
        "description": "Enable upload and processing of YouTube videos and web articles via URLs.",
        "details": "Integrate yt-dlp (latest) for YouTube transcription. Use BeautifulSoup4 for article scraping. Implement URL validator to detect content type. Add URL upload endpoint and Celery tasks for processing. Store extracted text and metadata.",
        "testStrategy": "Upload various URLs, verify transcript or article extraction. Test invalid/unsupported URLs are rejected.",
        "priority": "high",
        "dependencies": [
          6,
          14
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement URL validation and sanitization",
            "description": "Create a robust URL validation and sanitization service to verify and clean user-submitted URLs",
            "dependencies": [],
            "details": "Develop a URL validator service that checks for valid URL formats, sanitizes inputs to prevent injection attacks, and performs preliminary content-type detection (YouTube, article, PDF, etc.). Implement regex patterns for common URL formats and use Python's urllib.parse for validation. Add unit tests for various URL formats and edge cases.",
            "status": "done",
            "testStrategy": "Test with valid and invalid URLs across different formats (YouTube, news sites, PDFs, etc.). Verify sanitization prevents XSS and injection attacks. Ensure proper error handling for malformed URLs."
          },
          {
            "id": 2,
            "title": "Build web scraper service for content extraction",
            "description": "Develop a service to extract content from web articles using BeautifulSoup4 and handle different content structures",
            "dependencies": [
              1
            ],
            "details": "Implement a web scraper service using BeautifulSoup4 that extracts main content from articles while filtering out navigation, ads, and irrelevant content. Create content extraction strategies for common website patterns. Handle pagination if needed. Implement timeout and retry logic for reliability. Add logging for debugging extraction issues.",
            "status": "done",
            "testStrategy": "Test against various news sites, blogs, and article formats. Verify content extraction accuracy by comparing with manual extractions. Test handling of different HTML structures and error cases."
          },
          {
            "id": 3,
            "title": "Integrate YouTube video processing with yt-dlp",
            "description": "Implement YouTube video URL processing to extract transcripts and metadata using yt-dlp",
            "dependencies": [
              1
            ],
            "details": "Integrate the latest version of yt-dlp to download and process YouTube videos. Extract video transcripts, titles, descriptions, and other metadata. Handle different transcript formats and languages. Implement caching to avoid re-downloading videos. Add error handling for region-restricted or private videos. Create Celery tasks for asynchronous processing.",
            "status": "done",
            "testStrategy": "Test with various YouTube videos including different lengths, languages, and caption availability. Verify transcript extraction accuracy and metadata completeness. Test error handling for unavailable videos."
          },
          {
            "id": 4,
            "title": "Create URL metadata extraction service",
            "description": "Develop a service to extract metadata from URLs including title, description, author, and favicon",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Implement a metadata extraction service that retrieves title, description, author information, publication date, and favicon from URLs. Use Open Graph tags, Twitter cards, and HTML meta tags for extraction. Create fallback strategies when primary metadata sources are unavailable. Store extracted metadata in the database with proper schema mapping to existing metadata tables.",
            "status": "done",
            "testStrategy": "Test metadata extraction across various websites with different metadata implementations. Verify extraction of titles, descriptions, authors, dates, and favicons. Test fallback strategies when primary metadata is missing."
          },
          {
            "id": 5,
            "title": "Integrate URL processing with document pipeline and implement rate limiting",
            "description": "Connect URL processing to the existing document processing pipeline and add rate limiting with robots.txt compliance",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Create REST API endpoints for URL submission. Implement rate limiting to prevent abuse and ensure compliance with robots.txt rules for web scraping. Connect extracted content to the existing document processing pipeline for indexing and storage. Add monitoring and logging for URL processing jobs. Implement retry logic for failed processing attempts. Create admin dashboard components for URL processing status and configuration.",
            "status": "done",
            "testStrategy": "Test end-to-end URL processing flow with various content types. Verify rate limiting prevents abuse. Test robots.txt compliance with mock websites. Verify processed URL content appears correctly in search results and knowledge base."
          }
        ]
      },
      {
        "id": 21,
        "title": "Enable File and Image Upload in Chat",
        "description": "Allow users to upload files and images directly in chat for context-aware Q&A.",
        "details": "Implement file handler utility for multipart uploads. Integrate Claude Vision API for image analysis (latest version). Add chat file upload endpoint and Gradio component. Ensure file context is linked to chat session.",
        "testStrategy": "Upload images and PDFs in chat, verify analysis and context retention. Test with various file types.",
        "priority": "high",
        "dependencies": [
          6,
          13
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement File Handler Utility for Multipart Uploads",
            "description": "Create a utility module to handle file uploads with proper validation, storage, and metadata extraction.",
            "dependencies": [],
            "details": "Develop a file handler utility that supports multipart uploads, validates file types (images, PDFs, docs), implements size limits, generates unique filenames, and extracts basic metadata. Store files in a configurable location with proper error handling and logging.",
            "status": "done",
            "testStrategy": "Unit test with various file types and sizes. Test validation logic, error handling, and metadata extraction."
          },
          {
            "id": 2,
            "title": "Integrate Claude Vision API for Image Analysis",
            "description": "Connect to the latest Claude Vision API to enable AI analysis of uploaded images in chat conversations.",
            "dependencies": [
              1
            ],
            "details": "Implement a service that takes uploaded image files, prepares them for Claude Vision API, handles the API requests with proper error handling, and formats the response for use in the chat context. Include rate limiting, retries, and caching mechanisms. Update environment variables for API keys.",
            "status": "done",
            "testStrategy": "Test with various image types and sizes. Mock API responses for unit tests. Integration tests with actual API calls using test images."
          },
          {
            "id": 3,
            "title": "Create Chat File Upload Backend Endpoint",
            "description": "Develop a FastAPI endpoint that handles file uploads within chat sessions and maintains context.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create a new FastAPI endpoint at /api/chat/upload that accepts multipart form data, processes files using the file handler utility, links uploads to the current chat session ID in the database, and returns file metadata. Implement authentication checks, rate limiting, and proper error responses. Update OpenAPI documentation.",
            "status": "done",
            "testStrategy": "Unit tests for endpoint logic. Integration tests with authenticated requests. Test error cases like invalid files, authentication failures, and rate limiting."
          },
          {
            "id": 4,
            "title": "Implement Gradio File Upload Component",
            "description": "Add a file upload component to the chat interface using Gradio's file upload capabilities.",
            "dependencies": [
              3
            ],
            "details": "Extend the existing Gradio chat interface to include a file upload button that supports drag-and-drop. Style the component to match the UI design. Implement client-side validation for file types and sizes. Show upload progress and handle errors gracefully. Update the chat display to show uploaded files with thumbnails for images.",
            "status": "done",
            "testStrategy": "Manual testing of the UI component. Test with various file types. Verify error messages display correctly. Test accessibility compliance."
          },
          {
            "id": 5,
            "title": "Link File Context to Chat Session and Implement Persistence",
            "description": "Ensure uploaded files are properly linked to chat sessions and their context is maintained throughout the conversation.",
            "dependencies": [
              3,
              4
            ],
            "details": "Modify the chat session database schema to track uploaded files. Implement a mechanism to include file content in the conversation context sent to the LLM. For images, include Claude Vision analysis. For text documents, include extracted text. Ensure file context persists across page refreshes and session resumption. Add cleanup job for temporary files.",
            "status": "done",
            "testStrategy": "Test chat continuity with uploaded files. Verify context is maintained after refresh. Test with multiple files in a single chat. Verify file cleanup works correctly."
          }
        ]
      },
      {
        "id": 22,
        "title": "Implement Book Processing and Chapter Detection",
        "description": "Automatically detect chapters and create per-chapter knowledge base entries for uploaded books.",
        "details": "Develop chapter detector using regex and ML (e.g., spaCy or transformers). Create book service for chapter-aware processing. Add Celery task for chapter processing. Update book metadata tables. Add Gradio book upload component.",
        "testStrategy": "Upload books in various formats, verify chapters detected with >90% accuracy. Query by chapter and book-wide.",
        "priority": "high",
        "dependencies": [
          2,
          6,
          14
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Book Format Detection and Parsing",
            "description": "Create a service to detect and parse different book formats (EPUB, PDF) with appropriate extractors for each format.",
            "dependencies": [],
            "details": "Develop a BookFormatDetector class that identifies file types based on headers and extensions. Implement format-specific parsers for EPUB (using ebooklib) and PDF (using PyPDF2 or pdfplumber). Create a unified interface that returns extracted text and basic metadata regardless of the input format. Include error handling for corrupted or unsupported files.",
            "status": "done",
            "testStrategy": "Test with various book formats including EPUB, PDF, and malformed files. Verify correct format detection and text extraction with >95% accuracy."
          },
          {
            "id": 2,
            "title": "Build Table of Contents Parser for Chapter Extraction",
            "description": "Develop a component that extracts chapter information from book table of contents when available.",
            "dependencies": [
              1
            ],
            "details": "Create a TOCParser class that extracts structured chapter information from EPUB navigation and NCX files. For PDFs, implement bookmark extraction and analysis. Map extracted TOC entries to text positions in the document. Handle nested chapter structures and maintain hierarchy information. Store chapter titles, start/end positions, and parent-child relationships.",
            "status": "done",
            "testStrategy": "Test with books containing various TOC structures (flat, nested, missing). Verify chapter boundaries match actual content positions. Compare extracted TOC with manual analysis for accuracy."
          },
          {
            "id": 3,
            "title": "Implement Intelligent Chapter Boundary Detection",
            "description": "Create an ML-based chapter detector for books without clear TOC or when TOC extraction fails.",
            "dependencies": [
              1
            ],
            "details": "Implement regex patterns to identify common chapter headings (e.g., 'Chapter X', 'Section Y'). Use spaCy or transformers to detect semantic section breaks based on content shifts. Analyze page breaks, font changes, and whitespace patterns in PDFs. Combine heuristic and ML approaches with a confidence score. Fall back to fixed-size chunking when confidence is low. Create a ChapterDetector class that integrates with the book processing pipeline.",
            "status": "done",
            "testStrategy": "Test with books lacking TOC. Compare detected chapters with human-annotated boundaries. Measure precision/recall with target of >90% accuracy."
          },
          {
            "id": 4,
            "title": "Implement Chapter-Level Chunking Strategy",
            "description": "Develop a chunking strategy that respects chapter boundaries and creates optimal chunks for knowledge base entries.",
            "dependencies": [
              2,
              3
            ],
            "details": "Create a ChapterChunker class that processes each detected chapter separately. Implement semantic chunking within chapters using sentence and paragraph boundaries. Ensure chunks maintain context and don't split mid-sentence. Add overlap between chunks to improve retrieval quality. Implement metadata tagging to associate chunks with their source chapters. Update the Celery task for document processing to use chapter-aware chunking.",
            "status": "done",
            "testStrategy": "Test chunking with various book types and chapter structures. Verify chunks respect chapter boundaries and maintain semantic coherence. Measure retrieval quality with chunked vs. non-chunked approaches."
          },
          {
            "id": 5,
            "title": "Store Book and Chapter Metadata in Database",
            "description": "Update database schema and services to store book-specific metadata and chapter hierarchy information.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Extend the database schema to store book metadata (ISBN, publisher, edition, author, publication date). Create tables for chapter structure with parent-child relationships. Implement BookMetadataService for CRUD operations on book and chapter metadata. Update the Gradio interface with a dedicated book upload component that displays chapter structure. Integrate with existing knowledge base retrieval to support chapter-specific queries.",
            "status": "done",
            "testStrategy": "Test database operations with various book metadata scenarios. Verify chapter hierarchy is correctly stored and retrieved. Test queries that target specific chapters vs. whole-book searches."
          }
        ]
      },
      {
        "id": 23,
        "title": "Conduct End-to-End Integration and Load Testing",
        "description": "Test all new features together under realistic load and edge cases.",
        "details": "Use pytest, Playwright, and Locust for integration and load testing. Simulate concurrent uploads, queries, and agent routing. Validate against success metrics (latency, accuracy, error rate).",
        "testStrategy": "Run automated test suites and manual exploratory tests. Document and fix any failures.",
        "priority": "high",
        "dependencies": [
          9,
          10,
          13,
          15,
          16,
          18,
          19,
          20,
          21,
          22
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 24,
        "title": "Prepare Production Deployment and Release Notes",
        "description": "Finalize deployment scripts, update release notes, and prepare for production rollout.",
        "details": "Update Render.com deployment configuration. Write release notes summarizing new features, migrations, and breaking changes. Schedule deployment window and notify stakeholders.",
        "testStrategy": "Perform blue/green deployment on production. Monitor for errors and rollback if needed.",
        "priority": "high",
        "dependencies": [
          23
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 25,
        "title": "Post-Deployment Monitoring and Success Metrics Validation",
        "description": "Monitor system post-deployment and validate against defined success metrics.",
        "details": "Use Prometheus and Grafana to monitor uptime, error rate, processing time, and feature-specific metrics. Collect user feedback and bug reports. Prepare post-mortem if metrics are not met.",
        "testStrategy": "Review dashboards and logs for 48 hours post-launch. Confirm all metrics meet targets. Address any incidents immediately.",
        "priority": "high",
        "dependencies": [
          24
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 26,
        "title": "Implement Embedding Generation Service",
        "description": "Create embedding generation service using BGE-M3 (development via Ollama, production via API). Includes batch processing, caching, and regeneration on content updates.",
        "details": "Implement FR-017 from PRD: Development embeddings via BGE-M3 on Ollama (Mac Studio), production via Claude API or hosted endpoint. Batch processing (100 chunks per batch), embedding caching in Supabase, regenerate on content updates. Target latency: <100ms per chunk locally.",
        "testStrategy": "Measure embedding generation latency, verify batch processing works correctly, test cache hit rates, verify embeddings regenerate on content updates.",
        "status": "done",
        "dependencies": [
          2,
          6
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 27,
        "title": "Implement Hybrid Search with BM25 and Vector Fusion",
        "description": "Build hybrid search combining dense vector search (BGE-M3), sparse BM25 keyword matching, ILIKE pattern matching, and fuzzy search with Reciprocal Rank Fusion.",
        "details": "Implement FR-019 from PRD: Dense search (vector similarity), sparse search (BM25), ILIKE pattern matching, fuzzy search (Levenshtein). Combine results using Reciprocal Rank Fusion (RRF). Configurable weights per search type. Target: +40-60% improvement vs vector-only.",
        "testStrategy": "Compare search quality metrics (NDCG, MRR) between hybrid and single-method approaches. Verify RRF fusion produces better results than individual methods.",
        "status": "done",
        "dependencies": [
          2,
          6
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 28,
        "title": "Implement Query Expansion with Claude Haiku",
        "description": "Use Claude Haiku to generate 4-5 query variations for improved retrieval. Execute parallel searches and combine results with RRF.",
        "details": "Implement FR-020 from PRD: Claude Haiku generates query variations with different keywords but same intent. Parallel search with all variations, combine with RRF. Improves recall for ambiguous queries. Target latency: <500ms total.",
        "testStrategy": "Test query expansion with ambiguous queries, measure recall improvement, verify latency target is met, test parallel search execution.",
        "status": "done",
        "dependencies": [
          2,
          6
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 29,
        "title": "Implement Reranking with BGE-Reranker-v2",
        "description": "Add reranking layer using BGE-Reranker-v2 (development via Ollama, production via API) to rerank top 20-30 results and return top 10.",
        "details": "Implement FR-021 from PRD: Development reranking via BGE-Reranker-v2 on Ollama, production via Claude API or hosted reranker. Rerank top 20-30 results, score each (0-1 relevance), return top 10. Target latency: <200ms locally. Expected improvement: +15-25% precision.",
        "testStrategy": "Measure precision improvement with vs without reranking, verify latency targets, test with various query types.",
        "status": "done",
        "dependencies": [
          27
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 30,
        "title": "Implement Semantic Caching with Redis",
        "description": "Build tiered semantic caching using Redis with similarity thresholds for cache hits. Target 60-80% cache hit rate.",
        "details": "Implement FR-025 from PRD: Redis cache for frequent queries, cache embeddings to avoid recomputation, cache search results (5-minute TTL). Semantic cache thresholds: Exact match >0.98, High similarity 0.93-0.97, Medium 0.88-0.92 (fresh search), Low <0.88. Target: 60-80% cache hit rate.",
        "testStrategy": "Measure cache hit rates under various query loads, verify semantic similarity thresholds work correctly, test cache invalidation.",
        "status": "done",
        "dependencies": [
          26,
          27
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 31,
        "title": "Implement Neo4j Knowledge Graph Integration",
        "description": "Integrate Neo4j for entity storage, relationship traversal, and graph-enhanced context retrieval. Includes Cypher generation via Claude Sonnet.",
        "details": "Implement FR-023 from PRD: Store entities and relationships in Neo4j (Community on Mac Studio Docker). Knowledge graph queries for related content, entity-centric search, relationship traversal (2-3 hops). Cypher generation via Claude Sonnet for natural language queries. Graph-enhanced context retrieval.",
        "testStrategy": "Test entity extraction and storage, verify relationship traversal, test Cypher generation accuracy, measure graph query latency.",
        "status": "done",
        "dependencies": [
          2,
          26
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 32,
        "title": "Implement Conversation Memory with Graph Tables",
        "description": "Build production conversation memory using Supabase PostgreSQL graph tables (user_memory_nodes, user_memory_edges) with RLS policies per user.",
        "details": "Implement FR-037 from PRD: Store conversations in PostgreSQL graph tables. user_memory_nodes stores facts, preferences, entities with embeddings. user_memory_edges stores relationships between nodes. Row-level security (RLS) per user. Graph-based retrieval for relevant context. Long-term memory persistence.",
        "testStrategy": "Test memory node creation/retrieval, verify RLS policies isolate user data, test graph traversal for context retrieval, measure memory retrieval latency (<200ms target).",
        "status": "done",
        "dependencies": [
          2,
          10
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 33,
        "title": "Implement Context Management Service",
        "description": "Build context management for retrieving relevant memory nodes, graph traversal (1-2 hops), recency/access weighting, and context window management (4K tokens).",
        "details": "Implement FR-038 from PRD: Retrieve relevant memory nodes for query context. Graph traversal for related memories (1-2 hops). Recency-weighted retrieval (recent > old). Access-count-weighted retrieval (frequent > rare). Context window: 4K tokens (last 5 messages + memory). Target: >90% context relevance.",
        "testStrategy": "Test context retrieval relevance, verify weighting algorithms work correctly, test context window limits are respected, measure context relevance score.",
        "status": "in-progress",
        "dependencies": [
          32
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 34,
        "title": "Implement User Preferences Learning and Storage",
        "description": "Build system to learn user preferences from interactions, store as high-confidence memory nodes, and apply to search result boosting/filtering.",
        "details": "Implement FR-039 from PRD: Learn user preferences from interactions. Store as memory nodes with high confidence scores. Apply preferences to search results (boost/filter). Explicit preference setting via UI controls. Privacy controls (opt-out of memory).",
        "testStrategy": "Test preference extraction from conversations, verify preferences affect search results, test explicit preference setting, verify opt-out functionality.",
        "status": "pending",
        "dependencies": [
          32
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 35,
        "title": "Implement Session Management for Chat",
        "description": "Build session management supporting multiple concurrent sessions, persistence, timeout (30 min idle), clear, and export functionality.",
        "details": "Implement FR-040 from PRD: Multiple concurrent sessions per user. Session persistence (reload conversation). Session timeout (30 minutes idle). Clear session (delete history). Export conversation history (JSON, Markdown formats).",
        "testStrategy": "Test multiple concurrent sessions, verify session persistence across page reloads, test timeout behavior, verify export generates valid JSON/Markdown.",
        "status": "pending",
        "dependencies": [
          10,
          32
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 36,
        "title": "Implement Role-Based Access Control (RBAC)",
        "description": "Build RBAC system with user roles (admin, editor, viewer, guest), permission-based access, API key management, and audit logging.",
        "details": "Implement FR-047 from PRD: User roles (admin, editor, viewer, guest). Permission-based access (create, read, update, delete). Resource-level permissions (own documents vs all). API key management (create, rotate, revoke). Audit logs for admin actions. Database tables: users, api_keys, audit_logs with RLS policies.",
        "testStrategy": "Test role-based access restrictions, verify permission enforcement, test API key lifecycle, verify audit logs capture all admin actions.",
        "status": "pending",
        "dependencies": [
          2,
          6
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 37,
        "title": "Implement Document Management Admin Tools",
        "description": "Build admin tools for bulk document operations: upload (100 files), delete, reprocess embeddings, metadata update, versioning, and approval workflow.",
        "details": "Implement FR-048 from PRD: Bulk document upload (up to 100 files). Bulk delete with confirmation. Bulk reprocessing (regenerate embeddings). Bulk metadata update. Document versioning (track changes). Document approval workflow (draft → review → published).",
        "testStrategy": "Test bulk operations with large file sets, verify versioning tracks changes, test approval workflow transitions, measure bulk operation throughput (>100 docs/minute target).",
        "status": "pending",
        "dependencies": [
          36
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 38,
        "title": "Implement User Management Admin Interface",
        "description": "Build admin interface for user CRUD operations, role assignment, password reset, account suspension, activity logs, and GDPR data export.",
        "details": "Implement FR-049 from PRD: Create/edit/delete users (admin only). Assign roles and permissions. Reset passwords. Suspend/activate accounts. View user activity logs. Export user data (GDPR compliance).",
        "testStrategy": "Test all user CRUD operations, verify role assignment affects permissions, test GDPR export contains all user data, verify activity logs are comprehensive.",
        "status": "pending",
        "dependencies": [
          36
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 39,
        "title": "Implement Analytics Dashboard for Admin",
        "description": "Build admin analytics dashboard showing document stats, processing rates, top queries, active users, storage usage, and API usage metrics.",
        "details": "Implement FR-052 from PRD: Total documents by type. Documents by department. Processing success/failure rates. Top queries (last 7 days). Active users (last 30 days). Storage usage (PostgreSQL, B2). API usage by endpoint.",
        "testStrategy": "Verify dashboard metrics match database queries, test real-time updates, verify data accuracy across all metrics.",
        "status": "pending",
        "dependencies": [
          4,
          36
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 40,
        "title": "Implement CrewAI Multi-Agent Foundation Infrastructure",
        "description": "Set up core CrewAI infrastructure including agent base class, inter-agent communication, state management, database schema, and Render service integration.",
        "details": "Implement FR-053 from PRD: CrewAI framework on Render (already deployed at jb-crewai.onrender.com). Multi-agent orchestration for complex workflows. Task delegation and result aggregation. Agent collaboration. Long-running async tasks via Celery. Database tables: crewai_agents, crewai_crews, crewai_executions, crewai_task_executions.",
        "testStrategy": "Test agent registration and lifecycle, verify inter-agent communication, test async task execution, verify database schema supports all agent operations.",
        "status": "done",
        "dependencies": [
          17
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 41,
        "title": "Implement Main Orchestrator Agent (AGENT-001)",
        "description": "Build Master Content Analyzer & Asset Orchestrator that analyzes content, classifies departments, selects asset types, and delegates to specialized agents.",
        "details": "AGENT-001 from PRD: Role=Master Content Analyzer & Asset Orchestrator. Goal=Analyze content for department classification (10 depts), asset type selection (skill/command/agent/prompt/workflow), summary requirements. Tools: document_search, pattern_analyzer, department_classifier. LLM: Claude Sonnet 4.5. Allow Delegation: Yes.",
        "testStrategy": "Test department classification accuracy (>96% target), verify asset type decision logic, test delegation to appropriate agents.",
        "status": "done",
        "dependencies": [
          40
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 42,
        "title": "Implement Content Summarizer Agent (AGENT-002)",
        "description": "Build PDF summary generator with visual diagrams, flowcharts, key concepts, implementation guides, and quick reference sections.",
        "details": "AGENT-002 from PRD: Role=Content Summary & Visualization Expert. Goal=Generate comprehensive PDF summaries with visuals. Output to processed/crewai-summaries/{department}/. Tools: pdf_generator, diagram_creator, chart_builder. LLM: Claude Sonnet 4.5.",
        "testStrategy": "Verify PDF generation with proper formatting, test visual diagram creation, verify output file naming follows pattern.",
        "status": "done",
        "dependencies": [
          40
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Extract and preprocess PDF content",
            "description": "Develop functionality to extract text and relevant data from PDF documents and preprocess it for summarization and visualization.",
            "dependencies": [],
            "details": "Implement PDF parsing using appropriate libraries to extract text, images, and metadata. Clean and segment the extracted content into manageable chunks suitable for input to the LLM and visualization tools.",
            "status": "done",
            "testStrategy": "Test extraction accuracy on various PDF formats and verify preprocessing outputs correct text chunks and metadata."
          },
          {
            "id": 2,
            "title": "Generate textual summary using Claude Sonnet 4.5 LLM",
            "description": "Integrate Claude Sonnet 4.5 to generate comprehensive textual summaries including key concepts and implementation guides from the preprocessed PDF content.",
            "dependencies": [
              1
            ],
            "details": "Design prompts and workflows to feed preprocessed text chunks into Claude Sonnet 4.5, ensuring summaries cover key concepts and implementation details as specified in the PRD.",
            "status": "done",
            "testStrategy": "Validate summary completeness and relevance by comparing generated summaries against source documents and PRD requirements."
          },
          {
            "id": 3,
            "title": "Create visual diagrams, flowcharts, and charts",
            "description": "Use diagram_creator and chart_builder tools to generate visual aids such as diagrams and flowcharts that complement the textual summary.",
            "dependencies": [
              1
            ],
            "details": "Analyze summarized content to identify sections suitable for visualization. Programmatically generate diagrams and charts that illustrate workflows, concepts, or data referenced in the summary.",
            "status": "done",
            "testStrategy": "Verify visual outputs are accurate, clear, and correspond correctly to the summarized content sections."
          },
          {
            "id": 4,
            "title": "Assemble and format the PDF summary document",
            "description": "Combine textual summaries and visual elements into a well-formatted PDF document using pdf_generator, adhering to output path conventions.",
            "dependencies": [
              2,
              3
            ],
            "details": "Design PDF layout templates to include sections for key concepts, implementation guides, quick references, and visuals. Implement generation logic to produce final PDF files saved to processed/crewai-summaries/{department}/.",
            "status": "done",
            "testStrategy": "Check PDF formatting, section ordering, and file naming conventions. Confirm all content and visuals are correctly embedded."
          },
          {
            "id": 5,
            "title": "Implement validation and testing of PDF summary generator",
            "description": "Develop and execute tests to verify PDF generation quality, visual diagram creation, and output file correctness as per test strategy.",
            "dependencies": [
              4
            ],
            "details": "Create automated and manual test cases to validate formatting, content accuracy, visual correctness, and output path compliance. Include edge cases and performance benchmarks.",
            "status": "done",
            "testStrategy": "Run comprehensive tests covering PDF content integrity, visual element accuracy, and adherence to naming and storage conventions."
          }
        ]
      },
      {
        "id": 43,
        "title": "Implement Asset Generator Agents (AGENT-003 to AGENT-007)",
        "description": "Build 5 asset generation agents: Skill Generator (YAML), Command Generator (Markdown), Agent Generator (CrewAI YAML), Prompt Generator, and Workflow Generator (n8n JSON).",
        "details": "AGENTS 003-007 from PRD: AGENT-003=Skill Generator (YAML for Claude Code), AGENT-004=Command Generator (Markdown slash commands), AGENT-005=Agent Generator (CrewAI configs), AGENT-006=Prompt Generator (reusable templates), AGENT-007=Workflow Generator (n8n JSON). Each outputs to processed/crewai-suggestions/{asset-type}/drafts/.",
        "testStrategy": "Test each asset type generation, verify output format matches specification, test asset type decision logic from Main Orchestrator.",
        "status": "pending",
        "dependencies": [
          40,
          41
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 44,
        "title": "Implement Department Classifier Agent (AGENT-008)",
        "description": "Build specialized agent for accurate 10-department classification with confidence scores using keyword extraction and content analysis.",
        "details": "AGENT-008 from PRD: Role=Department Classification Specialist. Departments: it-engineering, sales-marketing, customer-support, operations-hr-supply, finance-accounting, project-management, real-estate, private-equity-ma, consulting, personal-continuing-ed. Tools: file_reader, department_classifier, keyword_extractor. LLM: Claude Haiku (fast). Output: Department + confidence score (0-1).",
        "testStrategy": "Test classification accuracy across all 10 departments (>96% target), verify confidence scores correlate with actual accuracy.",
        "status": "done",
        "dependencies": [
          40
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Department Classification Requirements and Data Preparation",
            "description": "Specify detailed classification criteria for each of the 10 departments and prepare the dataset for training and testing, ensuring data quality and relevance.",
            "dependencies": [],
            "details": "Gather representative documents and content samples for each department: it-engineering, sales-marketing, customer-support, operations-hr-supply, finance-accounting, project-management, real-estate, private-equity-ma, consulting, personal-continuing-ed. Clean and preprocess data for keyword extraction and content analysis. Define expected output format including department labels and confidence score range (0-1).",
            "status": "done",
            "testStrategy": "Verify dataset completeness and balance across departments; validate data preprocessing correctness."
          },
          {
            "id": 2,
            "title": "Develop Keyword Extraction Module",
            "description": "Implement and integrate a keyword extraction tool to identify relevant keywords from input documents to aid department classification.",
            "dependencies": [
              1
            ],
            "details": "Use or adapt the existing keyword_extractor tool to extract meaningful keywords from document content. Tune extraction parameters to maximize relevance for department-specific terms. Ensure output is compatible with downstream classification module.",
            "status": "done",
            "testStrategy": "Test keyword extraction accuracy and relevance on sample documents; compare extracted keywords against expected department-specific terms."
          },
          {
            "id": 3,
            "title": "Implement Department Classification Logic Using LLM and Content Analysis",
            "description": "Build the core classification agent leveraging Claude Haiku LLM and content analysis to assign one of the 10 departments with a confidence score.",
            "dependencies": [
              1,
              2
            ],
            "details": "Integrate file_reader, keyword_extractor, and department_classifier tools. Use extracted keywords and content features as input to Claude Haiku LLM for classification. Implement logic to generate confidence scores between 0 and 1 reflecting classification certainty. Ensure the agent outputs department label plus confidence score.",
            "status": "done",
            "testStrategy": "Evaluate classification accuracy on validation set aiming for >96% accuracy; verify confidence scores correlate with actual correctness."
          },
          {
            "id": 4,
            "title": "Integrate and Test Department Classifier Agent Workflow",
            "description": "Combine all components into a cohesive agent workflow and perform end-to-end testing to ensure correct operation and output consistency.",
            "dependencies": [
              3
            ],
            "details": "Develop orchestration code to read input files, extract keywords, classify department, and output results. Handle error cases and edge inputs gracefully. Validate that the agent meets performance and latency requirements. Prepare for deployment environment integration.",
            "status": "done",
            "testStrategy": "Conduct integration testing with diverse document inputs; verify output format and confidence scores; measure response times and error handling."
          },
          {
            "id": 5,
            "title": "Validate Classification Accuracy and Confidence Score Calibration",
            "description": "Perform comprehensive testing and calibration to ensure classification accuracy exceeds 96% and confidence scores reliably indicate prediction certainty.",
            "dependencies": [
              4
            ],
            "details": "Run extensive tests across all 10 departments using test datasets. Analyze misclassifications and adjust model or thresholds as needed. Calibrate confidence scores to reflect true probabilities. Document test results and prepare for production deployment.",
            "status": "done",
            "testStrategy": "Use confusion matrices and ROC curves to assess accuracy; perform reliability diagrams for confidence calibration; confirm target metrics are met before release."
          }
        ]
      },
      {
        "id": 45,
        "title": "Implement Document Analysis Agents (AGENT-009 to AGENT-011)",
        "description": "Build 3 document analysis agents: Research Analyst, Content Strategist, and Fact Checker for comprehensive document analysis workflows.",
        "details": "AGENTS 009-011 from PRD: AGENT-009=Senior Research Analyst (extract topics, entities, facts, quality assessment), AGENT-010=Content Strategist (executive summary, findings, recommendations), AGENT-011=Fact Checker (verify claims, confidence scores, citations). All use Claude Sonnet 4.5 with varying temperatures.",
        "testStrategy": "Test sequential workflow between agents, verify output quality, test fact verification accuracy.",
        "status": "done",
        "dependencies": [
          40
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Define Specifications for Each Document Analysis Agent",
            "description": "Define detailed functional specifications and input/output requirements for AGENT-009 (Senior Research Analyst), AGENT-010 (Content Strategist), and AGENT-011 (Fact Checker) based on the PRD and project context.",
            "dependencies": [],
            "details": "Review PRD for AGENTS 009-011 to specify exact extraction tasks for Research Analyst (topics, entities, facts, quality assessment), summary and recommendations for Content Strategist, and claim verification with confidence scoring for Fact Checker. Define input document formats, expected outputs, and temperature settings for Claude Sonnet 4.5.",
            "status": "done",
            "testStrategy": "Validate specifications with stakeholders and ensure clarity for implementation."
          },
          {
            "id": 2,
            "title": "Implement Senior Research Analyst Agent (AGENT-009)",
            "description": "Develop the Research Analyst agent to extract topics, entities, facts, and perform quality assessment from documents using Claude Sonnet 4.5.",
            "dependencies": [
              1
            ],
            "details": "Build the agent logic to process input documents, extract relevant information (topics, entities, facts), and assess document quality. Integrate Claude Sonnet 4.5 API with appropriate temperature settings. Ensure output format matches specification.",
            "status": "done",
            "testStrategy": "Test extraction accuracy on diverse document samples and verify quality assessment outputs."
          },
          {
            "id": 3,
            "title": "Implement Content Strategist Agent (AGENT-010)",
            "description": "Develop the Content Strategist agent to generate executive summaries, findings, and recommendations from analyzed documents.",
            "dependencies": [
              1
            ],
            "details": "Use Claude Sonnet 4.5 with configured temperature to generate concise executive summaries, highlight key findings, and formulate actionable recommendations based on input from Research Analyst outputs or raw documents.",
            "status": "done",
            "testStrategy": "Evaluate summary coherence, relevance of findings, and appropriateness of recommendations against benchmark documents."
          },
          {
            "id": 4,
            "title": "Implement Fact Checker Agent (AGENT-011)",
            "description": "Develop the Fact Checker agent to verify claims within documents, assign confidence scores, and provide citations for verification.",
            "dependencies": [
              1
            ],
            "details": "Integrate Claude Sonnet 4.5 to cross-check extracted claims against trusted sources or internal knowledge bases. Implement confidence scoring mechanism and citation generation. Ensure output includes verification status and references.",
            "status": "done",
            "testStrategy": "Validate fact verification accuracy, confidence scoring consistency, and citation correctness on test datasets."
          },
          {
            "id": 5,
            "title": "Integrate and Test Sequential Workflow of Document Analysis Agents",
            "description": "Integrate AGENT-009, AGENT-010, and AGENT-011 into a cohesive workflow and perform end-to-end testing to ensure quality and accuracy of document analysis.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Develop orchestration logic to pass outputs sequentially between agents: Research Analyst output feeds Content Strategist and Fact Checker. Test data flow, error handling, and output aggregation. Measure overall system performance and accuracy.",
            "status": "done",
            "testStrategy": "Conduct sequential workflow tests, verify output quality at each stage, and assess fact verification accuracy in integrated environment."
          }
        ]
      },
      {
        "id": 46,
        "title": "Implement Multi-Agent Orchestration Agents (AGENT-012 to AGENT-015)",
        "description": "Build 4 orchestration agents: Research Agent, Analysis Agent, Writing Agent, and Review Agent for complex multi-agent workflows.",
        "details": "AGENTS 012-015 from PRD (FR-012): AGENT-012=Research Agent (web/academic search), AGENT-013=Analysis Agent (pattern detection, statistics), AGENT-014=Writing Agent (reports, documentation), AGENT-015=Review Agent (quality assurance, consistency). Sequential execution for complex analysis tasks.",
        "testStrategy": "Test multi-agent workflow execution, verify agent collaboration, test result aggregation and quality review.",
        "status": "done",
        "dependencies": [
          40
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Research Agent (AGENT-012)",
            "description": "Build the Research Agent (AGENT-012) for web and academic search capabilities within the multi-agent workflow system.",
            "dependencies": [
              40
            ],
            "details": "Implement AGENT-012 from PRD (FR-012) with the following features: web search integration (Google/Bing APIs), academic database access (arXiv, PubMed), query formulation, source credibility assessment, information extraction, and structured data storage. Integrate with CrewAI foundation infrastructure for agent communication.",
            "status": "done",
            "testStrategy": "Test web search accuracy, academic database retrieval, query reformulation capabilities, and integration with other agents. Verify information extraction quality and structured output format."
          },
          {
            "id": 2,
            "title": "Implement Analysis Agent (AGENT-013)",
            "description": "Build the Analysis Agent (AGENT-013) for pattern detection and statistical analysis of data collected by the Research Agent.",
            "dependencies": [
              1,
              40
            ],
            "details": "Implement AGENT-013 from PRD (FR-012) with capabilities for: pattern recognition in textual/numerical data, statistical analysis (descriptive stats, correlation, regression), data visualization preparation, anomaly detection, and insight generation. Must accept structured data from Research Agent and prepare outputs for Writing Agent.",
            "status": "done",
            "testStrategy": "Test pattern detection accuracy, statistical analysis correctness, data transformation capabilities, and integration with Research and Writing agents. Verify performance with various data types and volumes."
          },
          {
            "id": 3,
            "title": "Implement Writing Agent (AGENT-014)",
            "description": "Build the Writing Agent (AGENT-014) for generating reports and documentation based on analysis results.",
            "dependencies": [
              2,
              40
            ],
            "details": "Implement AGENT-014 from PRD (FR-012) with capabilities for: report generation in multiple formats (text, markdown, HTML), section structuring, data visualization integration, citation management, terminology consistency, and style adaptation. Must consume Analysis Agent outputs and produce draft documents for Review Agent.",
            "status": "done",
            "testStrategy": "Test document generation quality, format consistency, citation accuracy, and integration with Analysis and Review agents. Verify adaptation to different writing styles and document types."
          },
          {
            "id": 4,
            "title": "Implement Review Agent (AGENT-015)",
            "description": "Build the Review Agent (AGENT-015) for quality assurance and consistency checking of documents produced by the Writing Agent.",
            "dependencies": [
              3,
              40
            ],
            "details": "Implement AGENT-015 from PRD (FR-012) with capabilities for: grammar/spelling verification, fact-checking against Research Agent data, consistency validation, citation verification, readability assessment, and improvement suggestions. Must provide feedback loop to Writing Agent and final approval mechanism.",
            "status": "done",
            "testStrategy": "Test error detection accuracy, fact-checking reliability, consistency validation, and integration with Writing Agent. Verify feedback loop functionality and document improvement metrics."
          },
          {
            "id": 5,
            "title": "Implement Multi-Agent Workflow Orchestration",
            "description": "Create the orchestration layer to manage sequential execution and data flow between all four agents (AGENT-012 to AGENT-015).",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Develop workflow orchestration system to coordinate the sequential execution of Research → Analysis → Writing → Review agents. Implement data transformation between agent outputs/inputs, progress tracking, error handling/recovery, parallel execution where possible, and workflow status reporting. Create unified API endpoint for initiating multi-agent workflows.",
            "status": "done",
            "testStrategy": "Test end-to-end workflow execution, agent transition handling, error recovery mechanisms, and overall system performance. Verify data integrity throughout the pipeline and measure total execution time for complex analysis tasks."
          }
        ]
      },
      {
        "id": 47,
        "title": "Implement CrewAI Asset Storage and B2 Integration",
        "description": "Build asset storage system for CrewAI outputs to B2 with department-based organization, metadata tracking, and URL signing.",
        "details": "B2 folder structure from PRD: processed/crewai-summaries/{10-departments}/, processed/crewai-suggestions/{asset-type}/drafts/ (claude-skills, claude-commands, agents, prompts, workflows). Database table: crewai_generated_assets with asset_type, department, b2_path, confidence_score.",
        "testStrategy": "Test asset storage to correct B2 paths, verify metadata tracking, test URL signing for asset retrieval.",
        "status": "pending",
        "dependencies": [
          40,
          43
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-11-24T19:36:45.295Z",
      "updated": "2025-12-27T23:07:34.877Z",
      "description": "Tasks for v7_3_features context"
    }
  }
}