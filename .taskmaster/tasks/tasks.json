{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Backend Environment Setup (FastAPI, Celery, Supabase, Redis, Neo4j)",
        "description": "Establish the production backend infrastructure using FastAPI, Celery, Supabase PostgreSQL (with pgvector), Redis (Upstash), and Neo4j Community (Docker).",
        "details": "Provision Render services for FastAPI and Celery. Configure Supabase PostgreSQL with pgvector and graph tables. Set up Redis (Upstash) for caching and Celery broker. Deploy Neo4j Community via Docker on Mac Studio for knowledge graph storage. Ensure all services use TLS 1.3 and encrypted environment variables. Recommended versions: FastAPI >=0.110, Celery >=5.3, supabase-py >=2.0, redis-py >=5.0, Neo4j Community 5.x.",
        "testStrategy": "Validate service connectivity, health endpoints, and database schema migrations. Run integration tests for API endpoints and Celery task execution. Confirm Redis and Neo4j connectivity.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Provision FastAPI and Celery Services on Render",
            "description": "Deploy FastAPI and Celery worker services using Render, ensuring production-grade configuration and separation.",
            "dependencies": [],
            "details": "Set up two separate Render services: one for FastAPI (API server) and one for Celery (background worker). Use recommended versions (FastAPI >=0.110, Celery >=5.3). Configure environment variables securely and ensure both services are reachable over the network.",
            "status": "done",
            "testStrategy": "Verify service deployment via Render dashboards. Access FastAPI health endpoint and confirm Celery worker logs show successful startup.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Configure Supabase PostgreSQL with pgvector Extension and Graph Tables",
            "description": "Set up Supabase PostgreSQL instance, enable pgvector, and create required tables for embeddings and graph data.",
            "dependencies": [],
            "details": "In Supabase dashboard, enable the 'vector' extension (pgvector) via Extensions panel or SQL command. Create tables for storing embeddings (e.g., documents with embedding vector columns) and graph structures as needed. Use recommended supabase-py >=2.0 for client access.\n<info added on 2025-11-03T04:12:52.520Z>\nSupabase PostgreSQL is already provisioned at qohsmuevxuetjpuherzo.supabase.co with credentials stored in the .env file. The database is accessible via Supabase Management Console Panel (MCP). \n\nTo complete this subtask:\n\n1. Connect to the Supabase PostgreSQL instance using the MCP or SQL editor.\n\n2. Enable the pgvector extension by executing:\n   ```sql\n   CREATE EXTENSION IF NOT EXISTS vector;\n   ```\n\n3. Create all 37+ required tables from /workflows/database_setup.md, including:\n   - documents\n   - document_chunks\n   - chat_sessions\n   - user_memory_nodes\n   - crewai_agents\n   - crewai_crews\n   - vector tables with embedding columns\n   - graph structure tables\n   - and all other tables specified in the database setup file\n\n4. Verify table creation and ensure proper relationships and constraints are established according to the schema definitions.\n\n5. Test database connectivity using supabase-py >=2.0 client from the application.\n</info added on 2025-11-03T04:12:52.520Z>",
            "status": "done",
            "testStrategy": "Run SQL queries to confirm pgvector is enabled and tables exist. Insert and retrieve sample data, including vector columns.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Set Up Redis (Upstash) for Caching and Celery Broker",
            "description": "Provision a Redis instance on Upstash and configure it for both caching and as the Celery message broker.",
            "dependencies": [],
            "details": "Create a new Redis database on Upstash. Obtain connection URL and credentials. Configure FastAPI and Celery to use this Redis instance for caching and as the Celery broker. Use redis-py >=5.0 for integration.",
            "status": "done",
            "testStrategy": "Connect to Redis from both FastAPI and Celery. Set and retrieve cache keys. Confirm Celery can enqueue and process tasks using Redis broker.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Deploy Neo4j Community Edition via Docker on Mac Studio",
            "description": "Install and run Neo4j Community Edition (5.x) using Docker on the Mac Studio for knowledge graph storage.",
            "dependencies": [],
            "details": "Pull the official Neo4j Community Docker image (version 5.x). Configure Docker container with appropriate ports, volumes for data persistence, and secure environment variables. Ensure Neo4j is accessible from the local network.",
            "status": "done",
            "testStrategy": "Access Neo4j Browser UI, run basic Cypher queries, and verify data persistence after container restart.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Configure TLS 1.3 and Encrypted Environment Variables for All Services",
            "description": "Ensure all backend services (FastAPI, Celery, Supabase, Redis, Neo4j) use TLS 1.3 for secure communication and store environment variables encrypted.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Update service configurations to enforce TLS 1.3 (e.g., Render custom domains with TLS, Upstash Redis with TLS, Supabase with SSL, Neo4j Docker with TLS certificates). Store all secrets and environment variables using encrypted storage mechanisms provided by each platform.",
            "status": "done",
            "testStrategy": "Attempt connections using only TLS 1.3. Inspect certificates and verify environment variables are not exposed in logs or process listings.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Validate Integration and Connectivity Across All Services",
            "description": "Test and confirm that FastAPI, Celery, Supabase, Redis, and Neo4j are correctly integrated and can communicate securely.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Implement health checks and integration tests: FastAPI connects to Supabase and Neo4j, Celery tasks use Redis broker and access Supabase, all over TLS. Run end-to-end tests for API endpoints and background tasks.",
            "status": "done",
            "testStrategy": "Run automated integration tests. Check logs for successful connections. Use tools like curl or Postman to verify TLS and endpoint health.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down the backend environment setup into subtasks for provisioning each service (FastAPI, Celery, Supabase PostgreSQL with pgvector, Redis, Neo4j), configuring secure communication (TLS 1.3), and validating integration and connectivity."
      },
      {
        "id": 2,
        "title": "File Upload Interface & Backblaze B2 Integration",
        "description": "Implement multi-file upload (up to 10 files, 100MB each) with drag-and-drop UI, progress indicators, and direct upload to Backblaze B2 pending/courses/ folder.",
        "details": "Use Gradio or Streamlit for the web UI. Integrate Backblaze B2 via b2sdk (Python >=1.20). Support Mountain Duck polling (30s) and immediate processing for web UI uploads. Enforce file size/type limits and progress feedback. Organize files per B2 folder structure.",
        "testStrategy": "Upload various file types and sizes, verify progress indicators, and confirm files appear in B2 pending/courses/. Test both Mountain Duck and web UI flows.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Implement Multi-File Upload UI with Drag-and-Drop",
            "description": "Create a user interface using Streamlit or Gradio that supports uploading up to 10 files (max 100MB each) via drag-and-drop, with progress indicators and file type/size validation.",
            "dependencies": [],
            "details": "Use Streamlit's st.file_uploader with accept_multiple_files=True or Gradio's file upload component. Implement drag-and-drop functionality, enforce file type and size limits, and display progress indicators for each file. Ensure the UI is intuitive and provides feedback on upload status and errors.",
            "status": "done",
            "testStrategy": "Upload various file types and sizes, verify drag-and-drop works, progress indicators display correctly, and validation prevents unsupported files.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Backblaze B2 Direct Upload via b2sdk",
            "description": "Connect the file upload UI to Backblaze B2 using b2sdk (Python >=1.20), enabling direct upload of files to the pending/courses/ folder and organizing files per B2 folder structure.",
            "dependencies": [
              1
            ],
            "details": "Configure b2sdk for authentication and folder management. Implement logic to upload files directly from the UI to the pending/courses/ folder, ensuring files are organized according to the required B2 structure. Handle upload errors and provide feedback to the user.",
            "status": "done",
            "testStrategy": "Upload files through the UI and confirm they appear in the correct B2 folder. Test error handling and folder organization.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Mountain Duck Polling and Immediate Processing Logic",
            "description": "Support file uploads via Mountain Duck by polling the local folder every 30 seconds and trigger immediate processing for files uploaded via the web UI.",
            "dependencies": [
              2
            ],
            "details": "Set up a polling mechanism to detect new files in the local folder synced by Mountain Duck every 30 seconds. For files uploaded via the web UI, initiate processing immediately after upload. Ensure both flows enforce file limits and integrate with the B2 upload logic.",
            "status": "done",
            "testStrategy": "Simulate uploads via Mountain Duck and web UI, verify polling detects new files, immediate processing works, and all files are uploaded to B2 with correct feedback.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on file upload interface & backblaze b2 integration."
      },
      {
        "id": 3,
        "title": "File Format Validation & Security Scanning",
        "description": "Validate file formats, check integrity, scan for malware, and enforce MIME/extension rules before upload.",
        "details": "Use python-magic for MIME detection, validate extensions, and run integrity checks (e.g., PDF header validation). Integrate ClamAV (clamd) for malware scanning. Reject unsupported formats with clear error messages.",
        "testStrategy": "Attempt uploads of valid, corrupted, and malicious files. Confirm correct rejection and error messaging. Validate security scan logs.",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement File Format and MIME Type Validation",
            "description": "Detect and validate the file's MIME type and extension before upload using python-magic and extension checks.",
            "dependencies": [],
            "details": "Use the python-magic library to inspect the file's magic number and determine its true MIME type. Cross-check this with the file extension to ensure consistency. Reject files with mismatched or unsupported MIME types/extensions, and provide clear error messages. Consider using additional libraries like file-validator for comprehensive checks if needed.",
            "status": "done",
            "testStrategy": "Attempt uploads with valid and invalid file types and extensions. Confirm correct acceptance or rejection and error messaging.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Perform File Integrity and Header Validation",
            "description": "Check file integrity and validate headers for supported formats (e.g., PDF, images) to ensure files are not corrupted or malformed.",
            "dependencies": [
              1
            ],
            "details": "For each supported file type, implement header validation (e.g., check PDF header for '%PDF', image headers for magic numbers). Reject files that fail integrity or header checks. Ensure that only structurally valid files proceed to the next stage.",
            "status": "done",
            "testStrategy": "Upload corrupted or partially valid files and verify that they are rejected with appropriate error messages.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Malware Scanning with ClamAV",
            "description": "Scan validated files for malware using ClamAV (clamd) before final acceptance.",
            "dependencies": [
              2
            ],
            "details": "After passing format and integrity checks, submit files to ClamAV for malware scanning. Reject any files flagged as malicious and log the incident. Ensure the scanning process is efficient and does not introduce significant upload latency.",
            "status": "done",
            "testStrategy": "Upload files containing known malware signatures and verify detection, rejection, and logging. Confirm clean files are accepted.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on file format validation & security scanning."
      },
      {
        "id": 4,
        "title": "Metadata Extraction & Supabase Storage",
        "description": "Extract basic and advanced metadata (filename, size, type, timestamps, EXIF, audio/video info) and store in Supabase documents table.",
        "details": "Use Python libraries: exifread for images, mutagen for audio/video, python-docx for DOCX metadata. Store extracted metadata in Supabase documents table as per schema. Ensure upload triggers metadata extraction.",
        "testStrategy": "Upload files of each supported type, verify metadata extraction accuracy, and confirm correct Supabase storage.",
        "priority": "high",
        "dependencies": [
          "3"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Metadata Extraction for Supported File Types",
            "description": "Develop Python functions to extract basic and advanced metadata from images, audio/video, DOCX, and PDF files using appropriate libraries.",
            "dependencies": [],
            "details": "Use exifread for image EXIF data, mutagen for audio/video metadata, python-docx for DOCX files, and PyPDF2 or pdfminer.six for PDF metadata extraction. Ensure extraction covers filename, size, type, timestamps, and relevant advanced fields (EXIF, audio/video info, document properties). Structure output as per Supabase schema requirements.",
            "status": "done",
            "testStrategy": "Unit test each extractor with sample files of each type. Validate that all required metadata fields are present and accurate.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Metadata Extraction with File Upload Workflow",
            "description": "Ensure that metadata extraction is automatically triggered upon file upload and that extracted data is prepared for storage.",
            "dependencies": [
              1
            ],
            "details": "Modify the upload handler to invoke the correct extraction function based on file type immediately after upload. Collect and format extracted metadata into a dictionary/object matching the Supabase documents table schema.\n<info added on 2025-11-05T22:11:51.333Z>\nImplementation completed for metadata extraction integration with upload workflow:\n\n1. Added metadata_extractor import to upload.py\n2. Modified upload flow to:\n   - Create temp file if not already created (for virus scanning)\n   - Extract metadata from temp file using MetadataExtractor\n   - Include extracted metadata in upload results\n3. Installed required libraries: exifread 3.5.1 and mutagen 1.47.0\n4. Metadata extraction happens after validation and virus scanning, before B2 upload\n5. Graceful error handling - if extraction fails, error is logged but upload continues\n6. Metadata is included in JSON response under \"metadata\" key for each uploaded file\n</info added on 2025-11-05T22:11:51.333Z>",
            "status": "done",
            "testStrategy": "Simulate file uploads via the interface and verify that metadata extraction is triggered and output is correctly formatted for storage.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Store Extracted Metadata in Supabase Documents Table",
            "description": "Insert the extracted metadata into the Supabase documents table, ensuring schema compliance and error handling.",
            "dependencies": [
              2
            ],
            "details": "Use the Supabase Python client to insert metadata records into the documents table. Implement error handling for failed inserts and log issues for debugging. Confirm that all required fields are populated and that the data matches the schema.\n<info added on 2025-11-05T22:21:11.569Z>\nSuccessfully implemented the SupabaseStorage class in app/services/supabase_storage.py with methods for managing document metadata: store_document_metadata(), get_document_by_file_id(), update_document_status(), and list_documents(). The implementation has been integrated into the upload workflow immediately after the B2 upload process. The API response now includes a \"supabase_stored\" boolean flag to indicate successful metadata storage. The system gracefully degrades if Supabase is not configured, allowing the application to function without interruption. Testing confirms that the complete upload workflow functions as expected - metadata extraction works perfectly and Supabase storage attempts are handled gracefully, returning false if not configured.\n</info added on 2025-11-05T22:21:11.569Z>",
            "status": "done",
            "testStrategy": "Upload files of each supported type, then query the Supabase documents table to verify that metadata is stored correctly and completely.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on metadata extraction & supabase storage."
      },
      {
        "id": 5,
        "title": "Duplicate Detection (SHA-256 & Fuzzy Matching)",
        "description": "Detect duplicate and near-duplicate files using SHA-256 hashes and optional fuzzy matching.",
        "details": "Compute SHA-256 hash for each file and check against Supabase documents table. Implement fuzzy matching using Levenshtein distance for filenames and content (rapidfuzz >=2.0). Provide skip/overwrite options.",
        "testStrategy": "Upload duplicate and near-duplicate files, verify detection and user options. Confirm deduplication accuracy.",
        "priority": "high",
        "dependencies": [
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement SHA-256 Hash-Based Duplicate Detection",
            "description": "Compute SHA-256 hashes for each file and compare against existing hashes in the Supabase documents table to identify exact duplicates.",
            "dependencies": [],
            "details": "Use a reliable hashing library to generate SHA-256 hashes for all files. Query the Supabase documents table for existing hashes and flag files with matching hashes as duplicates. Ensure efficient scanning and parallel processing for large file sets.",
            "status": "done",
            "testStrategy": "Upload files with identical content and verify that duplicates are detected solely by hash comparison. Confirm that files with different content are not flagged as duplicates.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Fuzzy Matching for Near-Duplicate Detection",
            "description": "Apply fuzzy matching algorithms (Levenshtein distance via rapidfuzz >=2.0) to filenames and file content to identify near-duplicate files.",
            "dependencies": [
              1
            ],
            "details": "After hash-based filtering, use rapidfuzz to compute similarity scores for filenames and optionally file contents. Set configurable thresholds for similarity to flag near-duplicates. Optimize for performance when comparing large numbers of files.",
            "status": "done",
            "testStrategy": "Upload files with similar but not identical names and/or content. Verify that near-duplicates are detected according to the configured similarity threshold.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement User Options for Duplicate Handling (Skip/Overwrite)",
            "description": "Provide user interface and backend logic for skip or overwrite actions when duplicates or near-duplicates are detected.",
            "dependencies": [
              1,
              2
            ],
            "details": "Design UI prompts and backend logic to allow users to choose whether to skip uploading duplicates, overwrite existing files, or take other actions. Ensure options are clearly presented and actions are reliably executed.",
            "status": "done",
            "testStrategy": "Simulate duplicate and near-duplicate uploads, test all user options (skip, overwrite), and verify correct file handling and user feedback.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on duplicate detection (sha-256 & fuzzy matching)."
      },
      {
        "id": 6,
        "title": "Celery Task Queue Management",
        "description": "Implement priority-based Celery task queue for async document processing, with status tracking, retries, and dead letter queue.",
        "details": "Configure Celery with Redis broker. Use priority queues (urgent, normal, low). Implement status tracking in Supabase file_uploads table. Add retry logic (3 attempts, exponential backoff) and dead letter queue for failed tasks.",
        "testStrategy": "Submit tasks with varying priorities, simulate failures, and verify retry and dead letter queue behavior.",
        "priority": "high",
        "dependencies": [
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Celery with Redis for Priority Queues",
            "description": "Set up Celery to use Redis as the broker and implement priority-based task queues (urgent, normal, low).",
            "dependencies": [],
            "details": "Update Celery configuration to use Redis as the broker. Define separate queues for each priority level (e.g., urgent, normal, low) and configure the broker_transport_options with 'queue_order_strategy': 'priority'. Adjust worker_prefetch_multiplier to 1 for effective prioritization. Ensure workers are started with the correct queue order.",
            "status": "done",
            "testStrategy": "Submit tasks with different priorities and verify that urgent tasks are processed before normal and low priority tasks.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Status Tracking with Supabase",
            "description": "Implement status updates for each task in the Supabase file_uploads table.",
            "dependencies": [
              1
            ],
            "details": "Modify Celery tasks to update the status field in the Supabase file_uploads table at key stages (queued, started, succeeded, failed). Ensure atomic updates and handle race conditions. Use Supabase client libraries for database operations.",
            "status": "done",
            "testStrategy": "Trigger tasks and verify that status changes are accurately reflected in the Supabase file_uploads table throughout the task lifecycle.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Retry Logic with Exponential Backoff",
            "description": "Add retry logic to Celery tasks with up to 3 attempts and exponential backoff on failure.",
            "dependencies": [
              1
            ],
            "details": "Configure Celery task decorators to include retry parameters: max_retries=3 and a backoff strategy (e.g., exponential). Ensure that exceptions trigger retries and that retry attempts are logged or tracked for observability.",
            "status": "done",
            "testStrategy": "Simulate task failures and verify that tasks are retried up to 3 times with increasing delays between attempts.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Set Up Dead Letter Queue for Failed Tasks",
            "description": "Configure a dead letter queue to capture tasks that fail after all retry attempts.",
            "dependencies": [
              3
            ],
            "details": "Create a dedicated dead letter queue in Celery/Redis. Update task failure handlers to route tasks to this queue after exhausting retries. Optionally, log or notify on dead letter events for monitoring.",
            "status": "done",
            "testStrategy": "Force tasks to fail beyond retry limits and verify their presence in the dead letter queue.",
            "updatedAt": "2025-11-05T23:00:18.337Z",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "End-to-End Testing of Priority Queue Management",
            "description": "Test the complete priority queue system, including status tracking, retries, and dead letter handling.",
            "dependencies": [
              2,
              4
            ],
            "details": "Design and execute test cases covering all priority levels, status transitions, retry scenarios, and dead letter queue routing. Validate system behavior under normal and failure conditions.",
            "status": "done",
            "testStrategy": "Run integration tests that submit tasks with various priorities, induce failures, and confirm correct processing, status updates, retries, and dead letter handling.",
            "parentId": "undefined",
            "updatedAt": "2025-11-05T23:00:42.292Z"
          }
        ],
        "complexity": 6.5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Decompose Celery task queue management into subtasks for priority queue configuration, status tracking integration, retry logic implementation, dead letter queue setup, and end-to-end testing.",
        "updatedAt": "2025-11-05T23:00:42.292Z"
      },
      {
        "id": 7,
        "title": "User Notification System (WebSocket & Email)",
        "description": "Provide real-time upload and processing notifications via WebSocket, with optional email alerts for long-running tasks.",
        "details": "Implement FastAPI WebSocket endpoints for progress and completion notifications. Use SMTP or SendGrid for email alerts. Integrate with frontend for actionable error messages.",
        "testStrategy": "Trigger uploads and processing, verify real-time notifications and email delivery for long tasks.",
        "priority": "medium",
        "dependencies": [
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement FastAPI WebSocket Endpoints for Real-Time Notifications",
            "description": "Develop FastAPI WebSocket endpoints to deliver real-time upload and processing progress and completion notifications to connected clients.",
            "dependencies": [],
            "details": "Set up FastAPI WebSocket routes (e.g., /ws/notifications). Manage client connections and broadcast progress/completion events. Ensure endpoints can handle multiple simultaneous connections and send actionable error messages. Integrate with backend processing logic to emit updates as tasks progress or complete.",
            "status": "done",
            "testStrategy": "Simulate uploads and processing tasks; verify clients receive real-time progress and completion notifications via WebSocket.",
            "parentId": "undefined",
            "updatedAt": "2025-11-05T23:01:15.700Z"
          },
          {
            "id": 2,
            "title": "Integrate Email Alert System for Long-Running Tasks",
            "description": "Add optional email notifications for users when uploads or processing tasks exceed a defined duration threshold.",
            "dependencies": [
              1
            ],
            "details": "Configure SMTP or SendGrid integration for sending emails. Implement logic to detect long-running tasks and trigger email alerts with relevant status and error details. Ensure emails are sent only when user opts in or when thresholds are exceeded. Handle email delivery failures gracefully.",
            "status": "done",
            "testStrategy": "Trigger long-running tasks and confirm that email alerts are sent to the correct recipients with accurate information.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Frontend Integration for Real-Time and Email Notifications",
            "description": "Connect frontend application to WebSocket endpoints and display real-time notifications, including actionable error messages. Provide UI for email alert preferences.",
            "dependencies": [
              1,
              2
            ],
            "details": "Update frontend to establish and manage WebSocket connections, display progress/completion notifications, and show errors in a user-friendly manner. Add UI controls for users to opt in/out of email alerts. Ensure seamless user experience for both notification channels.",
            "status": "done",
            "testStrategy": "Test frontend by uploading files and processing tasks; verify real-time updates and error messages appear, and email preferences are respected.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on user notification system (websocket & email).",
        "updatedAt": "2025-11-05T23:01:15.700Z"
      },
      {
        "id": 8,
        "title": "Backblaze B2 Folder Management & Encryption",
        "description": "Automate file movement across B2 folders (pending → processing → processed/failed) and support zero-knowledge encryption for sensitive files.",
        "details": "Use b2sdk for folder operations. Implement file movement logic based on processing status. Integrate PyCryptodome for optional AES encryption before upload.",
        "testStrategy": "Process files through all folder stages, verify correct organization and encryption for flagged files.",
        "priority": "high",
        "dependencies": [
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate b2sdk and Set Up B2 Folder Interfaces",
            "description": "Initialize b2sdk, authenticate, and set up interfaces for pending, processing, processed, and failed folders in the B2 bucket.",
            "dependencies": [],
            "details": "Use b2sdk's AccountInfo and B2Api to authenticate and connect to the B2 bucket. Instantiate B2Folder objects for each logical folder (pending, processing, processed, failed) to enable file operations between them.",
            "status": "done",
            "testStrategy": "Verify connection and folder listing for each B2 folder using b2sdk methods.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement File Movement Logic Based on Processing Status",
            "description": "Develop logic to move files between B2 folders according to their processing status (pending → processing → processed/failed).",
            "dependencies": [
              1
            ],
            "details": "Create functions to list files in each folder and move them to the next stage based on status. Ensure atomicity and handle errors during move operations using b2sdk's file copy and delete methods.",
            "status": "done",
            "testStrategy": "Simulate status changes and verify files are moved to the correct folders without duplication or loss.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate PyCryptodome for Optional AES Encryption",
            "description": "Add support for zero-knowledge AES encryption of sensitive files before upload to B2.",
            "dependencies": [
              1
            ],
            "details": "Use PyCryptodome to encrypt files with a user-supplied key before uploading to B2. Ensure encryption is optional and only applied to flagged files. Store encrypted files in the appropriate B2 folder.",
            "status": "done",
            "testStrategy": "Upload both encrypted and unencrypted files, then download and verify decryption for encrypted files.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Handle Status-Based Transitions and Error Recovery",
            "description": "Implement robust handling for file status transitions, including retries and error recovery for failed moves or uploads.",
            "dependencies": [
              2,
              3
            ],
            "details": "Add logic to detect and recover from failed moves or uploads. Implement retry mechanisms and ensure files are not lost or duplicated during transitions. Log all status changes and errors for auditability.",
            "status": "done",
            "testStrategy": "Intentionally trigger errors (e.g., network failures) and verify that files are correctly retried or moved to the failed folder.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Develop Comprehensive Tests for Folder Organization and Encryption",
            "description": "Create automated tests to verify correct file organization across all folder stages and validate encryption/decryption for sensitive files.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Write tests that process files through all folder stages, check their presence in the correct folders, and confirm that encryption is correctly applied and reversible for flagged files.",
            "status": "done",
            "testStrategy": "Run end-to-end tests covering all transitions and encryption scenarios, ensuring files are organized and protected as specified.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Divide B2 folder management and encryption into subtasks for implementing folder movement logic, integrating b2sdk, supporting AES encryption with PyCryptodome, handling status-based transitions, and verifying organization/encryption through tests."
      },
      {
        "id": 9,
        "title": "AI Department Classification Workflow (Claude Haiku)",
        "description": "Classify uploaded documents into 10 departments using Claude Haiku API, storing results in Supabase.",
        "details": "Integrate anthropic-py SDK. Implement async auto_classify_course function as per PRD. Store department, confidence, and subdepartment in documents and courses tables.",
        "testStrategy": "Upload sample documents for each department, verify classification accuracy and Supabase updates.",
        "priority": "high",
        "dependencies": [
          "8"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Claude Haiku API and anthropic-py SDK for Document Classification",
            "description": "Set up the Claude Haiku API and anthropic-py SDK to enable classification of uploaded documents into 10 departments.",
            "dependencies": [],
            "details": "Install and configure the anthropic-py SDK. Implement API authentication and error handling (e.g., retries, rate limits). Ensure the async auto_classify_course function is ready to send document content to Claude Haiku and receive department predictions. Tune parameters such as temperature and max_tokens for optimal classification accuracy.",
            "status": "done",
            "testStrategy": "Send sample documents to the API and verify department predictions are returned correctly. Test error handling by simulating API failures.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Async auto_classify_course Function and PRD Logic",
            "description": "Develop the async auto_classify_course function according to the Product Requirements Document (PRD), ensuring it processes documents and extracts department, confidence, and subdepartment.",
            "dependencies": [
              1
            ],
            "details": "Write the async function to handle document input, call the Claude Haiku API, and parse the response for department, confidence score, and subdepartment. Ensure the function supports batch processing and handles edge cases (e.g., ambiguous classifications). Document the function and its parameters for maintainability.",
            "status": "done",
            "testStrategy": "Unit test the function with mock API responses. Validate output structure and accuracy against expected department labels.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Store Classification Results in Supabase Documents and Courses Tables",
            "description": "Persist the classification results (department, confidence, subdepartment) in the Supabase documents and courses tables.",
            "dependencies": [
              2
            ],
            "details": "Map the classification output to the correct schema fields in Supabase. Implement transactional writes to ensure data consistency. Add logging for successful and failed writes. Verify that updates are reflected in both documents and courses tables as required.",
            "status": "done",
            "testStrategy": "Upload test documents, run classification, and confirm Supabase tables are updated with correct department, confidence, and subdepartment values. Check for data integrity and error handling.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on ai department classification workflow (claude haiku)."
      },
      {
        "id": 10,
        "title": "Universal Document Processing Pipeline",
        "description": "Extract text and structured data from all supported document types using specialized services and fallback methods.",
        "details": "Integrate LlamaIndex (REST API) for clean PDFs, Mistral OCR for scanned PDFs, Tesseract OCR for fallback. Use python-docx for DOCX, mutagen for audio/video, Claude Vision API for images. Implement table/image extraction and maintain page/section info.",
        "testStrategy": "Process each file type, verify extraction accuracy, structure preservation, and fallback logic.",
        "priority": "high",
        "dependencies": [
          "9"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Modular Document Ingestion and Classification",
            "description": "Design and build the pipeline's ingestion layer to accept documents from various sources and classify them by type (PDF, DOCX, image, audio/video).",
            "dependencies": [],
            "details": "Set up connectors for file sources (e.g., S3 buckets, local uploads). Integrate document type detection logic to route files to appropriate extraction modules. Log ingestion events and maintain audit trails for each document.",
            "status": "done",
            "testStrategy": "Submit sample files of each supported type, verify correct classification and routing, and check ingestion logs for completeness.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Specialized Extraction Services and Fallbacks",
            "description": "Connect and orchestrate specialized extraction services for each document type, with fallback logic for unsupported or failed cases.",
            "dependencies": [
              1
            ],
            "details": "Integrate LlamaIndex REST API for clean PDFs, Mistral OCR for scanned PDFs, Tesseract OCR as fallback, python-docx for DOCX, mutagen for audio/video, Claude Vision API for images. Implement logic to select extraction method based on classification and handle failures by cascading to fallback services.",
            "status": "done",
            "testStrategy": "Process a diverse set of documents, intentionally trigger extraction failures, and verify fallback mechanisms activate and extract data as expected.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Extract Structured Data and Metadata with Section/Page Tracking",
            "description": "Develop logic to extract tables, images, and maintain page/section metadata for all processed documents, ensuring structured outputs.",
            "dependencies": [
              2
            ],
            "details": "Implement table and image extraction for supported formats. Track and store page/section information alongside extracted text and structured data. Ensure outputs are normalized for downstream consumption.",
            "status": "done",
            "testStrategy": "Validate extracted outputs for structure, completeness, and correct association of metadata (page/section info) using test documents with known layouts.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on universal document processing pipeline."
      },
      {
        "id": 11,
        "title": "Audio & Video Processing (Soniox, Claude Vision)",
        "description": "Transcribe audio, extract speakers/timestamps, and analyze video frames using Soniox and Claude Vision APIs.",
        "details": "Integrate Soniox REST API for transcription and diarization. Use ffmpeg-python for frame/audio extraction from video. Analyze frames with Claude Vision API. Store transcripts and timeline metadata.",
        "testStrategy": "Process audio and video files, verify transcript accuracy, speaker identification, and frame analysis.",
        "priority": "high",
        "dependencies": [
          "10"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Extract Audio and Video Frames from Input Files",
            "description": "Use ffmpeg-python to extract audio tracks and video frames from input video files for downstream processing.",
            "dependencies": [],
            "details": "Implement a Python module using ffmpeg-python to separate audio from video files and extract video frames at configurable intervals. Ensure extracted audio is in a Soniox-compatible format (e.g., 16kHz mono WAV). Store extracted frames and audio in a structured directory or object storage for later processing.",
            "status": "done",
            "testStrategy": "Run extraction on sample video files, verify correct number and quality of frames, and check audio format compatibility with Soniox.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Transcribe Audio and Extract Speaker/Timestamps with Soniox API",
            "description": "Integrate Soniox REST API to transcribe extracted audio, enabling speaker diarization and timestamp extraction.",
            "dependencies": [
              1
            ],
            "details": "Authenticate with Soniox API using a project API key. Send extracted audio files for transcription using the async or streaming endpoints. Enable speaker diarization and timestamp options in the API request. Parse and store the returned transcript, speaker labels, and word-level timestamps in the database or metadata files.",
            "status": "done",
            "testStrategy": "Submit test audio files, verify transcript accuracy, correct speaker segmentation, and presence of timestamps. Compare results with ground truth if available.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Analyze Video Frames with Claude Vision API and Store Metadata",
            "description": "Send extracted video frames to Claude Vision API for analysis and store the resulting metadata alongside transcripts and timeline data.",
            "dependencies": [
              1
            ],
            "details": "Batch or stream video frames to the Claude Vision API, handling authentication and rate limits. Parse the returned analysis (e.g., scene description, object detection) and associate results with corresponding timestamps. Store all metadata in a structured format, linking frame analysis to transcript timeline.",
            "status": "done",
            "testStrategy": "Process sample frames, verify that analysis results are received and correctly mapped to frame timestamps. Check integration with transcript timeline and metadata storage.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on audio & video processing (soniox, claude vision)."
      },
      {
        "id": 12,
        "title": "Structured Data Extraction (LangExtract)",
        "description": "Extract entities, key-value pairs, and course metadata using LangExtract API.",
        "details": "Integrate LangExtract REST API for field/entity extraction. Store results in Supabase courses and document_chunks tables. Implement intelligent filename generation (M01-L02 format).",
        "testStrategy": "Process documents with structured fields, verify entity extraction and metadata accuracy.",
        "priority": "high",
        "dependencies": [
          "11"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Extraction Schema and Example Prompts for LangExtract",
            "description": "Specify the entity types, key-value pairs, and course metadata fields to be extracted. Create example prompts and sample extractions to guide the LangExtract API.",
            "dependencies": [],
            "details": "List all required fields (e.g., course title, module number, lesson number, instructor, date) and define their expected formats. Write natural language prompts and provide high-quality example extractions using LangExtract's ExampleData objects to ensure consistent output schema and accurate extraction.",
            "status": "done",
            "testStrategy": "Review extracted fields from test documents to confirm schema coverage and prompt effectiveness.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate LangExtract REST API for Automated Entity and Metadata Extraction",
            "description": "Connect to the LangExtract REST API and implement logic to process course documents, extracting entities, key-value pairs, and metadata as defined in the schema.",
            "dependencies": [
              1
            ],
            "details": "Set up API authentication and request handling. For each uploaded course document, send the text and extraction instructions/examples to LangExtract. Parse the returned structured data, ensuring source grounding and attribute mapping. Handle errors and edge cases (e.g., missing fields, ambiguous extractions).",
            "status": "done",
            "testStrategy": "Process a variety of course documents and verify that all required entities and metadata are extracted with correct attributes and source positions.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Store Extracted Data in Supabase and Implement Intelligent Filename Generation",
            "description": "Save the extracted entities and metadata into Supabase courses and document_chunks tables. Generate filenames using the M01-L02 format based on extracted module and lesson numbers.",
            "dependencies": [
              2
            ],
            "details": "Map extracted fields to Supabase table schemas, ensuring correct data types and relationships. Implement logic to generate filenames (e.g., M01-L02) from extracted metadata and associate them with stored records. Validate data integrity and handle duplicate or conflicting entries.",
            "status": "done",
            "testStrategy": "Insert extracted data from sample documents into Supabase, verify correct mapping and filename generation, and check for consistency across multiple uploads.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on structured data extraction (langextract)."
      },
      {
        "id": 13,
        "title": "Adaptive Chunking Strategy Implementation",
        "description": "Implement semantic, code, and transcript chunking with configurable size and overlap, preserving context.",
        "details": "Use LlamaIndex chunking for documents, custom logic for code (AST parsing), and time/topic-based chunking for transcripts. Store chunks in document_chunks table with metadata and overlap.",
        "testStrategy": "Chunk various document types, verify chunk boundaries, overlap, and context preservation.",
        "priority": "high",
        "dependencies": [
          "12"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Adaptive Semantic Chunking for Documents",
            "description": "Develop and configure semantic chunking for text documents using LlamaIndex, supporting adjustable chunk size and overlap to preserve context.",
            "dependencies": [],
            "details": "Use LlamaIndex's semantic chunker to split documents into contextually coherent chunks. Expose configuration for chunk size and overlap (e.g., via parameters or settings). Ensure chunk metadata (source_doc_id, chunk boundaries, overlap) is captured for each chunk and stored in the document_chunks table. Validate that semantic boundaries are respected and context is preserved across chunks.",
            "status": "done",
            "testStrategy": "Chunk a variety of document types, verify chunk boundaries align with semantic units, check overlap, and confirm metadata is correctly stored.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop Custom Code Chunking Using AST Parsing",
            "description": "Create a chunking mechanism for code files that leverages AST parsing to split code into logical units with configurable size and overlap.",
            "dependencies": [
              1
            ],
            "details": "Implement code chunking logic that parses source code into AST nodes (e.g., functions, classes) and groups them into chunks based on configurable parameters (lines per chunk, overlap). Support multiple programming languages if required. Store resulting code chunks with relevant metadata (e.g., language, function/class names, overlap) in the document_chunks table.",
            "status": "done",
            "testStrategy": "Process code files in different languages, verify chunking aligns with logical code units, check overlap, and ensure metadata accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Time/Topic-Based Chunking for Transcripts",
            "description": "Design and implement a chunking strategy for transcripts that splits content based on time intervals or topic shifts, with configurable overlap.",
            "dependencies": [
              1
            ],
            "details": "Develop logic to segment transcripts using either fixed time windows or detected topic boundaries. Allow configuration of chunk duration or topic sensitivity, as well as overlap between chunks. Store transcript chunks with metadata (e.g., start/end time, topic label, overlap) in the document_chunks table. Ensure context is preserved across chunk boundaries.",
            "status": "done",
            "testStrategy": "Chunk transcripts with varying lengths and topics, verify chunk boundaries match time/topic criteria, check overlap, and validate metadata storage.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on adaptive chunking strategy implementation."
      },
      {
        "id": 14,
        "title": "Error Handling & Graceful Degradation",
        "description": "Implement robust error handling, retry logic, partial processing, and detailed logging for all pipeline stages.",
        "details": "Use Python exception handling, Celery retry policies, and fallback to simpler methods. Log errors with stack traces in processing_logs table. Move failed files to B2 failed/ folder.",
        "testStrategy": "Simulate service failures, verify retries, partial saves, and error logs.",
        "priority": "high",
        "dependencies": [
          "13"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Robust Exception Handling and Retry Logic in Pipeline Tasks",
            "description": "Integrate structured Python exception handling and Celery retry policies for all pipeline stages to ensure resilience against transient and expected failures.",
            "dependencies": [],
            "details": "Wrap all critical pipeline operations in try/except blocks. Use Celery's retry mechanisms (e.g., autoretry_for, max_retries, retry_backoff) to handle transient errors such as network or service outages. Configure per-task retry parameters and ensure idempotency to avoid side effects on repeated execution. Avoid retrying on non-transient exceptions.",
            "status": "done",
            "testStrategy": "Simulate transient and permanent failures in pipeline tasks. Verify that retries occur as configured, and that non-retriable errors do not trigger retries.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Enable Graceful Degradation and Partial Processing with Fallbacks",
            "description": "Design pipeline stages to degrade gracefully by falling back to simpler or partial processing methods when primary logic fails.",
            "dependencies": [
              1
            ],
            "details": "For each pipeline stage, define fallback logic (e.g., simplified processing, skipping non-critical steps) to be invoked when primary processing fails after retries. Ensure that partial results are saved where possible, and that the system continues processing unaffected files or stages. Move unrecoverable files to the B2 failed/ folder for later inspection.",
            "status": "done",
            "testStrategy": "Force failures in primary processing logic and verify that fallback methods are invoked, partial results are saved, and failed files are moved appropriately.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Detailed Error Logging and Monitoring",
            "description": "Log all errors, stack traces, and processing outcomes in the processing_logs table to support debugging and monitoring.",
            "dependencies": [
              1,
              2
            ],
            "details": "On every exception or failure, capture the full stack trace and relevant context. Insert detailed error records into the processing_logs table, including task identifiers, error types, messages, and timestamps. Ensure logs are structured for easy querying and monitoring. Integrate with monitoring tools if available.",
            "status": "done",
            "testStrategy": "Trigger various error scenarios and verify that all relevant details are logged in the processing_logs table, including stack traces and context.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on error handling & graceful degradation."
      },
      {
        "id": 15,
        "title": "Processing Monitoring & Metrics Collection",
        "description": "Track real-time processing progress, resource usage, and cost per document using Prometheus metrics.",
        "details": "Integrate prometheus_client for FastAPI, Celery, and custom business metrics. Track processing time, resource usage, and cost. Store stage-wise metrics in processing_logs table.",
        "testStrategy": "Process documents, verify Prometheus metrics, Grafana dashboard updates, and Supabase logs.",
        "priority": "high",
        "dependencies": [
          "14"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Prometheus Metrics Collection in FastAPI and Celery",
            "description": "Instrument FastAPI and Celery services to expose Prometheus-compatible metrics endpoints for processing progress, resource usage, and cost tracking.",
            "dependencies": [],
            "details": "Install prometheus_client in both FastAPI and Celery environments. For FastAPI, mount the /metrics endpoint using make_asgi_app and add counters, histograms, and gauges for request counts, processing time, and resource usage. For Celery, use available Prometheus exporters or integrate prometheus_client to expose worker and task metrics. Ensure all relevant business and custom metrics are included.",
            "status": "done",
            "testStrategy": "Verify /metrics endpoints in FastAPI and Celery return expected metrics. Use Prometheus to scrape these endpoints and confirm metrics are ingested.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Track and Store Stage-wise Processing Metrics in Database",
            "description": "Capture and persist detailed stage-wise metrics (processing time, resource usage, cost per document) in the processing_logs table for audit and analysis.",
            "dependencies": [
              1
            ],
            "details": "Extend processing logic to record metrics at each pipeline stage. Store metrics such as start/end timestamps, CPU/memory usage, and cost estimates in the processing_logs table. Ensure schema supports all required fields and that writes are efficient and reliable.",
            "status": "done",
            "testStrategy": "Process sample documents and verify that processing_logs table contains accurate, stage-wise metrics matching Prometheus data.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Validate Metrics Collection and Visualization End-to-End",
            "description": "Test the full monitoring pipeline from metrics emission to visualization and logging, ensuring real-time and historical data is accurate and actionable.",
            "dependencies": [
              1,
              2
            ],
            "details": "Simulate document processing and monitor Prometheus for real-time metrics updates. Confirm that Grafana dashboards reflect current and historical metrics. Cross-check database logs with Prometheus data for consistency. Validate cost calculations and resource usage reporting.",
            "status": "done",
            "testStrategy": "Run end-to-end tests: process documents, check Prometheus and Grafana for live metrics, and verify processing_logs entries. Ensure all metrics are accurate and actionable.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on processing monitoring & metrics collection."
      },
      {
        "id": 16,
        "title": "Embedding Generation Pipeline (BGE-M3, Claude API)",
        "description": "Generate and cache embeddings for document chunks using BGE-M3 (Ollama for dev, Claude API for prod).",
        "details": "Integrate langchain.embeddings.OllamaEmbeddings for dev, Claude API for prod. Batch process 100 chunks, cache embeddings in Supabase pgvector. Regenerate on content updates.",
        "testStrategy": "Generate embeddings for sample chunks, verify latency, caching, and Supabase storage.",
        "priority": "high",
        "dependencies": [
          "15"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate BGE-M3 Embedding Generation for Development (Ollama)",
            "description": "Set up and integrate the BGE-M3 embedding model using Ollama for local development, enabling batch processing of document chunks.",
            "dependencies": [],
            "details": "Install and configure langchain_ollama and OllamaEmbeddings with the BGE-M3 model. Implement batch processing for 100 document chunks at a time. Ensure the pipeline can handle content updates by triggering re-embedding as needed. Optimize for local inference speed and resource usage.",
            "status": "done",
            "testStrategy": "Generate embeddings for a sample batch of 100 chunks, verify output shape and latency, and confirm embeddings are regenerated on content updates.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Claude API for Production Embedding Generation",
            "description": "Implement embedding generation using the Claude API for production, supporting batch processing and seamless switching from development to production.",
            "dependencies": [
              1
            ],
            "details": "Configure the Claude API integration within the embedding pipeline. Ensure batch processing of 100 chunks per request, with error handling and retry logic. Provide a configuration switch to toggle between Ollama (dev) and Claude API (prod). Ensure compatibility of embedding formats and dimensions.",
            "status": "done",
            "testStrategy": "Run embedding generation for a sample batch via Claude API, verify output consistency with dev pipeline, and test failover and error handling.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Embedding Caching and Regeneration Logic in Supabase pgvector",
            "description": "Design and implement caching of generated embeddings in Supabase pgvector, including logic to detect content updates and trigger regeneration.",
            "dependencies": [
              2
            ],
            "details": "Integrate with Supabase pgvector to store and retrieve embeddings. Implement logic to check for content changes and invalidate or update cached embeddings as needed. Ensure efficient batch inserts and retrievals. Maintain metadata for tracking embedding versions and update timestamps.",
            "status": "done",
            "testStrategy": "Insert, retrieve, and update embeddings in Supabase for sample documents. Simulate content updates and verify that embeddings are correctly regenerated and cached.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on embedding generation pipeline (bge-m3, claude api)."
      },
      {
        "id": 17,
        "title": "Vector Storage & Indexing (Supabase pgvector)",
        "description": "Store embeddings in Supabase pgvector, create HNSW index for fast similarity search, and optimize batch inserts.",
        "details": "Enable pgvector extension, create HNSW index, and optimize batch inserts using supabase-py bulk operations. Organize by namespace and support metadata filtering.",
        "testStrategy": "Insert and search embeddings, verify index performance and metadata filtering.",
        "priority": "high",
        "dependencies": [
          "16"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Enable pgvector Extension in Supabase",
            "description": "Activate the pgvector extension in the Supabase PostgreSQL database to support vector data types and similarity search operations.",
            "dependencies": [],
            "details": "Access the Supabase dashboard, navigate to the Extensions section, and enable the 'vector' extension. This step is required before creating tables with vector columns and using vector search features.",
            "status": "done",
            "testStrategy": "Verify that the 'vector' extension is listed as enabled in the Supabase dashboard and that SQL commands using the 'vector' data type execute without errors.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create Embeddings Table and HNSW Index",
            "description": "Design and create a table for storing embeddings, including metadata and namespace columns, and add an HNSW index for fast similarity search.",
            "dependencies": [
              1
            ],
            "details": "Define a table schema with columns for id, embedding (vector), metadata (JSONB), and namespace (text or UUID). Use SQL to create the table and then create an HNSW index on the embedding column for efficient ANN search.",
            "status": "done",
            "testStrategy": "Insert sample embeddings and confirm that the HNSW index exists and is used in EXPLAIN query plans for similarity searches.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Optimize Batch Inserts Using supabase-py Bulk Operations",
            "description": "Implement efficient batch insertion of embeddings and metadata using supabase-py or equivalent bulk insert methods.",
            "dependencies": [
              2
            ],
            "details": "Use supabase-py or another supported client to insert multiple embeddings in a single operation, minimizing transaction overhead and maximizing throughput. Ensure the code handles large batches and error cases.",
            "status": "done",
            "testStrategy": "Benchmark batch insert performance with varying batch sizes and verify that all records are correctly stored in the table.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Organize Embeddings by Namespace",
            "description": "Implement logic to assign and query embeddings by namespace to support multi-tenant or segmented storage.",
            "dependencies": [
              2
            ],
            "details": "Add a namespace column to the embeddings table if not already present. Ensure all insert and query operations include namespace filtering to logically separate data for different use cases or clients.",
            "status": "done",
            "testStrategy": "Insert embeddings with different namespaces and verify that queries scoped to a namespace only return relevant records.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Metadata Filtering in Similarity Search",
            "description": "Enable filtering of similarity search results based on metadata fields stored with each embedding.",
            "dependencies": [
              2
            ],
            "details": "Use PostgreSQL's JSONB operators to filter embeddings by metadata fields in combination with vector similarity queries. Update search queries to support metadata-based filtering (e.g., by document type, tags, or timestamps).",
            "status": "done",
            "testStrategy": "Run similarity searches with and without metadata filters, confirming that results are correctly filtered and performance remains acceptable.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7.5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Expand vector storage and indexing into subtasks for enabling pgvector, creating HNSW index, optimizing batch inserts, organizing by namespace, and implementing metadata filtering."
      },
      {
        "id": 18,
        "title": "Hybrid Search Implementation (Dense, Sparse, Fuzzy, RRF)",
        "description": "Implement hybrid search combining vector similarity, BM25, ILIKE, fuzzy matching, and reciprocal rank fusion.",
        "details": "Use pgvector for dense search, PostgreSQL full-text search for BM25, ILIKE for pattern matching, rapidfuzz for fuzzy search. Implement RRF for result fusion with configurable weights.",
        "testStrategy": "Run hybrid searches, verify result fusion, relevance, and latency targets.",
        "priority": "high",
        "dependencies": [
          "17"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Dense, Sparse, and Fuzzy Search Pipelines",
            "description": "Develop individual search pipelines for dense (vector), sparse (BM25), and fuzzy (ILIKE, rapidfuzz) retrieval methods.",
            "dependencies": [],
            "details": "Set up pgvector for dense search, configure PostgreSQL full-text search for BM25, implement ILIKE for pattern matching, and integrate rapidfuzz for fuzzy matching. Ensure each pipeline can independently retrieve and score results for a given query.",
            "status": "done",
            "testStrategy": "Run isolated queries for each pipeline and verify result relevance, accuracy, and latency.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Design and Implement Reciprocal Rank Fusion (RRF) Algorithm",
            "description": "Create a fusion algorithm to combine ranked results from dense, sparse, and fuzzy pipelines using reciprocal rank fusion.",
            "dependencies": [
              1
            ],
            "details": "Develop RRF logic to merge result lists from all pipelines, applying configurable weights. Ensure the algorithm penalizes lower-ranked results and boosts consensus across methods. Validate with sample queries and edge cases.",
            "status": "done",
            "testStrategy": "Test fusion with controlled input lists, verify ranking consistency, and check that top results reflect combined relevance.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Hybrid Search and Expose Unified API Endpoint",
            "description": "Combine all search pipelines and RRF fusion into a single hybrid search workflow, exposing it via an API endpoint.",
            "dependencies": [
              2
            ],
            "details": "Orchestrate parallel execution of all search methods, collect results, apply RRF fusion, and return unified ranked results. Implement API endpoint with configurable fusion weights and query parameters. Ensure robust error handling and logging.",
            "status": "done",
            "testStrategy": "Run end-to-end hybrid search queries through the API, validate result quality, latency, and error handling.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on hybrid search implementation (dense, sparse, fuzzy, rrf)."
      },
      {
        "id": 19,
        "title": "Query Expansion & Reranking (Claude Haiku, BGE-Reranker-v2)",
        "description": "Expand queries using Claude Haiku, execute parallel searches, and rerank results with BGE-Reranker-v2.",
        "details": "Integrate anthropic-py for query expansion, run parallel searches, and rerank top 20-30 results using Ollama BGE-Reranker-v2 (dev) or Claude API (prod).",
        "testStrategy": "Test query expansion and reranking, verify improved recall and precision.",
        "priority": "high",
        "dependencies": [
          "18"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Claude Haiku for Query Expansion",
            "description": "Implement query expansion using Claude Haiku via anthropic-py, ensuring queries are enriched for improved recall.",
            "dependencies": [],
            "details": "Set up anthropic-py client and configure Claude Haiku 4.5 API parameters (e.g., max_tokens, temperature, top_p). Design prompt templates to expand user queries, leveraging advanced prompt engineering techniques for optimal output. Handle API errors and retries for reliability.",
            "status": "done",
            "testStrategy": "Send sample queries and verify that expanded queries are generated as expected. Compare recall and diversity of results before and after expansion.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Execute Parallel Searches with Expanded Queries",
            "description": "Run parallel searches using the expanded queries to retrieve a broad set of relevant results.",
            "dependencies": [
              1
            ],
            "details": "Implement asynchronous or concurrent search logic to execute multiple queries in parallel. Aggregate results from all searches, ensuring deduplication and efficient handling of large result sets. Optimize for latency and throughput.",
            "status": "done",
            "testStrategy": "Test with multiple expanded queries and measure search latency. Confirm that all relevant results are retrieved and aggregated correctly.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Rerank Search Results Using BGE-Reranker-v2",
            "description": "Apply BGE-Reranker-v2 to rerank the top 20-30 search results for improved relevance and precision.",
            "dependencies": [
              2
            ],
            "details": "Integrate Ollama BGE-Reranker-v2 (dev) or Claude API (prod) to rerank aggregated results. Configure reranker model and permissions, and tune reranking parameters. Validate reranked output for relevance and consistency.",
            "status": "done",
            "testStrategy": "Compare original and reranked result sets using relevance metrics (precision, recall, NDCG). Conduct manual review of top results for quality assurance.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on query expansion & reranking (claude haiku, bge-reranker-v2)."
      },
      {
        "id": 20,
        "title": "Neo4j Graph Integration & Entity Storage",
        "description": "Store entities and relationships in Neo4j, enable graph-based queries and context retrieval.",
        "details": "Use neo4j Python driver (neo4j >=5.10) to create document and entity nodes, relationships, and vector index. Implement Cypher queries for entity-centric and relationship traversal.",
        "testStrategy": "Insert entities/relationships, run Cypher queries, verify graph traversal and context retrieval.",
        "priority": "high",
        "dependencies": [
          "19"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Set Up Neo4j Python Driver and Database Connection",
            "description": "Install the Neo4j Python driver and establish a secure connection to the Neo4j database instance.",
            "dependencies": [],
            "details": "Use pip to install the neo4j Python driver (neo4j >=5.10). Configure connection parameters (URI, username, password) and verify connectivity using GraphDatabase.driver and driver.verify_connectivity(). Ensure the database instance is running and accessible.",
            "status": "done",
            "testStrategy": "Attempt connection and run a simple Cypher query to confirm connectivity.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Entity and Relationship Node Creation",
            "description": "Create Cypher queries and Python functions to insert document and entity nodes, and define relationships between them in Neo4j.",
            "dependencies": [
              1
            ],
            "details": "Define node labels (e.g., Document, Entity) and relationship types. Use MERGE or CREATE Cypher statements to add nodes and relationships. Implement Python functions to batch insert entities and relationships, ensuring idempotency and data integrity.",
            "status": "done",
            "testStrategy": "Insert sample entities and relationships, then query the graph to verify correct node and relationship creation.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Enable Graph-Based Queries and Context Retrieval",
            "description": "Develop Cypher queries and Python interfaces for entity-centric and relationship traversal, including context retrieval and vector index integration.",
            "dependencies": [
              2
            ],
            "details": "Implement Cypher queries for traversing relationships (e.g., MATCH, OPTIONAL MATCH). Integrate vector index for similarity search if required. Provide Python functions to retrieve context around entities and relationships, supporting advanced graph queries.",
            "status": "done",
            "testStrategy": "Run entity-centric and relationship traversal queries, validate context retrieval, and test vector index search if applicable.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on neo4j graph integration & entity storage."
      },
      {
        "id": 21,
        "title": "Caching Strategy (Redis, Tiered Cache)",
        "description": "Implement Redis caching for frequent queries, embeddings, and search results with semantic thresholds and tiered cache.",
        "details": "Use redis-py for L1 cache (Redis), fallback to L2 (PostgreSQL). Implement semantic cache thresholds and 5-minute TTL. Track cache hit rate.",
        "testStrategy": "Run repeated queries, verify cache hits/misses, and cache update logic.",
        "priority": "high",
        "dependencies": [
          "20"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Redis L1 Cache with Semantic Thresholds and TTL",
            "description": "Set up Redis as the primary (L1) cache for frequent queries, embeddings, and search results, applying semantic thresholds and a 5-minute TTL.",
            "dependencies": [],
            "details": "Use redis-py to connect to Redis. Define cache keys for queries, embeddings, and search results. Implement logic to only cache results that meet semantic similarity thresholds. Set a 5-minute expiration (TTL) for all cache entries to ensure freshness. Ensure cache-aside pattern is used for read-heavy workloads, checking Redis first and falling back to the database on cache miss.",
            "status": "done",
            "testStrategy": "Run repeated queries and verify that results are cached in Redis, TTL is respected, and only semantically relevant results are cached.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Tiered Cache Fallback to PostgreSQL (L2)",
            "description": "Implement fallback logic to query PostgreSQL (L2) when Redis (L1) cache misses occur, and repopulate Redis cache as needed.",
            "dependencies": [
              1
            ],
            "details": "On cache miss in Redis, query PostgreSQL for the required data. If found, repopulate Redis with the result, applying the same semantic threshold and TTL logic. Ensure the fallback mechanism is robust and does not introduce significant latency. Use efficient serialization for storing and retrieving data between Redis and PostgreSQL.",
            "status": "done",
            "testStrategy": "Simulate cache misses and verify that data is correctly fetched from PostgreSQL, then cached in Redis for subsequent requests.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Monitor and Track Cache Hit Rate and Effectiveness",
            "description": "Implement monitoring to track cache hit/miss rates and overall cache effectiveness for both Redis and PostgreSQL tiers.",
            "dependencies": [
              1,
              2
            ],
            "details": "Instrument the caching logic to record cache hits, misses, and repopulation events. Aggregate metrics such as hit rate, miss rate, and average response time. Set up dashboards or logs to visualize cache performance and identify optimization opportunities. Use these metrics to tune semantic thresholds and TTL values.",
            "status": "done",
            "testStrategy": "Generate load with a mix of repeated and unique queries, then verify that hit/miss metrics are accurately tracked and reported.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on caching strategy (redis, tiered cache)."
      },
      {
        "id": 22,
        "title": "Query Type Detection & Routing",
        "description": "Classify incoming queries and route to optimal workflow framework (LangGraph, CrewAI, Simple) based on query complexity and requirements.",
        "status": "done",
        "dependencies": [
          "21"
        ],
        "priority": "high",
        "details": "Implemented WorkflowRouter class in app/workflows/workflow_router.py with Claude Haiku-powered classification. The system analyzes queries and routes them to the appropriate processing framework with confidence scoring and reasoning.",
        "testStrategy": "Submit queries of each type, verify correct classification and routing to appropriate workflow frameworks. Validate confidence scoring and fallback mechanisms.",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Query Type Taxonomy and Routing Logic",
            "description": "Define the taxonomy of query types (semantic, relational, hybrid, metadata) and specify routing logic for each type.",
            "dependencies": [],
            "details": "Analyze typical incoming queries and categorize them into clear types. Document routing rules for each category, mapping them to the appropriate search pipeline (vector, graph, metadata). Consider hierarchical classification if the taxonomy is complex, and ensure the design supports future extensibility.",
            "status": "done",
            "testStrategy": "Review taxonomy coverage against a sample set of queries. Validate routing logic with test cases for each query type.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Query Classifier Using Claude Haiku",
            "description": "Develop and deploy a query classifier leveraging Claude Haiku to assign incoming queries to the correct type.",
            "dependencies": [
              1
            ],
            "details": "Use prompt engineering and, if needed, hierarchical classification to maximize accuracy. Integrate Claude Haiku via API, ensuring the classifier outputs only the defined category names. Optimize for speed and reliability, and consider using vector similarity retrieval for highly variable queries.",
            "status": "done",
            "testStrategy": "Submit queries of each type and edge cases to the classifier. Measure classification accuracy and latency. Confirm output matches taxonomy.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Classifier with Search Pipeline Routing",
            "description": "Connect the classifier output to the routing system, ensuring queries are dispatched to the correct search pipeline.",
            "dependencies": [
              2
            ],
            "details": "Implement the routing logic that receives the classified query type and triggers the corresponding search pipeline (vector, graph, metadata, or hybrid). Ensure robust error handling and logging. Validate that each pipeline receives only relevant queries and that fallback logic is in place for unclassified or ambiguous queries.",
            "status": "done",
            "testStrategy": "End-to-end test: submit queries, verify correct classification and routing to the intended pipeline. Check logs and error handling for misrouted or unclassified queries.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Document Workflow Type Taxonomy",
            "description": "Document the implemented workflow type taxonomy (LANGGRAPH, CREWAI, SIMPLE) with detailed characteristics of each type.",
            "dependencies": [
              3
            ],
            "details": "Create comprehensive documentation of the three workflow types: LANGGRAPH for adaptive queries needing iterative refinement and external search; CREWAI for multi-agent tasks with specialized roles and sequential processing; and SIMPLE for straightforward factual lookups. Include examples and decision criteria for each type.",
            "status": "done",
            "testStrategy": "Review documentation with team members to ensure clarity and completeness. Validate with example queries for each workflow type.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Monitor and Optimize Classification Performance",
            "description": "Implement monitoring for classification decisions and optimize performance based on real-world usage patterns.",
            "dependencies": [
              3
            ],
            "details": "Set up analytics to track classification accuracy, confidence scores, and routing decisions in production. Analyze patterns of misclassification or low confidence scores. Refine the classifier based on this data to improve accuracy and reduce fallbacks to SIMPLE workflow.",
            "status": "done",
            "testStrategy": "Analyze classification logs over time. Compare predicted workflow types with actual performance. Measure improvements in classification accuracy after optimization.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on query type detection & routing."
      },
      {
        "id": 23,
        "title": "Query Processing Pipeline & Result Merging",
        "description": "Normalize, expand, execute, deduplicate, and merge query results using RRF and reranking.",
        "status": "done",
        "dependencies": [
          "22"
        ],
        "priority": "high",
        "details": "Implemented comprehensive query processing pipeline with services for query expansion, hybrid search, reranking, and parallel execution. Features include multiple expansion strategies, parallel execution across search methods, deduplication, RRF merging, BGE-Reranker-v2 reranking, logging, metrics tracking, score thresholding, and Top-K selection.",
        "testStrategy": "Process complex queries, verify result quality, deduplication, and latency. All tests passed successfully with the implemented pipeline.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Query Normalization and Expansion",
            "description": "Develop modules to normalize incoming queries and expand them for improved recall and relevance.",
            "dependencies": [],
            "details": "Created query_expansion_service.py using Claude Haiku for query expansion with multiple strategies including synonyms and reformulations. Implemented standardization of query formats (lowercasing, removing stopwords) and comprehensive logging of all normalized and expanded queries for traceability.",
            "status": "done",
            "testStrategy": "Test with diverse query inputs, verify normalization accuracy, and check that expansions improve recall without introducing irrelevant results.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Execute Queries in Parallel and Deduplicate Results",
            "description": "Design and implement parallel query execution across multiple sources, followed by deduplication of retrieved results.",
            "dependencies": [
              1
            ],
            "details": "Implemented parallel_search_service.py to run expanded queries against all relevant data sources concurrently. Applied deduplication algorithms to remove duplicate results based on content similarity and unique identifiers. Added logging for execution times and deduplication statistics.",
            "status": "done",
            "testStrategy": "Simulate concurrent queries, measure execution latency, and verify that deduplication removes all duplicates while retaining unique results.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Merge Results Using RRF and Rerank Final Output",
            "description": "Integrate Reciprocal Rank Fusion (RRF) for merging results and apply reranking models to optimize final result order.",
            "dependencies": [
              2
            ],
            "details": "Created hybrid_search_service.py for RRF merging of results from dense, sparse, and fuzzy search methods. Implemented reranking_service.py using BGE-Reranker-v2 via Ollama (dev) or Claude API (prod). Added score thresholding and Top-K selection for optimal result quality. Ensured comprehensive logging of all queries and merged results for audit and debugging.",
            "status": "done",
            "testStrategy": "Process sample queries, validate that RRF merging and reranking improve relevance, and check that final output matches expected quality benchmarks.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on query processing pipeline & result merging."
      },
      {
        "id": 24,
        "title": "Faceted Search & Result Presentation",
        "description": "Enable faceted filtering (department, type, date, entities) and present results with snippets, highlights, and relevance scores.",
        "status": "done",
        "dependencies": [
          "23"
        ],
        "priority": "medium",
        "details": "Implemented multi-select facets for department, file_type, date_range, and entity filtering. Generated snippets with keyword highlighting using HTML <mark> tags. Displayed relevance scores, source metadata, and B2 URL links. Added pagination support and SQL WHERE clause generation for filters.",
        "testStrategy": "Verified filtered searches functionality, result presentation with snippets and highlights, facet accuracy, and pagination. Confirmed authentication with Clerk JWT tokens works correctly.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Multi-Select Faceted Filtering UI",
            "description": "Develop the frontend components to support multi-select faceted filtering by department, type, date, and entities.",
            "dependencies": [],
            "details": "Design and build user interface elements for each facet (department, type, date, entities) with multi-select capability. Ensure facets are easy to find, mobile-friendly, and update results quickly when filters are applied. Facet values should be ordered logically (alphabetical, numerical, or by relevance) and selected values should be clearly indicated. Only display relevant facets for the current result set.",
            "status": "done",
            "testStrategy": "Test by applying various combinations of facet filters and verifying that the displayed results update accordingly and facet selections persist. Check usability on both desktop and mobile.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Generate and Present Search Result Snippets with Highlights",
            "description": "Create backend and frontend logic to generate result snippets, highlight matched keywords, and display relevant metadata.",
            "dependencies": [
              1
            ],
            "details": "For each search result, extract a relevant snippet containing the matched keywords. Highlight these keywords in the snippet. Display additional metadata such as relevance score, source, and department. Ensure that snippets are concise and informative, and that highlights are visually distinct.",
            "status": "done",
            "testStrategy": "Run searches with various queries and verify that snippets are generated, keywords are highlighted, and metadata is displayed correctly for each result.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Result Presentation with B2 URLs and Relevance Scores",
            "description": "Link each search result to its corresponding Backblaze B2 URL and ensure relevance scores are visible and accurate.",
            "dependencies": [
              2
            ],
            "details": "For each result, provide a clickable link to the B2 URL. Display the relevance score prominently, ensuring it is calculated and presented consistently. Confirm that the source and department fields are shown as specified. Validate that all links are functional and direct users to the correct B2 resource.",
            "status": "done",
            "testStrategy": "Click through result links to verify correct B2 URL redirection. Check that relevance scores match backend calculations and are displayed for all results.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Documentation of Faceted Search Implementation",
            "description": "Document the implementation details of the faceted search service and API endpoint.",
            "dependencies": [
              3
            ],
            "details": "Create comprehensive documentation for the faceted search implementation, including the faceted_search_service.py and the POST /api/query/search/faceted endpoint. Document the parameters accepted by the API (query, departments, file_types, date_from, date_to, entities, page, page_size), authentication requirements, and response format.",
            "status": "done",
            "testStrategy": "Review documentation for completeness and accuracy. Ensure all parameters, response formats, and authentication requirements are clearly explained.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Performance Optimization for Faceted Search",
            "description": "Analyze and optimize the performance of the faceted search implementation.",
            "dependencies": [
              3
            ],
            "details": "Profile the faceted search implementation to identify performance bottlenecks. Optimize SQL queries for facet value extraction and filtering. Implement caching strategies for frequently used facet values. Ensure pagination works efficiently with large result sets.",
            "status": "done",
            "testStrategy": "Benchmark search performance with various query combinations and result set sizes. Verify that response times remain acceptable under load and with complex facet combinations.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on faceted search & result presentation."
      },
      {
        "id": 25,
        "title": "Query Analytics & A/B Testing",
        "description": "Log queries, track latency, CTR, and support A/B testing for ranking algorithms.",
        "details": "Store query logs and result clicks in Supabase. Implement analytics dashboard and A/B test framework for ranking methods.",
        "testStrategy": "Analyze logs, verify CTR tracking, and run A/B tests.",
        "priority": "medium",
        "dependencies": [
          "24"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Query Logging and Metric Tracking in Supabase",
            "description": "Set up infrastructure to log all search queries, track latency, and record click-through rates (CTR) in Supabase.",
            "dependencies": [],
            "details": "Design Supabase tables to store query logs, including query text, timestamps, latency, and user interactions (clicks). Integrate logging into the query execution pipeline to ensure all relevant metrics are captured for each search event.",
            "status": "pending",
            "testStrategy": "Verify that queries, latency, and clicks are correctly logged in Supabase by running test queries and inspecting the stored data for completeness and accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop Analytics Dashboard for Query Metrics",
            "description": "Build a dashboard to visualize query volume, latency, and CTR using data from Supabase.",
            "dependencies": [
              1
            ],
            "details": "Use a dashboarding tool (e.g., Grafana or Streamlit) to connect to Supabase and display real-time and historical analytics for query metrics. Include filters for date ranges and ranking algorithm versions to support analysis.",
            "status": "pending",
            "testStrategy": "Check that the dashboard accurately reflects Supabase data by comparing dashboard metrics with direct database queries. Test responsiveness and filtering capabilities.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement and Run A/B Testing Framework for Ranking Algorithms",
            "description": "Create an A/B testing framework to compare different ranking algorithms by splitting user traffic and measuring impact on CTR and latency.",
            "dependencies": [
              1
            ],
            "details": "Randomly assign users or sessions to control and treatment groups, each using a different ranking algorithm. Log group assignment and outcomes in Supabase. Analyze results using statistical tests (e.g., t-test or Z-test) to determine significance of observed differences in metrics like CTR and latency[1][2][3].",
            "status": "pending",
            "testStrategy": "Simulate A/B tests with test users, verify correct group assignment and metric logging, and validate statistical analysis pipeline with sample data.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on query analytics & a/b testing.",
        "updatedAt": "2025-11-08T18:01:36.843Z"
      },
      {
        "id": 26,
        "title": "Chat UI Implementation (WebSocket, Streaming)",
        "description": "Build a mobile-responsive chat UI with real-time messaging and token-by-token streaming.",
        "status": "done",
        "dependencies": [
          "25"
        ],
        "priority": "high",
        "details": "Implemented with Gradio for frontend. Used HTTP-based streaming with async generators instead of WebSocket for simpler implementation and better reliability with Gradio. Integrated Clerk authentication, comprehensive error handling, retry logic, and mobile-responsive design.",
        "testStrategy": "Tested chat interactions, streaming, and mobile responsiveness across multiple devices. Verified error handling and retry logic.",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop Chat UI Frontend with Gradio or Streamlit",
            "description": "Create a mobile-responsive chat interface using Gradio or Streamlit, supporting user input, message display, and chat history.",
            "dependencies": [],
            "details": "Implemented the chat UI using Gradio's ChatInterface. Created chat_ui.py with mobile-responsive design and app_with_auth.py with Clerk authentication integration. Added custom CSS styling for improved mobile experience.",
            "status": "done",
            "testStrategy": "Manually tested UI on desktop and mobile browsers for responsiveness, usability, and correct message display.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Real-time Messaging for Chat Communication",
            "description": "Set up backend with HTTP-based streaming to handle real-time chat communication between frontend and backend.",
            "dependencies": [
              1
            ],
            "details": "Implemented HTTP-based streaming with async generators instead of WebSocket. This approach proved simpler to implement, more reliable with Gradio ChatInterface, and easier to deploy on Render while providing the same user experience.",
            "status": "done",
            "testStrategy": "Tested streaming implementation to verify real-time message delivery and connection stability.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Token-by-Token Streaming from Claude API",
            "description": "Enable streaming of Claude API responses token-by-token to the frontend for real-time chat experience.",
            "dependencies": [
              2
            ],
            "details": "Modified backend to call Claude API with streaming enabled. Implemented token-by-token streaming to the frontend using HTTP async generators. Updated frontend to append streamed tokens to the chat window in real time.",
            "status": "done",
            "testStrategy": "Sent prompts and verified that responses appeared incrementally in the chat UI, matching Claude API streaming output.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Loading Indicators and Error Handling",
            "description": "Add loading indicators for the assistant and robust error handling for network/API failures.",
            "dependencies": [
              3
            ],
            "details": "Added loading indicators (🔍 Processing your query...) while waiting for Claude API responses. Implemented comprehensive error handling with user-friendly messages. Added retry logic with exponential backoff for improved reliability.",
            "status": "done",
            "testStrategy": "Simulated slow responses and errors; verified loading indicator visibility and user-friendly error messages. Tested retry logic with network interruptions.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Test and Optimize Mobile Responsiveness",
            "description": "Thoroughly test the chat UI on various mobile devices and optimize for touch interaction and layout.",
            "dependencies": [
              4
            ],
            "details": "Used browser dev tools and real devices to test UI scaling, input usability, and scrolling. Applied custom CSS styling for optimal mobile experience. Deployed to production at https://jb-empire-chat.onrender.com with both authenticated (/chat) and non-authenticated versions.",
            "status": "done",
            "testStrategy": "Performed cross-device testing and collected feedback to ensure consistent, responsive behavior on phones and tablets.",
            "parentId": "undefined"
          }
        ],
        "complexity": 6.5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Expand chat UI implementation into subtasks for frontend development (Gradio/Streamlit), WebSocket endpoint implementation, streaming response handling, typing indicator/error handling, and mobile responsiveness testing."
      },
      {
        "id": 27,
        "title": "Conversation Memory System (Supabase Graph Tables)",
        "description": "Store and retrieve user conversation memory using PostgreSQL graph tables (user_memory_nodes, user_memory_edges).",
        "details": "Implement memory node/edge creation, context window management, and recency/access-weighted retrieval. Enforce RLS policies.",
        "testStrategy": "Simulate conversations, verify memory storage, retrieval, and RLS enforcement.",
        "priority": "high",
        "dependencies": [
          "26"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Implement Graph Tables for Conversation Memory",
            "description": "Create PostgreSQL tables (user_memory_nodes, user_memory_edges) to represent conversation memory as a graph structure, supporting efficient storage and retrieval.",
            "dependencies": [],
            "details": "Define schemas for user_memory_nodes and user_memory_edges, ensuring each node represents a memory item (e.g., message, context) and edges capture relationships (e.g., temporal, reference). Implement table creation scripts and indexes for efficient traversal. Ensure compatibility with Supabase and prepare for RLS enforcement.",
            "status": "done",
            "testStrategy": "Verify table creation, schema correctness, and ability to insert and query nodes/edges representing conversation history.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Memory Node/Edge Management and Context Window Logic",
            "description": "Develop logic for creating, updating, and deleting memory nodes and edges, and manage the context window for conversation retrieval.",
            "dependencies": [
              1
            ],
            "details": "Implement backend functions to add new conversation turns as nodes, link them with edges, and prune or limit history based on a context window (e.g., last N messages). Ensure recency and access-weighted retrieval logic is in place to prioritize relevant memory during retrieval. Integrate with Supabase API for transactional consistency.",
            "status": "done",
            "testStrategy": "Simulate conversations, add and remove nodes/edges, and verify that context window and recency/access-weighted retrieval return expected results.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Enforce Row-Level Security (RLS) and Validate Secure Access",
            "description": "Apply and test RLS policies to ensure users can only access their own conversation memory data in the graph tables.",
            "dependencies": [
              1,
              2
            ],
            "details": "Define and apply RLS policies on user_memory_nodes and user_memory_edges to restrict access by user identity. Test for unauthorized access attempts and verify that only the correct user's data is accessible. Document RLS configuration and integrate with Supabase authentication.",
            "status": "done",
            "testStrategy": "Attempt cross-user access, verify RLS enforcement, and run automated tests to confirm only authorized access to memory nodes and edges.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on conversation memory system (supabase graph tables)."
      },
      {
        "id": 28,
        "title": "Session & Preference Management",
        "description": "Support multiple concurrent sessions, session persistence, user preference learning, and privacy controls.",
        "details": "Implement session tracking, timeout, export, and deletion. Store preferences as memory nodes. Provide opt-out and explicit preference UI.",
        "testStrategy": "Test session persistence, preference learning, and privacy controls.",
        "priority": "medium",
        "dependencies": [
          "27"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Multi-Session Tracking and Persistence",
            "description": "Develop mechanisms to support multiple concurrent user sessions, ensure session data is persistent across server restarts, and enable session export and deletion.",
            "dependencies": [],
            "details": "Design a session management system that assigns unique, secure session IDs, supports concurrent sessions per user, and persists session data using a shared store (e.g., Redis). Implement session timeout, export, and deletion features. Ensure session data is securely stored and can be invalidated or removed on demand.",
            "status": "done",
            "testStrategy": "Simulate multiple concurrent sessions, verify session persistence after server restart, and test session export and deletion functionality.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop User Preference Learning and Storage",
            "description": "Create a system to learn, store, and update user preferences as memory nodes, ensuring preferences are associated with the correct session and user.",
            "dependencies": [
              1
            ],
            "details": "Implement logic to capture user actions and infer preferences, storing them as structured memory nodes linked to user profiles. Ensure updates are atomic and preferences persist across sessions. Provide mechanisms to retrieve and update preferences efficiently.",
            "status": "done",
            "testStrategy": "Test preference capture, retrieval, and update across multiple sessions and users. Validate that preferences persist and are correctly associated with users.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Design Privacy Controls and Explicit Preference UI",
            "description": "Provide user-facing controls for privacy, including opt-out options and an explicit UI for managing preferences and active sessions.",
            "dependencies": [
              1,
              2
            ],
            "details": "Develop UI components that allow users to view and manage their active sessions, export or delete session data, and opt out of preference learning. Ensure privacy controls are clear, accessible, and enforceable at the backend.",
            "status": "done",
            "testStrategy": "Perform UI/UX testing for privacy controls, verify backend enforcement of opt-out and deletion, and ensure users can manage sessions and preferences as intended.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on session & preference management."
      },
      {
        "id": 29,
        "title": "Monitoring & Observability (Prometheus, Grafana, Alertmanager)",
        "description": "Collect metrics, visualize in Grafana, set up alerting, and structured logging for all services.",
        "details": "Integrate prometheus_client for FastAPI, Celery, Redis, Neo4j. Build Grafana dashboards with pre-built panels. Configure Alertmanager for multi-channel alerts. Implement JSON logs and health check endpoints.",
        "testStrategy": "Simulate load, verify metrics, dashboard updates, alert triggers, and log accuracy.",
        "priority": "high",
        "dependencies": [
          "28"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Prometheus Metrics Collection for All Services",
            "description": "Set up Prometheus metrics collection for FastAPI, Celery, Redis, and Neo4j services using prometheus_client.",
            "dependencies": [],
            "details": "Install and configure prometheus_client in each service. Expose /metrics endpoints for FastAPI, Celery, Redis, and Neo4j. Ensure custom business metrics are included where relevant. Validate that metrics are accessible and correctly formatted for Prometheus scraping.",
            "status": "done",
            "testStrategy": "Simulate service activity and verify metrics are exposed and collected by Prometheus. Check for completeness and accuracy of metrics.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Build Grafana Dashboards and Panels for Metrics Visualization",
            "description": "Create Grafana dashboards with pre-built and custom panels to visualize collected metrics from all integrated services.",
            "dependencies": [
              1
            ],
            "details": "Connect Grafana to Prometheus as a data source. Design dashboards for FastAPI, Celery, Redis, and Neo4j, including panels for key metrics (e.g., request rates, error rates, resource usage). Use Grafana's dashboard editor to organize panels and set up useful visualizations for operational monitoring.",
            "status": "done",
            "testStrategy": "Verify dashboards update in real-time with incoming metrics. Confirm panels display accurate and actionable data for each service.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Configure Alertmanager for Multi-Channel Alerting and Notification Routing",
            "description": "Set up Alertmanager to handle alerts from Prometheus and Grafana, routing notifications to multiple channels (e.g., email, Slack).",
            "dependencies": [
              2
            ],
            "details": "Install and configure Alertmanager. Define alert rules in Prometheus and Grafana for critical metrics. Set up Alertmanager contact points for email, Slack, and other channels. Configure notification policies and silences as needed. Integrate Alertmanager with Grafana to manage and route alerts, ensuring unified notification handling[1][3][4][5][6].",
            "status": "done",
            "testStrategy": "Trigger test alerts and verify notifications are sent to all configured channels. Check alert deduplication, grouping, and routing logic.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Deploy Self-Hosted Langfuse on Render",
            "description": "Deploy Langfuse web service on Render using existing Supabase PostgreSQL database for LLM observability and cost tracking.",
            "details": "Deploy Langfuse Docker container to Render as a web service. Configure database connection to existing Supabase PostgreSQL (unified database architecture). Set environment variables: LANGFUSE_DATABASE_URL, NEXTAUTH_SECRET, NEXTAUTH_URL, SALT. Generate API keys after deployment and update .env file. Verify Langfuse UI is accessible and database tables are created. Cost: $7/month (Starter plan). Full deployment guide: .taskmaster/docs/LANGFUSE_INTEGRATION_PLAN.md (Phase 1: Deployment).",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 29,
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on monitoring & observability (prometheus, grafana, alertmanager)."
      },
      {
        "id": 30,
        "title": "Cost Tracking & Optimization",
        "description": "Track API, compute, and storage costs. Generate monthly reports and trigger budget alerts.",
        "details": "Integrate cost tracking for Claude, Soniox, Mistral, LangExtract, Render, Supabase, B2. Implement budget alert logic at 80% threshold.",
        "testStrategy": "Simulate usage, verify cost reports and alert triggers.",
        "priority": "medium",
        "dependencies": [
          "29"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Cost Tracking for All Services",
            "description": "Implement automated cost tracking for Claude, Soniox, Mistral, LangExtract, Render, Supabase, and B2, covering API, compute, and storage expenses.",
            "dependencies": [],
            "details": "Set up data pipelines or use APIs to collect cost and usage data from each provider. Normalize and aggregate costs by service and resource type. Ensure tracking supports multi-cloud and SaaS sources, and enables per-service breakdowns for accurate reporting.",
            "status": "done",
            "testStrategy": "Simulate usage across all services, verify that cost data is collected, normalized, and attributed correctly for each provider.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Generate Monthly Cost Reports",
            "description": "Develop automated monthly reporting that summarizes API, compute, and storage costs for all integrated services.",
            "dependencies": [
              1
            ],
            "details": "Design and implement a reporting system that compiles monthly cost data into clear, actionable reports. Include breakdowns by service, resource type, and time period. Reports should be exportable and support visualization for trend analysis.",
            "status": "done",
            "testStrategy": "Trigger monthly report generation with sample data, verify report accuracy, completeness, and clarity for all tracked services.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Budget Alert Logic at 80% Threshold",
            "description": "Set up automated alerts to notify stakeholders when spending reaches 80% of the defined monthly budget for any tracked service.",
            "dependencies": [
              1
            ],
            "details": "Configure monitoring logic to evaluate cumulative spend against budget thresholds in real time. Integrate with notification channels (e.g., email, Slack) to deliver timely alerts. Ensure alerts are actionable and include relevant cost breakdowns.",
            "status": "done",
            "testStrategy": "Simulate cost increases to exceed 80% of budget, confirm that alerts are triggered promptly and contain accurate, actionable information.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on cost tracking & optimization."
      },
      {
        "id": 31,
        "title": "Role-Based Access Control (RBAC) & API Key Management",
        "description": "Implement RBAC for users, documents, and API keys with audit logging and row-level security.",
        "details": "Use Supabase RLS policies, implement user roles (admin, editor, viewer, guest), API key creation/rotation/revocation, and audit logs. Hash API keys with bcrypt.",
        "testStrategy": "Test role permissions, API key flows, and audit log accuracy.",
        "priority": "high",
        "dependencies": [
          "30"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Implement User Roles and Row-Level Security (RLS) Policies in Supabase",
            "description": "Define user roles (admin, editor, viewer, guest) and implement row-level security (RLS) policies for users, documents, and API keys using Supabase.",
            "dependencies": [],
            "details": "Create a roles table and associate users with roles. Use Supabase's RLS policies to restrict access to tables based on user roles. Ensure that each role has clearly defined permissions for CRUD operations on users, documents, and API keys. Reference Supabase documentation and best practices for RLS and RBAC implementation.\n<info added on 2025-11-11T02:00:19.256Z>\nImplementation completed for User Roles and RLS Policies:\n\n✅ Database Schema Created:\n- Created roles table with 4 default roles (admin, editor, viewer, guest)\n- Created user_roles table for user-to-role mappings\n- Created api_keys table with bcrypt hashing\n- Created rbac_audit_logs table for immutable audit trail\n\n✅ RLS Policies Implemented:\n- Enabled RLS on all RBAC tables\n- roles table: read-only for authenticated users\n- api_keys table: users can only see/manage their own keys\n- user_roles table: users can read own roles, admins can manage all roles\n- rbac_audit_logs table: admin-only access\n\n✅ Default Roles Seeded:\n- admin: Full system access (all permissions)\n- editor: Can read/write documents\n- viewer: Can read documents only\n- guest: Limited read access\n\nFiles created:\n- app/models/rbac.py (Pydantic models)\n- app/core/supabase_client.py (Supabase helper)\n- Supabase migration applied successfully\n\nNext: Testing RLS policies and role permissions.\n</info added on 2025-11-11T02:00:19.256Z>",
            "status": "done",
            "testStrategy": "Test RLS policies by creating users with different roles and verifying access to resources. Attempt unauthorized actions to confirm enforcement of restrictions.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement API Key Lifecycle Management with Secure Storage",
            "description": "Develop endpoints and logic for API key creation, rotation, and revocation. Store API keys securely using bcrypt hashing.",
            "dependencies": [
              1
            ],
            "details": "Create endpoints for generating, rotating, and revoking API keys. Store only hashed versions of API keys using bcrypt in the database. Ensure that API keys are associated with users and roles, and that their permissions align with RBAC policies. Document the API key management process and enforce secure handling throughout the lifecycle.\n<info added on 2025-11-11T02:00:28.704Z>\nAPI Key Lifecycle Management Implementation Complete:\n\nAPI Key Generation:\n- Secure random token generation (64 hex chars)\n- Format: emp_[64-char-token]\n- Bcrypt hashing for secure storage\n- Key prefix extraction for fast lookup (emp_xxxxxxxx)\n\nAPI Key Operations Implemented:\n- create_api_key(): Generate new key with role assignment\n- validate_api_key(): Verify key with bcrypt check\n- list_api_keys(): List user's keys (prefix only, no full keys)\n- rotate_api_key(): Create new key, revoke old one atomically\n- revoke_api_key(): Permanently disable key with reason\n\nSecurity Features:\n- Full key shown ONLY once at creation\n- Automatic expiration checking\n- Usage tracking (last_used_at, usage_count)\n- Rate limiting support (rate_limit_per_hour field)\n- Ownership verification for all operations\n\nFiles Created:\n- app/services/rbac_service.py (Complete service implementation)\n- app/routes/rbac.py (FastAPI endpoints)\n- app/middleware/auth.py (Authentication middleware)\n\nIntegration:\n- RBAC router added to main.py at /api/rbac\n- Supports both API key and JWT authentication (JWT stub for future)\n</info added on 2025-11-11T02:00:28.704Z>",
            "status": "done",
            "testStrategy": "Verify API key creation, rotation, and revocation flows. Confirm that only hashed keys are stored and that revoked keys cannot be used for access.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Audit Logging for Access and Key Management Events",
            "description": "Track and log all access events, permission changes, and API key operations for auditing and compliance.",
            "dependencies": [
              1,
              2
            ],
            "details": "Set up audit logging for all RBAC-related actions, including role assignments, permission changes, and API key lifecycle events. Store logs in a dedicated audit table with relevant metadata (user, action, timestamp, resource). Ensure logs are immutable and accessible for compliance reviews.\n<info added on 2025-11-11T02:00:37.494Z>\nCompleted implementation of Audit Logging:\n\n✅ Audit Log Events Tracked:\n- api_key_created: When new key is generated\n- api_key_used: Every time key is validated/used\n- api_key_rotated: When key is rotated\n- api_key_revoked: When key is revoked\n- role_assigned: When role is granted to user\n- role_revoked: When role is removed from user\n\n✅ Audit Log Fields:\n- event_type: Type of event\n- actor_user_id: Who performed the action\n- target_user_id: Who was affected (for role operations)\n- target_resource_type: Type of resource (api_key, user_role)\n- target_resource_id: UUID of affected resource\n- action: Action performed (create, revoke, assign, etc.)\n- result: Outcome (success, failure, denied)\n- ip_address: IP of the request\n- user_agent: User agent string\n- metadata: Additional context (JSON)\n- error_message: Error details if failed\n- created_at: Immutable timestamp\n\n✅ Audit Features:\n- Immutable logs (insert-only, no updates)\n- Automatic logging in all RBAC operations\n- Admin-only access via RLS policies\n- Query filtering by event_type, user_id\n- Pagination support (limit/offset)\n\n✅ API Endpoint:\n- GET /api/rbac/audit-logs (admin only)\n- Supports filtering and pagination\n\nNext: Testing audit log accuracy and RLS enforcement.\n</info added on 2025-11-11T02:00:37.494Z>",
            "status": "done",
            "testStrategy": "Trigger various RBAC and API key events, then review audit logs to confirm accurate and complete recording of all relevant actions.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on role-based access control (rbac) & api key management."
      },
      {
        "id": 32,
        "title": "Bulk Document Management & Batch Operations",
        "description": "Enable bulk upload, delete, reprocessing, metadata update, versioning, and approval workflow for documents.",
        "details": "Implement batch endpoints for document operations. Track progress and support document versioning and approval states.",
        "testStrategy": "Perform bulk operations, verify throughput, versioning, and approval transitions.",
        "priority": "high",
        "dependencies": [
          "31"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Bulk Document Operations Endpoints",
            "description": "Develop RESTful API endpoints to support bulk upload, delete, reprocessing, and metadata update for documents.",
            "dependencies": [],
            "details": "Design and implement backend endpoints that accept batch requests for document operations. Ensure endpoints handle large payloads efficiently, support progress tracking, and provide clear error reporting for partial failures. Integrate with storage and indexing layers to maintain consistency and performance.\n<info added on 2025-11-11T21:02:25.181Z>\n## Investigation Results\n\n**Already Implemented:**\n1. ✅ All 4 bulk operation REST API endpoints in app/routes/documents.py:\n   - POST /bulk-upload\n   - POST /bulk-delete  \n   - POST /bulk-reprocess\n   - PATCH /bulk-metadata\n   - GET /batch-operations/{operation_id}\n   - GET /batch-operations\n\n2. ✅ All 4 Celery tasks in app/tasks/bulk_operations.py:\n   - bulk_upload_documents\n   - bulk_delete_documents\n   - bulk_reprocess_documents\n   - bulk_update_metadata\n   - Includes progress tracking and error handling\n\n**Missing - Need to Implement:**\nThe Celery tasks reference 4 functions from app.services.document_processor that don't exist yet:\n1. ❌ process_document_upload(file_path, filename, metadata, user_id, auto_process)\n2. ❌ delete_document(document_id, user_id, soft_delete)\n3. ❌ reprocess_document(document_id, user_id, force_reparse, update_embeddings, preserve_metadata)\n4. ❌ update_document_metadata(document_id, metadata, user_id)\n\nThe current document_processor.py only contains text extraction/parsing logic, not document management operations.\n\n**Next Steps:**\nNeed to create these 4 document management functions to complete Task 32.1.\n</info added on 2025-11-11T21:02:25.181Z>",
            "status": "done",
            "testStrategy": "Submit bulk operation requests (upload, delete, reprocess, metadata update) with varying batch sizes. Verify throughput, error handling, and data integrity for all operations.",
            "parentId": "undefined",
            "updatedAt": "2025-11-11T03:42:03.083Z"
          },
          {
            "id": 2,
            "title": "Integrate Document Versioning and Approval Workflow",
            "description": "Enable version control and approval states for documents, supporting batch transitions and rollbacks.",
            "dependencies": [
              1
            ],
            "details": "Extend the document model to support version history and approval status. Implement logic for batch versioning (e.g., uploading new versions in bulk) and batch approval/rejection. Ensure audit trails are maintained for all version and approval changes.",
            "status": "done",
            "testStrategy": "Perform bulk version uploads and approval transitions. Verify correct version history, approval state changes, and audit trail entries for all affected documents.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Progress Tracking and Operation Auditing",
            "description": "Track and expose the progress and audit logs of all batch document operations for transparency and compliance.",
            "dependencies": [
              1,
              2
            ],
            "details": "Develop mechanisms to monitor the status of ongoing batch operations, including per-document success/failure. Provide APIs or dashboards for users to query operation progress and review detailed audit logs. Ensure compliance with organizational and regulatory requirements for traceability.\n<info added on 2025-11-11T21:17:31.729Z>\n## Implementation Status: Complete\n\n**Progress Tracking (✅ Complete):**\n1. Models defined in app/models/documents.py:\n   - BatchOperationResponse (lines 92-106) - operation tracking with progress\n   - BatchOperationStatusResponse (lines 108-123) - detailed status with progress_percentage\n\n2. REST API endpoints in app/routes/documents.py:\n   - GET /api/documents/batch-operations/{operation_id} (lines 402-447) - Get specific operation status\n   - GET /api/documents/batch-operations (lines 450-505) - List all operations with filtering, pagination, and progress calculation\n\n3. Real-time progress updates in app/tasks/bulk_operations.py:\n   - _update_operation_status() helper function (lines 551-600)\n   - Called at start, during processing (per-document), and on completion\n   - Tracks: status, processed_items, successful_items, failed_items, results array\n\n**Operation Auditing (✅ Complete):**\n1. Database table: batch_operations (workflows/database_setup.md lines 609-624)\n   - Stores: operation_type, initiated_by, items counts, status, parameters, results\n   - Timestamps: started_at, completed_at, created_at\n   - JSONB fields for detailed parameters and results\n\n2. Approval workflow audit: approval_audit_log table with ApprovalAuditLogEntry model\n   - Tracks all approval state transitions\n   - Includes: event_type, status changes, user, IP address, user agent, timestamps\n\n3. Detailed result tracking:\n   - DocumentOperationResult model (lines 83-90) - per-document status with success/failure/error\n   - Stored in results JSONB array in batch_operations table\n\n**Compliance & Traceability (✅ Complete):**\n- Full audit trail for all batch operations\n- User tracking (initiated_by field)\n- Timestamp tracking (created_at, started_at, completed_at, updated_at)\n- Error message logging\n- Detailed per-document results\n</info added on 2025-11-11T21:17:31.729Z>",
            "status": "done",
            "testStrategy": "Initiate various batch operations and monitor progress tracking endpoints or dashboards. Validate that audit logs accurately reflect all actions, including errors and rollbacks.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on bulk document management & batch operations.",
        "updatedAt": "2025-11-11T03:42:03.083Z"
      },
      {
        "id": 33,
        "title": "User Management & GDPR Compliance",
        "description": "Support user creation, editing, role assignment, password reset, suspension, activity logs, and GDPR-compliant data export.",
        "details": "Implement admin endpoints for user management. Store activity logs and support data export/deletion per GDPR.",
        "testStrategy": "Test user flows, activity logging, and GDPR export/deletion.",
        "priority": "high",
        "dependencies": [
          "32"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement User Account and Role Management Endpoints",
            "description": "Develop admin endpoints to support user creation, editing, role assignment, password reset, and suspension.",
            "dependencies": [],
            "details": "Create RESTful endpoints for user CRUD operations, role assignment, and password management. Ensure endpoints allow for user suspension/reactivation and support both pre-defined and custom roles. Integrate secure authentication and authorization checks for all admin actions.\n<info added on 2025-11-11T21:36:53.612Z>\n## Implementation Details\n\n**Database Schema (Already Implemented)**\n- admin_users table (username, email, password_hash, role, is_active, etc.)\n- admin_sessions table (session tokens)\n- admin_activity_log table (action logging)\n\n**RBAC System (Already Implemented)**\n- API key lifecycle management\n- Role assignment/revocation functionality\n- Audit logging for RBAC events\n- Authentication middleware using API keys and JWT via Clerk\n- Authorization check middleware\n\n**Required User Management Endpoints**\n1. User CRUD operations:\n   - POST /api/users - Create new admin user\n   - GET /api/users - List all users with pagination/filtering\n   - GET /api/users/{user_id} - Retrieve specific user details\n   - PATCH /api/users/{user_id} - Update user information\n   - DELETE /api/users/{user_id} - Delete user account\n\n2. Password management:\n   - POST /api/users/{user_id}/reset-password - Admin-initiated reset\n   - POST /api/users/change-password - Self-service password change\n\n3. Account status management:\n   - POST /api/users/{user_id}/suspend - Suspend user account\n   - POST /api/users/{user_id}/activate - Reactivate suspended account\n\n**Implementation Plan**\n- Create app/routes/users.py with admin user management endpoints\n- Develop app/services/user_service.py for user operations\n- Define app/models/users.py for Pydantic models\n- Utilize bcrypt for password hashing\n- Integrate with admin_activity_log for comprehensive audit trail\n</info added on 2025-11-11T21:36:53.612Z>",
            "status": "done",
            "testStrategy": "Test user creation, editing, role assignment, password reset, and suspension via API and UI. Verify role-based access control and error handling.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Activity Logging for User Actions",
            "description": "Log all significant user management actions (creation, edits, role changes, suspensions, password resets) for audit and compliance.",
            "dependencies": [
              1
            ],
            "details": "Design and implement a logging mechanism to capture all admin and user actions related to user management. Store logs securely with timestamps, user IDs, action types, and relevant metadata. Ensure logs are immutable and accessible for compliance audits.\n<info added on 2025-11-11T21:42:57.585Z>\n## Investigation Status Update\n\nInitial investigation of logging mechanism reveals:\n\n1. Implementation Status:\n   - _log_activity() function is implemented in user_service.py\n   - Function is called by all user management operations\n   - Logs are written to admin_activity_log table\n\n2. Pending Verification:\n   - Database constraints and RLS policies need to be checked to ensure log immutability\n   - No endpoints currently exist for retrieving user activity logs for compliance audits\n\n3. Action Items:\n   - Implement read-only API endpoints for retrieving filtered activity logs\n   - Add database constraints to prevent modification of existing log entries\n   - Document the logging schema and retention policies\n   - Create test cases to verify logging functionality across all user management actions\n</info added on 2025-11-11T21:42:57.585Z>",
            "status": "done",
            "testStrategy": "Trigger user management actions and verify that logs are created with correct details. Test log retrieval and integrity.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Develop GDPR-Compliant Data Export and Deletion Features",
            "description": "Enable GDPR-compliant export and deletion of user data, including activity logs, upon user or admin request.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement endpoints to export all user-related data in a machine-readable format and to delete user data in accordance with GDPR requirements. Ensure deletion covers user profile, roles, and associated activity logs, and that exports are complete and secure.\n<info added on 2025-11-11T21:46:01.948Z>\n**Requirements Analysis:**\n\n1. Data Export Endpoint (GET /api/users/{user_id}/export):\n   - Export user profile data (username, email, full_name, role, etc.)\n   - Export all activity logs related to user (both as actor and subject)\n   - Export user sessions history\n   - Export API keys (without sensitive key material)\n   - Export user roles and permissions\n   - Format: JSON (machine-readable)\n   - Admin-only access\n\n2. Data Deletion Endpoint (DELETE /api/users/{user_id}/gdpr-delete):\n   - Delete user profile from admin_users table\n   - Delete/anonymize activity logs (preserve audit trail but remove PII)\n   - Delete all user sessions from admin_sessions table\n   - Revoke all user API keys\n   - Delete user role assignments\n   - Cascade deletion with proper foreign key handling\n   - Admin-only access with confirmation required\n\n**Implementation Plan:**\n- Add export_user_data() method to UserService\n- Add gdpr_delete_user() method to UserService\n- Add GDPR export/delete endpoints to users router\n- Add Pydantic models for export response\n- Consider: Activity logs should be anonymized rather than deleted for audit compliance\n</info added on 2025-11-11T21:46:01.948Z>",
            "status": "done",
            "testStrategy": "Request data export and deletion for test users. Verify completeness of exported data and confirm all user data is removed after deletion, including logs.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on user management & gdpr compliance."
      },
      {
        "id": 34,
        "title": "Analytics Dashboard Implementation",
        "description": "Build dashboard for document stats, query metrics, user activity, storage usage, and API endpoint usage.",
        "details": "Use Grafana or Streamlit for dashboard UI. Aggregate metrics from Supabase and Prometheus.",
        "testStrategy": "Verify dashboard accuracy and responsiveness under load.",
        "priority": "medium",
        "dependencies": [
          "33"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop Dashboard UI with Grafana or Streamlit",
            "description": "Set up the dashboard user interface using either Grafana or Streamlit, ensuring a logical layout for document stats, query metrics, user activity, storage usage, and API endpoint usage.",
            "dependencies": [],
            "details": "Install and configure Grafana or Streamlit. Design the dashboard structure, applying best practices such as focusing on key metrics, using consistent layouts, and providing clear panel documentation. Ensure the UI is intuitive and supports dynamic filtering or variable selection as needed.\n<info added on 2025-11-11T21:54:47.058Z>\nBased on the investigation findings, we will implement the analytics dashboard using Grafana since an existing infrastructure pattern is already established. We'll create a comprehensive dashboard with five main panel categories: document statistics, query metrics, user activity, storage usage, and API endpoint usage.\n\nThe implementation will follow this approach:\n1. Create a dedicated metrics service in app/services/metrics_service.py to collect and organize analytics data\n2. Add a Prometheus metrics endpoint in app/routes/monitoring.py to expose metrics for Grafana consumption\n3. Develop a Grafana dashboard JSON configuration at monitoring/grafana/dashboards/empire_analytics.json\n4. Follow the established pattern from the existing ragas_metrics.json dashboard for consistency\n\nThe dashboard will leverage the existing Grafana infrastructure while providing comprehensive visibility into system performance and usage patterns across all key operational areas.\n</info added on 2025-11-11T21:54:47.058Z>",
            "status": "done",
            "testStrategy": "Verify that all required metric categories are represented and the UI is navigable. Check for adherence to dashboard design best practices.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Aggregate Metrics from Supabase and Prometheus",
            "description": "Implement data aggregation logic to collect and preprocess metrics from Supabase and Prometheus for use in the dashboard.",
            "dependencies": [
              1
            ],
            "details": "Develop scripts or queries to extract relevant metrics (document stats, query metrics, user activity, storage usage, API endpoint usage) from Supabase and Prometheus. Transform and aggregate data as needed for efficient dashboard consumption. Ensure data freshness and reliability.",
            "status": "done",
            "testStrategy": "Validate that all required metrics are accurately aggregated and available for the dashboard. Test with sample data and edge cases.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Data Visualization Components",
            "description": "Create and configure visualizations for each metric category, ensuring clarity and actionable insights.",
            "dependencies": [
              2
            ],
            "details": "Select appropriate visualization types (e.g., graphs, tables, gauges) for each metric. Configure panels to highlight key signals and trends. Apply consistent color schemes and labeling. Add annotations or context where relevant to aid interpretation.",
            "status": "done",
            "testStrategy": "Review each visualization for accuracy, clarity, and alignment with dashboard goals. Solicit feedback from stakeholders and iterate as needed.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Test Dashboard Load and Responsiveness",
            "description": "Evaluate dashboard performance under expected and peak loads, optimizing for fast load times and responsive interactions.",
            "dependencies": [
              3
            ],
            "details": "Simulate concurrent users and high data volumes. Monitor dashboard load times, panel refresh rates, and responsiveness. Apply optimizations such as query aggregation, efficient variable usage, and appropriate refresh intervals. Document and address any bottlenecks.",
            "status": "done",
            "testStrategy": "Run load tests and measure key performance indicators (KPIs) such as load time and refresh latency. Confirm dashboard remains usable and responsive under stress.",
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down analytics dashboard implementation into subtasks for dashboard UI development (Grafana/Streamlit), metrics aggregation from Supabase/Prometheus, data visualization, and load/responsiveness testing."
      },
      {
        "id": 35,
        "title": "CrewAI Multi-Agent Integration & Orchestration",
        "description": "Integrate CrewAI service (REST API) for multi-agent workflows, agent management, and orchestration.",
        "details": "Connect to CrewAI REST API (srv-d2n0hh3uibrs73buafo0). Implement agent pool management, dynamic agent creation, lifecycle, and resource allocation. Support async task execution via Celery.",
        "testStrategy": "Run multi-agent workflows, verify orchestration, agent lifecycle, and result aggregation.",
        "priority": "high",
        "dependencies": [
          "34"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate CrewAI REST API for Agent Pool Management and Dynamic Agent Creation",
            "description": "Connect to the CrewAI REST API and implement logic for managing an agent pool, including dynamic creation, configuration, and lifecycle management of agents.",
            "dependencies": [],
            "details": "Establish secure connectivity to the CrewAI REST API (srv-d2n0hh3uibrs73buafo0). Implement endpoints and logic for creating, updating, and deleting agents dynamically. Support agent configuration (roles, goals, tools, memory, etc.) as per CrewAI's agent model. Ensure agents can be instantiated with custom parameters and maintain their lifecycle state.\n<info added on 2025-11-12T02:56:38.121Z>\nBased on the investigation, I'll enhance the CrewAI integration by implementing the following:\n\n1. Extend the existing crewai_service.py with comprehensive agent pool management methods:\n   - Agent CRUD operations: create_agent(), update_agent(), delete_agent(), get_agent(), get_agents()\n   - Crew management functions: create_crew(), update_crew(), delete_crew(), get_crew(), get_crews()\n   - Resource monitoring via get_agent_pool_stats() to track agent utilization and availability\n\n2. Implement Supabase database integration for the existing schema (crewai_agents, crewai_crews, crewai_task_templates, crewai_executions) to ensure persistent storage of agent configurations and execution history.\n\n3. Develop agent lifecycle management functionality including activation, deactivation, and status tracking.\n\n4. Create REST API routes in app/routes/crewai.py exposing agent and crew management endpoints.\n\n5. Connect with the existing CrewAI REST API at https://jb-crewai.onrender.com for agent execution and orchestration.\n</info added on 2025-11-12T02:56:38.121Z>",
            "status": "done",
            "testStrategy": "Create, update, and delete agents via API calls. Verify agent state transitions and configuration persistence.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Multi-Agent Workflow Orchestration and Resource Allocation",
            "description": "Develop orchestration logic to coordinate multi-agent workflows, manage task assignments, and allocate resources efficiently among agents.",
            "dependencies": [
              1
            ],
            "details": "Design and implement orchestration mechanisms using CrewAI's crew-and-flow model. Enable both sequential and parallel task execution modes. Assign tasks to agents based on their roles and goals, and manage dependencies between tasks. Implement resource allocation strategies to optimize agent utilization and prevent overload.",
            "status": "done",
            "testStrategy": "Run sample multi-agent workflows with varying complexity. Verify correct task sequencing, parallelism, and resource allocation.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Enable Asynchronous Task Execution and Monitoring via Celery",
            "description": "Integrate Celery to support asynchronous execution of agent tasks and implement monitoring for workflow progress and agent states.",
            "dependencies": [
              2
            ],
            "details": "Set up Celery workers to handle asynchronous task execution for CrewAI workflows. Ensure tasks can be queued, executed, and monitored independently. Capture logs and state changes for each agent and workflow. Implement error handling and alerting for failed tasks or agent exceptions.",
            "status": "done",
            "testStrategy": "Submit multiple concurrent workflows, monitor execution progress, and verify correct handling of asynchronous tasks and error scenarios.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on crewai multi-agent integration & orchestration."
      },
      {
        "id": 36,
        "title": "CrewAI Asset Generation Agents Implementation",
        "description": "Implement 8 asset generation agents (orchestrator, summarizer, skill, command, agent, prompt, workflow, department classifier) per PRD specs.",
        "details": "Define agent roles, goals, tools, and LLM configs in crewai_agents table. Integrate with CrewAI API for asset generation. Store outputs in B2 processed/ folders.",
        "testStrategy": "Trigger asset generation for sample documents, verify output formats and B2 storage.",
        "priority": "high",
        "dependencies": [
          "35"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define and Configure 8 Asset Generation Agents in crewai_agents Table",
            "description": "Specify roles, goals, tools, and LLM configurations for orchestrator, summarizer, skill, command, agent, prompt, workflow, and department classifier agents as per PRD specifications.",
            "dependencies": [],
            "details": "Draft detailed agent definitions in the crewai_agents table, ensuring each agent's role, goal, toolset, and LLM configuration aligns with PRD requirements. Use YAML or database schema as appropriate. Validate configuration completeness for all 8 agents.",
            "status": "done",
            "testStrategy": "Review crewai_agents table for correct entries and completeness. Validate agent configs load without errors in CrewAI.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Agents with CrewAI API for Asset Generation Workflows",
            "description": "Connect the configured agents to the CrewAI API, enabling them to generate assets according to workflow requirements.",
            "dependencies": [
              1
            ],
            "details": "Implement integration logic to instantiate and orchestrate the 8 agents using the CrewAI API. Ensure agents can receive tasks, execute asset generation, and interact as needed. Handle API authentication and error management.",
            "status": "done",
            "testStrategy": "Trigger asset generation for sample inputs via CrewAI API and verify that each agent performs its designated function.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Store Generated Assets in B2 Processed Folders",
            "description": "Implement logic to save all outputs from asset generation agents into the appropriate B2 processed/ folders.",
            "dependencies": [
              2
            ],
            "details": "Develop or update storage routines to ensure all generated assets are saved in the correct B2 processed/ directory structure. Confirm metadata and output formats match requirements. Handle storage errors and ensure data integrity.",
            "status": "done",
            "testStrategy": "Generate assets through the workflow and verify their presence, structure, and metadata in B2 processed/ folders.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on crewai asset generation agents implementation."
      },
      {
        "id": 37,
        "title": "CrewAI Document Analysis Agents Implementation",
        "description": "Implement 3 document analysis agents (research analyst, content strategist, fact checker) for structured analysis and verification.",
        "details": "Configure agents in crewai_agents table. Integrate with CrewAI API for analysis workflows. Store analysis outputs in Supabase and B2.",
        "testStrategy": "Run analysis workflows, verify structured outputs and fact verification accuracy.",
        "priority": "high",
        "dependencies": [
          "36"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Document Analysis Agents in crewai_agents Table",
            "description": "Define and register the three specialized agents (research analyst, content strategist, fact checker) in the crewai_agents table with appropriate roles, goals, and capabilities.",
            "dependencies": [],
            "details": "Specify agent roles, goals, and backstories in the crewai_agents table or agents.yaml. Ensure each agent is configured for its analysis specialization and can be referenced by workflows. Use CrewAI's agent configuration standards for compatibility.",
            "status": "done",
            "testStrategy": "Verify agents appear in the crewai_agents table and can be instantiated by CrewAI workflows.",
            "parentId": "undefined",
            "updatedAt": "2025-11-14T18:18:43.346Z"
          },
          {
            "id": 2,
            "title": "Integrate Agents with CrewAI API for Analysis Workflows",
            "description": "Connect the configured agents to the CrewAI API, enabling them to participate in document analysis workflows.",
            "dependencies": [
              1
            ],
            "details": "Implement API integration logic to allow the agents to receive tasks, process documents, and return structured outputs. Ensure agents can be triggered via the CrewAI API and handle input/output formats as required by the workflow.",
            "status": "done",
            "testStrategy": "Trigger sample analysis workflows via the API and confirm agents process and return structured results.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Document Analysis Workflow Execution",
            "description": "Design and execute workflows that coordinate the three agents for structured document analysis and verification.",
            "dependencies": [
              2
            ],
            "details": "Define workflow logic that assigns documents to the appropriate agents, sequences their tasks (e.g., research, content strategy, fact checking), and aggregates their outputs. Use CrewAI's workflow orchestration features to manage task flow.",
            "status": "done",
            "testStrategy": "Run end-to-end workflow executions and verify that each agent performs its designated analysis step.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Store Analysis Outputs in Supabase and B2",
            "description": "Persist the structured outputs from each agent in Supabase for structured data and B2 for file storage.",
            "dependencies": [
              3
            ],
            "details": "Implement logic to map agent outputs to Supabase tables for structured results and upload any relevant files or artifacts to B2. Ensure outputs are linked to the correct document and agent metadata.",
            "status": "done",
            "testStrategy": "Check Supabase and B2 for correct storage of outputs after workflow execution; verify data integrity and retrievability.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Test and Validate Agent Output Accuracy and Fact Verification",
            "description": "Systematically test the accuracy of agent outputs, with a focus on fact-checking reliability and structured result formats.",
            "dependencies": [
              4
            ],
            "details": "Develop test cases with known document inputs and expected outputs. Evaluate the correctness of research, content strategy, and fact-checking results. Measure fact-checker precision and recall, and validate output structure.",
            "status": "done",
            "testStrategy": "Run automated and manual tests comparing outputs to ground truth; review fact-checking results for accuracy and completeness.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down document analysis agent implementation into subtasks for agent configuration, CrewAI API integration, workflow execution, output storage in Supabase/B2, and accuracy testing.",
        "updatedAt": "2025-11-14T18:18:43.346Z"
      },
      {
        "id": 38,
        "title": "CrewAI Multi-Agent Orchestration Agents Implementation",
        "description": "Implement 4 orchestration agents (research, analysis, writing, review) for complex multi-document workflows.",
        "details": "Configure agents and crews in crewai_crews table. Support sequential and parallel execution modes. Integrate with CrewAI API for orchestration.",
        "testStrategy": "Run multi-agent orchestration workflows, verify execution order and output quality.",
        "priority": "high",
        "dependencies": [
          "37"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Orchestration Agents and Crews in crewai_crews Table",
            "description": "Define and register the four orchestration agents (research, analysis, writing, review) and their crew configurations in the crewai_crews table.",
            "dependencies": [],
            "details": "Specify agent roles, goals, backstories, and advanced options (e.g., LLM, delegation, tools) for each agent. Ensure each agent is correctly mapped to its crew and that the crew structure supports both sequential and parallel execution modes.",
            "status": "done",
            "testStrategy": "Verify agents and crews are correctly listed in the crewai_crews table and can be retrieved via API.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Sequential and Parallel Execution Logic for Agent Workflows",
            "description": "Develop logic to support both sequential and parallel execution of agent tasks within a crew for multi-document workflows.",
            "dependencies": [
              1
            ],
            "details": "Design execution engine to trigger agents in order (sequential) or concurrently (parallel) based on workflow configuration. Ensure correct handling of dependencies and data flow between agents.",
            "status": "done",
            "testStrategy": "Run sample workflows in both modes, confirm correct execution order and data handoff.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate CrewAI API for Orchestration and Agent Lifecycle Management",
            "description": "Connect orchestration logic to CrewAI API endpoints for agent invocation, status tracking, and result retrieval.",
            "dependencies": [
              2
            ],
            "details": "Implement API calls for agent task submission, monitor agent progress, and handle callbacks or polling for completion. Ensure robust error handling and retries.",
            "status": "done",
            "testStrategy": "Trigger agent workflows via API, verify correct agent lifecycle events and result collection.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Develop Workflow Management and State Tracking Mechanisms",
            "description": "Create workflow management logic to track the state, progress, and dependencies of multi-agent, multi-document workflows.",
            "dependencies": [
              3
            ],
            "details": "Implement state machine or workflow tracker to monitor each agent's status, handle transitions, and manage workflow metadata. Support resumption and recovery from failures.",
            "status": "done",
            "testStrategy": "Simulate workflow interruptions and restarts, verify accurate state tracking and recovery.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Output Validation and Quality Assurance for Agent Results",
            "description": "Design and apply validation checks to ensure agent outputs meet expected quality, format, and completeness standards.",
            "dependencies": [
              4
            ],
            "details": "Define validation rules for each agent type (e.g., research completeness, analysis accuracy, writing coherence, review thoroughness). Integrate automated and optional human-in-the-loop checks.",
            "status": "done",
            "testStrategy": "Run workflows with known-good and intentionally flawed inputs, verify validation catches errors and approves correct outputs.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Develop and Execute Comprehensive Orchestration Testing Suite",
            "description": "Create automated tests to validate orchestration logic, agent integration, workflow management, and output quality across various scenarios.",
            "dependencies": [
              5
            ],
            "details": "Design test cases for sequential and parallel workflows, error handling, state recovery, and output validation. Use both unit and integration tests to ensure system robustness.",
            "status": "done",
            "testStrategy": "Run full test suite, confirm all orchestration paths and edge cases are covered and pass.",
            "parentId": "undefined"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Expand orchestration agent implementation into subtasks for agent/crew configuration, sequential/parallel execution logic, CrewAI API integration, workflow management, output validation, and orchestration testing."
      },
      {
        "id": 39,
        "title": "CrewAI Inter-Agent Messaging & Collaboration",
        "description": "Enable inter-agent messaging, task delegation, result sharing, and conflict resolution within CrewAI workflows.",
        "details": "Implement agent interactions in crewai_agent_interactions table. Support direct/broadcast messaging, event publication, and state synchronization.",
        "testStrategy": "Simulate collaborative workflows, verify messaging, delegation, and result aggregation.",
        "priority": "high",
        "dependencies": [
          "38"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Inter-Agent Interaction Schema",
            "description": "Define the database schema and data model for agent interactions, supporting messaging, delegation, event publication, and state synchronization.",
            "dependencies": [],
            "details": "Create or update the crewai_agent_interactions table to capture direct/broadcast messages, event logs, delegation records, and state changes. Ensure extensibility for future collaboration features.",
            "status": "done",
            "testStrategy": "Review schema against requirements; validate with sample interaction records.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Direct and Broadcast Messaging Logic",
            "description": "Develop backend logic for agents to send direct and broadcast messages to other agents within a crew.",
            "dependencies": [
              1
            ],
            "details": "Implement API endpoints and internal functions for direct (agent-to-agent) and broadcast (agent-to-crew) messaging. Store messages in the interaction table and trigger notifications as needed.",
            "status": "done",
            "testStrategy": "Unit test message delivery, verify correct routing and storage for both direct and broadcast cases.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Develop Event Publication Mechanism",
            "description": "Enable agents to publish events (e.g., task completion, delegation, errors) for workflow coordination and monitoring.",
            "dependencies": [
              1
            ],
            "details": "Implement event publishing logic, allowing agents to emit structured events to the crewai_agent_interactions table. Support event subscription and notification for relevant agents.",
            "status": "done",
            "testStrategy": "Simulate event publication and subscription; verify event propagation and logging.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement State Synchronization Across Agents",
            "description": "Ensure agents maintain consistent shared state during collaborative workflows, including task progress and result sharing.",
            "dependencies": [
              1
            ],
            "details": "Design and implement mechanisms for agents to synchronize state changes (e.g., task status, shared data) via the interaction table or dedicated state sync service. Handle concurrent updates and conflict scenarios.",
            "status": "done",
            "testStrategy": "Test state updates under concurrent agent actions; verify consistency and conflict handling.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Integrate Conflict Resolution Logic",
            "description": "Develop logic for detecting and resolving conflicts between agents, such as task assignment disputes or inconsistent states.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Implement automated and/or human-in-the-loop conflict resolution workflows. Log conflict events, trigger resolution protocols, and update agent states accordingly.",
            "status": "done",
            "testStrategy": "Simulate conflict scenarios; verify detection, resolution, and state updates.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Simulate and Test Collaborative Workflow Scenarios",
            "description": "Create and execute end-to-end workflow simulations to validate inter-agent messaging, delegation, event handling, state sync, and conflict resolution.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "Design test scenarios covering typical and edge-case collaborative workflows. Automate simulation runs and verify expected outcomes in the interaction table and agent states.",
            "status": "done",
            "testStrategy": "Run integration tests for full workflows; check messaging, event logs, state consistency, and conflict resolution.",
            "parentId": "undefined"
          }
        ],
        "complexity": 8.5,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down inter-agent messaging and collaboration into subtasks for designing the interaction schema, implementing direct/broadcast messaging, event publication, state synchronization, conflict resolution, and workflow simulation testing."
      },
      {
        "id": 40,
        "title": "CrewAI Asset Storage & Retrieval",
        "description": "Store generated assets in crewai_generated_assets table and B2, enable retrieval by department, type, and confidence.",
        "details": "Implement asset storage logic, organize B2 folders, and support asset retrieval APIs. Track confidence scores and metadata.",
        "testStrategy": "Generate and retrieve assets, verify storage, organization, and retrieval accuracy.",
        "priority": "high",
        "dependencies": [
          "39"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Asset Storage Logic in crewai_generated_assets Table and B2",
            "description": "Design and implement the logic to store generated assets in the crewai_generated_assets database table and organize them in B2 cloud storage.",
            "dependencies": [],
            "details": "Define the schema for asset metadata, including department, type, and confidence score. Integrate asset generation outputs with the database and B2 storage. Ensure assets are stored in organized B2 folders based on department and type, and metadata is consistently tracked in the database.\n<info added on 2025-11-13T20:40:40.237Z>\nIMPLEMENTATION COMPLETE (2025-01-13):\n\n✅ Database Schema:\n- crewai_generated_assets table created in Supabase production\n- All columns implemented: id, execution_id, document_id, department, asset_type, asset_name, content, content_format, b2_path, file_size, mime_type, metadata, confidence_score, created_at\n- Foreign keys configured: execution_id → crewai_executions, document_id → documents\n\n✅ Service Implementation:\n- app/services/crewai_asset_service.py (324 lines)\n- store_asset() method handles both text-based and file-based assets\n- Text assets: stored in DB content column\n- File assets: uploaded to B2, b2_path stored in DB\n- B2 folder organization: crewai/assets/{department}/{asset_type}/{execution_id}/{filename}\n\n✅ Pydantic Models:\n- app/models/crewai_asset.py (173 lines)\n- AssetStorageRequest, AssetResponse, AssetUpdateRequest, AssetListResponse, AssetRetrievalFilters\n- Enums: AssetType, Department, ContentFormat\n\nStatus: READY FOR TESTING\n</info added on 2025-11-13T20:40:40.237Z>",
            "status": "done",
            "testStrategy": "Create sample assets, store them, and verify correct database entries and B2 folder organization.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop Asset Retrieval APIs by Department, Type, and Confidence",
            "description": "Build APIs to enable retrieval of stored assets filtered by department, asset type, and confidence score.",
            "dependencies": [
              1
            ],
            "details": "Design RESTful endpoints for asset retrieval. Implement query logic to filter assets using department, type, and confidence score from the crewai_generated_assets table and B2 storage. Ensure efficient and secure access to asset files and metadata.\n<info added on 2025-11-13T20:41:13.454Z>\nIMPLEMENTATION COMPLETE (2025-01-13):\n\n✅ API Routes Implemented:\n- app/routes/crewai_assets.py (284 lines)\n- Router prefix: /api/crewai/assets\n- Tags: [\"CrewAI Assets\"]\n\n✅ Endpoints:\n1. POST /api/crewai/assets/ - Store asset (text or file-based)\n2. GET /api/crewai/assets/ - Retrieve with filters (department, asset_type, confidence, pagination)\n3. GET /api/crewai/assets/{asset_id} - Get single asset by ID\n4. PATCH /api/crewai/assets/{asset_id} - Update confidence score and metadata\n5. GET /api/crewai/assets/execution/{execution_id} - Get all assets for execution\n\n✅ Filter Implementation:\n- execution_id (UUID)\n- department (enum: marketing, legal, hr, finance, etc.)\n- asset_type (enum: summary, analysis, report, etc.)\n- min_confidence / max_confidence (0-1)\n- limit (max 1000)\n- offset (pagination)\n\n✅ Service Integration:\n- Uses CrewAIAssetService via dependency injection\n- Full error handling (400, 404, 500)\n- Logging for all operations\n\nStatus: IMPLEMENTATION COMPLETE - Need to verify route registration in main.py\n</info added on 2025-11-13T20:41:13.454Z>",
            "status": "done",
            "testStrategy": "Test API endpoints with various filter combinations and validate that correct assets and metadata are returned.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Track and Update Asset Confidence Scores and Metadata",
            "description": "Implement mechanisms to track, update, and manage confidence scores and metadata for each asset throughout its lifecycle.",
            "dependencies": [
              1
            ],
            "details": "Add logic to update confidence scores and metadata in the crewai_generated_assets table as assets are processed or reviewed. Ensure changes are reflected in both the database and B2 storage organization if relevant. Provide audit trails for metadata updates.\n<info added on 2025-11-13T20:41:18.472Z>\nIMPLEMENTATION COMPLETE (2025-01-13):\n\n✅ Confidence Score Tracking:\n- Database column: confidence_score (float, nullable)\n- Validation: 0.0 - 1.0 range enforced in Pydantic models\n- Initial score set during asset creation\n- Update via PATCH /api/crewai/assets/{asset_id}\n\n✅ Metadata Management:\n- Database column: metadata (JSONB, default {})\n- Stored in Supabase as structured JSON\n- Full flexibility for custom metadata fields\n- MERGE behavior: new metadata merged with existing (preserves existing keys)\n- Update via AssetUpdateRequest model\n\n✅ Update Method (app/services/crewai_asset_service.py):\n- update_asset(asset_id, update_request)\n- Fetches existing asset\n- Merges metadata: {**existing.metadata, **update.metadata}\n- Updates confidence_score if provided\n- Returns updated AssetResponse\n\n✅ API Endpoint:\n- PATCH /api/crewai/assets/{asset_id}\n- Request: {confidence_score?: float, metadata?: dict}\n- Response: Updated AssetResponse\n- Errors: 404 (not found), 400 (invalid), 500 (server error)\n\nStatus: READY FOR TESTING\n</info added on 2025-11-13T20:41:18.472Z>",
            "status": "done",
            "testStrategy": "Simulate asset review and update workflows, verify that confidence scores and metadata are correctly updated and tracked.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on crewai asset storage & retrieval."
      },
      {
        "id": 41,
        "title": "Security Hardening & Compliance",
        "description": "Implement JWT authentication, RBAC, encrypted storage, input validation, and compliance features (GDPR, HIPAA, SOC 2).",
        "details": "Use PyJWT for authentication, enforce RBAC, encrypt Supabase volumes and B2 files, validate inputs, and implement audit trails. Support data export/deletion for GDPR.",
        "testStrategy": "Run security tests, penetration testing, and compliance checks.",
        "priority": "high",
        "dependencies": [
          "40"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement JWT Authentication with PyJWT",
            "description": "Set up secure JWT authentication using PyJWT, ensuring best practices for token issuance, validation, and storage.",
            "dependencies": [],
            "details": "Configure PyJWT to use strong signing algorithms (e.g., RS256), set short expiration times, validate all claims (issuer, audience, expiration), and store tokens securely (prefer HttpOnly cookies). Avoid storing sensitive data in JWTs and ensure all token transmission uses HTTPS.\n<info added on 2025-11-14T19:10:23.776Z>\n## Current Status\nJWT authentication implemented via Clerk integration (app/middleware/clerk_auth.py) with session token verification working.\n\n## Required Security Enhancements\n1. Add rate limiting to authentication endpoints using slowapi library\n2. Implement token refresh endpoint with refresh token rotation\n3. Add session timeout middleware with both idle and absolute timeout enforcement\n4. Ensure HTTPS-only transmission in production environment\n\n## Implementation Files\n- Existing: app/middleware/clerk_auth.py (JWT verification)\n- Existing: app/middleware/auth.py (JWT/API key validation)\n- New: app/middleware/rate_limit.py (for API rate limiting)\n- New: app/routes/auth.py (token refresh endpoint)\n\n## Security Assessment\nAuthentication foundation is solid. Focus should be on hardening through rate limiting and robust session management.\n</info added on 2025-11-14T19:10:23.776Z>\n<info added on 2025-11-14T19:38:41.247Z>\n## Implementation Complete\n\nSecurity hardening implementation for JWT authentication has been successfully completed with the following components:\n\n### Rate Limiting\n- Implemented using slowapi>=0.1.9\n- Created app/middleware/rate_limit.py with tiered limits:\n  - Auth endpoints: 5 login attempts/minute, 3 registrations/hour\n  - API key management: 10 creates/hour, 20 revocations/minute\n  - File uploads: 50/hour for single, 10/hour for bulk\n  - Query endpoints: 100/minute for simple, 20/minute for complex\n- Uses Redis in production, in-memory storage in development\n- Per-user and per-IP rate limiting with proper headers\n\n### Security Headers Middleware\n- Created app/middleware/security.py with SecurityHeadersMiddleware\n- Implemented headers: HSTS, X-Content-Type-Options, X-Frame-Options, X-XSS-Protection, Referrer-Policy, Permissions-Policy, and Content-Security-Policy\n- Environment-specific configurations with relaxed settings for documentation endpoints\n\n### CORS Hardening\n- Updated configuration in app/main.py with explicit HTTP methods\n- Environment-based configuration with production warnings\n\n### Testing\n- Created comprehensive test_task41_security.py (320 lines)\n- Tests for headers, rate limiting, CORS, and overall API health\n\n### Files Modified/Created\n- requirements.txt: Added slowapi and redis\n- app/middleware/security.py: NEW (180 lines)\n- app/middleware/rate_limit.py: NEW (260 lines)\n- app/main.py: MODIFIED\n- test_task41_security.py: NEW (320 lines)\n\n### Security Improvements\n- Protection against brute force, clickjacking, MIME sniffing, XSS\n- HTTPS enforcement in production\n- Information disclosure prevention\n- DoS protection through rate limiting\n</info added on 2025-11-14T19:38:41.247Z>",
            "status": "done",
            "testStrategy": "Unit test token issuance and validation, attempt token tampering, and verify rejection of invalid or expired tokens.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Enforce Role-Based Access Control (RBAC)",
            "description": "Integrate RBAC to restrict access to resources based on user roles and permissions.",
            "dependencies": [
              1
            ],
            "details": "Design a roles and permissions schema. Implement middleware to check user roles (from identity, not from JWT claims) on each protected endpoint. Ensure permissions are managed in the authorization layer, not embedded in JWTs.\n<info added on 2025-11-14T19:10:28.200Z>\n## Current Status\nRBAC fully implemented with 4 roles (admin, editor, viewer, guest). Complete lifecycle management in app/services/rbac_service.py. Database tables exist (users, roles, user_roles, api_keys).\n\n## Implementation Details\n- Role permission checking: app/middleware/auth.py:require_admin(), require_role()\n- API key lifecycle: generation, rotation, revocation, expiration\n- Bcrypt hashing for API keys (never stores plaintext)\n- Database schema ready in Supabase\n\n## Additional Work Needed\n- Row-Level Security (RLS) policies on all user-facing tables (CRITICAL)\n- API key scope validation (scopes field exists but not enforced)\n- Permission cache invalidation for role updates\n\n## Focus Areas\n1. Design and implement PostgreSQL RLS policies for data isolation\n2. Add scope validation middleware for API keys\n3. Test user data isolation at database level\n\n## Security Assessment\nAuthorization system is production-ready. Main gap is RLS enforcement.\n</info added on 2025-11-14T19:10:28.200Z>",
            "status": "done",
            "testStrategy": "Test endpoints with users of different roles, verify access is correctly granted or denied according to RBAC rules.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Set Up Encrypted Storage for Supabase and B2 Files",
            "description": "Encrypt all data at rest in Supabase volumes and Backblaze B2 file storage.",
            "dependencies": [],
            "details": "Enable encryption for Supabase storage volumes and configure server-side encryption for B2 buckets. Ensure encryption keys are securely managed and rotated according to policy.\n<info added on 2025-11-14T19:10:32.282Z>\n## Current Status\nFile encryption implementation is EXCELLENT with AES-256-GCM in app/services/encryption.py featuring:\n- 256-bit keys with PBKDF2 (100k iterations)\n- Random salts and nonces per file\n- Authenticated encryption with GCM mode\n- B2 integration ready\n- Test coverage in tests/test_encryption.py\n\n## Additional Work Needed\n- Verify Supabase encryption-at-rest is enabled\n- Confirm TLS for all database connections (Neo4j already using TLS with bolt+ssc://localhost:7687)\n- Add key rotation policies\n- Optional: Integrate with AWS KMS or HashiCorp Vault for key management\n\n## Verification Tasks\n1. Check Supabase project settings for encryption-at-rest\n2. Verify B2 server-side encryption configuration\n3. Document encryption key management procedures\n4. Test file encryption/decryption with B2 upload\n\n## Security Assessment\nEncryption implementation is production-grade. Focus should be on verification and key management procedures.\n</info added on 2025-11-14T19:10:32.282Z>",
            "status": "done",
            "testStrategy": "Verify files and database volumes are encrypted at rest, attempt unauthorized access to raw storage, and confirm data is unreadable without decryption keys.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Comprehensive Input Validation",
            "description": "Validate all user and API inputs to prevent injection and data integrity issues.",
            "dependencies": [],
            "details": "Apply strict input validation on all endpoints using whitelisting and schema validation. Sanitize inputs to prevent SQL injection, XSS, and other common attacks. Use libraries for validation where possible.\n<info added on 2025-11-14T19:10:37.398Z>\n## Current Status\nInput validation implementation is GOOD. Pydantic models are used throughout the codebase (7 model files) with:\n- Type hints and Field() constraints\n- File upload validation (whitelist, 100MB limit, 10 files max)\n- Email validation with EmailStr\n- Custom validators for key fields\n\n## Files With Validation\n- app/models/rbac.py (RBAC validation)\n- app/models/documents.py (document validation)\n- app/models/users.py (user validation)\n- app/api/upload.py (file upload validation)\n\n## Additional Work Needed\n- Request body size limits middleware (prevent DoS)\n- Custom validators for SQL injection prevention\n- Path traversal validation (no ../, null bytes)\n- XSS prevention in metadata fields\n- Rate limiting on all API endpoints\n\n## Hardening Tasks\n1. Add max_body_size middleware\n2. Create security validators for:\n   - Document paths\n   - Query parameters\n   - Metadata values\n3. Audit all database queries for parameterization\n4. Add input sanitization for user-generated content\n\n## Security Assessment\nValidation foundation is solid. Need additional hardening for edge cases.\n</info added on 2025-11-14T19:10:37.398Z>",
            "status": "done",
            "testStrategy": "Fuzz endpoints with invalid and malicious inputs, verify that invalid data is rejected and no vulnerabilities are introduced.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Audit Trail and Logging",
            "description": "Create an audit trail system to log security-relevant events and user actions for compliance and forensic analysis.",
            "dependencies": [
              1,
              2
            ],
            "details": "Log authentication events, access control decisions, data exports/deletions, and administrative actions. Ensure logs are tamper-evident and securely stored. Provide tools for querying and exporting audit logs.\n<info added on 2025-11-14T19:10:44.198Z>\n## Current Status\nAudit logging partially implemented with structlog throughout the application. AuditLogEntry model defined in app/models/rbac.py with all required fields (event_type, actor, target, IP, user_agent, metadata).\n\n## Events Currently Logged\n- Authentication attempts (success/failure)\n- API key creation/rotation/revocation\n- Role assignments\n- Access denials\n\n## Critical Gap\nLogs are only stored in application logs, not persisted to database, preventing querying for compliance or incident investigation purposes.\n\n## Implementation Plan\n1. Create audit_logs table in Supabase with AuditLogEntry schema\n2. Create app/middleware/audit.py to persist all security events\n3. Add audit log query/search endpoints in app/routes/audit.py\n4. Implement log retention policies (90 days active, 7 years archive)\n5. Extract IP address and User-Agent from requests\n6. Make logs tamper-evident (append-only, signed)\n\n## Database Schema\n- Table: audit_logs\n- Columns: id, event_type, actor_user_id, target_user_id, target_resource_type, target_resource_id, action, result, ip_address, user_agent, metadata (JSONB), error_message, created_at\n\n## Security Assessment\nFoundation exists but high priority to persist logs to database for compliance and security investigation capabilities.\n</info added on 2025-11-14T19:10:44.198Z>",
            "status": "done",
            "testStrategy": "Trigger various security events, verify logs are generated, immutable, and contain all required information.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Integrate Compliance Features (GDPR, HIPAA, SOC 2)",
            "description": "Implement features to meet GDPR, HIPAA, and SOC 2 requirements, including data export/deletion and privacy controls.",
            "dependencies": [
              3,
              4,
              5
            ],
            "details": "Support user data export and deletion (GDPR), ensure auditability and access controls (SOC 2), and implement privacy and security safeguards (HIPAA). Document compliance measures and provide user interfaces for data requests.\n<info added on 2025-11-14T19:10:53.454Z>\n## CURRENT STATUS\n- GDPR models exist in app/models/users.py (UserDataExport, GDPRDeleteResponse) but implementation needs verification.\n\n## COMPLIANCE REQUIREMENTS\n- GDPR: User data export, complete deletion, consent tracking, data retention\n- HIPAA: Encryption (✅), access controls (✅), audit trails (⚠️ needs DB persistence)\n- SOC 2: Auditability (⚠️), access controls (✅), security monitoring\n\n## VERIFICATION TASKS\n1. Test GDPR data export endpoint - verify all user PII is included\n2. Test GDPR deletion endpoint - verify complete removal from all tables\n3. Document data retention policies\n4. Add consent tracking for data processing\n5. Create user-facing data request interface\n\n## COMPLIANCE FEATURES TO IMPLEMENT\n- Data export: JSON download of all user data\n- Right to deletion: Remove all PII from documents, embeddings, graphs\n- Data portability: Export in machine-readable format\n- Privacy controls: User-configurable data retention\n- Breach notification: Automated alerts for security incidents\n\n## SOC 2 REQUIREMENTS\n- Access control documentation (✅ via RBAC)\n- Audit trail persistence (⚠️ task 41.5)\n- Security monitoring dashboards\n- Incident response procedures\n\n## HIPAA SAFEGUARDS\n- Technical safeguards: Encryption (✅), access controls (✅)\n- Physical safeguards: Document B2/Supabase security\n- Administrative safeguards: Policies and training documentation\n\n## DEPENDENCIES\nRequires audit logging (41.5) and RLS policies (41.2) to be complete first.\n</info added on 2025-11-14T19:10:53.454Z>",
            "status": "done",
            "testStrategy": "Perform compliance checks, simulate data subject requests, and verify all regulatory requirements are met.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Conduct Security and Compliance Testing",
            "description": "Perform security testing, penetration testing, and compliance verification across all implemented features.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Run automated security scans, manual penetration tests, and compliance audits. Address any vulnerabilities or compliance gaps identified. Document test results and remediation steps.\n<info added on 2025-11-14T19:11:00.118Z>\nCURRENT STATUS: No security testing suite exists yet. This is the final validation phase after all security features are implemented.\n\nTESTING PLAN:\n\n1. AUTOMATED SECURITY SCANS:\n   - OWASP ZAP for penetration testing\n   - Bandit for Python code security analysis\n   - Safety for dependency vulnerability scanning\n   - SQLMap for SQL injection testing\n\n2. MANUAL PENETRATION TESTING:\n   - Auth bypass attempts\n   - Token tampering and replay attacks\n   - RBAC privilege escalation tests\n   - Input fuzzing (SQL injection, XSS, path traversal)\n   - Rate limit bypass attempts\n   - CORS misconfiguration exploits\n\n3. COMPLIANCE VERIFICATION:\n   - GDPR data export/deletion validation\n   - HIPAA audit trail completeness\n   - SOC 2 access control verification\n   - Encryption verification (at-rest, in-transit)\n\n4. SECURITY TEST SUITE:\n   - Create tests/security/ directory\n   - Write pytest tests for:\n     - Authentication flows\n     - RBAC enforcement\n     - Input validation edge cases\n     - Audit log persistence\n     - Rate limiting\n     - Session management\n\n5. DOCUMENTATION:\n   - Security architecture document\n   - Threat model and mitigations\n   - Incident response playbook\n   - Compliance certification evidence\n\nDEPENDENCIES: All subtasks 41.1-41.6 must be complete before testing can begin.\n\nDELIVERABLES:\n- Security test report with findings\n- Remediation plan for any issues\n- Compliance certification readiness assessment\n</info added on 2025-11-14T19:11:00.118Z>",
            "status": "done",
            "testStrategy": "Review test reports, verify all critical issues are resolved, and confirm compliance with GDPR, HIPAA, and SOC 2.",
            "parentId": "undefined"
          }
        ],
        "complexity": 9,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Decompose security hardening and compliance into subtasks for JWT authentication, RBAC enforcement, encrypted storage setup, input validation, audit trail implementation, compliance feature integration (GDPR, HIPAA, SOC 2), and security/compliance testing."
      },
      {
        "id": 42,
        "title": "Reliability & Disaster Recovery Implementation",
        "description": "Set up automated backups, health checks, auto-restart, and disaster recovery procedures.",
        "details": "Configure daily B2 backups, implement health endpoints, auto-restart on failure, and document disaster recovery drills. Use Infrastructure as Code (Terraform/Ansible) for fast rebuild.",
        "testStrategy": "Simulate failures, verify backup/restore, health checks, and recovery procedures.",
        "priority": "high",
        "dependencies": [
          "41"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Automated Backups and Restore Procedures",
            "description": "Set up daily automated B2 backups and validate restore processes to ensure data durability and rapid recovery.",
            "dependencies": [],
            "details": "Configure daily automated backups to Backblaze B2 using Infrastructure as Code (Terraform/Ansible). Regularly test backup integrity and perform restore drills to verify data can be recovered quickly and accurately. Document backup schedules, retention policies, and restoration steps.",
            "status": "done",
            "testStrategy": "Simulate data loss scenarios and perform full and partial restores from backups to verify data integrity and recovery time objectives.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Deploy Health Checks and Auto-Restart Mechanisms",
            "description": "Implement health endpoints and configure automated service restarts on failure to maintain high availability.",
            "dependencies": [
              1
            ],
            "details": "Develop and expose health check endpoints for all critical services. Integrate monitoring tools to continuously check service health. Configure auto-restart policies (e.g., systemd, Kubernetes liveness probes) to automatically recover failed services. Ensure monitoring alerts are in place for failed health checks and restarts.",
            "status": "done",
            "testStrategy": "Induce service failures and verify that health checks detect issues and auto-restart mechanisms restore service availability without manual intervention.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Document and Test Disaster Recovery Procedures",
            "description": "Create, document, and regularly test disaster recovery (DR) drills to ensure readiness for major outages.",
            "dependencies": [
              1,
              2
            ],
            "details": "Develop comprehensive disaster recovery documentation covering failover, rebuild, and recovery steps using Infrastructure as Code. Schedule and execute regular DR drills simulating various failure scenarios (e.g., region outage, data corruption). Update documentation based on drill outcomes and lessons learned.",
            "status": "done",
            "testStrategy": "Conduct scheduled disaster recovery drills, measure recovery time and data loss, and review documentation for completeness and clarity after each drill.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on reliability & disaster recovery implementation."
      },
      {
        "id": 43,
        "title": "Load Testing & Performance Optimization",
        "description": "Conduct load testing for document processing, query execution, and WebSocket connections. Optimize for throughput and latency.",
        "details": "Use locust or k6 for load testing. Profile bottlenecks, optimize Celery worker scaling, database indexes, and caching. Tune API and WebSocket performance.",
        "testStrategy": "Run load tests at 2x expected traffic, verify performance metrics and optimize as needed.",
        "priority": "high",
        "dependencies": [
          "42"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Execute Load Testing Scenarios for Document Processing, Query Execution, and WebSocket Connections",
            "description": "Develop and run comprehensive load tests targeting document processing, query execution, and WebSocket endpoints using tools like Locust or k6.",
            "dependencies": [],
            "details": "Identify key user flows and endpoints for document processing, query execution, and WebSocket communication. Create load test scripts in Locust (Python) or k6 (JavaScript), simulating realistic traffic patterns and scaling up to at least 2x expected peak load. Collect baseline metrics for throughput, latency, and error rates.\n<info added on 2025-11-16T19:08:05.314Z>\nAuthentication setup for load testing completed:\n- Fixed bug in app/middleware/clerk_auth.py by replacing non-existent sessions.verify_token() with proper JWT verification\n- Implemented JWT verification using jwt.decode() with CLERK_SECRET_KEY\n- Created generate_test_token.py script to generate valid JWT tokens for load testing\n- Changes committed (6a67a3b) and pushed to main branch\n- System is now ready for authentication-enabled load testing scenarios\n</info added on 2025-11-16T19:08:05.314Z>",
            "status": "done",
            "testStrategy": "Verify that load tests execute as intended, generate reproducible results, and cover all critical workflows. Ensure metrics are collected for throughput, latency, and error rates under varying load conditions.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Profile System Performance and Identify Bottlenecks",
            "description": "Analyze system performance under load to pinpoint bottlenecks in Celery worker scaling, database indexing, caching, and API/WebSocket layers.",
            "dependencies": [
              1
            ],
            "details": "Use profiling tools and application logs to monitor CPU, memory, database query times, and network utilization during load tests. Focus on Celery worker queues, database slow queries, cache hit/miss ratios, and WebSocket throughput. Document all identified bottlenecks with supporting metrics.",
            "status": "done",
            "testStrategy": "Correlate load test results with profiling data to confirm bottleneck locations. Validate findings by reproducing issues under controlled load and measuring impact of each suspected bottleneck.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Optimize and Re-Test for Throughput and Latency Improvements",
            "description": "Implement targeted optimizations (e.g., Celery scaling, database indexes, caching strategies, API/WebSocket tuning) and validate improvements through iterative load testing.",
            "dependencies": [
              2
            ],
            "details": "Apply optimizations based on profiling results: adjust Celery worker counts, add or tune database indexes, refine caching logic, and optimize API/WebSocket configurations. Re-run load tests to measure improvements in throughput and latency. Iterate as needed until performance targets are met.\n<info added on 2025-11-17T00:16:21.618Z>\n## Progress Update (75% Complete)\n\n### Accomplishments\n1. Created comprehensive load testing framework (query_load_test.py)\n2. Identified and fixed 4 critical bugs:\n   - Langfuse decorator async bug (5e8c9c1) - adaptive endpoint 0% → 100% success\n   - Pydantic cache serialization (f2707f5) - enabled cache infrastructure\n   - LangGraph ToolNode error (7e0972f) - fixed 33% failure rate on complex queries\n   - Redis connection for Upstash (7e0972f) - SSL/TLS support for production\n3. Generated comprehensive documentation:\n   - PERFORMANCE_REPORT_TASK43_3.md (8 sections)\n   - TASK43_3_FINAL_STATUS.md (complete status)\n   - 3 JSON test result files\n4. Re-tested and validated all bug fixes in production\n5. Profiled performance bottlenecks and documented optimizations\n\n### Current Performance Metrics\n- Adaptive endpoint: 100% success (was 0%)\n- Auto-routed endpoint: 100% success\n- Cache hit rate: 0% (embedding service unavailable)\n- Adaptive P95 latency: 14.6s (target: <1s)\n- Auto-routed P95 latency: 7.1s (target: <2s)\n\n### Outstanding Issues\n1. Caching not functional - BGE-M3 via Ollama unavailable from Render\n   - Need OpenAI embeddings fallback\n   - Verify Redis connection in logs\n2. Performance too slow - sequential LLM calls taking 12-14s\n   - Need to combine analyze+plan into single call\n   - Add streaming responses\n   - Implement prompt caching\n\n### Next Steps\n- Add OpenAI embeddings fallback for production caching\n- Optimize LangGraph workflow (combine nodes, add streaming)\n- Final validation with working cache and optimized performance\n- Estimated: 2-4 hours to complete remaining 25%\n</info added on 2025-11-17T00:16:21.618Z>",
            "status": "done",
            "testStrategy": "Compare pre- and post-optimization metrics for throughput, latency, and error rates. Confirm that optimizations resolve identified bottlenecks and that the system meets or exceeds performance goals under 2x expected load.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on load testing & performance optimization."
      },
      {
        "id": 44,
        "title": "Documentation Finalization & User Onboarding",
        "description": "Prepare comprehensive documentation for developers and users. Implement onboarding flows and training materials.",
        "details": "Document API endpoints, workflows, agent configurations, and UI usage. Create onboarding guides and training videos.",
        "testStrategy": "Review documentation for completeness and clarity. Test onboarding flows with new users.",
        "priority": "medium",
        "dependencies": [
          "43"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Comprehensive API & Workflow Documentation",
            "description": "Create detailed, accurate documentation covering all API endpoints, workflows, agent configurations, and UI usage for both developers and end-users.",
            "dependencies": [
              43
            ],
            "details": "Document each API endpoint with request/response examples, authentication details, and error codes. Outline workflows with diagrams and step-by-step instructions. Describe agent configuration options and UI navigation paths. Use clear headings, code samples, and visuals to enhance readability and accessibility[1][2]. Ensure documentation is reviewed by technical stakeholders for accuracy before finalization.",
            "status": "done",
            "testStrategy": "Conduct peer reviews with developers and QA to verify completeness, clarity, and technical accuracy. Test documented workflows against the live system to ensure they match actual behavior.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Onboarding Guide & Training Material Development",
            "description": "Develop onboarding guides and training materials tailored to different user roles, including step-by-step tutorials, FAQs, and best practices.",
            "dependencies": [
              43
            ],
            "details": "Write onboarding guides for new users and developers, focusing on getting started, common tasks, and troubleshooting. Create training videos (e.g., using Loom or similar tools) demonstrating key features and workflows. Include exercises and real-world examples to reinforce learning. Structure content for easy navigation and quick reference, using consistent formatting and visual aids[1][2]. Collaborate with support and training teams to ensure materials address common user pain points.",
            "status": "done",
            "testStrategy": "Pilot onboarding materials with a group of new users and gather feedback on clarity, usefulness, and ease of understanding. Revise materials based on feedback before broad release.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Documentation Maintenance & Continuous Improvement Plan",
            "description": "Establish processes for ongoing documentation review, updates, and user feedback integration to keep materials accurate and relevant.",
            "dependencies": [
              43
            ],
            "details": "Set up a schedule for regular documentation reviews, especially after product updates or releases. Implement a feedback loop where users can report issues or suggest improvements. Use version control to track changes and ensure all stakeholders have access to the latest documentation. Standardize templates and update procedures to maintain consistency across all docs[2][4]. Assign clear ownership for documentation maintenance within the team.",
            "status": "done",
            "testStrategy": "Monitor documentation usage analytics and user feedback channels. Periodically audit docs for outdated information and verify that updates are correctly propagated. Test revised documentation with both new and experienced users to ensure continued effectiveness.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on documentation finalization & user onboarding."
      },
      {
        "id": 45,
        "title": "Integrate RAGAS Metrics Evaluation and Visualization for RAG Pipeline",
        "description": "Implement automated RAG quality evaluation using the RAGAS framework with 4 core metrics (Faithfulness, Answer Relevancy, Context Precision, Context Recall). Store results in Supabase and visualize in Grafana dashboards.",
        "details": "- Set up RAGAS framework integration for automated evaluation of RAG pipeline quality\n- Implement evaluation of 4 core metrics:\n  * Faithfulness (0-1): Measures if the generated answer is factually consistent with the retrieved context\n  * Answer Relevancy (0-1): Measures if the answer addresses the query intent\n  * Context Precision (0-1): Measures the proportion of relevant context chunks\n  * Context Recall (0-1): Measures if all necessary information is present in context\n- Create a test dataset of 30 curated samples from Empire documentation stored in .taskmaster/docs/ragas_test_dataset.json\n- Design and implement Supabase ragas_evaluations table with schema including:\n  * evaluation_id (UUID)\n  * timestamp (TIMESTAMP)\n  * query_text (TEXT)\n  * answer_text (TEXT)\n  * context_chunks (JSONB array)\n  * faithfulness_score (FLOAT)\n  * answer_relevancy_score (FLOAT)\n  * context_precision_score (FLOAT)\n  * context_recall_score (FLOAT)\n  * overall_score (FLOAT)\n  * metadata (JSONB)\n- Develop scripts/ragas_evaluation.py for batch evaluation with:\n  * Command-line interface for running evaluations\n  * Integration with existing RAG pipeline components\n  * Configurable parameters for evaluation settings\n  * Automatic storage of results in Supabase\n- Create Grafana dashboards showing:\n  * Metric trends over time\n  * Comparison between different RAG configurations\n  * Alerts when metrics fall below threshold (0.70)\n  * Drill-down capability to examine specific evaluation runs\n- Document expected baseline performance (0.70-0.85 overall scores)\n- Calculate and document cost estimates (~$0.20 per evaluation run for 30 samples)\n- Integrate with existing observability infrastructure from Task 25",
        "testStrategy": "1. Prepare test environment with sample RAG pipeline and test dataset\n2. Run baseline evaluation on the 30-sample test dataset from .taskmaster/docs/ragas_test_dataset.json\n3. Validate all 4 metrics (Faithfulness, Answer Relevancy, Context Precision, Context Recall) are calculated correctly\n4. Verify scores are within expected ranges (0-1) and reasonable for test data\n5. Confirm results are properly stored in Supabase ragas_evaluations table\n6. Check that all required fields in the schema are populated correctly\n7. Verify Grafana dashboard correctly displays:\n   - Individual metric scores\n   - Overall score trends\n   - Comparison between evaluation runs\n8. Test alert triggers by artificially setting scores below the 0.70 threshold\n9. Validate dashboard filtering and drill-down capabilities\n10. Perform a complete end-to-end test with a new document to ensure the entire evaluation pipeline works\n11. Measure performance and resource usage during evaluation runs\n12. Document baseline scores for the current RAG implementation",
        "status": "done",
        "dependencies": [
          "18",
          "25"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up RAGAS framework integration and test dataset",
            "description": "Integrate the RAGAS framework into the project and prepare the test dataset for evaluation.",
            "dependencies": [],
            "details": "Install RAGAS library and dependencies. Configure the framework to work with the existing RAG pipeline. Prepare and validate the test dataset of 30 curated samples from Empire documentation stored in .taskmaster/docs/ragas_test_dataset.json. Ensure the dataset contains appropriate query-answer-context triplets for evaluation.",
            "status": "done",
            "testStrategy": "Verify RAGAS installation and imports work correctly. Validate test dataset structure and content. Ensure sample queries cover diverse use cases from Empire documentation.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement core metrics evaluation logic",
            "description": "Develop the core functionality to evaluate the 4 RAGAS metrics: Faithfulness, Answer Relevancy, Context Precision, and Context Recall.",
            "dependencies": [
              1
            ],
            "details": "Create evaluation functions for each metric. Implement Faithfulness calculation to measure factual consistency between answers and context. Develop Answer Relevancy evaluation to assess query intent alignment. Build Context Precision measurement for relevant chunk proportion. Implement Context Recall to verify information completeness. Calculate overall combined score from individual metrics.",
            "status": "done",
            "testStrategy": "Run evaluations on sample data and verify each metric produces values between 0-1. Compare results with manual assessments of a subset of examples to validate accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create Supabase ragas_evaluations table and storage logic",
            "description": "Design and implement the Supabase database schema for storing RAGAS evaluation results and develop storage functionality.",
            "dependencies": [
              2
            ],
            "details": "Create ragas_evaluations table with schema including evaluation_id, timestamp, query_text, answer_text, context_chunks, all metric scores (faithfulness, answer_relevancy, context_precision, context_recall), overall_score, and metadata fields. Implement functions to store evaluation results in the database. Add batch processing capabilities for multiple evaluations.",
            "status": "done",
            "testStrategy": "Test database schema creation and data insertion. Verify all fields are properly stored and retrieved. Check batch processing with multiple evaluation records.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Develop scripts/ragas_evaluation.py with CLI interface",
            "description": "Create a command-line script for running RAGAS evaluations with configurable parameters and Supabase integration.",
            "dependencies": [
              3
            ],
            "details": "Develop scripts/ragas_evaluation.py with command-line arguments for evaluation settings. Integrate with existing RAG pipeline components to access retrieval and generation functions. Implement configurable parameters for batch size, metric weights, and thresholds. Add automatic storage of results in Supabase. Include logging and error handling. Document cost estimates (~$0.20 per evaluation run for 30 samples).",
            "status": "done",
            "testStrategy": "Test CLI with various parameter combinations. Verify integration with RAG pipeline components. Confirm results are properly stored in Supabase. Validate error handling for edge cases.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create Grafana dashboards for metrics visualization",
            "description": "Design and implement Grafana dashboards to visualize RAGAS metrics and integrate with existing observability infrastructure.",
            "dependencies": [
              4
            ],
            "details": "Create Grafana dashboards showing metric trends over time. Implement comparison views between different RAG configurations. Set up alerts when metrics fall below threshold (0.70). Add drill-down capability to examine specific evaluation runs. Document expected baseline performance (0.70-0.85 overall scores). Integrate with existing observability infrastructure from Task 25.",
            "status": "done",
            "testStrategy": "Verify dashboard displays all metrics correctly. Test alert functionality with below-threshold values. Confirm drill-down navigation works properly. Validate integration with existing observability infrastructure.",
            "parentId": "undefined"
          }
        ]
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-11-14T18:18:43.347Z",
      "taskCount": 45,
      "completedCount": 39,
      "tags": [
        "master"
      ],
      "created": "2025-11-14T19:10:19.748Z",
      "description": "Tasks for master context",
      "updated": "2025-11-17T18:28:30.661Z"
    }
  },
  "v7_3_features": {
    "tasks": [
      {
        "id": "101",
        "title": "Implement Neo4j HTTP Client",
        "description": "Create a production-optimized Neo4j HTTP client that directly accesses the transaction/commit endpoint for better performance than the driver approach.",
        "details": "Implement the Neo4jHTTPClient class in app/services/neo4j_http_client.py with the following features:\n\n1. Direct HTTP connection to Neo4j's transaction/commit endpoint\n2. Connection pooling for efficient resource usage\n3. Query batching capabilities\n4. Proper error handling and result parsing\n5. Async support for non-blocking operations\n\nImplementation should follow the pattern provided in the PRD:\n```python\nimport httpx\nfrom typing import Dict, Any, List\n\nclass Neo4jHTTPClient:\n    def __init__(self, uri: str, username: str, password: str):\n        self.base_url = uri.replace(\"bolt://\", \"http://\").replace(\"bolt+ssc://\", \"https://\")\n        self.base_url = f\"{self.base_url}/db/neo4j/tx/commit\"\n        self.auth = (username, password)\n        self.client = httpx.AsyncClient(timeout=30.0)\n\n    async def execute_query(\n        self,\n        query: str,\n        parameters: Dict[str, Any] = None\n    ) -> List[Dict[str, Any]]:\n        payload = {\n            \"statements\": [{\n                \"statement\": query,\n                \"parameters\": parameters or {}\n            }]\n        }\n\n        response = await self.client.post(\n            self.base_url,\n            json=payload,\n            auth=self.auth,\n            headers={\"Content-Type\": \"application/json\"}\n        )\n\n        result = response.json()\n        if result.get(\"errors\"):\n            raise Exception(result[\"errors\"][0][\"message\"])\n\n        return self._parse_results(result)\n\n    def _parse_results(self, result: Dict) -> List[Dict]:\n        rows = []\n        for statement_result in result.get(\"results\", []):\n            columns = statement_result.get(\"columns\", [])\n            for row in statement_result.get(\"data\", []):\n                rows.append(dict(zip(columns, row[\"row\"])))\n        return rows\n```\n\nAdd methods for batch query execution and connection management. Include unit tests to verify functionality against a Neo4j instance.",
        "testStrategy": "1. Unit tests with mocked HTTP responses to verify correct parsing\n2. Integration tests against a test Neo4j instance to verify actual connectivity\n3. Performance benchmarks comparing HTTP client vs. driver approach\n4. Test connection pooling under load\n5. Test error handling with malformed queries\n6. Test with various Neo4j query types (READ, WRITE, etc.)",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core HTTP Client with Connection Management",
            "description": "Implement the base Neo4jHTTPClient class with connection initialization, authentication, and proper connection management.",
            "dependencies": [],
            "details": "Create the Neo4jHTTPClient class in app/services/neo4j_http_client.py with proper initialization, authentication setup, and connection management. Implement the constructor that handles URI transformation from bolt to HTTP format, authentication setup, and httpx client initialization. Include methods for connection lifecycle management (open/close connections) and implement proper resource cleanup. Ensure the client handles connection timeouts and retries appropriately. Write unit tests to verify connection initialization and management functionality.",
            "status": "done",
            "testStrategy": "Unit test connection initialization with various URI formats (bolt://, bolt+ssc://, etc.). Mock HTTP responses to test connection management. Test proper resource cleanup on client shutdown. Test connection timeout handling and retry logic. Integration test with a real Neo4j instance to verify connectivity.",
            "updatedAt": "2026-01-12T01:46:41.108Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Query Execution and Result Parsing",
            "description": "Develop the core query execution functionality and result parsing logic for the Neo4j HTTP client.",
            "dependencies": [
              1
            ],
            "details": "Implement the execute_query method that sends Cypher queries to Neo4j's transaction/commit endpoint. Create the _parse_results method to transform Neo4j's JSON response into a more usable format. Handle different result types (nodes, relationships, paths, etc.) correctly. Implement proper error detection and exception handling for query execution failures. Add support for parameterized queries to prevent injection attacks. Write comprehensive unit tests for query execution and result parsing with various query types and response formats.",
            "status": "done",
            "testStrategy": "Unit test query execution with mocked HTTP responses. Test result parsing with various Neo4j response formats. Test error handling with different error scenarios. Integration test with actual Neo4j instance using various query types. Test parameterized queries for correctness and security.",
            "parentId": "undefined",
            "updatedAt": "2026-01-12T01:47:13.007Z"
          },
          {
            "id": 3,
            "title": "Implement Advanced Features: Batching, Pooling, and Async Support",
            "description": "Add advanced features to the Neo4j HTTP client including query batching, connection pooling, and asynchronous execution support.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement batch_execute_queries method to send multiple queries in a single HTTP request. Configure connection pooling for efficient resource usage under load. Optimize async support for non-blocking operations with proper concurrency handling. Add timeout configuration and circuit breaker patterns for resilience. Implement query result caching for frequently executed queries. Create performance benchmarks comparing the HTTP client against the driver approach. Write comprehensive tests for batching, pooling, and async execution under various load conditions.",
            "status": "done",
            "testStrategy": "Unit test batch query execution with various batch sizes. Test connection pooling under simulated load. Benchmark performance against standard Neo4j driver. Test async execution with concurrent queries. Test timeout handling and circuit breaker functionality. Integration test with Neo4j instance under load to verify pooling benefits.",
            "parentId": "undefined",
            "updatedAt": "2026-01-12T01:47:15.070Z"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down the Neo4j HTTP Client implementation into subtasks covering: 1) Core HTTP client implementation with connection handling, 2) Query execution and result parsing functionality, and 3) Advanced features like connection pooling, batching, and error handling.",
        "updatedAt": "2026-01-12T01:47:15.070Z"
      },
      {
        "id": "102",
        "title": "Extend Neo4j Schema for Graph Agent",
        "description": "Extend the existing Neo4j schema to support the new graph agent capabilities, including Customer 360, Document Structure, and Graph-Enhanced RAG.",
        "details": "Create Cypher scripts to extend the Neo4j schema with new node types and relationships:\n\n1. Customer 360 Nodes:\n   - Customer nodes with properties (id, name, type, industry, etc.)\n   - Ticket nodes for support tickets\n   - Order nodes for customer orders\n   - Interaction nodes for customer interactions\n   - Product nodes for products/services\n\n2. Document Structure Nodes:\n   - Section nodes for document hierarchy\n   - DefinedTerm nodes for document terminology\n   - Citation nodes for external references\n\n3. Relationships:\n   - Customer relationships (HAS_DOCUMENT, HAS_TICKET, etc.)\n   - Document structure relationships (HAS_SECTION, REFERENCES, etc.)\n   - Entity relationships for graph-enhanced RAG\n\n4. Indexes for performance optimization:\n   - Create indexes on frequently queried properties\n   - Create constraints for data integrity\n\nImplement these schema changes in a migration script that can be applied to the existing Neo4j database without data loss.",
        "testStrategy": "1. Create test script to verify schema changes were applied correctly\n2. Test indexes with EXPLAIN/PROFILE on common queries\n3. Verify constraints with intentionally invalid data\n4. Load test data for each node type and verify relationships\n5. Test backward compatibility with existing queries\n6. Verify performance with large datasets",
        "priority": "high",
        "dependencies": [
          "101"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 7,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down the Neo4j schema extension into subtasks covering: 1) Customer 360 schema components, 2) Document Structure schema components, 3) Graph-Enhanced RAG schema components, and 4) Index and constraint creation with migration strategy.",
        "updatedAt": "2026-01-12T01:53:12.016Z"
      },
      {
        "id": "103",
        "title": "Implement Pydantic Models for Graph Agent",
        "description": "Create Pydantic models for request/response objects used by the Graph Agent APIs, including Customer 360, Document Structure, and Graph-Enhanced RAG.",
        "details": "Create a new file app/models/graph_agent.py with Pydantic models as specified in the PRD:\n\n1. Base models:\n   - QueryType enum (CUSTOMER_360, DOCUMENT_STRUCTURE, GRAPH_ENHANCED_RAG)\n   - TraversalDepth enum (SHALLOW, MEDIUM, DEEP)\n\n2. Customer 360 models:\n   - Customer360Request\n   - CustomerNode\n   - Customer360Response\n\n3. Document Structure models:\n   - DocumentStructureRequest\n   - SectionNode\n   - DocumentStructureResponse\n   - SmartRetrievalRequest\n   - SmartRetrievalResponse\n\n4. Graph-Enhanced RAG models:\n   - GraphEnhancedRAGRequest\n   - GraphExpansionResult\n   - GraphEnhancedRAGResponse\n\nImplement all models following the schema provided in the PRD, with proper type hints, field validations, and documentation strings. Ensure models are compatible with FastAPI's automatic request validation and OpenAPI schema generation.",
        "testStrategy": "1. Unit tests for model validation\n2. Test serialization/deserialization\n3. Test with valid and invalid data\n4. Verify OpenAPI schema generation\n5. Test integration with FastAPI endpoints\n6. Verify documentation is generated correctly",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Base Models and Enums for Graph Agent",
            "description": "Create the foundational Pydantic models and enums that will be used across all Graph Agent APIs.",
            "dependencies": [],
            "details": "Create a new file app/models/graph_agent.py and implement the base models including QueryType enum (CUSTOMER_360, DOCUMENT_STRUCTURE, GRAPH_ENHANCED_RAG) and TraversalDepth enum (SHALLOW, MEDIUM, DEEP). Include proper type hints, field validations, and documentation strings. Ensure compatibility with FastAPI's automatic request validation and OpenAPI schema generation.",
            "status": "pending",
            "testStrategy": "Write unit tests to verify enum values are correctly defined. Test serialization/deserialization of the base models. Verify that the models generate correct OpenAPI schema documentation.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Customer 360 Request/Response Models",
            "description": "Create Pydantic models for Customer 360 functionality including request and response objects with proper validation rules.",
            "dependencies": [
              1
            ],
            "details": "In app/models/graph_agent.py, implement Customer360Request model with query parameters, CustomerNode model for representing customer data, and Customer360Response model for returning query results. Include validation rules for required fields, field types, and value constraints. Add comprehensive docstrings explaining each field's purpose and format requirements.",
            "status": "pending",
            "testStrategy": "Test model validation with valid and invalid data. Verify that validation errors are properly raised for invalid inputs. Test serialization/deserialization of complex nested structures. Ensure models work correctly with FastAPI's request parsing.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Document Structure and Graph-Enhanced RAG Models",
            "description": "Create Pydantic models for Document Structure and Graph-Enhanced RAG APIs with appropriate validation logic and relationships.",
            "dependencies": [
              1
            ],
            "details": "In app/models/graph_agent.py, implement DocumentStructureRequest, SectionNode, DocumentStructureResponse, SmartRetrievalRequest, SmartRetrievalResponse models for document structure functionality. Then implement GraphEnhancedRAGRequest, GraphExpansionResult, and GraphEnhancedRAGResponse models for graph-enhanced RAG functionality. Include proper validation rules, nested relationships, and comprehensive documentation for all fields. Ensure models handle complex nested structures and include appropriate default values where needed.",
            "status": "pending",
            "testStrategy": "Test validation of complex nested structures. Verify that models correctly handle optional and required fields. Test with edge cases like empty lists and deeply nested objects. Ensure models generate correct OpenAPI documentation. Test integration with FastAPI endpoints.",
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down the Pydantic models implementation into subtasks covering: 1) Base models and enums, 2) Customer 360 request/response models, 3) Document Structure and Graph-Enhanced RAG models with validation logic.",
        "updatedAt": "2026-01-12T01:56:34.761Z"
      },
      {
        "id": "104",
        "title": "Implement Customer 360 Service",
        "description": "Create the Customer 360 Service that provides unified customer views by traversing the Neo4j graph to consolidate data from multiple sources.",
        "details": "Implement the Customer360Service class in app/services/customer360_service.py with the following features:\n\n1. Query parser for customer-related natural language queries\n2. Multi-hop graph traversal to collect customer data\n3. Result aggregation and formatting\n4. Similar customer detection\n\nThe service should use the Neo4jHTTPClient for efficient graph queries and implement the following methods:\n\n```python\nfrom typing import Dict, List, Optional, Any\nfrom app.services.neo4j_http_client import Neo4jHTTPClient\nfrom app.models.graph_agent import Customer360Request, Customer360Response, CustomerNode\n\nclass Customer360Service:\n    def __init__(self, neo4j_client: Neo4jHTTPClient):\n        self.neo4j_client = neo4j_client\n    \n    async def process_customer_query(self, request: Customer360Request) -> Customer360Response:\n        # Parse natural language query to identify customer and query intent\n        # Execute appropriate graph traversal\n        # Format results into Customer360Response\n        pass\n    \n    async def get_customer_by_id(self, customer_id: str, include_documents: bool = True, \n                               include_tickets: bool = True, include_orders: bool = True,\n                               include_interactions: bool = True) -> Customer360Response:\n        # Direct lookup by customer ID with configurable related data\n        pass\n    \n    async def find_similar_customers(self, customer_id: str, limit: int = 5) -> List[CustomerNode]:\n        # Find customers with similar profiles/behaviors\n        pass\n    \n    async def _execute_customer_traversal(self, cypher_query: str, params: Dict[str, Any]) -> Dict[str, Any]:\n        # Execute graph traversal and process results\n        pass\n```\n\nImplement Cypher queries for customer data retrieval as shown in the PRD's Cypher Query Patterns section.",
        "testStrategy": "1. Unit tests with mocked Neo4j responses\n2. Integration tests with test customer data\n3. Test natural language query parsing with various customer queries\n4. Test multi-hop traversal with different depths\n5. Test result aggregation with complex customer data\n6. Performance testing with large customer datasets",
        "priority": "high",
        "dependencies": [
          "101",
          "102",
          "103"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 8,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down the Customer 360 Service implementation into subtasks covering: 1) Natural language query parsing for customer queries, 2) Graph traversal implementation for customer data retrieval, 3) Customer similarity detection algorithms, 4) Result aggregation and formatting, and 5) Integration with Neo4j HTTP client.",
        "updatedAt": "2026-01-12T02:02:21.283Z"
      },
      {
        "id": "105",
        "title": "Implement Document Structure Service",
        "description": "Create the Document Structure Service that extracts and navigates document hierarchy, clause references, and cross-links from complex documents.",
        "details": "Implement the DocumentStructureService class in app/services/document_structure_service.py with the following features:\n\n1. Structure extraction from documents using LLM\n2. Cross-reference detection and linking\n3. Smart retrieval with context expansion\n4. Definition linking\n\nThe service should implement these key methods:\n\n```python\nfrom typing import Dict, List, Optional, Any\nfrom app.services.neo4j_http_client import Neo4jHTTPClient\nfrom app.models.graph_agent import DocumentStructureRequest, DocumentStructureResponse, SmartRetrievalRequest, SmartRetrievalResponse\n\nclass DocumentStructureService:\n    def __init__(self, neo4j_client: Neo4jHTTPClient, llm_service):\n        self.neo4j_client = neo4j_client\n        self.llm_service = llm_service  # For structure extraction\n    \n    async def extract_document_structure(self, document_id: str, extract_cross_refs: bool = True,\n                                       extract_definitions: bool = True) -> DocumentStructureResponse:\n        # Extract document structure using LLM\n        # Store structure in Neo4j\n        # Return structured representation\n        pass\n    \n    async def get_document_structure(self, document_id: str) -> DocumentStructureResponse:\n        # Retrieve existing document structure from Neo4j\n        pass\n    \n    async def smart_retrieve(self, request: SmartRetrievalRequest) -> SmartRetrievalResponse:\n        # Context-aware retrieval with cross-references\n        pass\n    \n    async def get_cross_references(self, document_id: str, section_id: Optional[str] = None) -> List[Dict[str, Any]]:\n        # Get cross-references for document or specific section\n        pass\n    \n    async def _extract_sections(self, document_text: str) -> List[Dict[str, Any]]:\n        # Use LLM to extract sections and hierarchy\n        pass\n    \n    async def _extract_cross_references(self, document_text: str, sections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        # Identify cross-references between sections\n        pass\n```\n\nImplement the document structure extraction using a combination of LLM prompting and pattern recognition. Use the Neo4j graph to store and query the document structure.",
        "testStrategy": "1. Unit tests for structure extraction with sample documents\n2. Integration tests with Neo4j for storing and retrieving document structure\n3. Test cross-reference detection with complex legal documents\n4. Test smart retrieval with various query types\n5. Test definition linking and resolution\n6. Performance testing with large documents",
        "priority": "medium",
        "dependencies": [
          "101",
          "102",
          "103"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 8,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down the Document Structure Service implementation into subtasks covering: 1) Document structure extraction using LLM, 2) Cross-reference detection and linking, 3) Smart retrieval with context expansion, 4) Definition linking implementation, and 5) Integration with Neo4j for storing and querying document structures.",
        "updatedAt": "2026-01-12T02:08:07.891Z"
      },
      {
        "id": "106",
        "title": "Implement Graph-Enhanced RAG Service",
        "description": "Create the Graph-Enhanced RAG Service that augments vector search results with graph context by traversing relationships to find connected documents and entities.",
        "details": "Implement the GraphEnhancedRAGService class in app/services/graph_enhanced_rag_service.py with the following features:\n\n1. Entity extraction from retrieved chunks\n2. Graph expansion strategies (neighbor expansion, parent context, etc.)\n3. Context enrichment for retrieved results\n4. Result re-ranking based on graph relevance\n\nThe service should implement these key methods:\n\n```python\nfrom typing import Dict, List, Optional, Any\nfrom app.services.neo4j_http_client import Neo4jHTTPClient\nfrom app.models.graph_agent import GraphEnhancedRAGRequest, GraphEnhancedRAGResponse, GraphExpansionResult\n\nclass GraphEnhancedRAGService:\n    def __init__(self, neo4j_client: Neo4jHTTPClient, vector_search_service, entity_extractor):\n        self.neo4j_client = neo4j_client\n        self.vector_search_service = vector_search_service  # Existing RAG service\n        self.entity_extractor = entity_extractor  # For entity extraction\n    \n    async def query(self, request: GraphEnhancedRAGRequest) -> GraphEnhancedRAGResponse:\n        # Perform vector search\n        # Extract entities from results\n        # Expand with graph context\n        # Re-rank and format response\n        pass\n    \n    async def expand_results(self, chunks: List[Dict[str, Any]], \n                           expansion_depth: int = 1) -> GraphExpansionResult:\n        # Expand vector search results with graph context\n        pass\n    \n    async def get_entity_context(self, entity_id: str, depth: int = 1) -> Dict[str, Any]:\n        # Get context for a specific entity\n        pass\n    \n    async def _extract_entities(self, text: str) -> List[Dict[str, Any]]:\n        # Extract entities from text\n        pass\n    \n    async def _expand_by_neighbors(self, entity_ids: List[str], depth: int = 1) -> List[Dict[str, Any]]:\n        # Expand by traversing entity neighbors\n        pass\n    \n    async def _rerank_results(self, original_results: List[Dict[str, Any]], \n                             expanded_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        # Re-rank results based on graph relevance\n        pass\n```\n\nImplement the graph expansion using the Cypher queries outlined in the PRD's Cypher Query Patterns section. Integrate with the existing vector search service to enhance RAG results.",
        "testStrategy": "1. Unit tests for entity extraction\n2. Integration tests with Neo4j for graph expansion\n3. Test with various query types and expansion depths\n4. Test re-ranking with different relevance metrics\n5. Compare results with and without graph enhancement\n6. Performance testing with large result sets",
        "priority": "medium",
        "dependencies": [
          "101",
          "102",
          "103"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 9,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down the Graph-Enhanced RAG Service implementation into subtasks covering: 1) Entity extraction from retrieved chunks, 2) Graph expansion strategies implementation, 3) Context enrichment algorithms, 4) Result re-ranking based on graph relevance, and 5) Integration with existing vector search service.",
        "updatedAt": "2026-01-12T02:13:35.723Z"
      },
      {
        "id": "107",
        "title": "Implement Graph Agent Router",
        "description": "Create the Graph Agent Router that extends existing /api/graph routes with new capabilities for Customer 360, Document Structure, and Graph-Enhanced RAG.",
        "details": "Implement the Graph Agent Router in app/routes/graph_agent.py to handle the new graph agent endpoints:\n\n1. Customer 360 endpoints:\n   - POST /api/graph/customer360/query\n   - GET /api/graph/customer360/{customer_id}\n   - GET /api/graph/customer360/similar/{customer_id}\n\n2. Document Structure endpoints:\n   - POST /api/graph/document-structure/extract\n   - GET /api/graph/document-structure/{doc_id}\n   - POST /api/graph/document-structure/query\n   - GET /api/graph/document-structure/{doc_id}/cross-refs\n   - POST /api/graph/document-structure/smart-retrieve\n\n3. Graph-Enhanced RAG endpoints:\n   - POST /api/graph/enhanced-rag/query\n   - POST /api/graph/enhanced-rag/expand\n   - GET /api/graph/enhanced-rag/entities/{entity_id}/related\n   - POST /api/graph/enhanced-rag/context\n\nImplement the router using FastAPI with dependency injection for services:\n\n```python\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom app.services.customer360_service import Customer360Service\nfrom app.services.document_structure_service import DocumentStructureService\nfrom app.services.graph_enhanced_rag_service import GraphEnhancedRAGService\nfrom app.models.graph_agent import *\n\nrouter = APIRouter(prefix=\"/api/graph\", tags=[\"graph-agent\"])\n\n# Customer 360 endpoints\n@router.post(\"/customer360/query\", response_model=Customer360Response)\nasync def query_customer(request: Customer360Request, \n                       service: Customer360Service = Depends()):\n    return await service.process_customer_query(request)\n\n@router.get(\"/customer360/{customer_id}\", response_model=Customer360Response)\nasync def get_customer(customer_id: str, include_documents: bool = True,\n                     include_tickets: bool = True, include_orders: bool = True,\n                     include_interactions: bool = True,\n                     service: Customer360Service = Depends()):\n    return await service.get_customer_by_id(\n        customer_id, include_documents, include_tickets, \n        include_orders, include_interactions\n    )\n\n# Document Structure endpoints\n# ...\n\n# Graph-Enhanced RAG endpoints\n# ...\n```\n\nIntegrate the router with the main FastAPI application and ensure proper error handling and validation.",
        "testStrategy": "1. Unit tests for each endpoint with mocked service responses\n2. Integration tests with actual services\n3. Test request validation with valid and invalid inputs\n4. Test error handling with various error conditions\n5. Test authentication and authorization if applicable\n6. Load testing for concurrent requests",
        "priority": "high",
        "dependencies": [
          "103",
          "104",
          "105",
          "106"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down the Graph Agent Router implementation into subtasks covering: 1) Customer 360 endpoint implementation, 2) Document Structure endpoint implementation, and 3) Graph-Enhanced RAG endpoint implementation with proper error handling and validation.",
        "updatedAt": "2026-01-12T02:23:05.166Z"
      },
      {
        "id": "108",
        "title": "Implement Query Intent Detection for CKO Chat",
        "description": "Create a query intent detection system that can identify Customer 360, Document Structure, and Graph-Enhanced RAG queries to route them to the appropriate handler.",
        "details": "Implement a QueryIntentDetector class in app/services/query_intent_detector.py that can analyze natural language queries and determine the appropriate graph agent to handle them:\n\n```python\nfrom enum import Enum\nfrom typing import Dict, Any, Tuple\n\nclass QueryIntent(str, Enum):\n    CUSTOMER_360 = \"customer_360\"\n    DOCUMENT_STRUCTURE = \"document_structure\"\n    GRAPH_ENHANCED_RAG = \"graph_enhanced_rag\"\n    STANDARD_RAG = \"standard_rag\"\n\nclass QueryIntentDetector:\n    def __init__(self, llm_service):\n        self.llm_service = llm_service\n    \n    async def detect_intent(self, query: str) -> Tuple[QueryIntent, Dict[str, Any]]:\n        # Use pattern matching and/or LLM to detect query intent\n        # Return intent type and extracted parameters\n        \n        # Example implementation:\n        # 1. Check for customer-related keywords and patterns\n        if any(keyword in query.lower() for keyword in [\"customer\", \"client\", \"account\"]):\n            # Extract customer name/ID if present\n            return QueryIntent.CUSTOMER_360, self._extract_customer_params(query)\n        \n        # 2. Check for document structure patterns\n        if any(pattern in query.lower() for pattern in [\"section\", \"clause\", \"paragraph\", \"document structure\"]):\n            return QueryIntent.DOCUMENT_STRUCTURE, self._extract_document_params(query)\n        \n        # 3. Default to graph-enhanced RAG for general queries\n        return QueryIntent.GRAPH_ENHANCED_RAG, {\"query\": query}\n    \n    def _extract_customer_params(self, query: str) -> Dict[str, Any]:\n        # Extract customer name/ID and other parameters\n        # Use regex or LLM-based extraction\n        pass\n    \n    def _extract_document_params(self, query: str) -> Dict[str, Any]:\n        # Extract document ID, section references, etc.\n        pass\n```\n\nIntegrate this intent detector with the CKO Chat system to route queries to the appropriate graph agent endpoint. The detector should use a combination of pattern matching and LLM-based classification to identify query intents accurately.",
        "testStrategy": "1. Unit tests with sample queries for each intent type\n2. Test with ambiguous queries to verify classification\n3. Test parameter extraction accuracy\n4. Integration tests with CKO Chat\n5. Test with real user queries from logs if available\n6. Measure classification accuracy against human-labeled test set",
        "priority": "medium",
        "dependencies": [
          "104",
          "105",
          "106"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 7,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down the Query Intent Detection implementation into subtasks covering: 1) Pattern-based intent detection for common query types, 2) LLM-based intent classification for complex queries, 3) Parameter extraction from natural language queries, and 4) Integration with CKO Chat routing system.",
        "updatedAt": "2026-01-12T02:26:16.368Z"
      },
      {
        "id": "109",
        "title": "Implement Graph Result Formatter for Chat UI",
        "description": "Create a response formatter that presents graph query results in a user-friendly format for the CKO Chat interface, including expandable sections and graph visualizations.",
        "details": "Implement a GraphResultFormatter class in app/services/graph_result_formatter.py that formats graph query results for display in the chat UI:\n\n```python\nfrom typing import Dict, Any, List\nfrom app.models.graph_agent import Customer360Response, DocumentStructureResponse, GraphEnhancedRAGResponse\n\nclass GraphResultFormatter:\n    def format_customer_360(self, response: Customer360Response) -> Dict[str, Any]:\n        # Format Customer 360 response for chat UI\n        # Create expandable sections for documents, tickets, etc.\n        # Generate summary text\n        return {\n            \"type\": \"customer_360\",\n            \"content\": {\n                \"summary\": self._generate_customer_summary(response),\n                \"sections\": [\n                    {\n                        \"title\": \"Customer Profile\",\n                        \"content\": self._format_customer_profile(response.customer),\n                        \"expanded\": True\n                    },\n                    {\n                        \"title\": f\"Documents ({len(response.documents)})\",\n                        \"content\": self._format_document_list(response.documents),\n                        \"expanded\": False\n                    },\n                    # Additional sections for tickets, orders, etc.\n                ]\n            }\n        }\n    \n    def format_document_structure(self, response: DocumentStructureResponse) -> Dict[str, Any]:\n        # Format Document Structure response for chat UI\n        # Create expandable section tree\n        # Format cross-references as links\n        pass\n    \n    def format_graph_enhanced_rag(self, response: GraphEnhancedRAGResponse) -> Dict[str, Any]:\n        # Format Graph-Enhanced RAG response for chat UI\n        # Show answer with expandable context sections\n        # Include graph visualization data\n        pass\n    \n    def _generate_customer_summary(self, response: Customer360Response) -> str:\n        # Generate natural language summary of customer data\n        pass\n    \n    def _format_customer_profile(self, customer: Dict[str, Any]) -> Dict[str, Any]:\n        # Format customer profile data\n        pass\n    \n    def _format_document_list(self, documents: List[Dict[str, Any]]) -> Dict[str, Any]:\n        # Format document list with links\n        pass\n```\n\nThe formatter should create structured responses that can be rendered in the chat UI with expandable sections, links to source documents, and optional graph visualizations. The output should be compatible with the existing CKO Chat UI components.",
        "testStrategy": "1. Unit tests for each formatter method\n2. Test with various response structures\n3. Verify output format matches UI requirements\n4. Test with edge cases (empty results, large result sets)\n5. Integration tests with actual UI components\n6. User testing for readability and usability",
        "priority": "low",
        "dependencies": [
          "107",
          "108"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 6,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down the Graph Result Formatter implementation into subtasks covering: 1) Customer 360 result formatting with expandable sections, 2) Document Structure result formatting with hierarchical display, and 3) Graph-Enhanced RAG result formatting with answer highlighting and context sections.",
        "updatedAt": "2026-01-12T02:39:18.194Z"
      },
      {
        "id": "110",
        "title": "Implement Redis Caching for Graph Queries",
        "description": "Implement a caching layer using Redis to improve performance of graph queries by storing frequently accessed results.",
        "details": "Create a GraphQueryCache class in app/services/graph_query_cache.py that provides caching for graph query results:\n\n```python\nfrom typing import Dict, Any, Optional, List\nimport json\nimport hashlib\nfrom redis import Redis\n\nclass GraphQueryCache:\n    def __init__(self, redis_client: Redis, ttl: int = 3600):\n        self.redis = redis_client\n        self.default_ttl = ttl  # Default TTL in seconds\n    \n    async def get(self, query_type: str, query_key: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n        # Generate cache key from query type and parameters\n        cache_key = self._generate_cache_key(query_type, query_key)\n        \n        # Try to get from cache\n        cached_result = self.redis.get(cache_key)\n        if cached_result:\n            return json.loads(cached_result)\n        \n        return None\n    \n    async def set(self, query_type: str, query_key: Dict[str, Any], \n                result: Dict[str, Any], ttl: Optional[int] = None) -> None:\n        # Generate cache key and store result\n        cache_key = self._generate_cache_key(query_type, query_key)\n        self.redis.set(\n            cache_key,\n            json.dumps(result),\n            ex=ttl or self.default_ttl\n        )\n    \n    async def invalidate(self, query_type: str, query_key: Dict[str, Any]) -> None:\n        # Invalidate specific cache entry\n        cache_key = self._generate_cache_key(query_type, query_key)\n        self.redis.delete(cache_key)\n    \n    async def invalidate_by_prefix(self, prefix: str) -> None:\n        # Invalidate all cache entries with given prefix\n        keys = self.redis.keys(f\"{prefix}:*\")\n        if keys:\n            self.redis.delete(*keys)\n    \n    def _generate_cache_key(self, query_type: str, query_key: Dict[str, Any]) -> str:\n        # Generate deterministic cache key from query type and parameters\n        key_str = json.dumps(query_key, sort_keys=True)\n        hashed = hashlib.md5(key_str.encode()).hexdigest()\n        return f\"graph:{query_type}:{hashed}\"\n```\n\nIntegrate this cache with the graph agent services (Customer360Service, DocumentStructureService, GraphEnhancedRAGService) to cache query results. Implement cache invalidation strategies for data updates.",
        "testStrategy": "1. Unit tests for cache key generation\n2. Test cache hit/miss scenarios\n3. Test cache invalidation\n4. Integration tests with graph services\n5. Performance tests to measure cache impact\n6. Test with concurrent access patterns\n7. Test cache size growth over time",
        "priority": "medium",
        "dependencies": [
          "104",
          "105",
          "106"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down the Redis Caching implementation into subtasks covering: 1) Cache key generation and result storage, 2) Cache invalidation strategies, and 3) Integration with graph services for transparent caching.",
        "updatedAt": "2026-01-12T02:28:49.111Z"
      },
      {
        "id": "111",
        "title": "Implement Integration Tests for Graph Agent",
        "description": "Create comprehensive integration tests for the Graph Agent components to ensure they work together correctly and meet performance requirements.",
        "details": "Implement integration tests in tests/integration/test_graph_agent.py that verify the correct functioning of the Graph Agent components together:\n\n1. Test setup:\n   - Create test fixtures for Neo4j with sample data\n   - Set up test instances of all services\n   - Configure test client for API endpoints\n\n2. Customer 360 integration tests:\n   - Test end-to-end customer queries\n   - Verify correct data retrieval and formatting\n   - Test performance against SLAs\n\n3. Document Structure integration tests:\n   - Test document structure extraction\n   - Test smart retrieval with cross-references\n   - Verify correct handling of complex documents\n\n4. Graph-Enhanced RAG integration tests:\n   - Test query expansion with graph context\n   - Verify improved results compared to standard RAG\n   - Test performance impact\n\n5. End-to-end CKO Chat integration:\n   - Test query intent detection\n   - Verify correct routing to graph agents\n   - Test response formatting for UI\n\nImplement test utilities for loading test data, measuring performance, and comparing results:\n\n```python\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom app.main import app\nfrom app.services.neo4j_http_client import Neo4jHTTPClient\nfrom app.services.customer360_service import Customer360Service\n# Import other services\n\n@pytest.fixture\ndef test_client():\n    return TestClient(app)\n\n@pytest.fixture\ndef neo4j_client():\n    # Create test Neo4j client with test database\n    client = Neo4jHTTPClient(test_uri, test_user, test_password)\n    # Load test data\n    yield client\n    # Clean up test data\n\n@pytest.fixture\ndef customer360_service(neo4j_client):\n    return Customer360Service(neo4j_client)\n\n# Test cases\ndef test_customer360_query(test_client, neo4j_client):\n    # Test customer query endpoint\n    response = test_client.post(\n        \"/api/graph/customer360/query\",\n        json={\"query\": \"Show me everything about Test Corp\"}\n    )\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"customer\"][\"name\"] == \"Test Corp\"\n    # Additional assertions\n\n# Additional test cases for other components\n```\n\nInclude performance tests that verify the system meets the SLAs defined in the PRD.",
        "testStrategy": "1. Test with realistic test data that mimics production\n2. Measure response times against SLAs\n3. Test with various query patterns\n4. Test error handling and edge cases\n5. Test concurrent access patterns\n6. Compare results with expected outputs\n7. Test integration with existing Empire components",
        "priority": "medium",
        "dependencies": [
          "107",
          "108",
          "109",
          "110"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 7,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down the integration testing implementation into subtasks covering: 1) Test fixtures and data setup, 2) Customer 360 integration tests, 3) Document Structure and Graph-Enhanced RAG integration tests, and 4) Performance and SLA verification tests.",
        "updatedAt": "2026-01-12T03:12:51.429Z"
      },
      {
        "id": "112",
        "title": "Create MarkdownChunkerStrategy class implementing ChunkingStrategy interface",
        "description": "Implement a new chunking strategy that splits documents by markdown headers while preserving header context",
        "details": "Create a new class `MarkdownChunkerStrategy` that implements the existing `ChunkingStrategy` interface. This class will be responsible for splitting documents based on markdown headers.\n\nImplementation details:\n1. Implement the required interface methods from `ChunkingStrategy`\n2. Use regex pattern `^(#{1,6})\\s+(.+)$` to detect markdown headers\n3. Parse the document to identify all headers and their content\n4. Create a hierarchical structure of sections based on header levels\n5. For each section, create a chunk that includes the header and its content\n6. Store header metadata (level, text, hierarchy) with each chunk\n\nPseudo-code:\n```python\nclass MarkdownChunkerStrategy(ChunkingStrategy):\n    def __init__(self, max_chunk_size=1024, chunk_overlap=200):\n        self.max_chunk_size = max_chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.header_pattern = re.compile(r'^(#{1,6})\\s+(.+)$', re.MULTILINE)\n    \n    def split(self, document):\n        # Check if document has markdown headers\n        if not self._has_markdown_headers(document.text):\n            # Fall back to sentence-based chunking\n            return self._fallback_chunking(document)\n        \n        # Parse document into sections by headers\n        sections = self._parse_sections(document.text)\n        \n        # Convert sections to chunks\n        chunks = []\n        for section in sections:\n            if self._get_token_count(section.content) > self.max_chunk_size:\n                # Subdivide large sections\n                sub_chunks = self._subdivide_section(section)\n                chunks.extend(sub_chunks)\n            else:\n                chunks.append(self._create_chunk(section))\n        \n        return chunks\n```",
        "testStrategy": "1. Unit test the `MarkdownChunkerStrategy` class with various markdown documents\n2. Test with documents containing different header levels (h1-h6)\n3. Test with documents having no headers to verify fallback to sentence-based chunking\n4. Test with documents containing very large sections to verify proper subdivision\n5. Verify header metadata is correctly stored with each chunk\n6. Compare chunk quality with existing strategies using sample documents",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create MarkdownSection dataclass",
            "description": "Implement a dataclass to represent markdown sections with header information and content",
            "dependencies": [],
            "details": "Create a MarkdownSection dataclass that will store information about markdown sections including:\n- header_text: The text of the header\n- header_level: Integer representing the header level (1-6)\n- content: The content text under this header\n- parent_headers: List of parent headers for context preservation\n- position: Position in the original document\n\nThis class will be used to represent the hierarchical structure of markdown documents and will be essential for the chunking strategy.",
            "status": "pending",
            "testStrategy": "Unit test the MarkdownSection class to ensure it correctly stores and represents section data. Test initialization with various header levels and content. Verify parent header tracking works correctly.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create MarkdownChunkerConfig dataclass",
            "description": "Implement a configuration class for the markdown chunker with customizable parameters",
            "dependencies": [],
            "details": "Create a MarkdownChunkerConfig dataclass that will store configuration parameters for the markdown chunking strategy including:\n- max_chunk_size: Maximum size of each chunk in tokens\n- chunk_overlap: Number of tokens to overlap between chunks\n- preserve_headers: Boolean flag to determine if headers should be included in each chunk\n- min_chunk_size: Minimum size for a chunk to be considered valid\n- fallback_strategy: Strategy to use when no markdown headers are found\n\nThis configuration class will allow for flexible customization of the chunking behavior.",
            "status": "pending",
            "testStrategy": "Test the MarkdownChunkerConfig class with various parameter combinations. Verify default values are appropriate and parameter validation works correctly. Test serialization/deserialization if applicable.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create MarkdownChunkerStrategy class skeleton",
            "description": "Implement the basic structure of the MarkdownChunkerStrategy class implementing the ChunkingStrategy interface",
            "dependencies": [
              1,
              2
            ],
            "details": "Create the MarkdownChunkerStrategy class that implements the ChunkingStrategy interface. Include:\n- Constructor that accepts a MarkdownChunkerConfig\n- Implementation of required interface methods\n- Placeholder methods for section parsing and chunk creation\n- Basic structure for the split() method that will process documents\n- Method signatures for helper functions\n\nThis will establish the foundation for the chunking implementation while ensuring compliance with the interface contract.",
            "status": "pending",
            "testStrategy": "Test the class instantiation and verify it correctly implements the ChunkingStrategy interface. Test that configuration parameters are properly stored. Mock internal methods to verify the overall structure works as expected.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement header detection and section parsing",
            "description": "Add the regex pattern for header detection and implement the section parsing logic",
            "dependencies": [
              3
            ],
            "details": "Implement the core functionality for header detection and section parsing:\n- Add the regex pattern constant `^(#{1,6})\\s+(.+)$` for markdown header detection\n- Implement the _has_markdown_headers method to check if a document contains markdown headers\n- Create the _parse_sections method to break a document into hierarchical sections\n- Implement logic to track header hierarchy and parent-child relationships\n- Add methods to convert parsed sections into document chunks\n- Ensure proper metadata is attached to each chunk including header context\n\nThis completes the implementation of the markdown chunking strategy.",
            "status": "pending",
            "testStrategy": "Test header detection with various markdown formats. Verify section parsing correctly identifies headers of different levels. Test with complex documents containing nested headers. Ensure chunks maintain proper context and hierarchy information.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-12T02:18:09.066Z"
      },
      {
        "id": "113",
        "title": "Implement markdown header detection and section parsing",
        "description": "Create functionality to detect markdown headers and parse documents into hierarchical sections",
        "details": "Implement helper methods within the MarkdownChunkerStrategy class to detect markdown headers and parse documents into sections based on header hierarchy.\n\nImplementation details:\n1. Create a method to detect if a document contains valid markdown headers\n2. Implement section parsing logic that builds a hierarchical structure\n3. Handle edge cases like inconsistent header levels (e.g., h1 to h4 jumps)\n4. Create a MarkdownSection class to represent sections with header information\n5. Track parent-child relationships between headers\n\nPseudo-code:\n```python\ndef _has_markdown_headers(self, text):\n    return bool(self.header_pattern.search(text))\n\ndef _parse_sections(self, text):\n    lines = text.split('\\n')\n    sections = []\n    current_section = None\n    header_stack = []\n    \n    for line in lines:\n        header_match = self.header_pattern.match(line)\n        if header_match:\n            # New header found\n            level = len(header_match.group(1))  # Number of # symbols\n            header_text = header_match.group(2).strip()\n            \n            # Update header stack based on level\n            while header_stack and header_stack[-1]['level'] >= level:\n                header_stack.pop()\n            \n            # Create header hierarchy\n            hierarchy = [h['text'] for h in header_stack]\n            \n            # Save previous section if exists\n            if current_section:\n                sections.append(current_section)\n            \n            # Create new section\n            current_section = MarkdownSection(\n                header_text=header_text,\n                level=level,\n                content=line,  # Start with header line\n                parent_headers=hierarchy\n            )\n            \n            # Add to header stack\n            header_stack.append({'level': level, 'text': header_text})\n        elif current_section:\n            # Add line to current section content\n            current_section.content += '\\n' + line\n    \n    # Add final section\n    if current_section:\n        sections.append(current_section)\n    \n    return sections\n\nclass MarkdownSection:\n    def __init__(self, header_text, level, content, parent_headers):\n        self.header_text = header_text\n        self.level = level\n        self.content = content\n        self.parent_headers = parent_headers\n```",
        "testStrategy": "1. Unit test the header detection method with various inputs\n2. Test section parsing with documents having different header structures\n3. Verify correct handling of nested headers (h1 > h2 > h3)\n4. Test edge cases like empty sections, inconsistent header levels\n5. Verify parent-child relationships are correctly established\n6. Test with malformed headers to ensure they're treated as regular text",
        "priority": "high",
        "dependencies": [
          "112"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement _split_by_headers() method",
            "description": "Create a method to identify markdown headers and split document content into sections based on headers.",
            "dependencies": [],
            "details": "Implement the _split_by_headers() method in the MarkdownChunkerStrategy class that will: 1) Define a regex pattern to identify markdown headers (e.g., # Header, ## Subheader), 2) Split the document text into sections based on header boundaries, 3) Return a list of raw sections with header information including level and text. Handle edge cases like empty documents or documents without headers.",
            "status": "pending",
            "testStrategy": "Unit test with various markdown documents containing different header patterns. Test edge cases like documents with no headers, empty documents, and documents with only headers but no content.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement _build_header_hierarchy() method",
            "description": "Create a method to build hierarchical relationships between markdown headers of different levels.",
            "dependencies": [
              1
            ],
            "details": "Implement the _build_header_hierarchy() method that takes raw sections from _split_by_headers() and builds parent-child relationships between headers. This method should: 1) Track the current header stack, 2) Handle inconsistent header levels (e.g., h1 to h4 jumps), 3) Assign parent headers to each section, 4) Return a list of MarkdownSection objects with proper hierarchy information.",
            "status": "pending",
            "testStrategy": "Test with documents containing nested headers of various levels. Verify correct parent-child relationships are established. Test edge cases like inconsistent header levels (h1 followed by h3) and ensure proper hierarchy is maintained.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement chunk() main method",
            "description": "Create the main chunking method that uses header detection and hierarchy building to chunk markdown documents.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement the chunk() method in MarkdownChunkerStrategy that orchestrates the markdown chunking process: 1) Check if document contains markdown headers using _has_markdown_headers(), 2) If headers exist, use _split_by_headers() and _build_header_hierarchy() to create hierarchical sections, 3) Convert MarkdownSection objects to the standard chunk format required by the application, 4) Include metadata about section hierarchy in each chunk, 5) Handle fallback to default chunking if no headers are detected.",
            "status": "pending",
            "testStrategy": "Test the complete chunking process with various markdown documents. Verify chunks maintain proper hierarchy information. Test fallback behavior when no headers are detected. Verify chunk metadata correctly represents document structure.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add unit tests for header detection and section extraction",
            "description": "Create comprehensive unit tests for the markdown header detection and section parsing functionality.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Implement unit tests that cover: 1) Header detection with various markdown formats, 2) Section splitting with different document structures, 3) Hierarchy building with nested headers, 4) Edge cases like inconsistent header levels, empty sections, and documents without headers, 5) Integration tests for the complete chunking process, 6) Performance tests with large markdown documents containing many headers and sections.",
            "status": "pending",
            "testStrategy": "Create test fixtures with various markdown document structures. Use parameterized tests to cover multiple scenarios. Include edge cases and boundary conditions. Measure code coverage to ensure all code paths are tested.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-12T02:18:24.157Z"
      },
      {
        "id": "114",
        "title": "Implement large section subdivision with header context preservation",
        "description": "Create functionality to subdivide large sections exceeding token limits while preserving header context",
        "details": "Implement methods to handle sections that exceed the maximum chunk size (1024 tokens) by subdividing them into smaller chunks while preserving the header context.\n\nImplementation details:\n1. Create a method to calculate token count for sections\n2. Implement sentence-aware splitting for large sections\n3. Ensure each sub-chunk retains the original section's header metadata\n4. Apply chunk overlap (200 tokens) between subdivided chunks\n5. Maintain sequential ordering of subdivided chunks\n\nPseudo-code:\n```python\ndef _get_token_count(self, text):\n    # Use tokenizer to count tokens\n    return len(self.tokenizer.encode(text))\n\ndef _subdivide_section(self, section):\n    # Use sentence-aware splitting for large sections\n    sentences = self._split_into_sentences(section.content)\n    chunks = []\n    current_chunk = section.header_text + '\\n'  # Start with header\n    current_chunk_sentences = []\n    \n    for sentence in sentences:\n        # Check if adding this sentence would exceed the limit\n        test_chunk = current_chunk + ' ' + sentence\n        if self._get_token_count(test_chunk) > self.max_chunk_size and current_chunk_sentences:\n            # Create chunk with current content\n            chunk = self._create_chunk_with_header_context(\n                content=current_chunk,\n                section=section,\n                is_subdivision=True,\n                subdivision_index=len(chunks)\n            )\n            chunks.append(chunk)\n            \n            # Start new chunk with overlap\n            overlap_sentences = current_chunk_sentences[-3:]  # Approximate 200 token overlap\n            current_chunk = section.header_text + '\\n' + ' '.join(overlap_sentences) + ' ' + sentence\n            current_chunk_sentences = overlap_sentences + [sentence]\n        else:\n            # Add sentence to current chunk\n            current_chunk = test_chunk\n            current_chunk_sentences.append(sentence)\n    \n    # Add final chunk if there's content\n    if current_chunk_sentences:\n        chunk = self._create_chunk_with_header_context(\n            content=current_chunk,\n            section=section,\n            is_subdivision=True,\n            subdivision_index=len(chunks)\n        )\n        chunks.append(chunk)\n    \n    return chunks\n\ndef _create_chunk_with_header_context(self, content, section, is_subdivision=False, subdivision_index=0):\n    # Create chunk with header metadata\n    return Chunk(\n        text=content,\n        metadata={\n            'header_level': section.level,\n            'section_header': section.header_text,\n            'header_hierarchy': ' > '.join(section.parent_headers + [section.header_text]),\n            'is_header_split': True,\n            'is_subdivision': is_subdivision,\n            'subdivision_index': subdivision_index\n        }\n    )\n```",
        "testStrategy": "1. Test with sections of various sizes to verify subdivision logic\n2. Verify token counting is accurate\n3. Check that subdivided chunks maintain header context\n4. Verify chunk overlap is correctly applied\n5. Test with edge cases like very short sentences and very long sentences\n6. Verify sequential ordering of subdivided chunks",
        "priority": "medium",
        "dependencies": [
          "112",
          "113"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement _count_tokens() method using tiktoken",
            "description": "Create a method to accurately count tokens in text sections using the tiktoken library",
            "dependencies": [],
            "details": "Implement the _count_tokens() method that uses the tiktoken library to accurately count tokens in text sections. This method will be used to determine if a section needs to be subdivided based on the maximum token limit (1024 tokens). The implementation should handle different encoding models and cache the tokenizer for efficiency.",
            "status": "pending",
            "testStrategy": "Write unit tests to verify token counting accuracy with various text inputs. Compare results with OpenAI's tokenizer for validation. Test with edge cases like empty strings, special characters, and multilingual content.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement _chunk_oversized_section() using SentenceSplitter",
            "description": "Create a method to split large sections into smaller chunks while preserving sentence boundaries",
            "dependencies": [
              1
            ],
            "details": "Implement the _chunk_oversized_section() method that uses a SentenceSplitter to divide large sections into smaller chunks without breaking sentences. The method should ensure that each chunk starts with the section header for context preservation and implements sentence-aware splitting logic. It should handle edge cases like very long sentences that might exceed the token limit on their own.",
            "status": "pending",
            "testStrategy": "Test with sections of various sizes to verify subdivision logic. Ensure sentence boundaries are preserved. Verify that very long sections are properly split into multiple chunks. Test edge cases like sections with very few sentences but many tokens.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add chunk_index and total_section_chunks metadata",
            "description": "Enhance chunk metadata with indexing information to maintain sequential ordering of subdivided chunks",
            "dependencies": [
              2
            ],
            "details": "Modify the _create_chunk_with_header_context() method to include additional metadata fields: 'chunk_index' and 'total_section_chunks'. These fields will help maintain the sequential ordering of subdivided chunks and provide information about the total number of chunks a section was split into. Update the chunk creation logic to properly set these values when creating subdivided chunks.",
            "status": "pending",
            "testStrategy": "Test that chunks from subdivided sections have correct sequential index values. Verify that total_section_chunks accurately reflects the number of chunks created. Test with sections that produce different numbers of chunks to ensure consistency.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement chunk_overlap with 200 tokens between subdivided sections",
            "description": "Add functionality to ensure 200 token overlap between adjacent chunks from the same section",
            "dependencies": [
              2,
              3
            ],
            "details": "Enhance the _chunk_oversized_section() method to implement a 200 token overlap between adjacent chunks from the same section. This involves modifying the chunking algorithm to include approximately 200 tokens from the end of the previous chunk at the beginning of the next chunk. The implementation should use the _count_tokens() method to ensure accurate token counting and should handle edge cases where the overlap might be smaller due to section size constraints.",
            "status": "pending",
            "testStrategy": "Verify that adjacent chunks have approximately 200 tokens of overlapping content. Test with various section sizes to ensure consistent overlap behavior. Check that the first and last sentences of adjacent chunks properly overlap. Test edge cases where sections are just slightly larger than the maximum chunk size.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-12T02:18:38.723Z"
      },
      {
        "id": "115",
        "title": "Implement fallback to sentence-based chunking",
        "description": "Create fallback mechanism for documents without markdown headers",
        "details": "Implement a fallback mechanism that uses the existing sentence-based chunking strategy when a document contains no recognizable markdown headers.\n\nImplementation details:\n1. Create a method to delegate to the existing sentence-based chunker\n2. Ensure seamless transition between strategies based on document content\n3. Preserve all existing functionality of sentence-based chunking\n4. Add appropriate logging to indicate fallback was used\n\nPseudo-code:\n```python\ndef _fallback_chunking(self, document):\n    # Log that we're falling back to sentence-based chunking\n    logger.info(f\"No markdown headers found in document {document.id}, falling back to sentence-based chunking\")\n    \n    # Create and use sentence chunker with same parameters\n    sentence_chunker = SentenceChunkerStrategy(\n        max_chunk_size=self.max_chunk_size,\n        chunk_overlap=self.chunk_overlap\n    )\n    \n    # Get chunks from sentence chunker\n    chunks = sentence_chunker.split(document)\n    \n    # Add metadata to indicate fallback was used\n    for chunk in chunks:\n        chunk.metadata['chunking_strategy'] = 'sentence_fallback'\n        chunk.metadata['is_header_split'] = False\n    \n    return chunks\n```",
        "testStrategy": "1. Test with documents containing no markdown headers\n2. Verify fallback to sentence-based chunking works correctly\n3. Compare results with direct use of sentence chunker to ensure equivalence\n4. Verify appropriate metadata is added to chunks\n5. Check logging to ensure fallback is properly recorded\n6. Test with edge cases like documents with malformed headers",
        "priority": "medium",
        "dependencies": [
          "112"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add is_markdown_content() method with min_headers threshold",
            "description": "Create a method to detect if a document contains sufficient markdown headers to use header-based chunking",
            "dependencies": [],
            "details": "Implement a new method called `is_markdown_content()` that analyzes a document to determine if it contains enough markdown headers to be processed with header-based chunking. The method should accept a parameter for minimum number of headers required (default to 2 or 3). It should use regex patterns to identify markdown headers (e.g., lines starting with #, ##, etc.) and return a boolean indicating if the document meets the threshold. This will be used as the decision point for whether to use header-based chunking or fall back to sentence-based chunking.",
            "status": "pending",
            "testStrategy": "Write unit tests with various document samples: documents with many headers, few headers, no headers, and edge cases like headers with special characters. Verify the method correctly identifies documents that should use header-based chunking.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement fallback to SentenceSplitter when no headers detected",
            "description": "Create the fallback mechanism that delegates to sentence-based chunking when a document lacks sufficient markdown structure",
            "dependencies": [
              1
            ],
            "details": "Implement the `_fallback_chunking()` method as outlined in the pseudo-code. This method should be called when `is_markdown_content()` returns False. It should instantiate a SentenceChunkerStrategy with the same parameters as the current chunker, process the document using this strategy, and add appropriate metadata to each chunk to indicate the fallback strategy was used. Ensure proper logging is implemented to record when fallback occurs. Modify the main chunking method to check for markdown content first and delegate to the appropriate chunking strategy.",
            "status": "pending",
            "testStrategy": "Test with documents containing no markdown headers. Verify chunks are created correctly using sentence-based chunking. Check that metadata is properly added to chunks. Verify logging messages are generated correctly. Compare results with direct use of sentence chunker to ensure equivalence.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add integration test for non-markdown document processing",
            "description": "Create comprehensive integration tests to verify the fallback mechanism works end-to-end",
            "dependencies": [
              2
            ],
            "details": "Develop integration tests that process various document types through the chunking system. Include test cases for documents with no markdown headers, documents with insufficient headers (below threshold), and documents with adequate headers. Verify that the system correctly identifies document types, applies the appropriate chunking strategy, and produces expected results. Test that metadata is correctly applied in all cases. Also test edge cases such as very short documents, documents with unusual formatting, and documents with mixed content types.",
            "status": "pending",
            "testStrategy": "Create a test suite with multiple document fixtures. Compare chunking results against expected outputs for each document type. Verify metadata fields are correctly populated. Check logging output to confirm appropriate strategy selection messages. Test performance to ensure fallback mechanism doesn't introduce significant overhead.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-12T02:18:53.739Z"
      },
      {
        "id": "116",
        "title": "Extend Chunk class with header metadata fields",
        "description": "Extend the Chunk entity to include header-related metadata fields",
        "details": "Extend the existing Chunk class or its metadata structure to include new fields for header information.\n\nImplementation details:\n1. Add the following metadata fields to the Chunk class:\n   - header_level: Integer (1-6) representing the header level\n   - section_header: String containing the section header text\n   - header_hierarchy: String representing the full header hierarchy (e.g., \"Chapter 1 > Introduction > Overview\")\n   - is_header_split: Boolean indicating if the chunk was created by header splitting\n   - is_subdivision: Boolean indicating if the chunk is a subdivision of a larger section\n   - subdivision_index: Integer representing the position in a subdivided section\n\n2. Ensure backward compatibility with existing chunk metadata\n3. Update any relevant documentation or type definitions\n\nPseudo-code:\n```python\n# If Chunk is a dataclass or similar\nclass Chunk:\n    text: str\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    \n    # Helper methods for header metadata\n    @property\n    def header_level(self) -> Optional[int]:\n        return self.metadata.get('header_level')\n    \n    @property\n    def section_header(self) -> Optional[str]:\n        return self.metadata.get('section_header')\n    \n    @property\n    def header_hierarchy(self) -> Optional[str]:\n        return self.metadata.get('header_hierarchy')\n    \n    @property\n    def is_header_split(self) -> bool:\n        return self.metadata.get('is_header_split', False)\n```",
        "testStrategy": "1. Unit test the extended Chunk class\n2. Verify all new metadata fields can be set and retrieved\n3. Test backward compatibility with existing chunks\n4. Verify helper methods work correctly\n5. Test serialization/deserialization of chunks with header metadata\n6. Verify integration with existing systems that use Chunk objects",
        "priority": "medium",
        "dependencies": [
          "112"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-12T02:19:14.286Z"
      },
      {
        "id": "117",
        "title": "Implement automatic markdown detection for LlamaParse output",
        "description": "Create functionality to automatically detect markdown content from LlamaParse output",
        "details": "Implement logic to automatically detect when a document comes from LlamaParse with markdown formatting and apply the appropriate chunking strategy.\n\nImplementation details:\n1. Create a detection method to identify LlamaParse markdown output\n2. Integrate with the document processing pipeline to automatically select the markdown chunker\n3. Look for LlamaParse metadata or specific markdown patterns\n4. Handle edge cases where markdown might be malformed\n\nPseudo-code:\n```python\ndef is_llamaparse_markdown(document):\n    # Check document metadata for LlamaParse origin\n    if document.metadata.get('source') == 'llamaparse' and document.metadata.get('result_type') == 'markdown':\n        return True\n    \n    # Check for markdown header patterns\n    if re.search(r'^(#{1,6})\\s+(.+)$', document.text, re.MULTILINE):\n        # Count headers to ensure it's not just occasional use of # symbols\n        header_count = len(re.findall(r'^(#{1,6})\\s+(.+)$', document.text, re.MULTILINE))\n        if header_count >= 3:  # Arbitrary threshold for a structured document\n            return True\n    \n    return False\n\n# In document processor class\ndef select_chunking_strategy(document):\n    if is_llamaparse_markdown(document):\n        return MarkdownChunkerStrategy()\n    elif is_code_document(document):\n        return CodeChunkerStrategy()\n    elif is_transcript(document):\n        return TranscriptChunkerStrategy()\n    else:\n        return SentenceChunkerStrategy()\n```",
        "testStrategy": "1. Test with various LlamaParse outputs to verify detection\n2. Test with non-LlamaParse markdown documents\n3. Test with documents having different levels of markdown formatting\n4. Verify correct strategy selection based on document type\n5. Test edge cases with minimal or malformed markdown\n6. Verify integration with the document processing pipeline",
        "priority": "medium",
        "dependencies": [
          "112",
          "113"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement is_llamaparse_markdown detection function",
            "description": "Create a function to detect markdown content from LlamaParse output based on metadata and content patterns.",
            "dependencies": [],
            "details": "Implement the is_llamaparse_markdown() function in document_processor.py that analyzes document content and metadata to determine if it's markdown from LlamaParse. The function should check for LlamaParse metadata tags, examine markdown header patterns (using regex to find patterns like '## Header'), count the frequency of markdown elements, and determine if the threshold of markdown elements is met to classify as structured markdown. Include handling for edge cases with malformed markdown.",
            "status": "pending",
            "testStrategy": "Write unit tests with various document samples including: LlamaParse markdown with metadata, markdown without metadata, non-markdown content, edge cases with minimal markdown elements, and malformed markdown. Verify correct detection in each scenario.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Update document processing pipeline for markdown detection",
            "description": "Modify the document processing pipeline to automatically select the markdown chunker when LlamaParse markdown is detected.",
            "dependencies": [
              1
            ],
            "details": "Update the source_processing.py file to integrate the is_llamaparse_markdown detection function into the document processing workflow. Modify the select_chunking_strategy() method to check for markdown content and route documents to the MarkdownChunkerStrategy when appropriate. Ensure the detection happens early in the pipeline to optimize processing. Add logging for strategy selection decisions to aid debugging and monitoring. Handle the transition between detection and chunking strategy selection seamlessly.",
            "status": "pending",
            "testStrategy": "Create integration tests that verify the entire document processing pipeline correctly identifies LlamaParse markdown and selects the appropriate chunking strategy. Test with various document types to ensure other document types still route to their correct chunkers. Measure performance impact of the additional detection step.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add comprehensive tests for LlamaParse markdown flow",
            "description": "Create integration and unit tests to verify the end-to-end markdown detection and processing functionality.",
            "dependencies": [
              1,
              2
            ],
            "details": "Develop a comprehensive test suite in test_llamaparse_markdown.py that validates the entire markdown detection and processing flow. Include tests for various LlamaParse output formats, different markdown structures, and edge cases. Create mock LlamaParse outputs with varying degrees of markdown formatting. Test the integration between detection and chunking strategy selection. Verify that documents are correctly processed with the appropriate chunking strategy based on their content. Include performance tests to ensure the detection doesn't significantly impact processing time.",
            "status": "pending",
            "testStrategy": "Use pytest fixtures to create various test documents. Test the full pipeline from document ingestion through chunking. Verify chunk boundaries respect markdown structure. Test with real-world examples of LlamaParse output. Include regression tests to ensure other document types aren't affected by the changes.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-12T02:20:56.615Z"
      },
      {
        "id": "118",
        "title": "Implement chunking strategy factory and registration",
        "description": "Create a factory pattern for chunking strategies and register the new MarkdownChunkerStrategy",
        "details": "Implement a factory pattern for chunking strategies to allow dynamic selection and registration of strategies, including the new MarkdownChunkerStrategy.\n\nImplementation details:\n1. Create a ChunkerStrategyFactory class\n2. Implement registration mechanism for chunking strategies\n3. Add logic to select the appropriate strategy based on document type\n4. Register all existing strategies (semantic, code, transcript)\n5. Register the new MarkdownChunkerStrategy\n6. Ensure backward compatibility with existing code\n\nPseudo-code:\n```python\nclass ChunkerStrategyFactory:\n    _strategies = {}\n    \n    @classmethod\n    def register_strategy(cls, name, strategy_class):\n        cls._strategies[name] = strategy_class\n    \n    @classmethod\n    def get_strategy(cls, name, **kwargs):\n        strategy_class = cls._strategies.get(name)\n        if not strategy_class:\n            raise ValueError(f\"Unknown chunking strategy: {name}\")\n        return strategy_class(**kwargs)\n    \n    @classmethod\n    def get_strategy_for_document(cls, document, **kwargs):\n        # Select strategy based on document type\n        if is_llamaparse_markdown(document):\n            return cls.get_strategy('markdown', **kwargs)\n        elif is_code_document(document):\n            return cls.get_strategy('code', **kwargs)\n        elif is_transcript(document):\n            return cls.get_strategy('transcript', **kwargs)\n        else:\n            return cls.get_strategy('sentence', **kwargs)\n\n# Register strategies\nChunkerStrategyFactory.register_strategy('sentence', SentenceChunkerStrategy)\nChunkerStrategyFactory.register_strategy('code', CodeChunkerStrategy)\nChunkerStrategyFactory.register_strategy('transcript', TranscriptChunkerStrategy)\nChunkerStrategyFactory.register_strategy('markdown', MarkdownChunkerStrategy)\n```",
        "testStrategy": "1. Unit test the ChunkerStrategyFactory class\n2. Test registration of strategies\n3. Test retrieval of strategies by name\n4. Test automatic strategy selection based on document type\n5. Verify all existing strategies are properly registered\n6. Test with various document types to ensure correct strategy selection",
        "priority": "medium",
        "dependencies": [
          "112",
          "117"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-12T02:22:29.693Z"
      },
      {
        "id": "119",
        "title": "Implement chunk filtering by header metadata",
        "description": "Add functionality to filter chunks by header level or section name",
        "details": "Implement functionality to allow filtering of chunks based on header metadata, enabling advanced search capabilities.\n\nImplementation details:\n1. Extend the search/query interface to accept header filter parameters\n2. Implement filtering logic based on header level, section name, or header hierarchy\n3. Ensure efficient filtering without significant performance impact\n4. Add documentation for the new filtering capabilities\n\nPseudo-code:\n```python\nclass ChunkRepository:\n    # Existing methods...\n    \n    def search_with_filters(self, query, filters=None, **kwargs):\n        # Start with basic search\n        results = self.basic_search(query, **kwargs)\n        \n        # Apply header filters if specified\n        if filters and results:\n            if 'header_level' in filters:\n                results = [r for r in results if r.chunk.metadata.get('header_level') == filters['header_level']]\n            \n            if 'section_header' in filters:\n                section_pattern = re.compile(filters['section_header'], re.IGNORECASE)\n                results = [r for r in results if r.chunk.metadata.get('section_header') and \n                           section_pattern.search(r.chunk.metadata.get('section_header'))]\n            \n            if 'header_hierarchy' in filters:\n                hierarchy_pattern = re.compile(filters['header_hierarchy'], re.IGNORECASE)\n                results = [r for r in results if r.chunk.metadata.get('header_hierarchy') and \n                           hierarchy_pattern.search(r.chunk.metadata.get('header_hierarchy'))]\n        \n        return results\n\n# API endpoint or service method\ndef search_documents(query, header_level=None, section_name=None, header_path=None, **kwargs):\n    filters = {}\n    if header_level is not None:\n        filters['header_level'] = int(header_level)\n    if section_name:\n        filters['section_header'] = section_name\n    if header_path:\n        filters['header_hierarchy'] = header_path\n    \n    return chunk_repository.search_with_filters(query, filters, **kwargs)\n```",
        "testStrategy": "1. Test filtering by header level\n2. Test filtering by section name (exact and partial matches)\n3. Test filtering by header hierarchy\n4. Test combinations of multiple filters\n5. Verify performance impact is minimal\n6. Test with edge cases like non-existent headers or malformed filters",
        "priority": "low",
        "dependencies": [
          "112",
          "116"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-12T03:04:07.631Z"
      },
      {
        "id": "120",
        "title": "Implement logging and observability for markdown chunking",
        "description": "Add comprehensive logging and observability for the markdown chunking process",
        "details": "Implement logging and observability features to track the markdown chunking process, including strategy selection, chunk statistics, and performance metrics.\n\nImplementation details:\n1. Add structured logging throughout the MarkdownChunkerStrategy class\n2. Log key events: strategy selection, header detection, section parsing, chunking decisions\n3. Record metrics: number of chunks created, average chunk size, processing time\n4. Create summary statistics for each processed document\n5. Ensure logs can be used for debugging and performance analysis\n\nPseudo-code:\n```python\nclass MarkdownChunkerStrategy(ChunkingStrategy):\n    # Existing methods...\n    \n    def split(self, document):\n        start_time = time.time()\n        logger.info(f\"Starting markdown chunking for document {document.id}\")\n        \n        # Check if document has markdown headers\n        has_headers = self._has_markdown_headers(document.text)\n        logger.info(f\"Markdown headers detected: {has_headers}\")\n        \n        if not has_headers:\n            logger.info(f\"Falling back to sentence chunking for document {document.id}\")\n            chunks = self._fallback_chunking(document)\n        else:\n            # Parse document into sections by headers\n            sections = self._parse_sections(document.text)\n            logger.info(f\"Parsed {len(sections)} sections from document\")\n            \n            # Log section statistics\n            header_levels = Counter([section.level for section in sections])\n            logger.info(f\"Header level distribution: {dict(header_levels)}\")\n            \n            # Convert sections to chunks\n            chunks = []\n            subdivided_sections = 0\n            \n            for section in sections:\n                token_count = self._get_token_count(section.content)\n                if token_count > self.max_chunk_size:\n                    logger.info(f\"Subdividing section '{section.header_text}' with {token_count} tokens\")\n                    subdivided_sections += 1\n                    sub_chunks = self._subdivide_section(section)\n                    chunks.extend(sub_chunks)\n                else:\n                    chunks.append(self._create_chunk(section))\n            \n            logger.info(f\"Created {len(chunks)} chunks, subdivided {subdivided_sections} sections\")\n        \n        # Calculate and log metrics\n        processing_time = time.time() - start_time\n        avg_chunk_size = sum(len(c.text) for c in chunks) / len(chunks) if chunks else 0\n        avg_token_count = sum(self._get_token_count(c.text) for c in chunks) / len(chunks) if chunks else 0\n        \n        logger.info(f\"Markdown chunking completed in {processing_time:.2f}s\")\n        logger.info(f\"Chunks: {len(chunks)}, Avg size: {avg_chunk_size:.1f} chars, Avg tokens: {avg_token_count:.1f}\")\n        \n        # Add processing metadata to document\n        document.metadata['chunking_stats'] = {\n            'strategy': 'markdown' if has_headers else 'sentence_fallback',\n            'chunk_count': len(chunks),\n            'avg_chunk_size': avg_chunk_size,\n            'avg_token_count': avg_token_count,\n            'processing_time': processing_time,\n            'subdivided_sections': subdivided_sections if has_headers else 0\n        }\n        \n        return chunks\n```",
        "testStrategy": "1. Verify logs are generated for key events in the chunking process\n2. Test that metrics are accurately calculated and recorded\n3. Check that document metadata is properly updated with chunking statistics\n4. Test with various document types to ensure comprehensive logging\n5. Verify log levels are appropriate (info for normal operation, warning/error for issues)\n6. Test integration with existing logging and monitoring systems",
        "priority": "low",
        "dependencies": [
          "112",
          "113",
          "114"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-12T02:21:24.237Z"
      },
      {
        "id": "121",
        "title": "Create comprehensive tests and documentation",
        "description": "Develop comprehensive tests and documentation for the markdown chunking feature",
        "details": "Create a comprehensive test suite and documentation for the markdown chunking feature to ensure quality and usability.\n\nImplementation details:\n1. Create unit tests for all components of the MarkdownChunkerStrategy\n2. Develop integration tests with the full document processing pipeline\n3. Create performance tests to verify compliance with success criteria\n4. Write user documentation explaining the feature and its benefits\n5. Create developer documentation with examples and API references\n6. Update existing documentation to reference the new feature\n\nTest cases to implement:\n- Unit tests for header detection, section parsing, chunk creation\n- Tests for edge cases: empty documents, malformed markdown, inconsistent headers\n- Tests for large documents and sections requiring subdivision\n- Tests for fallback to sentence chunking\n- Integration tests with the full RAG pipeline\n- Performance tests comparing with existing chunking strategies\n\nDocumentation to create:\n- User guide explaining markdown chunking benefits\n- Developer guide for extending or customizing the chunking strategy\n- API reference for the new classes and methods\n- Examples of filtering by header metadata\n- Troubleshooting guide for common issues",
        "testStrategy": "1. Verify all tests pass and provide good coverage\n2. Review documentation for clarity and completeness\n3. Have team members review and provide feedback\n4. Test documentation examples to ensure they work as described\n5. Verify integration with existing documentation systems\n6. Check that all success criteria are verified by tests",
        "priority": "medium",
        "dependencies": [
          "112",
          "113",
          "114",
          "115",
          "116",
          "117",
          "118",
          "119",
          "120"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-12T02:34:20.210Z"
      },
      {
        "id": "122",
        "title": "Create Content Prep Agent Service",
        "description": "Implement the core Content Prep Agent (AGENT-016) service that will validate, order, and prepare content before ingestion into the knowledge base.",
        "details": "Create the `app/services/content_prep_agent.py` file with the following components:\n\n1. Implement the CrewAI agent definition as specified in the PRD\n2. Create the ContentSet, ContentFile, and ProcessingManifest data classes\n3. Implement the three main components:\n   - Set Detector: Pattern matching and grouping logic\n   - Order Resolver: Sequence parsing and gap detection\n   - Manifest Generator: Ordered queue and dependency tracking\n\nPseudo-code:\n```python\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import List, Dict, Optional\nimport re\nimport uuid\n\n# Data models as specified in PRD\n@dataclass\nclass ContentFile:\n    b2_path: str\n    filename: str\n    sequence_number: Optional[int] = None\n    dependencies: List[str] = None\n    estimated_complexity: str = \"medium\"\n    file_type: str = \"\"\n    size_bytes: int = 0\n    metadata: Dict = None\n    \n    def __post_init__(self):\n        if self.dependencies is None:\n            self.dependencies = []\n        if self.metadata is None:\n            self.metadata = {}\n\n@dataclass\nclass ContentSet:\n    id: str\n    name: str\n    detection_method: str\n    files: List[ContentFile]\n    is_complete: bool = False\n    missing_files: List[str] = None\n    processing_status: str = \"pending\"\n    created_at: datetime = None\n    metadata: Dict = None\n    \n    def __post_init__(self):\n        if self.missing_files is None:\n            self.missing_files = []\n        if self.created_at is None:\n            self.created_at = datetime.now()\n        if self.metadata is None:\n            self.metadata = {}\n\n@dataclass\nclass ProcessingManifest:\n    content_set_id: str\n    ordered_files: List[ContentFile]\n    total_files: int\n    estimated_processing_time: int\n    warnings: List[str] = None\n    context: Dict = None\n    \n    def __post_init__(self):\n        if self.warnings is None:\n            self.warnings = []\n        if self.context is None:\n            self.context = {}\n\n# Sequence detection patterns as specified in PRD\nSEQUENCE_PATTERNS = [\n    # Numeric prefix patterns\n    r\"^(\\d{1,3})[-_\\s]\",              # \"01-intro.pdf\", \"1_chapter.pdf\"\n    r\"[-_\\s](\\d{1,3})[-_\\s.]\",        # \"chapter-01-intro.pdf\"\n    r\"module[-_\\s]?(\\d{1,3})\",        # \"module01.pdf\", \"module-1.pdf\"\n    r\"chapter[-_\\s]?(\\d{1,3})\",       # \"chapter1.pdf\", \"chapter-01.pdf\"\n    r\"lesson[-_\\s]?(\\d{1,3})\",        # \"lesson5.pdf\"\n    r\"part[-_\\s]?(\\d{1,3})\",          # \"part-1.pdf\"\n    r\"week[-_\\s]?(\\d{1,3})\",          # \"week-01.pdf\"\n    r\"unit[-_\\s]?(\\d{1,3})\",          # \"unit-1.pdf\"\n\n    # Alpha sequence patterns\n    r\"^([a-z])[-_\\s]\",                # \"a-intro.pdf\", \"b-basics.pdf\"\n\n    # Roman numerals\n    r\"^(i{1,3}|iv|v|vi{0,3}|ix|x)[-_\\s]\",  # \"i-intro.pdf\", \"ii-basics.pdf\"\n]\n\nCONTENT_SET_INDICATORS = [\n    r\"course[-_\\s]?\",\n    r\"tutorial[-_\\s]?\",\n    r\"training[-_\\s]?\",\n    r\"documentation[-_\\s]?\",\n    r\"manual[-_\\s]?\",\n    r\"series[-_\\s]?\",\n    r\"book[-_\\s]?\",\n]\n\nclass ContentPrepAgent:\n    \"\"\"AGENT-016: Content Prep Agent implementation\"\"\"\n    \n    def __init__(self):\n        self.agent_config = {\n            \"id\": \"AGENT-016\",\n            \"name\": \"Content Prep Agent\",\n            \"role\": \"Content Preparation Specialist\",\n            \"goal\": \"Validate, order, and prepare content sets for optimal knowledge base ingestion\",\n            \"backstory\": \"\"\"You are an expert in content organization and curriculum design.\n            You understand that learning materials must be processed in logical sequence\n            to maintain prerequisite relationships. You detect patterns in file naming,\n            identify content sets, and ensure completeness before processing begins.\"\"\",\n            \"model\": \"claude-3-5-haiku-20241022\",  # Fast, cost-effective for ordering\n            \"tools\": [\n                \"file_metadata_reader\",\n                \"sequence_pattern_detector\",\n                \"b2_file_lister\",\n                \"manifest_generator\"\n            ],\n            \"allow_delegation\": False,\n            \"verbose\": True\n        }\n    \n    def detect_content_sets(self, files, detection_mode=\"auto\"):\n        \"\"\"Detect related content sets from a list of files\"\"\"\n        # Implementation of set detection logic\n        # Group by naming patterns, prefixes, etc.\n        pass\n        \n    def validate_completeness(self, content_set):\n        \"\"\"Check for missing files in sequence\"\"\"\n        # Gap detection in sequence numbers\n        # Return list of missing files\n        pass\n        \n    def resolve_order(self, files):\n        \"\"\"Determine the correct processing order\"\"\"\n        # Parse file names for sequence indicators\n        # Sort by detected sequence\n        # Fall back to LLM for ambiguous cases\n        pass\n        \n    def generate_manifest(self, content_set, proceed_incomplete=False):\n        \"\"\"Create processing manifest with ordered files\"\"\"\n        # Generate ordered queue\n        # Add dependencies and context\n        # Include warnings for missing files\n        pass\n        \n    def estimate_processing_time(self, files):\n        \"\"\"Estimate processing time based on file types and sizes\"\"\"\n        # Calculate based on file size, type, and complexity\n        pass\n        \n    def detect_sequence_number(self, filename):\n        \"\"\"Extract sequence number from filename using patterns\"\"\"\n        # Try each pattern in SEQUENCE_PATTERNS\n        # Return extracted number if found\n        pass\n```",
        "testStrategy": "1. Unit tests for each component (Set Detector, Order Resolver, Manifest Generator)\n2. Test with various file naming patterns to ensure correct sequence detection\n3. Test gap detection with incomplete sequences\n4. Test with edge cases (single files, very large sets)\n5. Verify correct data model instantiation\n6. Mock B2 file listing for integration testing",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create data classes for ContentFile, ContentSet, and ProcessingManifest",
            "description": "Implement the data models needed for the Content Prep Agent service as specified in the PRD.",
            "dependencies": [],
            "details": "Create the data classes ContentFile, ContentSet, and ProcessingManifest with all required fields and proper type annotations. Implement the __post_init__ methods to handle default values for lists and dictionaries. Ensure all fields match the specifications in the PRD and pseudo-code.",
            "status": "pending",
            "testStrategy": "Test data class instantiation with various parameters. Verify default values are properly initialized. Test serialization/deserialization of the data classes.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement sequence detection patterns and content set indicators",
            "description": "Define the regex patterns for sequence detection and content set identification.",
            "dependencies": [
              1
            ],
            "details": "Create the SEQUENCE_PATTERNS list containing all regex patterns for detecting sequence numbers in filenames. Implement the CONTENT_SET_INDICATORS list with patterns for identifying related content sets. Ensure patterns cover all specified cases in the PRD including numeric prefixes, alpha sequences, and roman numerals.",
            "status": "pending",
            "testStrategy": "Test each regex pattern against sample filenames to verify correct matching. Create comprehensive test cases covering all pattern variations.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create ContentPrepAgent class skeleton with CrewAI configuration",
            "description": "Implement the base ContentPrepAgent class with CrewAI agent configuration.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create the ContentPrepAgent class with proper initialization and CrewAI agent configuration as specified in the PRD. Include the agent_config dictionary with all required fields (id, name, role, goal, backstory, model, tools, etc.). Define method stubs for all required functionality.",
            "status": "pending",
            "testStrategy": "Verify agent initialization and configuration. Test that the agent_config contains all required fields with correct values.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement sequence detection and extraction methods",
            "description": "Create methods for extracting sequence numbers and prefixes from filenames.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement the detect_sequence_number method to extract sequence numbers from filenames using the defined regex patterns. Create helper methods for extracting prefixes and other metadata from filenames. Ensure proper handling of edge cases and different sequence formats.",
            "status": "pending",
            "testStrategy": "Test sequence detection with various filename formats. Verify correct extraction of sequence numbers, prefixes, and other metadata. Test edge cases like missing sequences or unusual formats.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement content set detection functionality",
            "description": "Create the detect_content_sets method to identify related content sets from file lists.",
            "dependencies": [
              3,
              4
            ],
            "details": "Implement the detect_content_sets method that groups files into related content sets based on naming patterns, prefixes, and other indicators. Include support for different detection modes (auto, pattern-based, prefix-based). Create helper methods for pattern matching and grouping logic.",
            "status": "pending",
            "testStrategy": "Test content set detection with various file collections. Verify correct grouping of related files. Test different detection modes and edge cases like single files or ambiguous groupings.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement order resolution and completeness validation",
            "description": "Create methods for determining processing order and detecting missing files.",
            "dependencies": [
              4,
              5
            ],
            "details": "Implement the resolve_order method to determine the correct processing sequence for files based on detected sequence numbers. Create the validate_completeness method to identify gaps in sequences and generate a list of potentially missing files. Include logic for handling ambiguous cases using LLM fallback.",
            "status": "pending",
            "testStrategy": "Test order resolution with various file sequences. Verify correct ordering based on sequence numbers. Test gap detection with incomplete sequences. Test LLM fallback for ambiguous cases.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Implement manifest generation and processing time estimation",
            "description": "Create methods for generating processing manifests and estimating processing times.",
            "dependencies": [
              5,
              6
            ],
            "details": "Implement the generate_manifest method to create a ProcessingManifest with ordered files, dependencies, and context information. Create the estimate_processing_time method to calculate expected processing duration based on file types, sizes, and complexity. Include handling for warnings about missing files and incomplete sets.",
            "status": "pending",
            "testStrategy": "Test manifest generation with various content sets. Verify correct ordering and dependency tracking. Test processing time estimation with different file types and sizes. Verify warning generation for incomplete sets.",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Implement database storage and retrieval methods",
            "description": "Create methods for persisting content sets and manifests to the database.",
            "dependencies": [
              1,
              7
            ],
            "details": "Implement methods for storing ContentSet and ProcessingManifest objects in the database. Create retrieval methods for loading existing content sets and manifests. Ensure proper error handling and transaction management. Implement status update methods for tracking processing progress.",
            "status": "pending",
            "testStrategy": "Test database storage and retrieval with mock database connections. Verify correct serialization and deserialization of objects. Test error handling and transaction management. Verify status updates work correctly.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-13T21:05:19.213Z"
      },
      {
        "id": "123",
        "title": "Create Content Sets Database Schema",
        "description": "Create the database schema for content sets and content set files in Supabase as specified in the PRD.",
        "details": "Create the migration file `migrations/create_content_sets.sql` with the SQL schema for the content_sets and content_set_files tables. This will store metadata about detected content sets and their files.\n\nThe schema should include:\n1. content_sets table with fields for id, name, detection method, completeness, etc.\n2. content_set_files table with fields for file metadata and sequence information\n3. Appropriate indexes for efficient querying\n4. Foreign key relationships\n\nSQL Schema:\n```sql\nCREATE TABLE content_sets (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    name VARCHAR(255) NOT NULL,\n    detection_method VARCHAR(50) NOT NULL,\n    is_complete BOOLEAN DEFAULT FALSE,\n    missing_files JSONB DEFAULT '[]',\n    file_count INTEGER NOT NULL,\n    processing_status VARCHAR(50) DEFAULT 'pending',\n    created_at TIMESTAMPTZ DEFAULT NOW(),\n    updated_at TIMESTAMPTZ DEFAULT NOW(),\n    metadata JSONB DEFAULT '{}'\n);\n\nCREATE TABLE content_set_files (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    content_set_id UUID REFERENCES content_sets(id) ON DELETE CASCADE,\n    b2_path VARCHAR(500) NOT NULL,\n    filename VARCHAR(255) NOT NULL,\n    sequence_number INTEGER,\n    dependencies JSONB DEFAULT '[]',\n    estimated_complexity VARCHAR(20),\n    file_type VARCHAR(50),\n    size_bytes BIGINT,\n    processing_status VARCHAR(50) DEFAULT 'pending',\n    created_at TIMESTAMPTZ DEFAULT NOW(),\n    metadata JSONB DEFAULT '{}'\n);\n\nCREATE INDEX idx_content_sets_status ON content_sets(processing_status);\nCREATE INDEX idx_content_set_files_set ON content_set_files(content_set_id);\nCREATE INDEX idx_content_set_files_sequence ON content_set_files(content_set_id, sequence_number);\n\n-- Index for retention policy\nCREATE INDEX idx_content_sets_updated ON content_sets(updated_at);\n```\n\nAlso create a Pydantic model file `app/models/content_sets.py` to define the data models for the API:\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Optional\nfrom datetime import datetime\nimport uuid\n\nclass ContentFileBase(BaseModel):\n    b2_path: str\n    filename: str\n    sequence_number: Optional[int] = None\n    dependencies: List[str] = Field(default_factory=list)\n    estimated_complexity: str = \"medium\"\n    file_type: str = \"\"\n    size_bytes: int = 0\n    metadata: Dict = Field(default_factory=dict)\n\nclass ContentFile(ContentFileBase):\n    id: uuid.UUID\n    content_set_id: uuid.UUID\n    processing_status: str = \"pending\"\n    created_at: datetime\n\nclass ContentFileCreate(ContentFileBase):\n    content_set_id: uuid.UUID\n\nclass ContentSetBase(BaseModel):\n    name: str\n    detection_method: str\n    is_complete: bool = False\n    missing_files: List[str] = Field(default_factory=list)\n    file_count: int\n    metadata: Dict = Field(default_factory=dict)\n\nclass ContentSet(ContentSetBase):\n    id: uuid.UUID\n    processing_status: str = \"pending\"\n    created_at: datetime\n    updated_at: datetime\n\nclass ContentSetCreate(ContentSetBase):\n    pass\n\nclass ProcessingManifest(BaseModel):\n    content_set_id: uuid.UUID\n    ordered_files: List[ContentFile]\n    total_files: int\n    estimated_processing_time: int\n    warnings: List[str] = Field(default_factory=list)\n    context: Dict = Field(default_factory=dict)\n```",
        "testStrategy": "1. Verify SQL schema with test database\n2. Test foreign key constraints\n3. Test index performance with large datasets\n4. Validate Pydantic models with sample data\n5. Test serialization/deserialization of JSON fields\n6. Ensure UUID generation works correctly",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T21:01:44.521Z"
      },
      {
        "id": "124",
        "title": "Implement Content Prep API Routes",
        "description": "Create the API routes for the Content Prep Agent as specified in the PRD, including endpoints for analyzing files, validating content sets, and generating processing manifests.",
        "details": "Create the file `app/routes/content_prep.py` with FastAPI routes for the Content Prep Agent. Implement all the endpoints specified in the PRD:\n\n1. POST /api/content-prep/analyze - Analyze pending files, detect sets\n2. POST /api/content-prep/validate - Validate completeness of content set\n3. POST /api/content-prep/order - Generate processing order\n4. GET /api/content-prep/sets - List detected content sets\n5. GET /api/content-prep/sets/{set_id} - Get content set details\n6. POST /api/content-prep/manifest - Generate processing manifest\n7. GET /api/content-prep/health - Service health check\n\nPseudo-code:\n```python\nfrom fastapi import APIRouter, Depends, HTTPException, BackgroundTasks\nfrom typing import List, Optional\nfrom app.models.content_sets import ContentSet, ContentSetCreate, ProcessingManifest\nfrom app.services.content_prep_agent import ContentPrepAgent\nfrom app.db.supabase import get_supabase_client\n\nrouter = APIRouter(prefix=\"/api/content-prep\", tags=[\"content-prep\"])\ncontent_prep_agent = ContentPrepAgent()\n\n@router.post(\"/analyze\", response_model=dict)\nasync def analyze_pending_files(data: dict):\n    \"\"\"Analyze pending files and detect content sets\"\"\"\n    b2_folder = data.get(\"b2_folder\", \"pending/\")\n    detection_mode = data.get(\"detection_mode\", \"auto\")\n    \n    # Get files from B2\n    # Detect content sets\n    # Return results\n    \n    return {\n        \"content_sets\": [],  # List of detected sets\n        \"standalone_files\": []  # List of files not in sets\n    }\n\n@router.post(\"/validate\", response_model=dict)\nasync def validate_content_set(data: dict):\n    \"\"\"Validate completeness of a content set\"\"\"\n    content_set_id = data.get(\"content_set_id\")\n    \n    # Get content set\n    # Validate completeness\n    # Return validation results\n    \n    return {\n        \"is_complete\": True,\n        \"missing_files\": [],\n        \"warnings\": []\n    }\n\n@router.post(\"/order\", response_model=dict)\nasync def generate_processing_order(data: dict):\n    \"\"\"Generate processing order for a content set\"\"\"\n    content_set_id = data.get(\"content_set_id\")\n    \n    # Get content set\n    # Generate order\n    # Return ordered files\n    \n    return {\n        \"ordered_files\": []\n    }\n\n@router.get(\"/sets\", response_model=List[ContentSet])\nasync def list_content_sets(status: Optional[str] = None):\n    \"\"\"List detected content sets\"\"\"\n    # Query database for content sets\n    # Filter by status if provided\n    # Return list\n    \n    return []\n\n@router.get(\"/sets/{set_id}\", response_model=ContentSet)\nasync def get_content_set(set_id: str):\n    \"\"\"Get content set details\"\"\"\n    # Query database for content set\n    # Return details or 404\n    \n    return {}\n\n@router.post(\"/manifest\", response_model=ProcessingManifest)\nasync def generate_processing_manifest(data: dict):\n    \"\"\"Generate processing manifest for a content set\"\"\"\n    content_set_id = data.get(\"content_set_id\")\n    proceed_incomplete = data.get(\"proceed_incomplete\", False)\n    add_context = data.get(\"add_context\", True)\n    \n    # Get content set\n    # Generate manifest\n    # Return manifest\n    \n    return {}\n\n@router.get(\"/health\")\nasync def health_check():\n    \"\"\"Service health check\"\"\"\n    return {\"status\": \"healthy\"}\n```\n\nAlso update the main FastAPI app to include these routes:\n\n```python\n# In app/main.py\nfrom app.routes import content_prep\n\napp.include_router(content_prep.router)\n```",
        "testStrategy": "1. Unit tests for each API endpoint\n2. Test with valid and invalid request data\n3. Test error handling and edge cases\n4. Integration tests with mocked B2 service\n5. Test response formats match API specifications\n6. Load testing with multiple concurrent requests",
        "priority": "high",
        "dependencies": [
          "122",
          "123"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T21:05:19.254Z"
      },
      {
        "id": "125",
        "title": "Implement Set Detection and Ordering Logic",
        "description": "Implement the core algorithms for detecting content sets, validating completeness, and determining the correct processing order.",
        "details": "Enhance the ContentPrepAgent class with detailed implementations of the set detection and ordering algorithms. This includes:\n\n1. Pattern-based content set detection\n2. Sequence number extraction from filenames\n3. Gap detection in sequences\n4. Chronological ordering logic\n\nPseudo-code for key methods:\n\n```python\nclass ContentPrepAgent:\n    # ... existing code ...\n    \n    def detect_content_sets(self, files, detection_mode=\"auto\"):\n        \"\"\"Detect related content sets from a list of files\"\"\"\n        content_sets = []\n        standalone_files = []\n        \n        # Group files by common prefixes\n        prefix_groups = self._group_by_prefix(files)\n        \n        for prefix, group_files in prefix_groups.items():\n            # Skip single files\n            if len(group_files) <= 1:\n                standalone_files.extend(group_files)\n                continue\n                \n            # Check if group matches content set indicators\n            is_content_set = any(re.search(pattern, prefix, re.IGNORECASE) \n                               for pattern in CONTENT_SET_INDICATORS)\n            \n            # If it's a potential content set or has >3 files with sequence numbers\n            if is_content_set or self._has_sequence_numbers(group_files, threshold=0.7):\n                # Create content set\n                content_set = ContentSet(\n                    id=str(uuid.uuid4()),\n                    name=self._generate_set_name(prefix, group_files),\n                    detection_method=detection_mode,\n                    files=[self._create_content_file(f) for f in group_files],\n                )\n                \n                # Validate completeness\n                self.validate_completeness(content_set)\n                \n                content_sets.append(content_set)\n            else:\n                standalone_files.extend(group_files)\n        \n        return content_sets, standalone_files\n    \n    def _group_by_prefix(self, files):\n        \"\"\"Group files by common prefixes\"\"\"\n        groups = {}\n        \n        for file in files:\n            filename = os.path.basename(file['path'])\n            \n            # Try to find a common prefix\n            prefix = self._extract_prefix(filename)\n            if prefix not in groups:\n                groups[prefix] = []\n            \n            groups[prefix].append(file)\n            \n        return groups\n    \n    def _extract_prefix(self, filename):\n        \"\"\"Extract prefix from filename\"\"\"\n        # Remove extension\n        name = os.path.splitext(filename)[0]\n        \n        # Try to match common content set patterns\n        for pattern in CONTENT_SET_INDICATORS:\n            match = re.search(pattern, name, re.IGNORECASE)\n            if match:\n                return name[:match.end()]\n        \n        # Fall back to first part of name (before first number or separator)\n        match = re.search(r'^([^\\d\\-_\\s]+)', name)\n        if match:\n            return match.group(1)\n            \n        return name\n    \n    def _has_sequence_numbers(self, files, threshold=0.7):\n        \"\"\"Check if files have sequence numbers\"\"\"\n        count = sum(1 for f in files if self.detect_sequence_number(os.path.basename(f['path'])) is not None)\n        return count / len(files) >= threshold\n    \n    def detect_sequence_number(self, filename):\n        \"\"\"Extract sequence number from filename using patterns\"\"\"\n        for pattern in SEQUENCE_PATTERNS:\n            match = re.search(pattern, filename, re.IGNORECASE)\n            if match:\n                # Extract the captured group\n                sequence_str = match.group(1)\n                \n                # Convert to integer if numeric\n                if sequence_str.isdigit():\n                    return int(sequence_str)\n                    \n                # Handle roman numerals\n                if re.match(r'^(i{1,3}|iv|v|vi{0,3}|ix|x)$', sequence_str, re.IGNORECASE):\n                    return self._roman_to_int(sequence_str.lower())\n                    \n                # Handle alpha sequence (a, b, c...)\n                if len(sequence_str) == 1 and sequence_str.isalpha():\n                    return ord(sequence_str.lower()) - ord('a') + 1\n        \n        return None\n    \n    def _roman_to_int(self, s):\n        \"\"\"Convert Roman numeral to integer\"\"\"\n        roman_map = {'i': 1, 'v': 5, 'x': 10}\n        result = 0\n        \n        for i in range(len(s)):\n            if i > 0 and roman_map[s[i]] > roman_map[s[i-1]]:\n                result += roman_map[s[i]] - 2 * roman_map[s[i-1]]\n            else:\n                result += roman_map[s[i]]\n                \n        return result\n    \n    def validate_completeness(self, content_set):\n        \"\"\"Check for missing files in sequence\"\"\"\n        # Get sequence numbers\n        sequence_numbers = []\n        for file in content_set.files:\n            if file.sequence_number is not None:\n                sequence_numbers.append(file.sequence_number)\n        \n        if not sequence_numbers:\n            # No sequence numbers detected\n            content_set.is_complete = True\n            return content_set\n        \n        # Find min and max sequence\n        min_seq = min(sequence_numbers)\n        max_seq = max(sequence_numbers)\n        \n        # Check for gaps\n        expected_range = set(range(min_seq, max_seq + 1))\n        actual_set = set(sequence_numbers)\n        missing = expected_range - actual_set\n        \n        if missing:\n            content_set.is_complete = False\n            content_set.missing_files = [f\"Missing sequence {num}\" for num in missing]\n        else:\n            content_set.is_complete = True\n            content_set.missing_files = []\n        \n        return content_set\n    \n    def resolve_order(self, content_set):\n        \"\"\"Determine the correct processing order\"\"\"\n        # Sort files by sequence number if available\n        files_with_sequence = []\n        files_without_sequence = []\n        \n        for file in content_set.files:\n            if file.sequence_number is not None:\n                files_with_sequence.append(file)\n            else:\n                files_without_sequence.append(file)\n        \n        # Sort by sequence number\n        files_with_sequence.sort(key=lambda f: f.sequence_number)\n        \n        # For files without sequence, try to use creation date or name\n        files_without_sequence.sort(key=lambda f: f.filename)\n        \n        # Combine the lists\n        ordered_files = files_with_sequence + files_without_sequence\n        \n        return ordered_files\n```",
        "testStrategy": "1. Unit tests for each algorithm (prefix grouping, sequence detection, gap detection)\n2. Test with various file naming patterns\n3. Test with edge cases (no sequence numbers, mixed formats)\n4. Test completeness validation with different gap scenarios\n5. Benchmark performance with large file sets\n6. Test with real-world examples of course materials",
        "priority": "high",
        "dependencies": [
          "122"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement analyze_folder() method for content set detection",
            "description": "Create the analyze_folder() method that scans a directory and identifies potential content sets based on file patterns.",
            "dependencies": [],
            "details": "Implement the analyze_folder() method in ContentPrepAgent class that takes a folder path as input, scans all files, and calls detect_content_sets() with the file list. Include logic to handle large directories by processing files in batches. The method should return a tuple of (content_sets, standalone_files).",
            "status": "pending",
            "testStrategy": "Unit test with various folder structures, test with empty folders, test with folders containing only standalone files, test with mixed content sets and standalone files.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement _create_content_set() builder method",
            "description": "Create a helper method to build ContentSet objects with proper metadata extraction and initialization.",
            "dependencies": [
              1
            ],
            "details": "Implement the _create_content_set() method that takes a group of files and creates a properly initialized ContentSet object. This includes generating a unique ID, determining an appropriate name based on common prefixes, setting the detection method, and initializing the files list with proper metadata for each file.",
            "status": "pending",
            "testStrategy": "Test with various file naming patterns, verify correct ID generation, test name generation with different prefix patterns, verify all file metadata is correctly extracted.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement sequence number extraction from filenames",
            "description": "Create robust methods to detect and extract sequence numbers from various filename formats.",
            "dependencies": [
              2
            ],
            "details": "Implement the detect_sequence_number() method to extract sequence numbers from filenames using regular expressions. Handle numeric sequences, roman numerals, and alphabetic sequences. Include the _roman_to_int() helper method for converting roman numerals to integers. Ensure the method can handle various delimiter patterns and position of sequence numbers in filenames.",
            "status": "pending",
            "testStrategy": "Test with various filename patterns including numeric sequences (1, 2, 3), roman numerals (i, ii, iii), alphabetic sequences (a, b, c), test with different delimiters, test with sequence numbers in different positions.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement gap detection for missing files in sequences",
            "description": "Create the validate_completeness() method to identify gaps in file sequences and mark content sets as incomplete when files are missing.",
            "dependencies": [
              3
            ],
            "details": "Implement the validate_completeness() method that analyzes a content set's files to detect missing sequence numbers. The method should extract sequence numbers from all files, determine the expected range, identify any gaps, and update the content set's is_complete flag and missing_files list accordingly. Handle edge cases where no sequence numbers are detected.",
            "status": "pending",
            "testStrategy": "Test with complete sequences, test with sequences missing files in the middle, at the beginning, and at the end, test with non-sequential files, test with mixed content types.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement chronological ordering logic for content sets",
            "description": "Create the resolve_order() method to determine the correct processing order for files within a content set.",
            "dependencies": [
              3,
              4
            ],
            "details": "Implement the resolve_order() method that sorts files within a content set based on sequence numbers when available. For files without sequence numbers, implement fallback ordering based on creation date or filename. The method should return an ordered list of files and handle mixed cases where some files have sequence numbers and others don't.",
            "status": "pending",
            "testStrategy": "Test ordering with sequence numbers, test fallback to creation date, test fallback to filename, test with mixed file types, verify correct ordering with various sequence patterns.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement batched analysis for large file sets",
            "description": "Enhance the content set detection to handle large directories by processing files in batches and merging results.",
            "dependencies": [
              1,
              2,
              5
            ],
            "details": "Modify the analyze_folder() method to process large directories (>100 files) in batches to prevent memory issues. Implement a merge_content_sets() helper method that combines results from multiple batches while preserving chronological ordering. Include progress tracking and logging for batch processing. Ensure the final merged result maintains all metadata and relationships between files.",
            "status": "pending",
            "testStrategy": "Test with directories containing >100 files, verify memory usage remains stable, test merging of content sets across batches, verify chronological ordering is preserved after merging, test with edge cases like all files belonging to one content set split across batches.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-13T21:06:53.806Z"
      },
      {
        "id": "126",
        "title": "Implement Processing Manifest Generation",
        "description": "Create the functionality to generate processing manifests that specify the ordered processing queue, dependencies, and context for downstream agents.",
        "details": "Enhance the ContentPrepAgent class with methods to generate processing manifests. This includes:\n\n1. Creating an ordered processing queue\n2. Identifying dependencies between files\n3. Estimating processing complexity and time\n4. Adding context metadata for downstream agents\n\nPseudo-code:\n\n```python\nclass ContentPrepAgent:\n    # ... existing code ...\n    \n    def generate_manifest(self, content_set, proceed_incomplete=False):\n        \"\"\"Create processing manifest with ordered files\"\"\"\n        # Check if content set is complete\n        if not content_set.is_complete and not proceed_incomplete:\n            raise ValueError(\"Content set is incomplete. Set proceed_incomplete=True to generate manifest anyway.\")\n        \n        # Get ordered files\n        ordered_files = self.resolve_order(content_set)\n        \n        # Identify dependencies\n        self._identify_dependencies(ordered_files)\n        \n        # Estimate processing time\n        estimated_time = self.estimate_processing_time(ordered_files)\n        \n        # Create manifest\n        manifest = ProcessingManifest(\n            content_set_id=content_set.id,\n            ordered_files=ordered_files,\n            total_files=len(ordered_files),\n            estimated_processing_time=estimated_time,\n            warnings=content_set.missing_files if not content_set.is_complete else [],\n            context={\n                \"content_set_name\": content_set.name,\n                \"is_complete\": content_set.is_complete,\n                \"detection_method\": content_set.detection_method,\n                \"total_files\": len(ordered_files),\n            }\n        )\n        \n        return manifest\n    \n    def _identify_dependencies(self, ordered_files):\n        \"\"\"Identify dependencies between files based on order\"\"\"\n        # Simple sequential dependencies - each file depends on the previous one\n        for i in range(1, len(ordered_files)):\n            prev_file = ordered_files[i-1]\n            curr_file = ordered_files[i]\n            \n            # Add dependency on previous file\n            curr_file.dependencies.append(prev_file.b2_path)\n    \n    def estimate_processing_time(self, files):\n        \"\"\"Estimate processing time based on file types and sizes\"\"\"\n        total_time = 0\n        \n        for file in files:\n            # Base time by file type\n            if file.file_type.lower() == \"pdf\":\n                base_time = 30  # 30 seconds base for PDF\n            elif file.file_type.lower() in [\"docx\", \"doc\"]:\n                base_time = 25  # 25 seconds base for Word docs\n            elif file.file_type.lower() in [\"txt\", \"md\"]:\n                base_time = 10  # 10 seconds base for text files\n            else:\n                base_time = 20  # Default base time\n            \n            # Adjust by file size (very simple heuristic)\n            size_factor = max(1, file.size_bytes / (1024 * 1024))  # Size in MB, minimum 1\n            \n            # Adjust by complexity\n            complexity_factor = 1.0\n            if file.estimated_complexity == \"high\":\n                complexity_factor = 1.5\n            elif file.estimated_complexity == \"low\":\n                complexity_factor = 0.8\n            \n            # Calculate time for this file\n            file_time = base_time * size_factor * complexity_factor\n            total_time += file_time\n        \n        return int(total_time)  # Return as integer seconds\n    \n    def estimate_file_complexity(self, file):\n        \"\"\"Estimate file complexity based on size, type, and content\"\"\"\n        # Simple heuristic based on file size\n        if file.size_bytes > 5 * 1024 * 1024:  # > 5MB\n            return \"high\"\n        elif file.size_bytes < 100 * 1024:  # < 100KB\n            return \"low\"\n        else:\n            return \"medium\"\n```",
        "testStrategy": "1. Unit tests for manifest generation\n2. Test dependency identification with different file orders\n3. Test processing time estimation with various file types and sizes\n4. Verify manifest structure matches expected schema\n5. Test error handling for incomplete sets\n6. Test with large file sets to ensure performance",
        "priority": "medium",
        "dependencies": [
          "122",
          "125"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T21:06:53.847Z"
      },
      {
        "id": "127",
        "title": "Integrate with B2 Workflow Service",
        "description": "Integrate the Content Prep Agent with the B2 Workflow Service to intercept file uploads and trigger content preparation.",
        "details": "Modify the existing B2 Workflow Service to hook into the Content Prep Agent. This involves:\n\n1. Intercepting new file uploads to the `pending/` folder\n2. Triggering content set detection and analysis\n3. Moving files to `processing/` in the correct order based on the manifest\n\nPseudo-code for the integration:\n\n```python\n# In app/services/b2_workflow.py\n\nfrom app.services.content_prep_agent import ContentPrepAgent\n\nclass B2WorkflowService:\n    # ... existing code ...\n    \n    def __init__(self):\n        # ... existing init ...\n        self.content_prep_agent = ContentPrepAgent()\n    \n    async def process_pending_folder(self):\n        \"\"\"Process files in the pending folder\"\"\"\n        # Get files from pending folder\n        pending_files = await self.list_pending_files()\n        \n        if not pending_files:\n            return\n        \n        # Check if we need content preparation\n        if len(pending_files) > 1:\n            # Detect content sets\n            content_sets, standalone_files = self.content_prep_agent.detect_content_sets(pending_files)\n            \n            # Process standalone files immediately\n            for file in standalone_files:\n                await self.move_to_processing(file['path'])\n                await self.trigger_processing(file['path'])\n            \n            # Process content sets\n            for content_set in content_sets:\n                # Store content set in database\n                await self.store_content_set(content_set)\n                \n                # Check completeness\n                if not content_set.is_complete:\n                    # Log warning about incomplete set\n                    self.logger.warning(f\"Incomplete content set detected: {content_set.name}\")\n                    # Send notification to user (implementation depends on notification system)\n                    await self.notify_incomplete_set(content_set)\n                    # Skip processing until user acknowledges\n                    continue\n                \n                # Generate manifest\n                manifest = self.content_prep_agent.generate_manifest(content_set)\n                \n                # Process files in order\n                for file in manifest.ordered_files:\n                    await self.move_to_processing(file.b2_path)\n                    # Pass content set context to processing task\n                    await self.trigger_processing(file.b2_path, context=manifest.context)\n        else:\n            # Single file - process immediately\n            file = pending_files[0]\n            await self.move_to_processing(file['path'])\n            await self.trigger_processing(file['path'])\n    \n    async def store_content_set(self, content_set):\n        \"\"\"Store content set in database\"\"\"\n        # Implementation depends on database access method\n        pass\n    \n    async def notify_incomplete_set(self, content_set):\n        \"\"\"Notify user about incomplete content set\"\"\"\n        # Implementation depends on notification system\n        pass\n    \n    async def trigger_processing(self, file_path, context=None):\n        \"\"\"Trigger processing task for a file\"\"\"\n        # ... existing code ...\n        \n        # Add content set context if available\n        task_data = {\n            \"file_path\": file_path,\n            # ... existing task data ...\n        }\n        \n        if context:\n            task_data[\"content_set_context\"] = context\n        \n        # Trigger Celery task\n        # ... existing code ...\n```\n\nAlso create a Celery task for content set detection and processing:\n\n```python\n# In app/tasks/content_prep_tasks.py\n\nfrom celery import shared_task\nfrom app.services.content_prep_agent import ContentPrepAgent\n\n@shared_task\ndef detect_content_sets(b2_folder):\n    \"\"\"Detect content sets in a B2 folder\"\"\"\n    agent = ContentPrepAgent()\n    \n    # List files in folder\n    # Detect content sets\n    # Store in database\n    \n    return {\"content_sets\": [], \"standalone_files\": []}\n\n@shared_task\ndef process_content_set(content_set_id, proceed_incomplete=False):\n    \"\"\"Process a content set\"\"\"\n    agent = ContentPrepAgent()\n    \n    # Get content set from database\n    # Generate manifest\n    # Trigger processing for each file in order\n    \n    return {\"status\": \"processing\", \"files_count\": 0}\n```",
        "testStrategy": "1. Integration tests with mocked B2 service\n2. Test single file pass-through\n3. Test multi-file content set detection\n4. Test incomplete set handling and notification\n5. Verify correct processing order\n6. Test with various file naming patterns\n7. Test error handling and recovery",
        "priority": "high",
        "dependencies": [
          "122",
          "125",
          "126"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T21:10:04.660Z"
      },
      {
        "id": "128",
        "title": "Implement Source Processing Task Integration",
        "description": "Modify the existing Source Processing Task (Celery) to accept processing manifests and handle content set context.",
        "details": "Update the Source Processing Task to work with the Content Prep Agent by:\n\n1. Accepting manifest-based processing instructions\n2. Handling content set context metadata\n3. Passing context to downstream services (chunking, embedding)\n\nPseudo-code for the integration:\n\n```python\n# In app/tasks/source_processing.py\n\nfrom celery import shared_task\nfrom app.services.content_prep_agent import ContentPrepAgent\n\n@shared_task\ndef process_source(file_path, **kwargs):\n    \"\"\"Process a source file\"\"\"\n    # ... existing code ...\n    \n    # Check for content set context\n    content_set_context = kwargs.get(\"content_set_context\")\n    \n    # Process the file\n    result = process_file(file_path)\n    \n    # Pass content set context to chunking task\n    if content_set_context:\n        # Add content set context to chunking task\n        chunking_task.delay(result[\"processed_path\"], content_set_context=content_set_context)\n    else:\n        # Normal processing without content set context\n        chunking_task.delay(result[\"processed_path\"])\n    \n    return result\n\n# Update the chunking task to accept content set context\n@shared_task\ndef chunking_task(processed_path, **kwargs):\n    \"\"\"Chunk a processed file\"\"\"\n    # ... existing code ...\n    \n    # Check for content set context\n    content_set_context = kwargs.get(\"content_set_context\")\n    \n    # Process chunks\n    chunks = chunk_document(processed_path)\n    \n    # Add content set metadata to chunks if available\n    if content_set_context:\n        for chunk in chunks:\n            chunk.metadata[\"content_set\"] = content_set_context.get(\"content_set_name\")\n            chunk.metadata[\"content_set_id\"] = content_set_context.get(\"content_set_id\")\n            chunk.metadata[\"is_part_of_sequence\"] = True\n    \n    # Pass to embedding task with context\n    embedding_task.delay(chunks, content_set_context=content_set_context if content_set_context else None)\n    \n    return {\"chunks_count\": len(chunks)}\n```\n\nAlso update the knowledge graph integration to create relationships for content sets:\n\n```python\n# In app/services/knowledge_graph.py\n\nclass KnowledgeGraphService:\n    # ... existing code ...\n    \n    def add_document_node(self, document, **kwargs):\n        \"\"\"Add a document node to the knowledge graph\"\"\"\n        # ... existing code ...\n        \n        # Check for content set context\n        content_set_context = kwargs.get(\"content_set_context\")\n        \n        if content_set_context:\n            # Create content set node if it doesn't exist\n            self.create_content_set_node(content_set_context)\n            \n            # Create PART_OF relationship\n            self.create_part_of_relationship(document.id, content_set_context.get(\"content_set_id\"))\n            \n            # Create PRECEDES/FOLLOWS relationships if this document has dependencies\n            for dependency in document.get(\"dependencies\", []):\n                self.create_dependency_relationship(document.id, dependency)\n    \n    def create_content_set_node(self, content_set_context):\n        \"\"\"Create a ContentSet node in Neo4j\"\"\"\n        query = \"\"\"\n        MERGE (cs:ContentSet {id: $id})\n        ON CREATE SET \n            cs.name = $name,\n            cs.is_complete = $is_complete,\n            cs.total_files = $total_files,\n            cs.created_at = datetime()\n        \"\"\"\n        \n        params = {\n            \"id\": content_set_context.get(\"content_set_id\"),\n            \"name\": content_set_context.get(\"content_set_name\"),\n            \"is_complete\": content_set_context.get(\"is_complete\", False),\n            \"total_files\": content_set_context.get(\"total_files\", 0)\n        }\n        \n        self.graph.run(query, params)\n    \n    def create_part_of_relationship(self, document_id, content_set_id):\n        \"\"\"Create PART_OF relationship between document and content set\"\"\"\n        query = \"\"\"\n        MATCH (d:Document {id: $document_id})\n        MATCH (cs:ContentSet {id: $content_set_id})\n        MERGE (d)-[:PART_OF]->(cs)\n        \"\"\"\n        \n        params = {\n            \"document_id\": document_id,\n            \"content_set_id\": content_set_id\n        }\n        \n        self.graph.run(query, params)\n    \n    def create_dependency_relationship(self, document_id, dependency_id):\n        \"\"\"Create FOLLOWS/PRECEDES relationships between documents\"\"\"\n        query = \"\"\"\n        MATCH (d1:Document {id: $document_id})\n        MATCH (d2:Document {id: $dependency_id})\n        MERGE (d1)-[:FOLLOWS]->(d2)\n        MERGE (d2)-[:PRECEDES]->(d1)\n        \"\"\"\n        \n        params = {\n            \"document_id\": document_id,\n            \"dependency_id\": dependency_id\n        }\n        \n        self.graph.run(query, params)\n```",
        "testStrategy": "1. Integration tests with mocked Celery tasks\n2. Test context propagation through the pipeline\n3. Verify Neo4j relationships are created correctly\n4. Test with various content set scenarios\n5. Test error handling and recovery\n6. Verify metadata is preserved throughout processing",
        "priority": "medium",
        "dependencies": [
          "122",
          "127"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T21:12:14.216Z"
      },
      {
        "id": "129",
        "title": "Implement Chat-Based Ordering Clarification",
        "description": "Implement the chat-based ordering clarification feature that allows the Content Prep Agent to ask users questions via CKO Chat when ordering confidence is below threshold.",
        "details": "Create functionality for the Content Prep Agent to interact with users via CKO Chat when it needs clarification about file ordering. This includes:\n\n1. Detecting when ordering confidence is below threshold\n2. Sending clarification messages to CKO Chat\n3. Parsing user responses\n4. Updating ordering based on user input\n\nPseudo-code:\n\n```python\nclass ContentPrepAgent:\n    # ... existing code ...\n    \n    async def resolve_order_with_clarification(self, content_set, confidence_threshold=0.8):\n        \"\"\"Resolve order with user clarification if needed\"\"\"\n        # Try automatic ordering first\n        ordered_files = self.resolve_order(content_set)\n        \n        # Calculate confidence in ordering\n        confidence = self._calculate_ordering_confidence(ordered_files)\n        \n        if confidence < confidence_threshold:\n            # Need user clarification\n            clarification = await self._request_user_clarification(content_set, ordered_files)\n            \n            if clarification:\n                # Update ordering based on user input\n                ordered_files = self._update_ordering(ordered_files, clarification)\n        \n        return ordered_files\n    \n    def _calculate_ordering_confidence(self, ordered_files):\n        \"\"\"Calculate confidence in the ordering\"\"\"\n        # Count files with explicit sequence numbers\n        files_with_sequence = sum(1 for f in ordered_files if f.sequence_number is not None)\n        total_files = len(ordered_files)\n        \n        # Base confidence on percentage of files with sequence numbers\n        if total_files == 0:\n            return 1.0\n        \n        return files_with_sequence / total_files\n    \n    async def _request_user_clarification(self, content_set, ordered_files):\n        \"\"\"Request clarification from user via CKO Chat\"\"\"\n        from app.services.cko_chat import CKOChatService\n        \n        chat_service = CKOChatService()\n        \n        # Identify ambiguous files (those without sequence numbers)\n        ambiguous_files = [f for f in ordered_files if f.sequence_number is None]\n        \n        if not ambiguous_files:\n            return None\n        \n        # Create clarification message\n        message = f\"I'm processing your content set '{content_set.name}' and need help with the correct order. \"\n        message += \"I've detected these files without clear sequence indicators:\\n\"\n        \n        for i, file in enumerate(ambiguous_files[:5]):  # Limit to 5 files to avoid long messages\n            message += f\"- {file.filename}\\n\"\n        \n        if len(ambiguous_files) > 5:\n            message += f\"...and {len(ambiguous_files) - 5} more.\\n\"\n        \n        message += \"\\nCould you please tell me the correct order for these files? \"\n        message += \"You can respond with file names in the desired order, or with instructions like 'File A comes before File B'.\"\n        \n        # Send message to CKO Chat\n        chat_id = await chat_service.send_agent_message(\n            agent_id=\"AGENT-016\",\n            user_id=content_set.metadata.get(\"user_id\"),\n            message=message\n        )\n        \n        # Wait for user response (with timeout)\n        response = await chat_service.wait_for_user_response(chat_id, timeout=3600)  # 1 hour timeout\n        \n        if not response:\n            # No response within timeout\n            return None\n        \n        # Log the conversation for audit trail\n        await self._log_clarification_conversation(content_set.id, message, response)\n        \n        return response\n    \n    def _update_ordering(self, ordered_files, clarification):\n        \"\"\"Update ordering based on user clarification\"\"\"\n        # This would use LLM to parse the user's response and update the ordering\n        # For simplicity, we'll assume the response is a comma-separated list of filenames\n        \n        # Create a map of filename to file object\n        file_map = {f.filename: f for f in ordered_files}\n        \n        # Try to extract filenames from the response\n        # This is a simplified approach - in reality, you'd use an LLM to parse natural language\n        filenames = [name.strip() for name in clarification.split(',')]\n        \n        # Create new ordered list based on user input\n        new_order = []\n        for filename in filenames:\n            if filename in file_map:\n                new_order.append(file_map[filename])\n                # Remove from map to avoid duplicates\n                del file_map[filename]\n        \n        # Add any remaining files at the end\n        new_order.extend(file_map.values())\n        \n        return new_order\n    \n    async def _log_clarification_conversation(self, content_set_id, question, answer):\n        \"\"\"Log the clarification conversation for audit trail\"\"\"\n        # Implementation depends on logging system\n        pass\n```\n\nCreate a simple CKO Chat service interface:\n\n```python\n# In app/services/cko_chat.py\n\nclass CKOChatService:\n    \"\"\"Interface to the CKO Chat system\"\"\"\n    \n    async def send_agent_message(self, agent_id, user_id, message):\n        \"\"\"Send a message from an agent to a user\"\"\"\n        # Implementation depends on chat system\n        # This would create a new chat or add to existing chat\n        # Return chat ID\n        pass\n    \n    async def wait_for_user_response(self, chat_id, timeout=3600):\n        \"\"\"Wait for user response with timeout\"\"\"\n        # Implementation depends on chat system\n        # This would poll or use websockets to wait for response\n        # Return user message or None if timeout\n        pass\n```",
        "testStrategy": "1. Unit tests for ordering confidence calculation\n2. Test message generation for different ambiguous file scenarios\n3. Test response parsing with various user input formats\n4. Integration tests with mocked CKO Chat service\n5. Test timeout handling\n6. Verify audit trail logging\n7. Test end-to-end flow with simulated user responses",
        "priority": "medium",
        "dependencies": [
          "122",
          "125"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement ordering confidence calculation method",
            "description": "Create the _calculate_ordering_confidence() method that determines when user clarification is needed",
            "dependencies": [],
            "details": "Implement the method that calculates confidence based on the percentage of files with explicit sequence numbers. Set the threshold at 80% as specified. Include edge case handling for empty file sets and ensure the method returns a value between 0 and 1.",
            "status": "pending",
            "testStrategy": "Unit test with various file sets: all files with sequence numbers, mixed sets, empty sets. Verify threshold behavior at boundary conditions (79% vs 81%).",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create CKOChatService interface",
            "description": "Implement the CKOChatService class with methods for agent-user communication",
            "dependencies": [],
            "details": "Create the CKOChatService class in app/services/cko_chat.py with methods for sending agent messages and waiting for user responses. Implement timeout handling for response waiting and proper error handling for communication failures.",
            "status": "pending",
            "testStrategy": "Mock the underlying chat system API. Test successful message sending, response waiting with and without timeouts, and error handling scenarios.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement clarification message generation",
            "description": "Create logic to generate clear, user-friendly clarification messages about ambiguous file ordering",
            "dependencies": [
              1
            ],
            "details": "Implement the first part of _request_user_clarification() that identifies ambiguous files and creates a well-formatted message explaining the issue to the user. Include file listing with truncation for large sets and clear instructions on how to respond.",
            "status": "pending",
            "testStrategy": "Test message generation with various file sets, including edge cases like all files being ambiguous or only one ambiguous file. Verify message formatting and clarity.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement chat integration for sending clarification requests",
            "description": "Connect the Content Prep Agent to CKO Chat for sending clarification requests to users",
            "dependencies": [
              2,
              3
            ],
            "details": "Complete the _request_user_clarification() method to send the generated clarification message to users via the CKOChatService. Implement proper error handling for failed message delivery and configure the agent ID ('AGENT-016') and user ID extraction from content set metadata.",
            "status": "pending",
            "testStrategy": "Integration test with mocked CKOChatService. Verify correct message delivery, proper agent and user ID handling, and error case management.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement natural language response parsing",
            "description": "Create logic to interpret user responses to ordering clarification requests",
            "dependencies": [
              4
            ],
            "details": "Implement the _update_ordering() method that parses user responses and updates file ordering accordingly. Handle various response formats including comma-separated lists, natural language descriptions ('file A before file B'), and numbered lists. Use pattern matching or LLM-based parsing as appropriate.",
            "status": "pending",
            "testStrategy": "Test with various response formats including comma-separated lists, natural language instructions, and edge cases like partial responses or responses with unknown files.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement clarification conversation logging",
            "description": "Create audit trail logging for clarification conversations",
            "dependencies": [
              4
            ],
            "details": "Implement the _log_clarification_conversation() method to record all clarification questions and user responses for audit purposes. Store conversation details including timestamps, content set ID, question text, response text, and outcome (whether ordering was successfully updated).",
            "status": "pending",
            "testStrategy": "Verify log entries are created with correct data. Test with various conversation scenarios including successful clarifications and timeouts.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Integrate all components in resolve_order_with_clarification method",
            "description": "Connect all components in the main workflow method that handles the entire clarification process",
            "dependencies": [
              1,
              5,
              6
            ],
            "details": "Implement the resolve_order_with_clarification() method that orchestrates the entire process: calculating confidence, requesting clarification when needed, waiting for and processing responses, updating ordering, and returning the final ordered files. Include proper state management to pause processing while awaiting responses.",
            "status": "pending",
            "testStrategy": "End-to-end testing with mocked dependencies. Test the full workflow with scenarios above and below confidence threshold. Verify correct handling of timeouts and user responses.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-13T21:19:08.764Z"
      },
      {
        "id": "130",
        "title": "Implement Retention Policy and Cleanup",
        "description": "Implement the 90-day retention policy for content set metadata and create a scheduled cleanup task.",
        "details": "Create a scheduled task to enforce the 90-day retention policy for content set metadata. This includes:\n\n1. Creating a Celery periodic task for cleanup\n2. Implementing the database cleanup logic\n3. Adding logging and monitoring\n\nPseudo-code:\n\n```python\n# In app/tasks/scheduled_tasks.py\n\nfrom celery import shared_task\nfrom celery.schedules import crontab\nfrom app.db.supabase import get_supabase_client\nfrom datetime import datetime, timedelta\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Register as a periodic task to run daily at 3 AM\n@shared_task\ndef cleanup_content_sets():\n    \"\"\"Clean up content sets older than 90 days\"\"\"\n    logger.info(\"Starting content set cleanup task\")\n    \n    try:\n        # Calculate cutoff date (90 days ago)\n        cutoff_date = datetime.now() - timedelta(days=90)\n        \n        # Get Supabase client\n        supabase = get_supabase_client()\n        \n        # Find completed content sets older than 90 days\n        response = supabase.table(\"content_sets\") \\\n            .select(\"id\") \\\n            .eq(\"processing_status\", \"complete\") \\\n            .lt(\"updated_at\", cutoff_date.isoformat()) \\\n            .execute()\n        \n        if response.data:\n            content_set_ids = [item[\"id\"] for item in response.data]\n            count = len(content_set_ids)\n            logger.info(f\"Found {count} content sets to clean up\")\n            \n            # Delete in batches to avoid timeout\n            batch_size = 50\n            for i in range(0, count, batch_size):\n                batch = content_set_ids[i:i+batch_size]\n                \n                # Delete content set files first (due to foreign key constraint)\n                supabase.table(\"content_set_files\") \\\n                    .delete() \\\n                    .in_(\"content_set_id\", batch) \\\n                    .execute()\n                \n                # Delete content sets\n                supabase.table(\"content_sets\") \\\n                    .delete() \\\n                    .in_(\"id\", batch) \\\n                    .execute()\n                \n                logger.info(f\"Deleted batch {i//batch_size + 1} ({len(batch)} content sets)\")\n            \n            return {\"status\": \"success\", \"deleted_count\": count}\n        else:\n            logger.info(\"No content sets to clean up\")\n            return {\"status\": \"success\", \"deleted_count\": 0}\n    \n    except Exception as e:\n        logger.error(f\"Error in content set cleanup: {str(e)}\")\n        return {\"status\": \"error\", \"error\": str(e)}\n```\n\nRegister the periodic task in Celery beat schedule:\n\n```python\n# In app/celery_app.py\n\napp.conf.beat_schedule = {\n    # ... existing scheduled tasks ...\n    \n    'cleanup-content-sets-daily': {\n        'task': 'app.tasks.scheduled_tasks.cleanup_content_sets',\n        'schedule': crontab(hour=3, minute=0),  # Run at 3:00 AM\n    },\n}\n```\n\nAdd monitoring and metrics:\n\n```python\n# In app/monitoring/metrics.py\n\nfrom prometheus_client import Counter, Gauge\n\n# Counters for content set operations\ncontent_sets_created = Counter('content_sets_created', 'Number of content sets created')\ncontent_sets_processed = Counter('content_sets_processed', 'Number of content sets processed')\ncontent_sets_deleted = Counter('content_sets_deleted', 'Number of content sets deleted by retention policy')\n\n# Gauges for current state\ncontent_sets_pending = Gauge('content_sets_pending', 'Number of pending content sets')\ncontent_sets_processing = Gauge('content_sets_processing', 'Number of processing content sets')\ncontent_sets_complete = Gauge('content_sets_complete', 'Number of complete content sets')\n```",
        "testStrategy": "1. Unit tests for cleanup logic\n2. Test with mock database responses\n3. Test date calculation and filtering\n4. Test batch processing\n5. Test error handling\n6. Integration test with test database\n7. Verify metrics are updated correctly",
        "priority": "low",
        "dependencies": [
          "123"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T21:23:08.753Z"
      },
      {
        "id": "131",
        "title": "Create Comprehensive Test Suite",
        "description": "Create a comprehensive test suite for the Content Prep Agent, including unit tests, integration tests, and performance benchmarks.",
        "details": "Create the file `tests/test_content_prep_agent.py` with a comprehensive test suite for the Content Prep Agent. This includes:\n\n1. Unit tests for each component (set detection, ordering, manifest generation)\n2. Integration tests with mocked dependencies\n3. Performance benchmarks for large file sets\n4. Test cases for all user stories\n\nPseudo-code:\n\n```python\n# In tests/test_content_prep_agent.py\n\nimport unittest\nfrom unittest.mock import patch, MagicMock\nimport pytest\nfrom app.services.content_prep_agent import ContentPrepAgent, ContentSet, ContentFile, ProcessingManifest\n\nclass TestContentPrepAgent(unittest.TestCase):\n    def setUp(self):\n        self.agent = ContentPrepAgent()\n        \n        # Sample test files\n        self.test_files = [\n            {\"path\": \"pending/course/01-intro.pdf\", \"size\": 1024 * 1024, \"type\": \"pdf\"},\n            {\"path\": \"pending/course/02-basics.pdf\", \"size\": 2 * 1024 * 1024, \"type\": \"pdf\"},\n            {\"path\": \"pending/course/03-advanced.pdf\", \"size\": 3 * 1024 * 1024, \"type\": \"pdf\"},\n            {\"path\": \"pending/course/10-appendix.pdf\", \"size\": 500 * 1024, \"type\": \"pdf\"},\n            {\"path\": \"pending/random.txt\", \"size\": 10 * 1024, \"type\": \"txt\"},\n        ]\n    \n    def test_detect_sequence_number(self):\n        \"\"\"Test sequence number detection from filenames\"\"\"\n        test_cases = [\n            (\"01-intro.pdf\", 1),\n            (\"chapter-02-basics.pdf\", 2),\n            (\"module03.pdf\", 3),\n            (\"lesson_4_advanced.pdf\", 4),\n            (\"part-v-conclusion.pdf\", 5),  # Roman numeral\n            (\"a-intro.pdf\", 1),  # Alpha sequence\n            (\"random.pdf\", None),  # No sequence\n        ]\n        \n        for filename, expected in test_cases:\n            result = self.agent.detect_sequence_number(filename)\n            self.assertEqual(result, expected, f\"Failed for {filename}\")\n    \n    def test_detect_content_sets(self):\n        \"\"\"Test content set detection\"\"\"\n        content_sets, standalone = self.agent.detect_content_sets(self.test_files)\n        \n        # Should detect one content set with 4 files\n        self.assertEqual(len(content_sets), 1)\n        self.assertEqual(len(content_sets[0].files), 4)\n        \n        # Should have one standalone file\n        self.assertEqual(len(standalone), 1)\n        self.assertEqual(standalone[0][\"path\"], \"pending/random.txt\")\n    \n    def test_validate_completeness(self):\n        \"\"\"Test completeness validation\"\"\"\n        # Create a content set with files 1, 2, 4 (missing 3)\n        files = [\n            ContentFile(b2_path=\"file1.pdf\", filename=\"file1.pdf\", sequence_number=1),\n            ContentFile(b2_path=\"file2.pdf\", filename=\"file2.pdf\", sequence_number=2),\n            ContentFile(b2_path=\"file4.pdf\", filename=\"file4.pdf\", sequence_number=4),\n        ]\n        \n        content_set = ContentSet(\n            id=\"test-id\",\n            name=\"Test Set\",\n            detection_method=\"pattern\",\n            files=files\n        )\n        \n        # Validate completeness\n        self.agent.validate_completeness(content_set)\n        \n        # Should be incomplete with missing file 3\n        self.assertFalse(content_set.is_complete)\n        self.assertEqual(len(content_set.missing_files), 1)\n        \n        # Add the missing file and revalidate\n        content_set.files.append(\n            ContentFile(b2_path=\"file3.pdf\", filename=\"file3.pdf\", sequence_number=3)\n        )\n        \n        self.agent.validate_completeness(content_set)\n        \n        # Should now be complete\n        self.assertTrue(content_set.is_complete)\n        self.assertEqual(len(content_set.missing_files), 0)\n    \n    def test_resolve_order(self):\n        \"\"\"Test ordering resolution\"\"\"\n        # Create a content set with files in random order\n        files = [\n            ContentFile(b2_path=\"file3.pdf\", filename=\"file3.pdf\", sequence_number=3),\n            ContentFile(b2_path=\"file1.pdf\", filename=\"file1.pdf\", sequence_number=1),\n            ContentFile(b2_path=\"file2.pdf\", filename=\"file2.pdf\", sequence_number=2),\n        ]\n        \n        content_set = ContentSet(\n            id=\"test-id\",\n            name=\"Test Set\",\n            detection_method=\"pattern\",\n            files=files\n        )\n        \n        # Resolve order\n        ordered_files = self.agent.resolve_order(content_set)\n        \n        # Should be ordered by sequence number\n        self.assertEqual(ordered_files[0].sequence_number, 1)\n        self.assertEqual(ordered_files[1].sequence_number, 2)\n        self.assertEqual(ordered_files[2].sequence_number, 3)\n    \n    def test_generate_manifest(self):\n        \"\"\"Test manifest generation\"\"\"\n        # Create a complete content set\n        files = [\n            ContentFile(b2_path=\"file1.pdf\", filename=\"file1.pdf\", sequence_number=1),\n            ContentFile(b2_path=\"file2.pdf\", filename=\"file2.pdf\", sequence_number=2),\n            ContentFile(b2_path=\"file3.pdf\", filename=\"file3.pdf\", sequence_number=3),\n        ]\n        \n        content_set = ContentSet(\n            id=\"test-id\",\n            name=\"Test Set\",\n            detection_method=\"pattern\",\n            files=files,\n            is_complete=True\n        )\n        \n        # Generate manifest\n        manifest = self.agent.generate_manifest(content_set)\n        \n        # Verify manifest\n        self.assertEqual(manifest.content_set_id, \"test-id\")\n        self.assertEqual(manifest.total_files, 3)\n        self.assertEqual(len(manifest.ordered_files), 3)\n        self.assertEqual(len(manifest.warnings), 0)\n        \n        # Check dependencies\n        self.assertEqual(len(manifest.ordered_files[0].dependencies), 0)  # First file has no dependencies\n        self.assertEqual(len(manifest.ordered_files[1].dependencies), 1)  # Second depends on first\n        self.assertEqual(len(manifest.ordered_files[2].dependencies), 1)  # Third depends on second\n    \n    @patch('app.services.content_prep_agent.ContentPrepAgent._request_user_clarification')\n    async def test_resolve_order_with_clarification(self, mock_clarification):\n        \"\"\"Test ordering with user clarification\"\"\"\n        # Create a content set with some files missing sequence numbers\n        files = [\n            ContentFile(b2_path=\"file1.pdf\", filename=\"file1.pdf\", sequence_number=1),\n            ContentFile(b2_path=\"fileA.pdf\", filename=\"fileA.pdf\", sequence_number=None),\n            ContentFile(b2_path=\"fileB.pdf\", filename=\"fileB.pdf\", sequence_number=None),\n        ]\n        \n        content_set = ContentSet(\n            id=\"test-id\",\n            name=\"Test Set\",\n            detection_method=\"pattern\",\n            files=files\n        )\n        \n        # Mock user clarification response\n        mock_clarification.return_value = \"fileA.pdf, fileB.pdf\"\n        \n        # Resolve order with clarification\n        ordered_files = await self.agent.resolve_order_with_clarification(content_set, confidence_threshold=0.9)\n        \n        # Should have requested clarification\n        mock_clarification.assert_called_once()\n        \n        # Should be ordered according to user input\n        self.assertEqual(ordered_files[0].filename, \"file1.pdf\")  # Has sequence number, comes first\n        self.assertEqual(ordered_files[1].filename, \"fileA.pdf\")  # From user input\n        self.assertEqual(ordered_files[2].filename, \"fileB.pdf\")  # From user input\n\n# Performance tests\n@pytest.mark.benchmark\ndef test_performance_large_set(benchmark):\n    \"\"\"Benchmark performance with large file sets\"\"\"\n    agent = ContentPrepAgent()\n    \n    # Generate 100 test files\n    large_set = []\n    for i in range(1, 101):\n        large_set.append({\n            \"path\": f\"pending/course/module-{i:02d}.pdf\",\n            \"size\": 1024 * 1024,\n            \"type\": \"pdf\"\n        })\n    \n    # Benchmark content set detection\n    result = benchmark(agent.detect_content_sets, large_set)\n    \n    # Verify result\n    content_sets, standalone = result\n    assert len(content_sets) == 1\n    assert len(content_sets[0].files) == 100\n```\n\nCreate integration tests for the API endpoints:\n\n```python\n# In tests/test_content_prep_api.py\n\nfrom fastapi.testclient import TestClient\nfrom app.main import app\nimport pytest\nfrom unittest.mock import patch\n\nclient = TestClient(app)\n\n@pytest.fixture\ndef mock_content_prep_agent():\n    with patch('app.routes.content_prep.ContentPrepAgent') as mock:\n        yield mock\n\ndef test_analyze_endpoint(mock_content_prep_agent):\n    \"\"\"Test the analyze endpoint\"\"\"\n    # Mock agent response\n    mock_instance = mock_content_prep_agent.return_value\n    mock_instance.detect_content_sets.return_value = ([], [])\n    \n    # Test API\n    response = client.post(\n        \"/api/content-prep/analyze\",\n        json={\"b2_folder\": \"pending/test/\", \"detection_mode\": \"auto\"}\n    )\n    \n    # Verify response\n    assert response.status_code == 200\n    assert \"content_sets\" in response.json()\n    assert \"standalone_files\" in response.json()\n    \n    # Verify agent was called correctly\n    mock_instance.detect_content_sets.assert_called_once()\n\ndef test_manifest_endpoint(mock_content_prep_agent):\n    \"\"\"Test the manifest endpoint\"\"\"\n    # Mock agent response\n    mock_instance = mock_content_prep_agent.return_value\n    mock_instance.generate_manifest.return_value = {\n        \"content_set_id\": \"test-id\",\n        \"ordered_files\": [],\n        \"total_files\": 0,\n        \"estimated_processing_time\": 0,\n        \"warnings\": [],\n        \"context\": {}\n    }\n    \n    # Test API\n    response = client.post(\n        \"/api/content-prep/manifest\",\n        json={\"content_set_id\": \"test-id\", \"proceed_incomplete\": True}\n    )\n    \n    # Verify response\n    assert response.status_code == 200\n    assert \"content_set_id\" in response.json()\n    assert \"ordered_files\" in response.json()\n    \n    # Verify agent was called correctly\n    mock_instance.generate_manifest.assert_called_once()\n```",
        "testStrategy": "1. Run unit tests for each component\n2. Run integration tests with mocked dependencies\n3. Run performance benchmarks with large file sets\n4. Test with various file naming patterns\n5. Test error handling and edge cases\n6. Verify test coverage meets targets (>90%)\n7. Run tests in CI/CD pipeline",
        "priority": "medium",
        "dependencies": [
          "122",
          "124",
          "125",
          "126",
          "127",
          "128",
          "129",
          "130"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T21:30:07.791Z"
      },
      {
        "id": "132",
        "title": "Implement Prometheus Metrics for Agent Services",
        "description": "Add standardized Prometheus metrics to all agent services (AGENT-001 through AGENT-015) including counters for success/failure, histograms for duration, and gauges for active executions.",
        "details": "Implement Prometheus metrics across all agent services following the pattern established in monitoring_service.py (AGENT-016). The implementation should include:\n\n1. Add the following metrics to each agent:\n   - Counter: Track success/failure outcomes of agent operations\n   - Histogram: Measure duration of agent operations\n   - Gauge: Monitor active executions in progress\n\n2. Update the following files:\n   - content_summarizer_agent.py\n   - department_classifier_agent.py\n   - document_analysis_agents.py\n   - multi_agent_orchestration.py\n   - asset_generator_agents.py\n   - orchestrator_agent_service.py\n\n3. Implementation pattern for each agent:\n```python\nfrom prometheus_client import Counter, Histogram, Gauge\n\n# Define metrics with appropriate labels\nAGENT_REQUESTS_TOTAL = Counter(\n    'agent_requests_total', \n    'Total number of requests processed by the agent',\n    ['agent_id', 'status']\n)\n\nAGENT_REQUEST_DURATION = Histogram(\n    'agent_request_duration_seconds',\n    'Time spent processing agent requests',\n    ['agent_id', 'operation']\n)\n\nAGENT_ACTIVE_EXECUTIONS = Gauge(\n    'agent_active_executions',\n    'Number of currently active agent executions',\n    ['agent_id']\n)\n\nclass SomeAgent:\n    def __init__(self, agent_id):\n        self.agent_id = agent_id\n        \n    async def process(self, request):\n        # Track active executions\n        AGENT_ACTIVE_EXECUTIONS.labels(agent_id=self.agent_id).inc()\n        \n        # Track request duration\n        with AGENT_REQUEST_DURATION.labels(agent_id=self.agent_id, operation='process').time():\n            try:\n                result = await self._process_implementation(request)\n                # Record success\n                AGENT_REQUESTS_TOTAL.labels(agent_id=self.agent_id, status='success').inc()\n                return result\n            except Exception as e:\n                # Record failure\n                AGENT_REQUESTS_TOTAL.labels(agent_id=self.agent_id, status='failure').inc()\n                raise\n            finally:\n                # Decrement active executions\n                AGENT_ACTIVE_EXECUTIONS.labels(agent_id=self.agent_id).dec()\n```\n\n4. Ensure consistent naming conventions across all agents\n5. Add appropriate labels to distinguish between different agent types and operations\n6. Update any agent factory or registration code to ensure metrics are properly initialized\n7. Ensure thread safety for concurrent agent operations\n8. Add documentation comments explaining the metrics and their usage",
        "testStrategy": "1. Unit tests:\n   - Create test cases for each agent type to verify metrics are incremented/decremented correctly\n   - Test success and failure scenarios to ensure counters are updated appropriately\n   - Test concurrent operations to verify thread safety\n\n2. Integration tests:\n   - Verify metrics are exposed correctly via the Prometheus endpoint\n   - Test that metrics have the correct labels and values\n   - Verify histogram buckets are appropriate for the expected duration ranges\n\n3. Load testing:\n   - Verify metrics behave correctly under concurrent load\n   - Check for any performance impact from metrics collection\n\n4. Manual verification:\n   - Use Prometheus UI to query metrics and verify they appear as expected\n   - Create test Grafana dashboards to visualize the metrics\n   - Verify alerts can be configured based on the new metrics\n\n5. Specific test cases:\n   - Test agent success path: verify counter increments and gauge behaves correctly\n   - Test agent failure path: verify failure counter increments\n   - Test long-running operations: verify histogram captures duration correctly\n   - Test agent restart: verify gauges reset to zero appropriately",
        "status": "done",
        "dependencies": [
          "107",
          "118",
          "128",
          "131"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T05:03:46.813Z"
      },
      {
        "id": "133",
        "title": "Implement Orchestrator API Routes",
        "description": "Create API routes for the Master Orchestrator (AGENT-001) to expose its functionality, including coordination, agent listing, health checks, and statistics endpoints.",
        "details": "Create the file `app/routes/orchestrator.py` with the following FastAPI routes:\n\n1. POST /api/orchestrator/coordinate - Main endpoint for orchestrating agent workflows\n   - Implement request validation using Pydantic models\n   - Parse incoming requests and delegate to the Orchestrator service\n   - Handle response formatting and error cases\n\n2. GET /api/orchestrator/agents - Endpoint to list all available agents\n   - Return metadata about registered agents including capabilities and status\n   - Support filtering by agent type, status, or capability\n\n3. GET /api/orchestrator/health - Health check endpoint\n   - Verify Orchestrator service is running properly\n   - Check connections to dependent services\n   - Return appropriate health status codes\n\n4. GET /api/orchestrator/stats - Statistics endpoint\n   - Collect and return performance metrics\n   - Include agent usage statistics, response times, and success rates\n   - Support time-range filtering\n\nRegister the router in main.py by adding:\n```python\nfrom app.routes import orchestrator\n\napp.include_router(orchestrator.router, tags=[\"orchestrator\"])\n```\n\nCreate Pydantic models in `app/models/orchestrator.py`:\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Optional, Any\nfrom enum import Enum\n\nclass AgentType(str, Enum):\n    CONTENT_PREP = \"content_prep\"\n    GRAPH = \"graph\"\n    RETRIEVAL = \"retrieval\"\n    # Add other agent types\n\nclass AgentStatus(str, Enum):\n    AVAILABLE = \"available\"\n    BUSY = \"busy\"\n    OFFLINE = \"offline\"\n\nclass OrchestrationRequest(BaseModel):\n    workflow_id: Optional[str] = Field(None, description=\"Optional workflow identifier\")\n    task: str = Field(..., description=\"Task description for orchestration\")\n    agents: List[str] = Field(default=[], description=\"Specific agents to include\")\n    parameters: Dict[str, Any] = Field(default={}, description=\"Task parameters\")\n    \nclass AgentInfo(BaseModel):\n    id: str\n    name: str\n    type: AgentType\n    status: AgentStatus\n    capabilities: List[str]\n    \nclass OrchestrationResponse(BaseModel):\n    workflow_id: str\n    status: str\n    result: Optional[Dict[str, Any]] = None\n    \nclass HealthStatus(BaseModel):\n    status: str\n    version: str\n    dependencies: Dict[str, str]\n    \nclass StatsResponse(BaseModel):\n    total_requests: int\n    success_rate: float\n    average_response_time: float\n    agent_usage: Dict[str, int]\n    recent_workflows: List[Dict[str, Any]]\n```\n\nEnsure proper error handling and validation throughout the implementation.",
        "testStrategy": "1. Create unit tests in `tests/routes/test_orchestrator.py` for each endpoint:\n   - Test POST /api/orchestrator/coordinate with valid and invalid payloads\n   - Test GET /api/orchestrator/agents with various filter parameters\n   - Test GET /api/orchestrator/health in normal and degraded states\n   - Test GET /api/orchestrator/stats with different time ranges\n\n2. Create integration tests that verify:\n   - Router registration works correctly in main.py\n   - Endpoints interact properly with the Orchestrator service\n   - Authentication and authorization are enforced correctly\n\n3. Test error handling:\n   - Verify appropriate status codes for various error conditions\n   - Test validation errors with malformed requests\n   - Test service unavailable scenarios\n\n4. Test Pydantic models:\n   - Verify model validation works as expected\n   - Test serialization/deserialization\n   - Ensure OpenAPI schema generation is correct\n\n5. Performance testing:\n   - Test response times under load\n   - Verify endpoints can handle concurrent requests\n   - Check memory usage during extended operation\n\n6. Documentation verification:\n   - Ensure API documentation is generated correctly\n   - Verify examples in documentation match implementation",
        "status": "done",
        "dependencies": [
          "103",
          "122",
          "124"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T03:50:30.220Z"
      },
      {
        "id": "134",
        "title": "Implement Health Endpoint for Asset Generators",
        "description": "Add a health endpoint to Asset Generators (AGENT-003 through AGENT-007) that returns status information and capabilities for all five generators: Skill, Command, Agent, Prompt, and Workflow.",
        "details": "Create a new health endpoint in the asset_generators.py routes file to provide status information for all five Asset Generator agents:\n\n1. Add the following route to app/routes/asset_generators.py:\n```python\n@router.get(\"/health\", response_model=AssetGeneratorsHealthResponse)\nasync def get_health():\n    \"\"\"\n    Get health status for all Asset Generator agents.\n    \n    Returns:\n        AssetGeneratorsHealthResponse: Status and capabilities for all five generators\n    \"\"\"\n    health_service = AssetGeneratorsHealthService()\n    return await health_service.get_health_status()\n```\n\n2. Create a Pydantic model for the health response in app/models/asset_generators.py:\n```python\nclass GeneratorHealth(BaseModel):\n    status: str  # \"healthy\", \"degraded\", or \"offline\"\n    capabilities: List[str]\n    last_check: datetime\n    error_message: Optional[str] = None\n\nclass AssetGeneratorsHealthResponse(BaseModel):\n    skill_generator: GeneratorHealth\n    command_generator: GeneratorHealth\n    agent_generator: GeneratorHealth\n    prompt_generator: GeneratorHealth\n    workflow_generator: GeneratorHealth\n    timestamp: datetime = Field(default_factory=datetime.utcnow)\n```\n\n3. Implement the health service in app/services/asset_generators_health.py:\n```python\nfrom datetime import datetime\nfrom app.models.asset_generators import AssetGeneratorsHealthResponse, GeneratorHealth\nfrom app.services.skill_generator import SkillGeneratorService\nfrom app.services.command_generator import CommandGeneratorService\nfrom app.services.agent_generator import AgentGeneratorService\nfrom app.services.prompt_generator import PromptGeneratorService\nfrom app.services.workflow_generator import WorkflowGeneratorService\n\nclass AssetGeneratorsHealthService:\n    async def get_health_status(self) -> AssetGeneratorsHealthResponse:\n        \"\"\"Get health status for all Asset Generator agents.\"\"\"\n        \n        # Get health status from each generator service\n        skill_health = await self._check_skill_generator()\n        command_health = await self._check_command_generator()\n        agent_health = await self._check_agent_generator()\n        prompt_health = await self._check_prompt_generator()\n        workflow_health = await self._check_workflow_generator()\n        \n        return AssetGeneratorsHealthResponse(\n            skill_generator=skill_health,\n            command_generator=command_health,\n            agent_generator=agent_health,\n            prompt_generator=prompt_health,\n            workflow_generator=workflow_health\n        )\n    \n    async def _check_skill_generator(self) -> GeneratorHealth:\n        try:\n            service = SkillGeneratorService()\n            capabilities = await service.get_capabilities()\n            return GeneratorHealth(\n                status=\"healthy\",\n                capabilities=capabilities,\n                last_check=datetime.utcnow()\n            )\n        except Exception as e:\n            return GeneratorHealth(\n                status=\"offline\",\n                capabilities=[],\n                last_check=datetime.utcnow(),\n                error_message=str(e)\n            )\n    \n    # Implement similar _check methods for other generators\n    # _check_command_generator, _check_agent_generator, etc.\n```\n\n4. Update each generator service to implement a get_capabilities() method that returns a list of capabilities specific to that generator.\n\n5. Ensure proper error handling and timeouts for health checks to prevent cascading failures.\n\n6. Add appropriate logging for health check operations.\n\n7. Update API documentation to include the new health endpoint.",
        "testStrategy": "1. Create unit tests in tests/routes/test_asset_generators.py:\n   ```python\n   async def test_health_endpoint_returns_correct_structure():\n       # Arrange\n       client = TestClient(app)\n       \n       # Act\n       response = client.get(\"/api/asset-generators/health\")\n       \n       # Assert\n       assert response.status_code == 200\n       data = response.json()\n       assert \"skill_generator\" in data\n       assert \"command_generator\" in data\n       assert \"agent_generator\" in data\n       assert \"prompt_generator\" in data\n       assert \"workflow_generator\" in data\n       assert \"timestamp\" in data\n       \n       # Check structure of each generator's health data\n       for generator in [\"skill_generator\", \"command_generator\", \"agent_generator\", \n                         \"prompt_generator\", \"workflow_generator\"]:\n           assert \"status\" in data[generator]\n           assert \"capabilities\" in data[generator]\n           assert \"last_check\" in data[generator]\n   ```\n\n2. Create unit tests for the AssetGeneratorsHealthService:\n   - Test successful health checks for all generators\n   - Test scenarios where one or more generators are offline\n   - Test error handling when exceptions occur\n\n3. Create integration tests that verify the health endpoint with mocked generator services:\n   ```python\n   @patch(\"app.services.skill_generator.SkillGeneratorService\")\n   @patch(\"app.services.command_generator.CommandGeneratorService\")\n   # ... patches for other services\n   async def test_health_endpoint_with_mocked_services(mock_skill, mock_command, ...):\n       # Configure mocks\n       mock_skill.return_value.get_capabilities.return_value = [\"create_skill\", \"update_skill\"]\n       # ... configure other mocks\n       \n       # Make request\n       response = client.get(\"/api/asset-generators/health\")\n       \n       # Verify response\n       # ...\n   ```\n\n4. Test error scenarios:\n   - Test when one generator is down but others are up\n   - Test timeout handling\n   - Test with various error conditions\n\n5. Performance testing:\n   - Measure response time under normal conditions\n   - Test with simulated slow responses from generators\n\n6. Manual testing:\n   - Verify the endpoint in development environment\n   - Check that all capabilities are correctly listed\n   - Verify status values are accurate\n\n7. Update API documentation tests to include the new endpoint.",
        "status": "done",
        "dependencies": [
          "107",
          "132",
          "133"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T05:07:18.068Z"
      },
      {
        "id": "135",
        "title": "Implement Standardized Error Response Model and Handling",
        "description": "Create a standardized error response model and consistent error handling across all agents with a centralized error_handler middleware.",
        "details": "Implement a standardized approach to error handling across all agent routes:\n\n1. Create `app/models/errors.py` with the following structure:\n```python\nfrom pydantic import BaseModel\nfrom typing import Optional, Dict, Any\nfrom datetime import datetime\nfrom enum import Enum\n\nclass ErrorType(str, Enum):\n    RETRIABLE = \"retriable\"\n    PERMANENT = \"permanent\"\n\nclass AgentErrorResponse(BaseModel):\n    error_code: str\n    error_type: ErrorType\n    agent_id: str\n    message: str\n    details: Optional[Dict[str, Any]] = None\n    request_id: Optional[str] = None\n    timestamp: datetime = datetime.utcnow()\n    \n    class Config:\n        schema_extra = {\n            \"example\": {\n                \"error_code\": \"AGENT_PROCESSING_ERROR\",\n                \"error_type\": \"retriable\",\n                \"agent_id\": \"content_prep_agent\",\n                \"message\": \"Failed to process content set\",\n                \"details\": {\"reason\": \"Invalid file format\"},\n                \"request_id\": \"req-123456\",\n                \"timestamp\": \"2023-06-15T10:30:00Z\"\n            }\n        }\n```\n\n2. Create an error handler middleware in `app/middleware/error_handler.py`:\n```python\nfrom fastapi import Request, status\nfrom fastapi.responses import JSONResponse\nfrom app.models.errors import AgentErrorResponse, ErrorType\nimport logging\nimport uuid\nfrom starlette.middleware.base import BaseHTTPMiddleware\n\nlogger = logging.getLogger(__name__)\n\nclass ErrorHandlerMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):\n        try:\n            return await call_next(request)\n        except Exception as e:\n            # Extract agent_id from path if possible\n            path_parts = request.url.path.split(\"/\")\n            agent_id = \"unknown\"\n            for i, part in enumerate(path_parts):\n                if part == \"agent\" and i + 1 < len(path_parts):\n                    agent_id = path_parts[i + 1]\n                    break\n            \n            # Generate request_id if not present\n            request_id = request.headers.get(\"X-Request-ID\", str(uuid.uuid4()))\n            \n            # Log the error\n            logger.error(f\"Request error: {str(e)}\", \n                        extra={\"request_id\": request_id, \"agent_id\": agent_id})\n            \n            # Create standardized error response\n            error_response = AgentErrorResponse(\n                error_code=\"INTERNAL_SERVER_ERROR\",\n                error_type=ErrorType.RETRIABLE,\n                agent_id=agent_id,\n                message=\"An unexpected error occurred\",\n                details={\"exception\": str(e)},\n                request_id=request_id\n            )\n            \n            return JSONResponse(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                content=error_response.dict()\n            )\n```\n\n3. Update all agent route handlers to use the standardized error format:\n```python\nfrom fastapi import HTTPException\nfrom app.models.errors import AgentErrorResponse, ErrorType\n\n# Example of updated route handler\n@router.post(\"/process\")\nasync def process_content(request: ProcessRequest):\n    try:\n        # Processing logic\n        result = await agent.process(request.content)\n        return result\n    except ValidationError as e:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=AgentErrorResponse(\n                error_code=\"VALIDATION_ERROR\",\n                error_type=ErrorType.PERMANENT,\n                agent_id=\"content_prep_agent\",\n                message=\"Invalid request format\",\n                details={\"errors\": e.errors()}\n            ).dict()\n        )\n    except ResourceNotFoundError as e:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=AgentErrorResponse(\n                error_code=\"RESOURCE_NOT_FOUND\",\n                error_type=ErrorType.PERMANENT,\n                agent_id=\"content_prep_agent\",\n                message=\"Requested resource not found\",\n                details={\"resource_id\": e.resource_id}\n            ).dict()\n        )\n```\n\n4. Register the middleware in the main FastAPI application:\n```python\n# In app/main.py\nfrom app.middleware.error_handler import ErrorHandlerMiddleware\n\napp = FastAPI()\napp.add_middleware(ErrorHandlerMiddleware)\n```\n\n5. Create common error code constants in `app/constants/error_codes.py`:\n```python\n# Common error codes\nVALIDATION_ERROR = \"VALIDATION_ERROR\"\nRESOURCE_NOT_FOUND = \"RESOURCE_NOT_FOUND\"\nUNAUTHORIZED = \"UNAUTHORIZED\"\nFORBIDDEN = \"FORBIDDEN\"\nINTERNAL_SERVER_ERROR = \"INTERNAL_SERVER_ERROR\"\n\n# Agent-specific error codes\nAGENT_PROCESSING_ERROR = \"AGENT_PROCESSING_ERROR\"\nAGENT_TIMEOUT = \"AGENT_TIMEOUT\"\nAGENT_UNAVAILABLE = \"AGENT_UNAVAILABLE\"\n```\n\n6. Update all existing agent implementations to use the new error model and handling approach.",
        "testStrategy": "1. Unit test the AgentErrorResponse model:\n   - Test serialization/deserialization\n   - Test validation of required fields\n   - Test default values (timestamp)\n   - Test enum validation for error_type\n\n2. Unit test the ErrorHandlerMiddleware:\n   - Test with various exception types\n   - Verify correct error response format\n   - Test agent_id extraction from different URL patterns\n   - Test request_id propagation\n   - Test logging behavior\n\n3. Integration tests for error handling:\n   - Test each agent endpoint with invalid inputs\n   - Test with simulated processing errors\n   - Verify consistent error response format across all endpoints\n   - Test with and without X-Request-ID header\n\n4. Test error handling for specific scenarios:\n   - Authentication failures\n   - Authorization failures\n   - Resource not found\n   - Validation errors\n   - Timeout errors\n   - External service failures\n\n5. End-to-end tests:\n   - Verify client applications can properly parse and handle error responses\n   - Test error recovery flows for retriable errors\n   - Test appropriate client behavior for permanent errors\n\n6. Performance testing:\n   - Measure overhead of error handling middleware\n   - Test under high load with frequent errors",
        "status": "done",
        "dependencies": [
          "107",
          "110",
          "125",
          "129"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T05:19:17.976Z"
      },
      {
        "id": "136",
        "title": "Implement X-Request-ID Tracing Across Agent Chains",
        "description": "Create a middleware and utility functions to generate, propagate, and log X-Request-ID headers throughout the entire agent ecosystem, enabling end-to-end request tracing across multi-agent workflows.",
        "details": "Implement a comprehensive request tracing system using X-Request-ID headers:\n\n1. Create middleware in `app/middleware/request_tracing.py`:\n   - Generate a unique UUID for each incoming request if X-Request-ID is not present\n   - Extract existing X-Request-ID from incoming requests if available\n   - Add the request_id to the request context for access throughout the request lifecycle\n\n2. Create utility functions in `app/utils/tracing.py`:\n   - `get_request_id()`: Retrieve the current request_id from context\n   - `with_request_id(headers)`: Add the current request_id to outgoing request headers\n   - `log_with_request_id(message, level)`: Log with request_id included\n\n3. Modify agent service calls:\n   - Update all HTTP client calls to include X-Request-ID header using `with_request_id()`\n   - Ensure all CrewAI agent chains propagate the request_id to downstream agents\n   - Add request_id to agent context objects for access during execution\n\n4. Update logging configuration:\n   - Modify logging formatters to include request_id in all log entries\n   - Create a custom log filter to inject request_id into log records\n\n5. Enhance error responses:\n   - Update error handling middleware to include request_id in all error responses\n   - Add request_id to standardized error response format\n\n6. Add request_id to metrics:\n   - Include request_id as a label in Prometheus metrics\n   - Enable correlation between metrics and logs\n\n7. Create documentation:\n   - Document the request tracing system for developers\n   - Provide examples of how to use the tracing utilities\n\nExample middleware implementation:\n```python\nfrom fastapi import Request\nfrom starlette.middleware.base import BaseHTTPMiddleware\nimport uuid\nfrom contextvars import ContextVar\n\n# Context variable to store request_id\nrequest_id_var = ContextVar(\"request_id\", default=None)\n\nclass RequestTracingMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):\n        # Extract or generate request_id\n        request_id = request.headers.get(\"X-Request-ID\")\n        if not request_id:\n            request_id = str(uuid.uuid4())\n        \n        # Store in context for this request\n        token = request_id_var.set(request_id)\n        \n        # Process request\n        response = await call_next(request)\n        \n        # Add request_id to response headers\n        response.headers[\"X-Request-ID\"] = request_id\n        \n        # Reset context\n        request_id_var.reset(token)\n        \n        return response\n```\n\nExample utility functions:\n```python\nfrom app.middleware.request_tracing import request_id_var\nimport logging\n\ndef get_request_id():\n    \"\"\"Get the current request ID from context.\"\"\"\n    return request_id_var.get()\n\ndef with_request_id(headers=None):\n    \"\"\"Add request_id to headers for outgoing requests.\"\"\"\n    headers = headers or {}\n    request_id = get_request_id()\n    if request_id:\n        headers[\"X-Request-ID\"] = request_id\n    return headers\n\ndef log_with_request_id(message, level=logging.INFO):\n    \"\"\"Log with request_id included.\"\"\"\n    logger = logging.getLogger()\n    request_id = get_request_id()\n    extra = {\"request_id\": request_id} if request_id else {}\n    logger.log(level, message, extra=extra)\n```",
        "testStrategy": "1. Create unit tests in `tests/middleware/test_request_tracing.py`:\n   - Test middleware with no existing X-Request-ID header\n   - Test middleware with existing X-Request-ID header\n   - Verify request_id is properly stored in context\n   - Verify request_id is added to response headers\n\n2. Create unit tests in `tests/utils/test_tracing.py`:\n   - Test `get_request_id()` returns correct value\n   - Test `with_request_id()` adds header correctly\n   - Test `log_with_request_id()` includes request_id in logs\n\n3. Create integration tests in `tests/integration/test_request_tracing.py`:\n   - Test request_id propagation through multiple API calls\n   - Test request_id propagation through agent chains\n   - Verify logs contain consistent request_id across service boundaries\n\n4. Test error scenarios:\n   - Verify error responses include request_id\n   - Test behavior when invalid request_id is provided\n\n5. Performance testing:\n   - Measure overhead of request tracing middleware\n   - Verify no significant performance impact\n\n6. Manual testing:\n   - Use tools like Postman to trace requests through the system\n   - Verify request_id appears in logs and responses\n   - Test multi-agent workflows and verify end-to-end tracing\n\n7. Create a test script that:\n   - Initiates a complex multi-agent workflow\n   - Verifies the same request_id appears in all logs\n   - Checks metrics contain the request_id label\n   - Validates error responses include the request_id",
        "status": "done",
        "dependencies": [
          "133",
          "122",
          "131"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T03:56:29.663Z"
      },
      {
        "id": "137",
        "title": "Implement Circuit Breaker for Anthropic API Calls",
        "description": "Implement a circuit breaker pattern for Anthropic API calls across all agent services to improve resilience and prevent cascading failures during API outages.",
        "details": "Create a robust circuit breaker implementation for Anthropic API calls that will be used across all agent services:\n\n1. Create a new module `app/services/api_resilience.py` to implement the circuit breaker pattern:\n   - Use the `tenacity` or `circuit-breaker` library for implementation\n   - Configure exponential backoff with a maximum of 3 retries\n   - Implement proper circuit state tracking (closed/open/half-open)\n   - Set appropriate thresholds for opening the circuit (e.g., 5 failures in 30 seconds)\n   - Implement half-open state with test requests to check if service is restored\n\n2. Create a wrapper class for Anthropic API calls:\n```python\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom prometheus_client import Counter, Gauge\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Metrics for circuit breaker state\nanthropic_circuit_state = Gauge(\n    'anthropic_circuit_state',\n    'Current state of Anthropic API circuit breaker (0=closed, 1=half-open, 2=open)',\n    ['service']\n)\n\nanthropic_api_failures = Counter(\n    'anthropic_api_failures',\n    'Number of Anthropic API call failures',\n    ['service', 'endpoint']\n)\n\nclass AnthropicCircuitBreaker:\n    CLOSED = 0\n    HALF_OPEN = 1\n    OPEN = 2\n    \n    def __init__(self, service_name, failure_threshold=5, recovery_timeout=60):\n        self.service_name = service_name\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.failure_count = 0\n        self.last_failure_time = None\n        self.state = self.CLOSED\n        self._update_state_metric()\n    \n    def _update_state_metric(self):\n        anthropic_circuit_state.labels(service=self.service_name).set(self.state)\n    \n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=2, max=10),\n        retry_error_callback=lambda retry_state: retry_state.outcome.result()\n    )\n    async def call_api(self, func, *args, **kwargs):\n        \"\"\"Wrapper for Anthropic API calls with circuit breaker pattern\"\"\"\n        # Check if circuit is open\n        if self.state == self.OPEN:\n            # Check if recovery timeout has elapsed to transition to half-open\n            if self.last_failure_time and (time.time() - self.last_failure_time) > self.recovery_timeout:\n                self.state = self.HALF_OPEN\n                self._update_state_metric()\n                logger.info(f\"Circuit for {self.service_name} transitioned to HALF-OPEN state\")\n            else:\n                # Fail fast when circuit is open\n                logger.warning(f\"Circuit for {self.service_name} is OPEN, failing fast\")\n                raise CircuitOpenError(f\"Circuit for {self.service_name} is open\")\n        \n        try:\n            result = await func(*args, **kwargs)\n            \n            # If we're in half-open and call succeeded, close the circuit\n            if self.state == self.HALF_OPEN:\n                self.state = self.CLOSED\n                self.failure_count = 0\n                self._update_state_metric()\n                logger.info(f\"Circuit for {self.service_name} restored to CLOSED state\")\n            \n            return result\n            \n        except Exception as e:\n            endpoint = kwargs.get('endpoint', 'unknown')\n            anthropic_api_failures.labels(service=self.service_name, endpoint=endpoint).inc()\n            \n            self.failure_count += 1\n            self.last_failure_time = time.time()\n            \n            # Check if we need to open the circuit\n            if self.state == self.CLOSED and self.failure_count >= self.failure_threshold:\n                self.state = self.OPEN\n                self._update_state_metric()\n                logger.error(f\"Circuit for {self.service_name} transitioned to OPEN state after {self.failure_count} failures\")\n            \n            # Re-raise the exception for retry handling\n            raise\n```\n\n3. Apply the circuit breaker to all agent services that call Anthropic API:\n   - content_summarizer\n   - department_classifier\n   - document_analysis\n   - multi_agent_orchestration\n   - asset_generators\n\n4. Update each service to use the circuit breaker wrapper:\n```python\n# Example integration in content_summarizer\nfrom app.services.api_resilience import AnthropicCircuitBreaker\n\nclass ContentSummarizerAgent:\n    def __init__(self):\n        # Initialize circuit breaker\n        self.circuit_breaker = AnthropicCircuitBreaker(service_name=\"content_summarizer\")\n        # Other initialization...\n    \n    async def summarize_content(self, content):\n        try:\n            # Use circuit breaker to call Anthropic API\n            summary = await self.circuit_breaker.call_api(\n                self._call_anthropic_api,\n                content=content,\n                endpoint=\"completion\"\n            )\n            return summary\n        except CircuitOpenError:\n            # Handle circuit open case - return cached response or fallback\n            return self._generate_fallback_summary(content)\n```\n\n5. Implement appropriate fallback mechanisms for each service when the circuit is open:\n   - Return cached responses when available\n   - Use simpler models or rules-based approaches as fallbacks\n   - Provide clear error messages to users about temporary service limitations\n\n6. Add comprehensive logging for circuit state changes and API failures to aid in debugging and monitoring.",
        "testStrategy": "1. Unit tests for the circuit breaker implementation:\n   - Test state transitions (closed → open → half-open → closed)\n   - Test failure counting and threshold behavior\n   - Test exponential backoff retry logic\n   - Test timeout handling and recovery\n\n2. Integration tests with mocked Anthropic API responses:\n   - Test successful API calls\n   - Test handling of different error types (rate limits, server errors, etc.)\n   - Test retry behavior with temporary failures\n   - Test circuit opening after threshold failures\n\n3. Test each agent service with the circuit breaker:\n   - Verify circuit breaker is properly integrated in each service\n   - Test fallback mechanisms when circuit is open\n   - Verify metrics are correctly updated for each service\n\n4. Load testing:\n   - Simulate high load scenarios to verify circuit breaker prevents cascading failures\n   - Test recovery behavior under load\n\n5. Monitoring tests:\n   - Verify circuit state metrics are correctly exposed to Prometheus\n   - Test dashboard alerts for circuit open states\n   - Verify logging provides sufficient information for troubleshooting\n\n6. End-to-end tests:\n   - Test the complete flow from API call through circuit breaker to response\n   - Verify correct behavior during simulated Anthropic API outages\n\n7. Test fallback quality:\n   - Evaluate the quality of fallback responses compared to normal operation\n   - Ensure degraded service is still usable when circuit is open",
        "status": "done",
        "dependencies": [
          "110"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-13T22:28:26.078Z"
      },
      {
        "id": "138",
        "title": "Implement Review Agent Revision Loop",
        "description": "Implement a revision loop for AGENT-015 Review Agent that routes content back to AGENT-014 Writing Agent when quality thresholds aren't met, with a maximum of 3 iterations and metrics tracking.",
        "details": "Create a revision loop system between the Review Agent and Writing Agent with the following components:\n\n1. Enhance the Review Agent (AGENT-015) output schema:\n   ```python\n   class ReviewOutput(BaseModel):\n       content_id: str\n       passed: bool\n       quality_score: float  # New field for quantitative assessment\n       feedback: List[FeedbackItem]\n       revision_count: int = 0  # Track number of revisions\n   ```\n\n2. Modify the orchestration workflow to handle revisions:\n   ```python\n   class WorkflowState(BaseModel):\n       # Existing fields\n       revision_requested: bool = False  # New flag to indicate revision needed\n       current_revision: int = 0  # Track current revision number\n   ```\n\n3. Implement the revision loop logic in the workflow orchestrator:\n   ```python\n   async def process_review_result(review_output: ReviewOutput, workflow_state: WorkflowState):\n       # Update metrics\n       metrics.record_quality_score(review_output.content_id, review_output.quality_score)\n       \n       if not review_output.passed and workflow_state.current_revision < 3:\n           # Route back to writing agent with feedback\n           workflow_state.revision_requested = True\n           workflow_state.current_revision += 1\n           \n           # Record metrics for revision request\n           metrics.increment_revision_count(review_output.content_id)\n           \n           # Send to writing agent with feedback\n           return await writing_agent.revise_content(\n               content_id=review_output.content_id,\n               feedback=review_output.feedback,\n               revision_number=workflow_state.current_revision\n           )\n       elif not review_output.passed:\n           # Max revisions reached, flag for human review\n           workflow_state.requires_human_review = True\n           metrics.record_failed_after_max_revisions(review_output.content_id)\n           return workflow_state\n       else:\n           # Content passed review\n           metrics.record_successful_content(\n               content_id=review_output.content_id, \n               revision_count=workflow_state.current_revision\n           )\n           return workflow_state\n   ```\n\n4. Enhance the Writing Agent (AGENT-014) to handle revision requests:\n   ```python\n   async def revise_content(self, content_id: str, feedback: List[FeedbackItem], revision_number: int):\n       # Retrieve original content\n       original_content = await self.content_repository.get_content(content_id)\n       \n       # Format feedback for the LLM\n       formatted_feedback = self.format_feedback_for_revision(feedback)\n       \n       # Create revision prompt with specific focus on addressing feedback\n       revision_prompt = f\"\"\"\n       You are revising content based on review feedback. This is revision #{revision_number}.\n       \n       ORIGINAL CONTENT:\n       {original_content.text}\n       \n       REVIEW FEEDBACK:\n       {formatted_feedback}\n       \n       Please revise the content to address all feedback points while maintaining the original purpose.\n       Focus especially on improving the areas mentioned in the feedback.\n       \"\"\"\n       \n       # Get revised content from LLM\n       revised_content = await self.llm_service.generate_content(revision_prompt)\n       \n       # Store revision history\n       await self.content_repository.save_revision(\n           content_id=content_id,\n           revision_number=revision_number,\n           original_content=original_content.text,\n           revised_content=revised_content,\n           feedback=feedback\n       )\n       \n       # Update content\n       await self.content_repository.update_content(content_id, revised_content)\n       \n       return revised_content\n   ```\n\n5. Implement metrics tracking for revisions:\n   ```python\n   # In app/services/metrics.py\n   \n   def record_quality_score(content_id: str, quality_score: float):\n       # Record quality score in metrics system\n       pass\n       \n   def increment_revision_count(content_id: str):\n       # Increment revision counter for this content\n       pass\n       \n   def record_quality_improvement(content_id: str, original_score: float, new_score: float):\n       # Record delta between original and new quality scores\n       pass\n       \n   def record_failed_after_max_revisions(content_id: str):\n       # Record instances where content failed even after max revisions\n       pass\n       \n   def record_successful_content(content_id: str, revision_count: int):\n       # Record successful content with the number of revisions needed\n       pass\n   ```\n\n6. Create a dashboard view for revision metrics:\n   - Average quality score improvement per revision\n   - Distribution of revision counts\n   - Success rate after revisions\n   - Content requiring max revisions",
        "testStrategy": "1. Unit tests for the revision loop logic:\n   - Test that content with quality_score below threshold gets flagged for revision\n   - Test that revision_count increments correctly\n   - Test that max revisions (3) is enforced\n   - Test that feedback is properly passed to the Writing Agent\n\n2. Integration tests for the Review-Write cycle:\n   - Test end-to-end flow with mock agents\n   - Verify content is properly updated after revision\n   - Test with various quality scores to ensure threshold logic works\n   - Verify revision history is properly stored\n\n3. Metrics validation:\n   - Verify quality scores are recorded correctly\n   - Test that revision counts are accurately tracked\n   - Validate quality improvement calculations\n   - Test dashboard data aggregation\n\n4. Edge case testing:\n   - Test behavior when a revision improves but still doesn't meet threshold\n   - Test when Writing Agent fails during revision\n   - Test concurrent revisions for different content pieces\n   - Test with malformed feedback data\n\n5. Performance testing:\n   - Measure latency impact of revision loops\n   - Test system under load with multiple simultaneous revision cycles\n   - Verify database performance with revision history storage\n\n6. User acceptance testing:\n   - Verify that revision feedback is clear and actionable\n   - Test that quality improvements are meaningful\n   - Validate dashboard metrics against manual calculations",
        "status": "done",
        "dependencies": [
          "107",
          "110",
          "137"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T04:03:47.665Z"
      },
      {
        "id": "139",
        "title": "Create Integration Tests for Agent API Routes",
        "description": "Develop comprehensive integration tests for all agent API routes using FastAPI TestClient to verify the full request-response cycle, including error scenarios and multi-agent workflow chains.",
        "details": "Create a comprehensive integration test suite in the `tests/integration/` directory to test all agent API routes:\n\n1. Set up the test environment:\n   - Create `tests/integration/test_api_routes.py` as the main test file\n   - Implement test fixtures for FastAPI TestClient setup\n   - Configure test database and mock external dependencies\n\n2. Implement test cases for each agent endpoint group:\n   - Content Summarizer endpoints:\n     - Test document summarization with various input types\n     - Test customization parameters (length, style, format)\n   \n   - Department Classifier endpoints:\n     - Test classification with sample documents\n     - Verify confidence scores and multi-label classification\n   \n   - Document Analysis endpoints:\n     - Test document parsing and metadata extraction\n     - Test analysis with different document types (PDF, DOCX, TXT)\n   \n   - Multi-agent Orchestration endpoints:\n     - Test workflow creation and execution\n     - Test agent coordination and task delegation\n   \n   - Asset Generator endpoints:\n     - Test generation of various asset types\n     - Verify output formats and customization options\n   \n   - Content Prep endpoints:\n     - Test content set detection and ordering\n     - Test manifest generation and validation\n\n3. Implement error scenario tests:\n   - Invalid input parameters (wrong format, missing required fields)\n   - Timeout scenarios using mocked delayed responses\n   - API failure handling with simulated service errors\n   - Rate limiting and throttling tests\n   - Authentication and authorization failures\n\n4. Implement end-to-end workflow tests:\n   - Test complete multi-agent workflows from request to final response\n   - Verify correct data passing between agents\n   - Test complex scenarios involving multiple agent interactions\n   - Validate response formats and content\n\n5. Implement performance tests:\n   - Test response times under various load conditions\n   - Verify handling of concurrent requests\n   - Test with large payloads to ensure stability\n\nSample test code structure:\n```python\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom app.main import app\nfrom unittest.mock import patch, MagicMock\n\nclient = TestClient(app)\n\n@pytest.fixture\ndef mock_dependencies():\n    # Set up mocks for external services\n    with patch(\"app.services.content_summarizer.SummarizerService\") as mock_summarizer, \\\n         patch(\"app.services.department_classifier.ClassifierService\") as mock_classifier:\n        yield {\n            \"summarizer\": mock_summarizer,\n            \"classifier\": mock_classifier,\n            # Add other mocked dependencies\n        }\n\ndef test_content_summarizer_endpoint(mock_dependencies):\n    # Arrange\n    mock_dependencies[\"summarizer\"].return_value.summarize.return_value = {\n        \"summary\": \"This is a test summary\",\n        \"confidence\": 0.95\n    }\n    \n    # Act\n    response = client.post(\n        \"/api/content_summarizer/summarize\",\n        json={\"text\": \"This is a test document that needs to be summarized.\", \"max_length\": 100}\n    )\n    \n    # Assert\n    assert response.status_code == 200\n    assert \"summary\" in response.json()\n    assert \"confidence\" in response.json()\n    \ndef test_invalid_input_handling():\n    # Test with missing required field\n    response = client.post(\n        \"/api/content_summarizer/summarize\",\n        json={\"max_length\": 100}  # Missing 'text' field\n    )\n    assert response.status_code == 422\n    \n    # Test with invalid parameter type\n    response = client.post(\n        \"/api/content_summarizer/summarize\",\n        json={\"text\": \"Sample text\", \"max_length\": \"not_a_number\"}\n    )\n    assert response.status_code == 422\n\ndef test_multi_agent_workflow():\n    # Test a complete workflow involving multiple agents\n    # ...\n```",
        "testStrategy": "1. Run individual endpoint tests:\n   - Execute tests for each agent endpoint in isolation\n   - Verify correct HTTP status codes (200 for success, appropriate error codes for failures)\n   - Validate response structure against API specifications\n   - Check that response data matches expected format and content\n\n2. Run error scenario tests:\n   - Verify that invalid inputs return appropriate 400-level status codes\n   - Confirm error messages are descriptive and helpful\n   - Test that timeout scenarios are handled gracefully\n   - Ensure API failures return appropriate 500-level status codes with useful error information\n   - Verify authentication failures return 401/403 status codes\n\n3. Run end-to-end workflow tests:\n   - Execute complete multi-agent workflow tests\n   - Verify data consistency throughout the workflow\n   - Check that final outputs match expected results\n   - Test with various input combinations to ensure robustness\n\n4. Verify test coverage:\n   - Generate test coverage reports using pytest-cov\n   - Ensure all API routes have at least 90% test coverage\n   - Identify and address any gaps in test coverage\n\n5. Run performance tests:\n   - Execute tests with timing measurements\n   - Verify response times are within acceptable limits\n   - Test with concurrent requests to ensure stability\n\n6. Validate in CI/CD pipeline:\n   - Configure tests to run automatically in the CI/CD pipeline\n   - Ensure tests pass consistently in the pipeline environment\n   - Set up test reports for easy review\n\n7. Conduct code review:\n   - Have team members review test code for completeness and correctness\n   - Verify that tests align with API specifications and requirements",
        "status": "done",
        "dependencies": [
          "131",
          "133",
          "122",
          "121",
          "103"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T17:29:02.885Z"
      },
      {
        "id": "140",
        "title": "Enhance Content Prep Agent Health Endpoint",
        "description": "Update the existing GET /api/content-prep/health endpoint to include comprehensive agent status information including agent_id, version, uptime, error metrics, and processing statistics.",
        "details": "Enhance the existing health endpoint in the content_prep.py routes file to provide more detailed status information:\n\n1. Modify the route in app/routes/content_prep.py:\n```python\n@router.get(\"/health\", response_model=ContentPrepHealthResponse)\nasync def get_health():\n    \"\"\"\n    Get health status for the Content Prep Agent (AGENT-016).\n    \n    Returns:\n        ContentPrepHealthResponse: Detailed health status including connectivity and metrics\n    \"\"\"\n    health_service = get_health_service()\n    \n    # Get basic agent information\n    agent_info = {\n        \"agent_id\": \"AGENT-016\",\n        \"name\": \"Content Prep Agent\",\n        \"version\": get_version(),\n        \"uptime\": calculate_uptime()\n    }\n    \n    # Get processing metrics\n    metrics = {\n        \"recent_error_count\": await health_service.get_recent_error_count(),\n        \"pending_content_sets\": await health_service.get_pending_content_sets_count(),\n        \"active_processing_count\": await health_service.get_active_processing_count()\n    }\n    \n    # Check database connectivity\n    connectivity = {\n        \"supabase\": await health_service.check_supabase_connectivity(),\n        \"neo4j\": await health_service.check_neo4j_connectivity()\n    }\n    \n    return ContentPrepHealthResponse(\n        status=\"healthy\" if all(connectivity.values()) else \"degraded\",\n        agent=agent_info,\n        metrics=metrics,\n        connectivity=connectivity\n    )\n```\n\n2. Create or update the response model in app/models/health.py:\n```python\nclass ContentPrepHealthResponse(BaseModel):\n    status: Literal[\"healthy\", \"degraded\", \"unhealthy\"]\n    agent: Dict[str, Any]  # Contains agent_id, name, version, uptime\n    metrics: Dict[str, int]  # Contains error_count, pending_sets, active_processing\n    connectivity: Dict[str, bool]  # Contains status of connected services\n```\n\n3. Implement the health service methods in app/services/health_service.py:\n```python\nclass HealthService:\n    # ... existing code ...\n    \n    async def get_recent_error_count(self) -> int:\n        \"\"\"Get count of errors in the last 24 hours\"\"\"\n        try:\n            # Query error logs from database\n            return await self.db.fetch_error_count(hours=24)\n        except Exception:\n            return -1  # Indicate error retrieving count\n    \n    async def get_pending_content_sets_count(self) -> int:\n        \"\"\"Get count of content sets waiting to be processed\"\"\"\n        try:\n            return await self.db.fetch_content_sets_count(status=\"pending\")\n        except Exception:\n            return -1\n    \n    async def get_active_processing_count(self) -> int:\n        \"\"\"Get count of content sets currently being processed\"\"\"\n        try:\n            return await self.db.fetch_content_sets_count(status=\"processing\")\n        except Exception:\n            return -1\n    \n    async def check_supabase_connectivity(self) -> bool:\n        \"\"\"Check if Supabase connection is working\"\"\"\n        try:\n            # Simple query to verify connection\n            await self.db.execute(\"SELECT 1\")\n            return True\n        except Exception:\n            return False\n    \n    async def check_neo4j_connectivity(self) -> bool:\n        \"\"\"Check if Neo4j connection is working\"\"\"\n        try:\n            # Simple query to verify connection\n            result = await self.neo4j_client.run(\"MATCH (n) RETURN count(n) LIMIT 1\")\n            return result is not None\n        except Exception:\n            return False\n```\n\n4. Implement the utility functions in app/utils/system.py:\n```python\ndef get_version() -> str:\n    \"\"\"Get the current version of the application\"\"\"\n    try:\n        with open(\"VERSION\", \"r\") as f:\n            return f.read().strip()\n    except Exception:\n        return \"unknown\"\n\ndef calculate_uptime() -> int:\n    \"\"\"Calculate uptime in seconds since application start\"\"\"\n    global _start_time\n    return int(time.time() - _start_time)\n```\n\n5. Update the application startup to record start time:\n```python\n# In app/main.py\nimport time\n\n_start_time = time.time()\n\ndef start_application():\n    # ... existing code ...\n```",
        "testStrategy": "1. Create unit tests in tests/routes/test_content_prep.py:\n```python\nasync def test_health_endpoint_returns_correct_structure():\n    # Arrange\n    client = TestClient(app)\n    \n    # Act\n    response = client.get(\"/api/content-prep/health\")\n    \n    # Assert\n    assert response.status_code == 200\n    data = response.json()\n    assert \"status\" in data\n    assert \"agent\" in data\n    assert \"metrics\" in data\n    assert \"connectivity\" in data\n    \n    # Check agent info\n    assert \"agent_id\" in data[\"agent\"]\n    assert data[\"agent\"][\"agent_id\"] == \"AGENT-016\"\n    assert \"version\" in data[\"agent\"]\n    assert \"uptime\" in data[\"agent\"]\n    \n    # Check metrics\n    assert \"recent_error_count\" in data[\"metrics\"]\n    assert \"pending_content_sets\" in data[\"metrics\"]\n    assert \"active_processing_count\" in data[\"metrics\"]\n    \n    # Check connectivity\n    assert \"supabase\" in data[\"connectivity\"]\n    assert \"neo4j\" in data[\"connectivity\"]\n\nasync def test_health_endpoint_with_mocked_services():\n    # Arrange\n    app.dependency_overrides[get_health_service] = lambda: MockHealthService(\n        supabase_healthy=True,\n        neo4j_healthy=True,\n        error_count=5,\n        pending_sets=10,\n        active_processing=3\n    )\n    client = TestClient(app)\n    \n    # Act\n    response = client.get(\"/api/content-prep/health\")\n    \n    # Assert\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"status\"] == \"healthy\"\n    assert data[\"metrics\"][\"recent_error_count\"] == 5\n    assert data[\"metrics\"][\"pending_content_sets\"] == 10\n    assert data[\"metrics\"][\"active_processing_count\"] == 3\n    assert data[\"connectivity\"][\"supabase\"] is True\n    assert data[\"connectivity\"][\"neo4j\"] is True\n    \n    # Clean up\n    app.dependency_overrides.clear()\n\nasync def test_health_endpoint_with_degraded_services():\n    # Arrange\n    app.dependency_overrides[get_health_service] = lambda: MockHealthService(\n        supabase_healthy=True,\n        neo4j_healthy=False,\n        error_count=5,\n        pending_sets=10,\n        active_processing=3\n    )\n    client = TestClient(app)\n    \n    # Act\n    response = client.get(\"/api/content-prep/health\")\n    \n    # Assert\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"status\"] == \"degraded\"\n    assert data[\"connectivity\"][\"supabase\"] is True\n    assert data[\"connectivity\"][\"neo4j\"] is False\n    \n    # Clean up\n    app.dependency_overrides.clear()\n```\n\n2. Create the MockHealthService class in tests/mocks/health_service.py:\n```python\nclass MockHealthService:\n    def __init__(self, supabase_healthy=True, neo4j_healthy=True, \n                 error_count=0, pending_sets=0, active_processing=0):\n        self.supabase_healthy = supabase_healthy\n        self.neo4j_healthy = neo4j_healthy\n        self.error_count = error_count\n        self.pending_sets = pending_sets\n        self.active_processing = active_processing\n    \n    async def get_recent_error_count(self) -> int:\n        return self.error_count\n    \n    async def get_pending_content_sets_count(self) -> int:\n        return self.pending_sets\n    \n    async def get_active_processing_count(self) -> int:\n        return self.active_processing\n    \n    async def check_supabase_connectivity(self) -> bool:\n        return self.supabase_healthy\n    \n    async def check_neo4j_connectivity(self) -> bool:\n        return self.neo4j_healthy\n```\n\n3. Perform integration testing:\n   - Test the endpoint with actual database connections\n   - Verify correct status reporting when services are unavailable\n   - Test with various load conditions to ensure metrics are accurate\n\n4. Verify the endpoint follows the pattern established by other agent health endpoints by comparing the response structure with existing health endpoints.",
        "status": "done",
        "dependencies": [
          "122",
          "124"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-13T22:34:55.721Z"
      },
      {
        "id": "141",
        "title": "Design and Implement RAG Quality Metrics Database Schema",
        "description": "Create the database tables and indexes required to store RAG quality metrics, agent performance history, retrieval parameter configurations, and grounding results.",
        "details": "Implement the following tables in Supabase:\n- rag_quality_metrics (with indexes on intent_type, agent_id, created_at, and quality scores)\n- agent_performance_history (with indexes on agent_id and task_type)\n- retrieval_parameter_configs (with unique constraint on intent_type and query_complexity)\n- grounding_results (with indexes on query_id and grounding_score)\nEnsure all tables use UUID primary keys and appropriate foreign key relationships where needed.",
        "testStrategy": "Validate schema creation with SQL scripts. Confirm indexes are created and test insert/query performance with sample data.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:33:44.923Z"
      },
      {
        "id": "142",
        "title": "Implement Query Intent Analyzer Service",
        "description": "Develop a service that classifies user queries into intent types and extracts relevant metadata.",
        "details": "Create app/services/query_intent_analyzer.py with a class that:\n- Classifies queries as factual, analytical, comparative, procedural, or creative\n- Extracts entities and calculates complexity score (0-1)\n- Suggests optimal retrieval strategy and expected output format\n- Uses lightweight LLM (Claude Haiku) for classification and entity extraction\n- Returns QueryIntent object with all required fields",
        "testStrategy": "Test with labeled query dataset. Validate classification accuracy (>90%) and entity extraction completeness.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:37:04.909Z"
      },
      {
        "id": "143",
        "title": "Implement Retrieval Evaluator Service with RAGAS",
        "description": "Build a service that evaluates retrieval quality using RAGAS metrics.",
        "details": "Create app/services/retrieval_evaluator.py with:\n- Real-time evaluation for high-value queries using Claude 3.5 Haiku\n- Batch evaluation with 10% sampling for high-volume queries using Anthropic Batch API\n- Storage of metrics in rag_quality_metrics table\n- Configurable quality thresholds and retry logic (expanded retrieval, low confidence warning)\n- /api/rag/metrics endpoint for admin access",
        "testStrategy": "Test with various query types. Validate metric calculation, storage, and threshold enforcement. Confirm batch vs real-time evaluation works as specified.",
        "priority": "high",
        "dependencies": [
          "141"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:39:13.312Z"
      },
      {
        "id": "144",
        "title": "Implement Answer Grounding Evaluator Service",
        "description": "Develop a service that verifies answer claims against source documents to prevent hallucinations.",
        "details": "Create app/services/answer_grounding_evaluator.py with:\n- Claim extraction from answers using LLM\n- Claim-to-source alignment scoring (0-1)\n- Calculation of overall grounding score and confidence level\n- Flagging of ungrounded claims\n- Storage of results in grounding_results table\n- Blocking of answers below critical grounding threshold",
        "testStrategy": "Test with answers containing known hallucinations. Validate claim extraction, grounding scoring, and blocking logic.",
        "priority": "high",
        "dependencies": [
          "141"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:41:24.087Z"
      },
      {
        "id": "145",
        "title": "Implement Adaptive Retrieval Service",
        "description": "Build a service that dynamically adjusts retrieval parameters based on query characteristics and historical performance.",
        "details": "Create app/services/adaptive_retrieval_service.py with:\n- Retrieval parameter configuration per intent type and complexity\n- Adjustment of weights (dense, sparse, fuzzy), top_k, rerank_threshold, graph_expansion_depth\n- Feedback recording and parameter optimization\n- Manual override capability\n- Audit logging of parameter decisions",
        "testStrategy": "Test with different query types and feedback scenarios. Validate parameter adjustment and optimization logic.",
        "priority": "medium",
        "dependencies": [
          "141",
          "142"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:44:12.061Z"
      },
      {
        "id": "146",
        "title": "Implement Agent Selector Service",
        "description": "Develop a service that intelligently routes tasks to optimal agents based on capabilities and performance history.",
        "details": "Create app/services/agent_selector_service.py with:\n- Agent capability mapping for all 17 agents\n- Historical performance tracking per agent per task type\n- Optimal agent selection based on performance history\n- Exploration factor for underutilized agents\n- Selection explanation for transparency\n- Outcome recording for future optimization",
        "testStrategy": "Test with various task types and agent performance scenarios. Validate selection logic and exploration factor.",
        "priority": "medium",
        "dependencies": [
          "141",
          "142"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:46:27.280Z"
      },
      {
        "id": "147",
        "title": "Implement Output Validator Service",
        "description": "Build a service that validates and corrects agent outputs before delivery.",
        "details": "Create app/services/output_validator_service.py with:\n- Format compliance checking (JSON, markdown, etc.)\n- Completeness validation (required sections present)\n- Consistency checking (no internal contradictions)\n- Style guideline enforcement\n- Auto-correction of formatting issues\n- Flagging of uncorrectable issues for human review",
        "testStrategy": "Test with outputs containing various issues. Validate detection, correction, and flagging logic.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:48:43.869Z"
      },
      {
        "id": "148",
        "title": "Integrate Services into RAG Pipeline",
        "description": "Connect all enhancement services into the existing RAG pipeline.",
        "details": "Update the RAG pipeline to:\n- Route queries through Query Intent Analyzer\n- Use Adaptive Retrieval Service for parameter adjustment\n- Evaluate retrieval quality with Retrieval Evaluator\n- Select agent with Agent Selector Service\n- Validate answer grounding with Answer Grounding Evaluator\n- Validate output with Output Validator Service\n- Ensure proper error handling and fallback mechanisms",
        "testStrategy": "Test end-to-end pipeline with various query types. Validate service integration and error handling.",
        "priority": "high",
        "dependencies": [
          "142",
          "143",
          "144",
          "145",
          "146",
          "147"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:50:59.604Z"
      },
      {
        "id": "149",
        "title": "Develop Metrics Dashboard",
        "description": "Create a dashboard for administrators to monitor RAG quality metrics.",
        "details": "Build a web interface that:\n- Displays real-time RAGAS scores (Context Relevance, Answer Relevance, Faithfulness, Coverage)\n- Allows drill-down by query type, time period, and agent\n- Shows trends and optimization opportunities\n- Loads within 2 seconds with 30-day data",
        "testStrategy": "Test dashboard performance and data accuracy with sample metrics data.",
        "priority": "medium",
        "dependencies": [
          "143"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:54:03.894Z"
      },
      {
        "id": "150",
        "title": "Implement Quality Gate and Fallback Logic",
        "description": "Add logic to handle quality gate failures and provide fallback options.",
        "details": "Implement:\n- Retry with expanded retrieval (increased top_k, graph expansion) when quality threshold breached\n- Visible 'low confidence' warning if still below threshold\n- Logging of quality gate failures and fallback actions\n- Alert system for persistent quality issues",
        "testStrategy": "Test with queries that trigger quality gate failures. Validate retry logic and warning display.",
        "priority": "medium",
        "dependencies": [
          "143",
          "144"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:56:19.646Z"
      },
      {
        "id": "151",
        "title": "Implement PostgreSQL Session Variables for Row-Level Security",
        "description": "Create PostgreSQL functions for managing row-level security context, including set_rls_context, get_rls_context, clear_rls_context, current_user_id, and session_has_role functions. Update middleware to call these functions via Supabase RPC.",
        "details": "Implement the following PostgreSQL functions in a new migration file `migrations/rls_context_functions.sql`:\n\n1. `set_rls_context(key text, value text)`: Sets a session variable for RLS context\n   ```sql\n   CREATE OR REPLACE FUNCTION set_rls_context(key text, value text)\n   RETURNS void AS $$\n   BEGIN\n     PERFORM set_config('app.rls_' || key, value, false);\n   EXCEPTION\n     WHEN OTHERS THEN\n       RAISE EXCEPTION 'Failed to set RLS context: %', SQLERRM;\n   END;\n   $$ LANGUAGE plpgsql SECURITY DEFINER;\n   ```\n\n2. `get_rls_context(key text)`: Retrieves a session variable\n   ```sql\n   CREATE OR REPLACE FUNCTION get_rls_context(key text)\n   RETURNS text AS $$\n   BEGIN\n     RETURN current_setting('app.rls_' || key, true);\n   EXCEPTION\n     WHEN OTHERS THEN\n       RETURN NULL;\n   END;\n   $$ LANGUAGE plpgsql SECURITY DEFINER;\n   ```\n\n3. `clear_rls_context()`: Clears all RLS context variables\n   ```sql\n   CREATE OR REPLACE FUNCTION clear_rls_context()\n   RETURNS void AS $$\n   DECLARE\n     setting_name text;\n   BEGIN\n     FOR setting_name IN \n       SELECT name FROM pg_settings WHERE name LIKE 'app.rls_%'\n     LOOP\n       PERFORM set_config(setting_name, NULL, false);\n     END LOOP;\n   EXCEPTION\n     WHEN OTHERS THEN\n       RAISE EXCEPTION 'Failed to clear RLS context: %', SQLERRM;\n   END;\n   $$ LANGUAGE plpgsql SECURITY DEFINER;\n   ```\n\n4. `current_user_id()`: Returns the current user ID from context\n   ```sql\n   CREATE OR REPLACE FUNCTION current_user_id()\n   RETURNS uuid AS $$\n   BEGIN\n     RETURN get_rls_context('user_id')::uuid;\n   EXCEPTION\n     WHEN OTHERS THEN\n       RETURN NULL;\n   END;\n   $$ LANGUAGE plpgsql SECURITY DEFINER;\n   ```\n\n5. `session_has_role(role_name text)`: Checks if the current session has a specific role\n   ```sql\n   CREATE OR REPLACE FUNCTION session_has_role(role_name text)\n   RETURNS boolean AS $$\n   DECLARE\n     roles text;\n   BEGIN\n     roles := get_rls_context('roles');\n     RETURN roles LIKE '%' || role_name || '%';\n   EXCEPTION\n     WHEN OTHERS THEN\n       RETURN false;\n   END;\n   $$ LANGUAGE plpgsql SECURITY DEFINER;\n   ```\n\nNext, update the middleware in `app/middleware/auth.py` to set these context variables via Supabase RPC calls:\n\n```python\nfrom fastapi import Request, Response\nfrom app.db.supabase import get_supabase_client\n\nasync def rls_middleware(request: Request, call_next):\n    # Extract user information from request (JWT token)\n    user_id = extract_user_id(request)\n    user_roles = extract_user_roles(request)\n    \n    try:\n        # Get Supabase client\n        supabase = get_supabase_client()\n        \n        # Set RLS context via RPC\n        if user_id:\n            await supabase.rpc('set_rls_context', {'key': 'user_id', 'value': str(user_id)})\n            \n        if user_roles:\n            roles_str = ','.join(user_roles)\n            await supabase.rpc('set_rls_context', {'key': 'roles', 'value': roles_str})\n        \n        # Process the request\n        response = await call_next(request)\n        \n        # Clear RLS context after request is processed\n        await supabase.rpc('clear_rls_context')\n        \n        return response\n    except Exception as e:\n        # Security-first error handling - return 503 on failure\n        return Response(\n            content={\"error\": \"Service temporarily unavailable\"},\n            status_code=503,\n            media_type=\"application/json\"\n        )\n```\n\nFinally, register the middleware in the main FastAPI application:\n\n```python\n# In app/main.py\nfrom app.middleware.auth import rls_middleware\n\napp = FastAPI()\napp.middleware(\"http\")(rls_middleware)\n```",
        "testStrategy": "1. **Unit Tests for PostgreSQL Functions**:\n   - Create a test file `tests/db/test_rls_functions.sql` to test each function individually\n   - Test `set_rls_context` with various key-value pairs\n   - Test `get_rls_context` retrieves the correct values\n   - Test `clear_rls_context` properly removes all context variables\n   - Test `current_user_id` returns the expected UUID\n   - Test `session_has_role` correctly identifies roles in the context\n\n2. **Integration Tests for Middleware**:\n   - Create a test file `tests/middleware/test_rls_middleware.py`\n   - Test middleware correctly extracts user information from requests\n   - Test middleware makes appropriate RPC calls to Supabase\n   - Test middleware clears context after request processing\n   - Test error handling returns 503 status code on failure\n\n3. **Security Tests**:\n   - Verify RLS policies correctly use the context variables\n   - Test that unauthorized access is properly prevented\n   - Test that context variables are properly isolated between concurrent requests\n   - Verify SQL injection prevention in the functions\n\n4. **Performance Tests**:\n   - Measure overhead of setting and clearing context variables\n   - Test with concurrent requests to ensure no context leakage\n   - Benchmark database queries with RLS enabled vs. disabled\n\n5. **End-to-End Tests**:\n   - Create test scenarios that use RLS to restrict data access\n   - Verify different user roles see appropriate data\n   - Test API endpoints that rely on RLS for data filtering",
        "status": "done",
        "dependencies": [
          "101",
          "123"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T20:19:52.791Z"
      },
      {
        "id": "152",
        "title": "WebSocket Authentication with JWT Validation",
        "description": "Implement JWT validation for WebSocket connections, add token refresh mechanism for long-running connections, implement connection timeout and heartbeat functionality, and create integration tests.",
        "details": "Implement secure WebSocket authentication with the following components:\n\n1. JWT Validation for WebSocket Connections:\n   - Create a middleware for validating JWT tokens during WebSocket handshake\n   - Implement token extraction from the connection request headers or query parameters\n   - Verify token signature, expiration, and claims\n   - Store authenticated user information in the WebSocket connection context\n\n```python\n# In app/middleware/websocket_auth.py\nfrom fastapi import WebSocket, status\nfrom jose import jwt, JWTError\nfrom app.core.config import settings\nfrom app.core.security import ALGORITHM\n\nasync def websocket_auth_middleware(websocket: WebSocket):\n    try:\n        # Extract token from query params or headers\n        token = websocket.query_params.get(\"token\") or websocket.headers.get(\"Authorization\", \"\").replace(\"Bearer \", \"\")\n        \n        if not token:\n            await websocket.close(code=status.WS_1008_POLICY_VIOLATION, reason=\"Missing authentication token\")\n            return None\n            \n        # Validate token\n        payload = jwt.decode(token, settings.SECRET_KEY, algorithms=[ALGORITHM])\n        user_id = payload.get(\"sub\")\n        \n        if not user_id:\n            await websocket.close(code=status.WS_1008_POLICY_VIOLATION, reason=\"Invalid token payload\")\n            return None\n            \n        # Store user info in connection state\n        websocket.state.user_id = user_id\n        return user_id\n    except JWTError:\n        await websocket.close(code=status.WS_1008_POLICY_VIOLATION, reason=\"Invalid authentication token\")\n        return None\n```\n\n2. Token Refresh Mechanism:\n   - Implement a token refresh protocol for WebSocket connections\n   - Create a refresh event type that clients can send before token expiration\n   - Implement server-side refresh logic that validates the current token and issues a new one\n   - Send the new token back to the client through the WebSocket connection\n\n```python\n# In app/websockets/handlers.py\nfrom fastapi import WebSocket\nfrom app.core.security import create_access_token\nfrom app.models.user import User\nfrom datetime import datetime, timedelta\nimport json\n\nasync def handle_token_refresh(websocket: WebSocket, data: dict):\n    user_id = websocket.state.user_id\n    \n    # Get user from database to ensure they still exist and are active\n    user = await User.get(user_id)\n    if not user or not user.is_active:\n        await websocket.close(code=status.WS_1008_POLICY_VIOLATION, reason=\"User no longer valid\")\n        return\n        \n    # Create new token\n    new_token = create_access_token(\n        data={\"sub\": user_id},\n        expires_delta=timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)\n    )\n    \n    # Send new token to client\n    await websocket.send_text(json.dumps({\n        \"event\": \"token_refresh\",\n        \"data\": {\"token\": new_token}\n    }))\n```\n\n3. Connection Timeout and Heartbeat:\n   - Implement a heartbeat mechanism to detect stale connections\n   - Set up a configurable connection timeout (default: 60 seconds)\n   - Create a background task for each connection to monitor heartbeat status\n   - Implement client-side ping messages and server-side pong responses\n\n```python\n# In app/websockets/connection_manager.py\nimport asyncio\nfrom fastapi import WebSocket\nfrom typing import Dict, Set\nimport time\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: Dict[str, WebSocket] = {}\n        self.last_heartbeat: Dict[str, float] = {}\n        self.heartbeat_interval = 30  # seconds\n        self.connection_timeout = 60  # seconds\n        \n    async def connect(self, websocket: WebSocket, user_id: str):\n        await websocket.accept()\n        self.active_connections[user_id] = websocket\n        self.last_heartbeat[user_id] = time.time()\n        \n        # Start heartbeat monitor\n        asyncio.create_task(self._heartbeat_monitor(user_id))\n        \n    async def disconnect(self, user_id: str):\n        if user_id in self.active_connections:\n            del self.active_connections[user_id]\n        if user_id in self.last_heartbeat:\n            del self.last_heartbeat[user_id]\n            \n    async def handle_heartbeat(self, user_id: str):\n        self.last_heartbeat[user_id] = time.time()\n        if user_id in self.active_connections:\n            await self.active_connections[user_id].send_text(json.dumps({\n                \"event\": \"pong\",\n                \"timestamp\": time.time()\n            }))\n            \n    async def _heartbeat_monitor(self, user_id: str):\n        while user_id in self.active_connections:\n            await asyncio.sleep(5)  # Check every 5 seconds\n            \n            if user_id not in self.last_heartbeat:\n                break\n                \n            elapsed = time.time() - self.last_heartbeat[user_id]\n            if elapsed > self.connection_timeout:\n                # Connection timed out\n                if user_id in self.active_connections:\n                    await self.active_connections[user_id].close(\n                        code=status.WS_1008_POLICY_VIOLATION,\n                        reason=\"Connection timeout\"\n                    )\n                await self.disconnect(user_id)\n                break\n```\n\n4. WebSocket Route Implementation:\n   - Create WebSocket endpoint with authentication middleware\n   - Implement message handling with event routing\n   - Add support for the token refresh and heartbeat events\n\n```python\n# In app/routes/websocket.py\nfrom fastapi import APIRouter, WebSocket, Depends\nfrom app.middleware.websocket_auth import websocket_auth_middleware\nfrom app.websockets.connection_manager import ConnectionManager\nimport json\n\nrouter = APIRouter()\nconnection_manager = ConnectionManager()\n\n@router.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    user_id = await websocket_auth_middleware(websocket)\n    if not user_id:\n        return  # Connection was closed by middleware\n        \n    await connection_manager.connect(websocket, user_id)\n    \n    try:\n        while True:\n            data = await websocket.receive_text()\n            message = json.loads(data)\n            \n            event_type = message.get(\"event\")\n            event_data = message.get(\"data\", {})\n            \n            if event_type == \"heartbeat\":\n                await connection_manager.handle_heartbeat(user_id)\n            elif event_type == \"refresh_token\":\n                await handle_token_refresh(websocket, event_data)\n            else:\n                # Handle other message types\n                pass\n                \n    except Exception as e:\n        # Log the error\n        pass\n    finally:\n        await connection_manager.disconnect(user_id)\n```\n\n5. Client-Side Implementation Guidance:\n   - Provide example client code for handling token refresh and heartbeat\n   - Document the WebSocket protocol for authentication and token refresh\n   - Include error handling recommendations",
        "testStrategy": "1. Unit Tests:\n   - Test JWT validation with valid and invalid tokens\n   - Test token extraction from different sources (headers, query params)\n   - Test token refresh logic with various token states (valid, near-expiry, expired)\n   - Test heartbeat mechanism with simulated timeouts\n   - Test connection manager with multiple concurrent connections\n\n2. Integration Tests:\n   - Create a test WebSocket client that connects to the server\n   - Test the full authentication flow from connection to token refresh\n   - Test heartbeat mechanism with real timing\n   - Test connection timeout by deliberately missing heartbeats\n   - Test reconnection scenarios after disconnection\n\n3. Security Tests:\n   - Test with tampered JWT tokens to ensure proper validation\n   - Test with expired tokens to verify rejection\n   - Test token refresh with invalid refresh attempts\n   - Verify that unauthenticated connections are properly rejected\n   - Test rate limiting for connection attempts and token refreshes\n\n4. Load Tests:\n   - Test with multiple concurrent WebSocket connections\n   - Measure performance under load for token validation and refresh\n   - Test heartbeat mechanism with many connections\n   - Verify resource cleanup after disconnections\n\n5. End-to-End Tests:\n   - Create a test client application that uses the WebSocket connection\n   - Test the complete user flow including authentication, heartbeat, and token refresh\n   - Verify proper handling of network interruptions\n   - Test long-running connections with multiple token refreshes\n\n6. Test Automation:\n   - Create automated tests that can be run in CI/CD pipeline\n   - Implement test fixtures for WebSocket connections\n   - Create mock authentication services for testing\n   - Document test coverage and results",
        "status": "done",
        "dependencies": [
          "137",
          "110"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-15T05:23:50.170Z"
      },
      {
        "id": "153",
        "title": "Neo4j Graph Sync Error Handling",
        "description": "Implement robust error handling for Neo4j graph synchronization operations including retry logic with exponential backoff, circuit breaker pattern, and a dead letter queue for failed operations.",
        "details": "Enhance the Neo4j synchronization system with comprehensive error handling:\n\n1. Implement retry logic with exponential backoff:\n   ```python\n   from tenacity import retry, stop_after_attempt, wait_exponential\n   \n   @retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=2, max=60))\n   async def perform_graph_sync_operation(operation_data):\n       # Existing sync operation code\n       try:\n           result = await neo4j_client.execute_query(operation_data.query, operation_data.params)\n           return result\n       except Exception as e:\n           logger.error(f\"Graph sync operation failed: {str(e)}\")\n           raise  # Let tenacity handle the retry\n   ```\n\n2. Implement circuit breaker pattern:\n   ```python\n   from circuitbreaker import circuit\n   \n   # Configure the circuit breaker\n   @circuit(failure_threshold=5, recovery_timeout=60, expected_exception=Neo4jConnectionError)\n   async def protected_graph_operation(operation_data):\n       # Existing operation code\n       return await neo4j_client.execute_query(operation_data.query, operation_data.params)\n   ```\n\n3. Create a dead letter queue for failed operations:\n   ```python\n   class DeadLetterQueue:\n       def __init__(self, redis_client):\n           self.redis = redis_client\n           self.queue_key = \"neo4j:sync:dead_letter_queue\"\n           \n       async def add_failed_operation(self, operation_data, error_info):\n           entry = {\n               \"operation_id\": str(uuid.uuid4()),\n               \"timestamp\": datetime.utcnow().isoformat(),\n               \"operation_data\": operation_data,\n               \"error_info\": error_info,\n               \"retry_count\": 0\n           }\n           await self.redis.lpush(self.queue_key, json.dumps(entry))\n           \n       async def get_failed_operations(self, limit=100):\n           # Retrieve operations for manual inspection or retry\n           operations = await self.redis.lrange(self.queue_key, 0, limit-1)\n           return [json.loads(op) for op in operations]\n           \n       async def remove_operation(self, operation_id):\n           # Remove after successful manual processing\n           operations = await self.get_failed_operations()\n           for i, op in enumerate(operations):\n               if op[\"operation_id\"] == operation_id:\n                   await self.redis.lrem(self.queue_key, 1, json.dumps(op))\n                   return True\n           return False\n   ```\n\n4. Implement monitoring alerts:\n   ```python\n   class GraphSyncMonitor:\n       def __init__(self, alert_service):\n           self.alert_service = alert_service\n           self.failure_counts = {}\n           \n       async def record_operation_result(self, operation_type, success):\n           # Track success/failure rates\n           if operation_type not in self.failure_counts:\n               self.failure_counts[operation_type] = {\"success\": 0, \"failure\": 0}\n               \n           if success:\n               self.failure_counts[operation_type][\"success\"] += 1\n           else:\n               self.failure_counts[operation_type][\"failure\"] += 1\n               \n           # Check if alert threshold is reached\n           self._check_alert_threshold(operation_type)\n           \n       def _check_alert_threshold(self, operation_type):\n           stats = self.failure_counts[operation_type]\n           total = stats[\"success\"] + stats[\"failure\"]\n           \n           if total >= 10:  # Minimum sample size\n               failure_rate = stats[\"failure\"] / total\n               \n               if failure_rate > 0.2:  # 20% failure rate threshold\n                   self.alert_service.send_alert(\n                       level=\"warning\",\n                       title=f\"High Neo4j sync failure rate for {operation_type}\",\n                       message=f\"Failure rate of {failure_rate:.1%} detected for {operation_type} operations\",\n                       metadata={\"operation_type\": operation_type, \"stats\": stats}\n                   )\n   ```\n\n5. Integration with existing Neo4j client:\n   ```python\n   class EnhancedNeo4jClient:\n       def __init__(self, base_client, dead_letter_queue, monitor):\n           self.base_client = base_client\n           self.dlq = dead_letter_queue\n           self.monitor = monitor\n           \n       @circuit(failure_threshold=5, recovery_timeout=60)\n       @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=30))\n       async def execute_sync_operation(self, operation_type, query, params):\n           try:\n               result = await self.base_client.execute_query(query, params)\n               await self.monitor.record_operation_result(operation_type, success=True)\n               return result\n           except Exception as e:\n               await self.monitor.record_operation_result(operation_type, success=False)\n               await self.dlq.add_failed_operation(\n                   {\"type\": operation_type, \"query\": query, \"params\": params},\n                   {\"error\": str(e), \"traceback\": traceback.format_exc()}\n               )\n               raise\n   ```\n\n6. Create a background worker for processing the dead letter queue:\n   ```python\n   @shared_task\n   async def process_dead_letter_queue():\n       dlq = DeadLetterQueue(get_redis_client())\n       client = Neo4jHTTPClient()\n       \n       failed_ops = await dlq.get_failed_operations(limit=50)\n       for op in failed_ops:\n           if op[\"retry_count\"] < 5:  # Max retry attempts\n               try:\n                   # Attempt to reprocess\n                   await client.execute_query(op[\"operation_data\"][\"query\"], op[\"operation_data\"][\"params\"])\n                   # If successful, remove from queue\n                   await dlq.remove_operation(op[\"operation_id\"])\n               except Exception as e:\n                   # Update retry count and push back to queue\n                   op[\"retry_count\"] += 1\n                   op[\"last_error\"] = str(e)\n                   await dlq.remove_operation(op[\"operation_id\"])\n                   await dlq.add_failed_operation(op[\"operation_data\"], \n                                                {\"error\": str(e), \"retry_count\": op[\"retry_count\"]})\n   ```",
        "testStrategy": "1. Unit tests for retry logic:\n   - Test successful operation after temporary failures\n   - Test that retry count respects maximum attempts\n   - Verify exponential backoff timing between retries\n   - Test behavior when max retries are exhausted\n\n2. Unit tests for circuit breaker:\n   - Test circuit transitions from closed to open state after threshold failures\n   - Test that requests fail fast when circuit is open\n   - Test half-open state behavior after recovery timeout\n   - Test circuit closing after successful operations in half-open state\n\n3. Unit tests for dead letter queue:\n   - Test adding failed operations to the queue\n   - Test retrieving operations from the queue\n   - Test removing operations from the queue\n   - Test queue persistence across application restarts\n\n4. Integration tests:\n   - Test end-to-end flow with simulated Neo4j failures\n   - Verify operations eventually succeed after temporary failures\n   - Verify operations go to dead letter queue after permanent failures\n   - Test background worker processing of dead letter queue\n\n5. Monitoring and alerting tests:\n   - Test alert generation when failure thresholds are reached\n   - Test alert suppression during circuit open state\n   - Verify metrics are correctly updated for success/failure counts\n   - Test alert resolution when failure rates return to normal\n\n6. Performance tests:\n   - Measure impact of retry logic on system performance\n   - Test system behavior under high load with partial Neo4j outages\n   - Verify dead letter queue performance with large numbers of failed operations\n\n7. Chaos testing:\n   - Simulate Neo4j service outages and verify system resilience\n   - Test recovery behavior after Neo4j service restoration\n   - Verify no data loss during outage scenarios",
        "status": "done",
        "dependencies": [
          "101",
          "110",
          "137"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-15T05:44:15.804Z"
      },
      {
        "id": "154",
        "title": "Implement Standardized Exception Handling Framework",
        "description": "Create a comprehensive exception handling framework with a custom exception hierarchy, global middleware, structured error responses with request_id tracking, and contextual error logging.",
        "details": "Implement a standardized exception handling framework to improve error management across the application:\n\n1. Create a custom exception hierarchy in `app/exceptions/`:\n   ```python\n   # app/exceptions/base.py\n   from typing import Optional, Dict, Any\n   \n   class BaseAppException(Exception):\n       \"\"\"Base exception class for all application exceptions\"\"\"\n       def __init__(\n           self, \n           message: str, \n           error_code: str = \"INTERNAL_ERROR\",\n           status_code: int = 500,\n           details: Optional[Dict[str, Any]] = None\n       ):\n           self.message = message\n           self.error_code = error_code\n           self.status_code = status_code\n           self.details = details or {}\n           super().__init__(self.message)\n   \n   # app/exceptions/client_errors.py\n   from .base import BaseAppException\n   \n   class BadRequestException(BaseAppException):\n       \"\"\"Exception for invalid request data\"\"\"\n       def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):\n           super().__init__(\n               message=message,\n               error_code=\"BAD_REQUEST\",\n               status_code=400,\n               details=details\n           )\n   \n   class NotFoundException(BaseAppException):\n       \"\"\"Exception for resource not found\"\"\"\n       def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):\n           super().__init__(\n               message=message,\n               error_code=\"NOT_FOUND\",\n               status_code=404,\n               details=details\n           )\n   \n   # app/exceptions/server_errors.py\n   from .base import BaseAppException\n   \n   class DatabaseException(BaseAppException):\n       \"\"\"Exception for database errors\"\"\"\n       def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):\n           super().__init__(\n               message=message,\n               error_code=\"DATABASE_ERROR\",\n               status_code=500,\n               details=details\n           )\n   ```\n\n2. Implement a global exception handler middleware in `app/middleware/exception_handler.py`:\n   ```python\n   import uuid\n   import logging\n   import traceback\n   from fastapi import Request, Response\n   from fastapi.responses import JSONResponse\n   from starlette.middleware.base import BaseHTTPMiddleware\n   \n   from app.exceptions.base import BaseAppException\n   from app.models.errors import AgentErrorResponse, ErrorType\n   \n   logger = logging.getLogger(__name__)\n   \n   class ExceptionHandlerMiddleware(BaseHTTPMiddleware):\n       async def dispatch(self, request: Request, call_next):\n           # Generate unique request ID for tracking\n           request_id = str(uuid.uuid4())\n           request.state.request_id = request_id\n           \n           try:\n               response = await call_next(request)\n               return response\n           except BaseAppException as exc:\n               # Handle custom application exceptions\n               return self._handle_app_exception(exc, request)\n           except Exception as exc:\n               # Handle unexpected exceptions\n               return self._handle_unexpected_exception(exc, request)\n       \n       def _handle_app_exception(self, exc: BaseAppException, request: Request) -> JSONResponse:\n           logger.error(\n               f\"Application exception: {exc.error_code} - {exc.message}\",\n               extra={\n                   \"request_id\": request.state.request_id,\n                   \"path\": request.url.path,\n                   \"method\": request.method,\n                   \"details\": exc.details,\n                   \"error_code\": exc.error_code\n               }\n           )\n           \n           return JSONResponse(\n               status_code=exc.status_code,\n               content=AgentErrorResponse(\n                   error_code=exc.error_code,\n                   error_type=ErrorType.PERMANENT if exc.status_code >= 400 and exc.status_code < 500 else ErrorType.RETRIABLE,\n                   agent_id=request.app.state.agent_id,\n                   message=exc.message,\n                   details=exc.details,\n                   request_id=request.state.request_id\n               ).dict()\n           )\n       \n       def _handle_unexpected_exception(self, exc: Exception, request: Request) -> JSONResponse:\n           # Log full traceback for unexpected exceptions\n           error_details = {\n               \"traceback\": traceback.format_exc(),\n               \"exception_type\": exc.__class__.__name__\n           }\n           \n           logger.error(\n               f\"Unexpected exception: {str(exc)}\",\n               extra={\n                   \"request_id\": request.state.request_id,\n                   \"path\": request.url.path,\n                   \"method\": request.method,\n                   \"details\": error_details\n               },\n               exc_info=True\n           )\n           \n           return JSONResponse(\n               status_code=500,\n               content=AgentErrorResponse(\n                   error_code=\"INTERNAL_SERVER_ERROR\",\n                   error_type=ErrorType.RETRIABLE,\n                   agent_id=request.app.state.agent_id,\n                   message=\"An unexpected error occurred\",\n                   details={\"request_id\": request.state.request_id},\n                   request_id=request.state.request_id\n               ).dict()\n           )\n   ```\n\n3. Enhance the existing error response model in `app/models/errors.py` to include request_id:\n   ```python\n   from pydantic import BaseModel, Field\n   from typing import Optional, Dict, Any\n   from datetime import datetime\n   from enum import Enum\n   \n   class ErrorType(str, Enum):\n       RETRIABLE = \"retriable\"\n       PERMANENT = \"permanent\"\n   \n   class AgentErrorResponse(BaseModel):\n       error_code: str\n       error_type: ErrorType\n       agent_id: str\n       message: str\n       details: Optional[Dict[str, Any]] = None\n       timestamp: datetime = Field(default_factory=datetime.utcnow)\n       request_id: str\n   ```\n\n4. Create a contextual error logging utility in `app/utils/error_logging.py`:\n   ```python\n   import logging\n   import inspect\n   import uuid\n   from typing import Dict, Any, Optional\n   from fastapi import Request\n   \n   logger = logging.getLogger(__name__)\n   \n   class ErrorLogger:\n       @staticmethod\n       def log_error(\n           error: Exception,\n           request: Optional[Request] = None,\n           context: Optional[Dict[str, Any]] = None,\n           level: int = logging.ERROR\n       ):\n           # Get calling function and module\n           frame = inspect.currentframe().f_back\n           func_name = frame.f_code.co_name\n           module_name = frame.f_globals['__name__']\n           \n           # Build error context\n           error_context = {\n               \"exception_type\": error.__class__.__name__,\n               \"function\": func_name,\n               \"module\": module_name\n           }\n           \n           # Add request context if available\n           if request:\n               request_id = getattr(request.state, \"request_id\", str(uuid.uuid4()))\n               error_context.update({\n                   \"request_id\": request_id,\n                   \"path\": request.url.path,\n                   \"method\": request.method,\n                   \"client_ip\": request.client.host\n               })\n           \n           # Add custom context if provided\n           if context:\n               error_context.update(context)\n           \n           # Log the error with context\n           logger.log(\n               level,\n               f\"{error.__class__.__name__} in {module_name}.{func_name}: {str(error)}\",\n               extra=error_context,\n               exc_info=True\n           )\n   ```\n\n5. Register the middleware in the FastAPI application setup:\n   ```python\n   # app/main.py\n   from fastapi import FastAPI\n   from app.middleware.exception_handler import ExceptionHandlerMiddleware\n   \n   app = FastAPI()\n   app.add_middleware(ExceptionHandlerMiddleware)\n   ```\n\n6. Create usage examples in `app/docs/exception_handling.md` to demonstrate proper usage:\n   ```markdown\n   # Exception Handling Guidelines\n   \n   ## Raising Custom Exceptions\n   \n   ```python\n   from app.exceptions.client_errors import BadRequestException\n   \n   def validate_user_input(data):\n       if not data.get(\"username\"):\n           raise BadRequestException(\n               message=\"Username is required\",\n               details={\"field\": \"username\", \"provided\": data.get(\"username\")}\n           )\n   ```\n   \n   ## Using the Error Logger\n   \n   ```python\n   from fastapi import APIRouter, Request, Depends\n   from app.utils.error_logging import ErrorLogger\n   \n   router = APIRouter()\n   \n   @router.get(\"/items/{item_id}\")\n   async def get_item(item_id: int, request: Request):\n       try:\n           # Attempt to get item\n           item = await db.get_item(item_id)\n           if not item:\n               raise ItemNotFoundException(f\"Item with ID {item_id} not found\")\n           return item\n       except Exception as e:\n           # Log error with context\n           ErrorLogger.log_error(\n               error=e,\n               request=request,\n               context={\"item_id\": item_id}\n           )\n           # Let middleware handle the exception\n           raise\n   ```\n   ```",
        "testStrategy": "1. Unit test the custom exception hierarchy:\n   - Test each exception class to ensure it properly inherits from BaseAppException\n   - Verify that status codes, error codes, and messages are correctly set\n   - Test serialization of exception details\n\n2. Test the ExceptionHandlerMiddleware:\n   - Create test cases for each type of custom exception\n   - Verify correct status codes are returned in responses\n   - Ensure request_id is properly generated and included in responses\n   - Test handling of unexpected exceptions\n   - Verify error response format matches the AgentErrorResponse model\n\n3. Test the enhanced AgentErrorResponse model:\n   - Verify the model correctly validates with and without optional fields\n   - Test serialization/deserialization with request_id included\n   - Ensure timestamp is automatically generated\n\n4. Test the ErrorLogger utility:\n   - Mock the logging system to capture log output\n   - Test logging with and without request context\n   - Verify all context fields are properly included in log entries\n   - Test different logging levels\n\n5. Integration tests:\n   - Create test endpoints that raise different types of exceptions\n   - Verify middleware correctly intercepts and formats all exceptions\n   - Test request_id propagation through the request lifecycle\n   - Verify log entries contain the same request_id as the response\n\n6. Performance tests:\n   - Measure overhead of exception handling middleware\n   - Test with high concurrency to ensure no performance degradation\n\n7. Documentation verification:\n   - Review exception handling documentation for clarity\n   - Ensure all examples work as described\n   - Verify documentation covers all exception types and usage patterns",
        "status": "in-progress",
        "dependencies": [
          "135",
          "153"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-15T05:45:32.754Z"
      },
      {
        "id": "155",
        "title": "Implement Claude Haiku-based Entity Extraction for Research Tasks",
        "description": "Develop a system that uses Claude Haiku to extract entities, topics, and facts from research tasks with structured output and store the extracted information in Neo4j graph database.",
        "details": "Implement a Claude Haiku-based entity extraction system for research tasks:\n\n1. Create a new service class `app/services/entity_extraction_service.py`:\n   ```python\n   from typing import Dict, List, Any, Optional\n   from app.clients.claude_client import ClaudeClient\n   from app.models.research_task import ResearchTask\n   from app.repositories.neo4j_repository import Neo4jRepository\n   \n   class EntityExtractionService:\n       def __init__(self, claude_client: ClaudeClient, neo4j_repo: Neo4jRepository):\n           self.claude_client = claude_client\n           self.neo4j_repo = neo4j_repo\n           \n       async def extract_entities(self, research_task: ResearchTask) -> Dict[str, Any]:\n           \"\"\"Extract entities from a research task using Claude Haiku\"\"\"\n           # Prepare prompt for Claude\n           prompt = self._build_extraction_prompt(research_task)\n           \n           # Call Claude Haiku\n           extraction_result = await self.claude_client.generate(\n               prompt=prompt,\n               model=\"claude-3-haiku-20240307\",\n               max_tokens=2000,\n               temperature=0.2,\n               response_format={\"type\": \"json\"}\n           )\n           \n           # Validate and process the extraction result\n           validated_result = self._validate_extraction_result(extraction_result)\n           \n           # Store entities in Neo4j\n           await self._store_entities_in_graph(research_task.id, validated_result)\n           \n           return validated_result\n       \n       def _build_extraction_prompt(self, research_task: ResearchTask) -> str:\n           \"\"\"Build a prompt for Claude to extract entities\"\"\"\n           return f\"\"\"\n           Extract the following types of information from this research task:\n           \n           1. Main topics\n           2. Key entities (people, organizations, technologies, concepts)\n           3. Important facts and findings\n           4. Relationships between entities\n           \n           Research task:\n           Title: {research_task.title}\n           Description: {research_task.description}\n           Content: {research_task.content}\n           \n           Return the extracted information in the following JSON format:\n           {{\n               \"topics\": [\n                   {{ \"name\": \"topic name\", \"relevance_score\": 0.95 }}\n               ],\n               \"entities\": [\n                   {{ \n                       \"name\": \"entity name\", \n                       \"type\": \"PERSON|ORGANIZATION|TECHNOLOGY|CONCEPT|LOCATION|OTHER\",\n                       \"mentions\": [\"text mention 1\", \"text mention 2\"],\n                       \"relevance_score\": 0.85\n                   }}\n               ],\n               \"facts\": [\n                   {{ \n                       \"statement\": \"factual statement\",\n                       \"confidence_score\": 0.75,\n                       \"source_text\": \"original text from which fact was derived\"\n                   }}\n               ],\n               \"relationships\": [\n                   {{\n                       \"source\": \"entity or topic name\",\n                       \"target\": \"entity or topic name\",\n                       \"type\": \"RELATED_TO|PART_OF|CREATED_BY|USED_BY|etc\",\n                       \"description\": \"description of relationship\"\n                   }}\n               ]\n           }}\n           \"\"\"\n       \n       def _validate_extraction_result(self, extraction_result: Dict[str, Any]) -> Dict[str, Any]:\n           \"\"\"Validate and clean up the extraction result\"\"\"\n           # Implement validation logic using the OutputValidatorService\n           # Ensure all required fields are present and properly formatted\n           # Return the validated result\n           # TODO: Implement validation logic\n           return extraction_result\n       \n       async def _store_entities_in_graph(self, task_id: str, extraction_result: Dict[str, Any]) -> None:\n           \"\"\"Store extracted entities and relationships in Neo4j\"\"\"\n           # Create transaction for batch operations\n           tx = await self.neo4j_repo.begin_transaction()\n           \n           try:\n               # Store topics\n               for topic in extraction_result.get(\"topics\", []):\n                   await self.neo4j_repo.execute_query(\n                       \"\"\"\n                       MERGE (t:Topic {name: $name})\n                       SET t.relevance_score = $relevance_score\n                       WITH t\n                       MATCH (rt:ResearchTask {id: $task_id})\n                       MERGE (rt)-[:HAS_TOPIC]->(t)\n                       \"\"\",\n                       {\"name\": topic[\"name\"], \"relevance_score\": topic[\"relevance_score\"], \"task_id\": task_id},\n                       tx=tx\n                   )\n               \n               # Store entities\n               for entity in extraction_result.get(\"entities\", []):\n                   await self.neo4j_repo.execute_query(\n                       \"\"\"\n                       MERGE (e:Entity {name: $name})\n                       SET e.type = $type,\n                           e.mentions = $mentions,\n                           e.relevance_score = $relevance_score\n                       WITH e\n                       MATCH (rt:ResearchTask {id: $task_id})\n                       MERGE (rt)-[:MENTIONS]->(e)\n                       \"\"\",\n                       {\n                           \"name\": entity[\"name\"],\n                           \"type\": entity[\"type\"],\n                           \"mentions\": entity[\"mentions\"],\n                           \"relevance_score\": entity[\"relevance_score\"],\n                           \"task_id\": task_id\n                       },\n                       tx=tx\n                   )\n               \n               # Store facts\n               for i, fact in enumerate(extraction_result.get(\"facts\", [])):\n                   fact_id = f\"{task_id}_fact_{i}\"\n                   await self.neo4j_repo.execute_query(\n                       \"\"\"\n                       CREATE (f:Fact {id: $fact_id, statement: $statement, confidence_score: $confidence_score, source_text: $source_text})\n                       WITH f\n                       MATCH (rt:ResearchTask {id: $task_id})\n                       CREATE (rt)-[:CONTAINS_FACT]->(f)\n                       \"\"\",\n                       {\n                           \"fact_id\": fact_id,\n                           \"statement\": fact[\"statement\"],\n                           \"confidence_score\": fact[\"confidence_score\"],\n                           \"source_text\": fact[\"source_text\"],\n                           \"task_id\": task_id\n                       },\n                       tx=tx\n                   )\n               \n               # Store relationships\n               for rel in extraction_result.get(\"relationships\", []):\n                   await self.neo4j_repo.execute_query(\n                       \"\"\"\n                       MATCH (source {name: $source_name})\n                       MATCH (target {name: $target_name})\n                       WHERE source:Topic OR source:Entity\n                       AND target:Topic OR target:Entity\n                       MERGE (source)-[r:RELATED {type: $rel_type}]->(target)\n                       SET r.description = $description\n                       \"\"\",\n                       {\n                           \"source_name\": rel[\"source\"],\n                           \"target_name\": rel[\"target\"],\n                           \"rel_type\": rel[\"type\"],\n                           \"description\": rel[\"description\"]\n                       },\n                       tx=tx\n                   )\n               \n               # Commit transaction\n               await self.neo4j_repo.commit_transaction(tx)\n           except Exception as e:\n               # Rollback transaction on error\n               await self.neo4j_repo.rollback_transaction(tx)\n               raise e\n   ```\n\n2. Register the service in the dependency injection container:\n   ```python\n   # In app/di/container.py\n   from app.services.entity_extraction_service import EntityExtractionService\n   \n   # Add to the container setup\n   container.register(EntityExtractionService)\n   ```\n\n3. Create an API endpoint to trigger entity extraction for a research task:\n   ```python\n   # In app/api/routes/research_tasks.py\n   from fastapi import APIRouter, Depends, HTTPException\n   from app.services.entity_extraction_service import EntityExtractionService\n   from app.repositories.research_task_repository import ResearchTaskRepository\n   \n   router = APIRouter()\n   \n   @router.post(\"/{task_id}/extract-entities\", response_model=Dict[str, Any])\n   async def extract_entities(\n       task_id: str,\n       entity_extraction_service: EntityExtractionService = Depends(),\n       research_task_repository: ResearchTaskRepository = Depends()\n   ):\n       \"\"\"Extract entities from a research task and store them in Neo4j\"\"\"\n       # Get the research task\n       task = await research_task_repository.get_by_id(task_id)\n       if not task:\n           raise HTTPException(status_code=404, detail=\"Research task not found\")\n       \n       # Extract entities\n       try:\n           extraction_result = await entity_extraction_service.extract_entities(task)\n           return extraction_result\n       except Exception as e:\n           # Use the standardized exception handling\n           raise HTTPException(status_code=500, detail=f\"Entity extraction failed: {str(e)}\")\n   ```\n\n4. Implement a background task processor for asynchronous entity extraction:\n   ```python\n   # In app/tasks/entity_extraction_task.py\n   from app.services.entity_extraction_service import EntityExtractionService\n   from app.repositories.research_task_repository import ResearchTaskRepository\n   \n   async def process_entity_extraction(task_id: str):\n       \"\"\"Background task to extract entities from a research task\"\"\"\n       # Get dependencies\n       from app.di.container import container\n       entity_extraction_service = container.resolve(EntityExtractionService)\n       research_task_repository = container.resolve(ResearchTaskRepository)\n       \n       # Get the research task\n       task = await research_task_repository.get_by_id(task_id)\n       if not task:\n           # Log error and return\n           print(f\"Research task {task_id} not found\")\n           return\n       \n       # Extract entities\n       try:\n           await entity_extraction_service.extract_entities(task)\n           print(f\"Entity extraction completed for task {task_id}\")\n       except Exception as e:\n           # Log error\n           print(f\"Entity extraction failed for task {task_id}: {str(e)}\")\n   ```\n\n5. Add a trigger to automatically extract entities when a research task is created or updated:\n   ```python\n   # In app/services/research_task_service.py\n   from app.tasks.task_queue import enqueue_task\n   \n   async def create_research_task(self, task_data: Dict[str, Any]) -> ResearchTask:\n       # Existing code to create a research task\n       task = await self.repository.create(task_data)\n       \n       # Enqueue entity extraction task\n       await enqueue_task(\"process_entity_extraction\", {\"task_id\": task.id})\n       \n       return task\n   \n   async def update_research_task(self, task_id: str, task_data: Dict[str, Any]) -> ResearchTask:\n       # Existing code to update a research task\n       task = await self.repository.update(task_id, task_data)\n       \n       # Enqueue entity extraction task\n       await enqueue_task(\"process_entity_extraction\", {\"task_id\": task.id})\n       \n       return task\n   ```\n\n6. Create a utility to query the extracted entities from Neo4j:\n   ```python\n   # In app/services/entity_extraction_service.py (additional method)\n   async def get_entities_for_task(self, task_id: str) -> Dict[str, Any]:\n       \"\"\"Get all extracted entities for a research task\"\"\"\n       # Get topics\n       topics_result = await self.neo4j_repo.execute_query(\n           \"\"\"\n           MATCH (rt:ResearchTask {id: $task_id})-[:HAS_TOPIC]->(t:Topic)\n           RETURN t.name as name, t.relevance_score as relevance_score\n           ORDER BY t.relevance_score DESC\n           \"\"\",\n           {\"task_id\": task_id}\n       )\n       \n       # Get entities\n       entities_result = await self.neo4j_repo.execute_query(\n           \"\"\"\n           MATCH (rt:ResearchTask {id: $task_id})-[:MENTIONS]->(e:Entity)\n           RETURN e.name as name, e.type as type, e.mentions as mentions, e.relevance_score as relevance_score\n           ORDER BY e.relevance_score DESC\n           \"\"\",\n           {\"task_id\": task_id}\n       )\n       \n       # Get facts\n       facts_result = await self.neo4j_repo.execute_query(\n           \"\"\"\n           MATCH (rt:ResearchTask {id: $task_id})-[:CONTAINS_FACT]->(f:Fact)\n           RETURN f.statement as statement, f.confidence_score as confidence_score, f.source_text as source_text\n           ORDER BY f.confidence_score DESC\n           \"\"\",\n           {\"task_id\": task_id}\n       )\n       \n       # Get relationships\n       relationships_result = await self.neo4j_repo.execute_query(\n           \"\"\"\n           MATCH (rt:ResearchTask {id: $task_id})-[:MENTIONS|HAS_TOPIC]->(source)\n           MATCH (source)-[r:RELATED]->(target)\n           RETURN source.name as source, target.name as target, r.type as type, r.description as description\n           \"\"\",\n           {\"task_id\": task_id}\n       )\n       \n       return {\n           \"topics\": topics_result,\n           \"entities\": entities_result,\n           \"facts\": facts_result,\n           \"relationships\": relationships_result\n       }\n   ```\n\n7. Integrate with the OutputValidatorService to ensure proper formatting of Claude's responses:\n   ```python\n   # In app/services/entity_extraction_service.py (update _validate_extraction_result method)\n   from app.services.output_validator_service import OutputValidatorService\n   \n   def __init__(self, claude_client: ClaudeClient, neo4j_repo: Neo4jRepository, output_validator: OutputValidatorService):\n       self.claude_client = claude_client\n       self.neo4j_repo = neo4j_repo\n       self.output_validator = output_validator\n   \n   def _validate_extraction_result(self, extraction_result: Dict[str, Any]) -> Dict[str, Any]:\n       \"\"\"Validate and clean up the extraction result\"\"\"\n       # Define the expected schema\n       schema = {\n           \"type\": \"object\",\n           \"required\": [\"topics\", \"entities\", \"facts\", \"relationships\"],\n           \"properties\": {\n               \"topics\": {\n                   \"type\": \"array\",\n                   \"items\": {\n                       \"type\": \"object\",\n                       \"required\": [\"name\", \"relevance_score\"],\n                       \"properties\": {\n                           \"name\": {\"type\": \"string\"},\n                           \"relevance_score\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1}\n                       }\n                   }\n               },\n               \"entities\": {\n                   \"type\": \"array\",\n                   \"items\": {\n                       \"type\": \"object\",\n                       \"required\": [\"name\", \"type\", \"mentions\", \"relevance_score\"],\n                       \"properties\": {\n                           \"name\": {\"type\": \"string\"},\n                           \"type\": {\"type\": \"string\", \"enum\": [\"PERSON\", \"ORGANIZATION\", \"TECHNOLOGY\", \"CONCEPT\", \"LOCATION\", \"OTHER\"]},\n                           \"mentions\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                           \"relevance_score\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1}\n                       }\n                   }\n               },\n               \"facts\": {\n                   \"type\": \"array\",\n                   \"items\": {\n                       \"type\": \"object\",\n                       \"required\": [\"statement\", \"confidence_score\", \"source_text\"],\n                       \"properties\": {\n                           \"statement\": {\"type\": \"string\"},\n                           \"confidence_score\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                           \"source_text\": {\"type\": \"string\"}\n                       }\n                   }\n               },\n               \"relationships\": {\n                   \"type\": \"array\",\n                   \"items\": {\n                       \"type\": \"object\",\n                       \"required\": [\"source\", \"target\", \"type\", \"description\"],\n                       \"properties\": {\n                           \"source\": {\"type\": \"string\"},\n                           \"target\": {\"type\": \"string\"},\n                           \"type\": {\"type\": \"string\"},\n                           \"description\": {\"type\": \"string\"}\n                       }\n                   }\n               }\n           }\n       }\n       \n       # Validate against schema\n       validated_result = self.output_validator.validate_json(extraction_result, schema)\n       return validated_result\n   ```\n\n8. Implement error handling using the standardized exception framework:\n   ```python\n   # In app/exceptions/entity_extraction_exceptions.py\n   from app.exceptions.base import BaseAppException\n   \n   class EntityExtractionError(BaseAppException):\n       \"\"\"Base exception for entity extraction errors\"\"\"\n       def __init__(self, message: str, error_code: str = \"ENTITY_EXTRACTION_ERROR\", status_code: int = 500):\n           super().__init__(message, error_code, status_code)\n   \n   class InvalidExtractionResultError(EntityExtractionError):\n       \"\"\"Exception for invalid extraction results\"\"\"\n       def __init__(self, message: str):\n           super().__init__(message, \"INVALID_EXTRACTION_RESULT\", 422)\n   \n   class GraphStorageError(EntityExtractionError):\n       \"\"\"Exception for errors storing entities in Neo4j\"\"\"\n       def __init__(self, message: str):\n           super().__init__(message, \"GRAPH_STORAGE_ERROR\", 500)\n   ```\n\n9. Update the entity extraction service to use these custom exceptions:\n   ```python\n   # In app/services/entity_extraction_service.py\n   from app.exceptions.entity_extraction_exceptions import InvalidExtractionResultError, GraphStorageError\n   \n   # In _validate_extraction_result method\n   try:\n       validated_result = self.output_validator.validate_json(extraction_result, schema)\n       return validated_result\n   except Exception as e:\n       raise InvalidExtractionResultError(f\"Invalid extraction result: {str(e)}\")\n   \n   # In _store_entities_in_graph method\n   try:\n       # Existing code\n   except Exception as e:\n       await self.neo4j_repo.rollback_transaction(tx)\n       raise GraphStorageError(f\"Failed to store entities in graph: {str(e)}\")\n   ```",
        "testStrategy": "1. Unit Tests for EntityExtractionService:\n   - Create test file `tests/services/test_entity_extraction_service.py`\n   - Test `_build_extraction_prompt` method to ensure it generates correct prompts\n   - Test `_validate_extraction_result` with valid and invalid extraction results\n   - Mock Claude client responses and test the full extraction process\n   - Test error handling with various failure scenarios\n\n2. Integration Tests for Neo4j Storage:\n   - Create test file `tests/integration/test_entity_extraction_neo4j.py`\n   - Set up a test Neo4j instance or use a mock\n   - Test storing different types of entities and relationships\n   - Verify that all data is correctly persisted in the graph\n   - Test transaction rollback on errors\n   - Test querying stored entities and relationships\n\n3. API Endpoint Tests:\n   - Create test file `tests/api/test_research_task_entity_extraction.py`\n   - Test the entity extraction endpoint with valid research tasks\n   - Test error handling for non-existent tasks\n   - Test handling of malformed responses from Claude\n   - Test authentication and authorization for the endpoint\n\n4. End-to-End Tests:\n   - Create test file `tests/e2e/test_entity_extraction_workflow.py`\n   - Test the complete workflow from research task creation to entity extraction\n   - Verify that entities are correctly extracted and stored\n   - Test the background task processing\n   - Verify that entity extraction is triggered on task updates\n\n5. Performance Tests:\n   - Test extraction performance with research tasks of varying sizes\n   - Measure and optimize Neo4j query performance\n   - Test concurrent extraction requests\n   - Verify system behavior under load\n\n6. Validation Tests:\n   - Test the integration with OutputValidatorService\n   - Verify that malformed Claude responses are properly handled\n   - Test schema validation with various edge cases\n   - Ensure all required fields are properly validated\n\n7. Manual Testing:\n   - Create a sample research task with rich content\n   - Trigger entity extraction and verify the results\n   - Examine the extracted entities in Neo4j using the Neo4j Browser\n   - Verify the relationships between entities\n   - Test the quality of extracted entities and facts",
        "status": "pending",
        "dependencies": [
          "147",
          "153"
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": "156",
        "title": "LlamaIndex Integration Hardening",
        "description": "Implement reliability improvements for the LlamaIndex service including connection pooling, request timeout handling, retry logic for transient failures, and a health check endpoint.",
        "details": "Enhance the LlamaIndex integration with the following reliability improvements:\n\n1. Connection Pooling:\n   - Implement a connection pool for LlamaIndex service to efficiently manage and reuse connections\n   - Create a `LlamaIndexConnectionPool` class in `app/services/llama_index/connection_pool.py`\n   - Configure pool size based on expected load (default: min=5, max=20)\n   - Implement connection lifecycle management (creation, validation, recycling)\n   - Add monitoring for pool statistics\n\n2. Request Timeout Handling:\n   - Add configurable timeout parameters for all LlamaIndex operations\n   - Implement graceful timeout handling with appropriate error messages\n   - Create a timeout configuration in `app/config/llama_index_config.py`\n   - Add logging for timeout events with context information\n\n3. Retry Logic:\n   - Implement exponential backoff retry mechanism for transient failures\n   - Define retry policies for different types of operations (queries, indexing, etc.)\n   - Create a `RetryHandler` class in `app/services/llama_index/retry_handler.py`\n   - Configure max retry attempts, backoff factor, and jitter\n   - Add detailed logging of retry attempts and outcomes\n\n4. Health Check Endpoint:\n   - Create a `/api/llama-index/health` endpoint in `app/api/routes/llama_index.py`\n   - Implement comprehensive health checks that verify:\n     - Connection to LlamaIndex service\n     - Index availability and status\n     - Query functionality with a simple test query\n     - Resource availability (memory, storage)\n   - Return detailed health status with component-level information\n   - Add integration with the application's overall health monitoring system\n\n5. Error Handling Improvements:\n   - Create standardized error responses for different failure scenarios\n   - Implement detailed logging with context information\n   - Add error classification to distinguish between transient and permanent failures\n\nSample code for connection pool implementation:\n```python\n# app/services/llama_index/connection_pool.py\nfrom typing import List, Optional\nimport time\nimport threading\nfrom llama_index import ServiceContext, StorageContext, load_index_from_storage\n\nclass LlamaIndexConnection:\n    def __init__(self, index_id: str):\n        self.index_id = index_id\n        self.last_used = time.time()\n        self.created_at = time.time()\n        self.service_context = ServiceContext.from_defaults()\n        self.storage_context = StorageContext.from_defaults()\n        self.index = load_index_from_storage(self.storage_context)\n        \n    def is_valid(self) -> bool:\n        # Implement validation logic\n        return True\n        \n    def refresh(self) -> None:\n        # Implement refresh logic\n        self.last_used = time.time()\n\nclass LlamaIndexConnectionPool:\n    def __init__(self, min_connections: int = 5, max_connections: int = 20):\n        self.min_connections = min_connections\n        self.max_connections = max_connections\n        self.connections: List[LlamaIndexConnection] = []\n        self.lock = threading.RLock()\n        self.initialize_pool()\n        \n    def initialize_pool(self) -> None:\n        with self.lock:\n            for _ in range(self.min_connections):\n                self.connections.append(LlamaIndexConnection(\"default\"))\n                \n    def get_connection(self) -> LlamaIndexConnection:\n        with self.lock:\n            if not self.connections:\n                if len(self.connections) < self.max_connections:\n                    return LlamaIndexConnection(\"default\")\n                else:\n                    # Wait for a connection to become available\n                    # Implement waiting logic\n                    pass\n            \n            connection = self.connections.pop(0)\n            if not connection.is_valid():\n                connection = LlamaIndexConnection(\"default\")\n            \n            return connection\n            \n    def release_connection(self, connection: LlamaIndexConnection) -> None:\n        with self.lock:\n            connection.refresh()\n            self.connections.append(connection)\n```",
        "testStrategy": "1. Connection Pool Testing:\n   - Unit test the `LlamaIndexConnectionPool` class with various pool sizes\n   - Verify connections are properly created, managed, and recycled\n   - Test concurrent access patterns with multiple threads\n   - Validate connection validation logic works correctly\n   - Measure performance improvements with and without connection pooling\n\n2. Timeout Handling Testing:\n   - Create test cases with deliberately slow operations\n   - Verify timeout configuration is correctly applied\n   - Test different timeout values and their effects\n   - Ensure proper error messages are returned on timeout\n   - Validate that resources are properly cleaned up after timeout\n\n3. Retry Logic Testing:\n   - Create mock LlamaIndex service that fails intermittently\n   - Test retry behavior with various failure patterns\n   - Verify exponential backoff works as expected\n   - Test different retry configurations\n   - Ensure maximum retry limit is respected\n   - Validate that permanent failures are handled correctly\n\n4. Health Check Endpoint Testing:\n   - Test the endpoint returns correct status when all systems are operational\n   - Simulate various failure conditions and verify correct reporting\n   - Test response format and content\n   - Verify integration with monitoring systems\n   - Test performance impact of health checks\n\n5. Integration Testing:\n   - Create end-to-end tests that verify all components work together\n   - Test under load to ensure stability\n   - Verify error propagation and handling\n   - Test recovery scenarios after simulated failures\n   - Validate logging and monitoring integration\n\n6. Performance Testing:\n   - Measure throughput with and without the improvements\n   - Test under various load conditions\n   - Measure resource utilization (CPU, memory)\n   - Identify and address any bottlenecks",
        "status": "pending",
        "dependencies": [
          "143",
          "145",
          "147"
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": "157",
        "title": "B2 Storage Error Handling",
        "description": "Implement robust error handling for B2 storage operations including retry logic for uploads/downloads, checksum verification, dead letter handling for failed operations, and storage metrics collection.",
        "details": "Enhance the B2 storage service with comprehensive error handling mechanisms:\n\n1. Implement retry logic with exponential backoff for upload/download operations:\n   ```python\n   from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n   from b2sdk.exception import B2ConnectionError, B2RequestTimeout\n   \n   @retry(\n       stop=stop_after_attempt(5),\n       wait=wait_exponential(multiplier=1, min=2, max=60),\n       retry=retry_if_exception_type((B2ConnectionError, B2RequestTimeout))\n   )\n   async def resilient_b2_upload(file_path, destination_path):\n       try:\n           return await b2_client.upload_file(file_path, destination_path)\n       except Exception as e:\n           logger.error(f\"B2 upload error: {str(e)}\")\n           raise\n   ```\n\n2. Implement checksum verification for data integrity:\n   ```python\n   import hashlib\n   \n   def calculate_sha1(file_path):\n       sha1 = hashlib.sha1()\n       with open(file_path, 'rb') as f:\n           while chunk := f.read(8192):\n               sha1.update(chunk)\n       return sha1.hexdigest()\n   \n   async def verify_upload_integrity(local_file_path, b2_file_info):\n       local_checksum = calculate_sha1(local_file_path)\n       remote_checksum = b2_file_info.content_sha1\n       \n       if local_checksum != remote_checksum:\n           logger.error(f\"Checksum mismatch for {local_file_path}\")\n           raise IntegrityError(f\"Upload verification failed: checksums don't match\")\n       \n       return True\n   ```\n\n3. Implement dead letter queue for failed uploads:\n   ```python\n   class B2DeadLetterQueue:\n       def __init__(self, redis_client, queue_name=\"b2_dead_letter_queue\"):\n           self.redis = redis_client\n           self.queue_name = queue_name\n       \n       async def add_failed_operation(self, operation_type, file_path, error_details, retry_count):\n           entry = {\n               \"operation_type\": operation_type,\n               \"file_path\": file_path,\n               \"error_details\": str(error_details),\n               \"retry_count\": retry_count,\n               \"timestamp\": datetime.utcnow().isoformat()\n           }\n           await self.redis.lpush(self.queue_name, json.dumps(entry))\n       \n       async def get_failed_operations(self, limit=100):\n           entries = await self.redis.lrange(self.queue_name, 0, limit-1)\n           return [json.loads(entry) for entry in entries]\n       \n       async def retry_operation(self, entry_index):\n           entry = json.loads(await self.redis.lindex(self.queue_name, entry_index))\n           # Logic to retry the operation\n           # If successful, remove from queue\n           await self.redis.lrem(self.queue_name, 1, json.dumps(entry))\n           return True\n   ```\n\n4. Implement storage metrics collection:\n   ```python\n   from prometheus_client import Counter, Histogram, Gauge\n   \n   # Define metrics\n   b2_operation_counter = Counter(\n       'b2_operations_total',\n       'Total number of B2 operations',\n       ['operation_type', 'status']\n   )\n   \n   b2_operation_latency = Histogram(\n       'b2_operation_latency_seconds',\n       'Latency of B2 operations in seconds',\n       ['operation_type']\n   )\n   \n   b2_storage_usage = Gauge(\n       'b2_storage_usage_bytes',\n       'Current B2 storage usage in bytes'\n   )\n   \n   # Usage in code\n   async def track_b2_operation(operation_type, start_time, status):\n       duration = time.time() - start_time\n       b2_operation_counter.labels(operation_type=operation_type, status=status).inc()\n       b2_operation_latency.labels(operation_type=operation_type).observe(duration)\n   \n   async def update_storage_metrics():\n       usage = await b2_client.get_bucket_usage()\n       b2_storage_usage.set(usage)\n   ```\n\n5. Create a B2StorageService class that integrates all these components:\n   ```python\n   class B2StorageService:\n       def __init__(self, b2_client, redis_client):\n           self.b2_client = b2_client\n           self.dead_letter_queue = B2DeadLetterQueue(redis_client)\n       \n       async def upload_file(self, local_path, remote_path, retry_count=0):\n           start_time = time.time()\n           try:\n               result = await resilient_b2_upload(local_path, remote_path)\n               await verify_upload_integrity(local_path, result)\n               await track_b2_operation('upload', start_time, 'success')\n               return result\n           except Exception as e:\n               await track_b2_operation('upload', start_time, 'failure')\n               if retry_count >= 5:\n                   await self.dead_letter_queue.add_failed_operation(\n                       'upload', local_path, str(e), retry_count\n                   )\n               raise\n       \n       async def download_file(self, remote_path, local_path, retry_count=0):\n           start_time = time.time()\n           try:\n               result = await resilient_b2_download(remote_path, local_path)\n               await track_b2_operation('download', start_time, 'success')\n               return result\n           except Exception as e:\n               await track_b2_operation('download', start_time, 'failure')\n               if retry_count >= 5:\n                   await self.dead_letter_queue.add_failed_operation(\n                       'download', remote_path, str(e), retry_count\n                   )\n               raise\n       \n       async def process_dead_letter_queue(self, batch_size=10):\n           failed_operations = await self.dead_letter_queue.get_failed_operations(batch_size)\n           for i, operation in enumerate(failed_operations):\n               try:\n                   if operation['operation_type'] == 'upload':\n                       await self.upload_file(\n                           operation['file_path'],\n                           operation['destination_path'],\n                           retry_count=operation['retry_count'] + 1\n                       )\n                   elif operation['operation_type'] == 'download':\n                       await self.download_file(\n                           operation['remote_path'],\n                           operation['local_path'],\n                           retry_count=operation['retry_count'] + 1\n                       )\n                   await self.dead_letter_queue.retry_operation(i)\n               except Exception as e:\n                   logger.error(f\"Failed to process dead letter queue item: {str(e)}\")\n   ```\n\n6. Create a scheduled task to periodically process the dead letter queue and update metrics:\n   ```python\n   @shared_task\n   async def process_b2_maintenance():\n       storage_service = B2StorageService(get_b2_client(), get_redis_client())\n       await storage_service.process_dead_letter_queue()\n       await update_storage_metrics()\n   ```",
        "testStrategy": "1. Unit tests for retry logic:\n   - Test successful upload/download after temporary failures\n   - Test that retry count respects maximum attempts\n   - Verify exponential backoff timing between retries\n   - Test behavior when max retries are exhausted\n\n2. Unit tests for checksum verification:\n   - Test successful verification with matching checksums\n   - Test failure detection with mismatched checksums\n   - Test handling of corrupted files\n   - Test with various file sizes (small, medium, large)\n\n3. Unit tests for dead letter queue:\n   - Test adding failed operations to the queue\n   - Test retrieving operations from the queue\n   - Test retrying operations from the queue\n   - Test queue persistence across service restarts\n\n4. Unit tests for metrics collection:\n   - Test counter increments for successful operations\n   - Test counter increments for failed operations\n   - Test latency histogram recording\n   - Test storage usage gauge updates\n\n5. Integration tests:\n   - Test end-to-end upload with simulated network failures\n   - Test end-to-end download with simulated network failures\n   - Test dead letter queue processing with real Redis instance\n   - Test metrics reporting to Prometheus endpoint\n\n6. Performance tests:\n   - Benchmark upload/download speeds with retry logic enabled\n   - Test system under high concurrency\n   - Measure impact of checksum verification on throughput\n   - Test dead letter queue processing with large backlogs\n\n7. Chaos testing:\n   - Test behavior during B2 service outages\n   - Test behavior during Redis outages\n   - Test behavior with network packet loss and latency\n   - Test recovery after system restarts",
        "status": "pending",
        "dependencies": [
          "137",
          "153"
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": "158",
        "title": "CrewAI Workflow Improvements",
        "description": "Enhance CrewAI workflow management with state persistence, graceful shutdown handling, task cancellation support, and comprehensive metrics and logging.",
        "details": "Implement the following workflow improvements to the CrewAI system:\n\n1. **Workflow State Persistence**:\n   - Create a `WorkflowStateManager` class that serializes and persists workflow state\n   - Implement checkpointing at critical workflow stages\n   - Store state in a configurable backend (file system, database, or Redis)\n   - Add state recovery mechanisms for workflow resumption\n   - Include versioning for backward compatibility\n   - Example implementation:\n   ```python\n   class WorkflowStateManager:\n       def __init__(self, storage_backend=\"file\", storage_path=\"./workflow_states\"):\n           self.storage_backend = storage_backend\n           self.storage_path = storage_path\n           \n       def save_state(self, workflow_id, state_data):\n           \"\"\"Serialize and save workflow state\"\"\"\n           serialized_state = self._serialize_state(state_data)\n           if self.storage_backend == \"file\":\n               self._save_to_file(workflow_id, serialized_state)\n           elif self.storage_backend == \"redis\":\n               self._save_to_redis(workflow_id, serialized_state)\n           # Add other backends as needed\n           \n       def load_state(self, workflow_id):\n           \"\"\"Load and deserialize workflow state\"\"\"\n           # Implementation for loading state\n   ```\n\n2. **Graceful Shutdown Handling**:\n   - Implement signal handlers for SIGTERM and SIGINT\n   - Add workflow pause functionality to safely stop at checkpoints\n   - Create cleanup procedures for resources (connections, temp files)\n   - Implement state saving before shutdown\n   - Add logging for shutdown events\n   - Example implementation:\n   ```python\n   import signal\n   import sys\n   \n   class GracefulShutdownHandler:\n       def __init__(self, workflow_manager):\n           self.workflow_manager = workflow_manager\n           signal.signal(signal.SIGTERM, self.handle_shutdown)\n           signal.signal(signal.SIGINT, self.handle_shutdown)\n           \n       def handle_shutdown(self, signum, frame):\n           \"\"\"Handle shutdown signals gracefully\"\"\"\n           logger.info(f\"Received signal {signum}, initiating graceful shutdown\")\n           self.workflow_manager.pause_workflows()\n           self.workflow_manager.save_all_states()\n           self.workflow_manager.cleanup_resources()\n           sys.exit(0)\n   ```\n\n3. **Task Cancellation Support**:\n   - Implement a cancellation token system for tasks\n   - Add API endpoints for cancelling specific tasks or workflows\n   - Create cleanup procedures for cancelled tasks\n   - Implement notification system for cancellation events\n   - Handle dependent task cancellation logic\n   - Example implementation:\n   ```python\n   class CancellationToken:\n       def __init__(self):\n           self.cancelled = False\n           self._callbacks = []\n           \n       def cancel(self):\n           \"\"\"Mark as cancelled and execute callbacks\"\"\"\n           self.cancelled = True\n           for callback in self._callbacks:\n               callback()\n               \n       def register_callback(self, callback):\n           \"\"\"Register a callback to be executed on cancellation\"\"\"\n           self._callbacks.append(callback)\n           \n       def is_cancelled(self):\n           \"\"\"Check if cancellation has been requested\"\"\"\n           return self.cancelled\n   ```\n\n4. **Workflow Metrics and Logging**:\n   - Implement structured logging with contextual information\n   - Create metrics collection for workflow performance (duration, resource usage)\n   - Add task-level timing and status metrics\n   - Implement error rate and failure tracking\n   - Create dashboard-ready metrics output (Prometheus format)\n   - Example implementation:\n   ```python\n   class WorkflowMetricsCollector:\n       def __init__(self):\n           self.metrics = {}\n           \n       def record_task_start(self, task_id, metadata=None):\n           \"\"\"Record the start of a task\"\"\"\n           self.metrics[task_id] = {\n               \"start_time\": time.time(),\n               \"status\": \"running\",\n               \"metadata\": metadata or {}\n           }\n           \n       def record_task_completion(self, task_id, success=True, error=None):\n           \"\"\"Record the completion of a task\"\"\"\n           if task_id in self.metrics:\n               self.metrics[task_id].update({\n                   \"end_time\": time.time(),\n                   \"duration\": time.time() - self.metrics[task_id][\"start_time\"],\n                   \"status\": \"completed\" if success else \"failed\",\n                   \"error\": error\n               })\n   ```\n\nIntegration points:\n- Update the main CrewAI workflow controller to use these new components\n- Modify the API layer to expose cancellation and metrics endpoints\n- Update documentation to reflect new capabilities\n- Create migration path for existing workflows",
        "testStrategy": "1. **Workflow State Persistence Testing**:\n   - Unit test the `WorkflowStateManager` class with different storage backends\n   - Test serialization/deserialization with various workflow states\n   - Verify state recovery after simulated crashes\n   - Test with corrupted state files to ensure proper error handling\n   - Benchmark performance impact of state persistence\n   - Test concurrent access patterns\n\n2. **Graceful Shutdown Testing**:\n   - Create test harness that sends shutdown signals to running workflows\n   - Verify all resources are properly cleaned up after shutdown\n   - Test shutdown during different workflow stages\n   - Measure shutdown completion time under various loads\n   - Verify state is correctly saved during shutdown\n   - Test with multiple concurrent workflows\n\n3. **Task Cancellation Testing**:\n   - Unit test the cancellation token implementation\n   - Test API endpoints for task and workflow cancellation\n   - Verify dependent tasks are properly handled during cancellation\n   - Test cancellation at different stages of task execution\n   - Verify resource cleanup after cancellation\n   - Test cancellation propagation in complex workflow graphs\n\n4. **Metrics and Logging Testing**:\n   - Verify metrics are correctly collected for all workflow events\n   - Test structured logging format and content\n   - Validate metrics accuracy with controlled workflow executions\n   - Test integration with monitoring systems (Prometheus, Grafana)\n   - Verify performance impact of metrics collection\n   - Test with high-volume workflows to ensure scalability\n\n5. **Integration Testing**:\n   - End-to-end tests with all components integrated\n   - Test recovery from various failure scenarios\n   - Verify backward compatibility with existing workflows\n   - Performance testing under production-like conditions\n   - Stress testing with concurrent workflows and cancellations",
        "status": "pending",
        "dependencies": [
          "127",
          "131",
          "150",
          "156",
          "157"
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": "159",
        "title": "Implement Circuit Breaker Pattern for External API Services",
        "description": "Implement a comprehensive circuit breaker pattern for all external API calls including Anthropic, Neo4j, and Supabase with configurable thresholds, fallback responses, and monitoring capabilities.",
        "details": "Extend the existing circuit breaker implementation to cover all external API services with enhanced functionality:\n\n1. Refactor the existing `app/services/api_resilience.py` module to support multiple API services:\n   - Create a configurable CircuitBreaker class that can be instantiated for different services\n   - Implement service-specific configuration profiles for Anthropic, Neo4j, and Supabase\n   - Add support for per-service configurable thresholds:\n     ```python\n     class CircuitBreakerConfig:\n         def __init__(self, \n                     failure_threshold: int = 5,\n                     recovery_timeout: int = 30,\n                     retry_max_attempts: int = 3,\n                     retry_backoff_factor: float = 2.0,\n                     timeout: int = 10):\n             self.failure_threshold = failure_threshold\n             self.recovery_timeout = recovery_timeout\n             self.retry_max_attempts = retry_max_attempts\n             self.retry_backoff_factor = retry_backoff_factor\n             self.timeout = timeout\n     \n     class CircuitBreaker:\n         def __init__(self, service_name: str, config: CircuitBreakerConfig = None):\n             self.service_name = service_name\n             self.config = config or CircuitBreakerConfig()\n             self.state = \"CLOSED\"  # CLOSED, OPEN, HALF-OPEN\n             self.failure_count = 0\n             self.last_failure_time = None\n             # Additional state tracking\n     ```\n\n2. Implement fallback response mechanisms:\n   - Create a FallbackRegistry to store and retrieve fallback handlers\n   - Implement default fallback responses for each service type\n   - Allow custom fallback handlers to be registered per operation\n   - Example implementation:\n     ```python\n     class FallbackRegistry:\n         def __init__(self):\n             self.fallbacks = {}\n         \n         def register(self, service_name: str, operation: str, handler: Callable):\n             if service_name not in self.fallbacks:\n                 self.fallbacks[service_name] = {}\n             self.fallbacks[service_name][operation] = handler\n         \n         def get_fallback(self, service_name: str, operation: str) -> Optional[Callable]:\n             return self.fallbacks.get(service_name, {}).get(operation)\n     ```\n\n3. Add circuit state monitoring and metrics:\n   - Implement Prometheus metrics for circuit state changes\n   - Track success/failure rates, response times, and circuit open duration\n   - Create a dashboard endpoint for current circuit states\n   - Log all circuit state transitions with context\n   - Example metrics:\n     ```python\n     # In app/monitoring/metrics.py\n     from prometheus_client import Counter, Gauge, Histogram\n     \n     # Counters\n     circuit_state_changes = Counter('circuit_breaker_state_changes_total', \n                                    'Circuit breaker state transitions',\n                                    ['service', 'from_state', 'to_state'])\n     circuit_requests = Counter('circuit_breaker_requests_total',\n                               'Requests through circuit breaker',\n                               ['service', 'result'])\n     \n     # Gauges\n     circuit_state = Gauge('circuit_breaker_state',\n                          'Current circuit breaker state (0=closed, 1=half-open, 2=open)',\n                          ['service'])\n     \n     # Histograms\n     circuit_response_time = Histogram('circuit_breaker_response_time_seconds',\n                                      'Response time for requests through circuit breaker',\n                                      ['service'])\n     ```\n\n4. Create service-specific circuit breaker wrappers:\n   - Implement AnthropicCircuitBreaker (extending existing implementation)\n   - Implement Neo4jCircuitBreaker for database operations\n   - Implement SupabaseCircuitBreaker for storage operations\n   - Example wrapper:\n     ```python\n     class Neo4jCircuitBreaker:\n         def __init__(self, client, config=None):\n             self.client = client\n             self.circuit = CircuitBreaker(\"neo4j\", config)\n             self.fallback_registry = FallbackRegistry()\n             \n             # Register default fallbacks\n             self.fallback_registry.register(\"neo4j\", \"query\", self._default_query_fallback)\n         \n         async def query(self, cypher, params=None):\n             try:\n                 return await self.circuit.execute(\n                     lambda: self.client.query(cypher, params),\n                     operation=\"query\"\n                 )\n             except CircuitOpenError:\n                 fallback = self.fallback_registry.get_fallback(\"neo4j\", \"query\")\n                 return await fallback(cypher, params)\n         \n         async def _default_query_fallback(self, cypher, params=None):\n             # Return cached results or empty response\n             return {\"results\": [], \"from_fallback\": True}\n     ```\n\n5. Update service initialization to use circuit breakers:\n   - Modify service factory methods to wrap clients with circuit breakers\n   - Update dependency injection to provide circuit-protected clients\n   - Add configuration loading from environment variables\n\n6. Implement a circuit breaker management API:\n   - Create endpoints to view circuit states\n   - Allow manual reset of circuits\n   - Provide configuration update capabilities\n   - Example API routes:\n     ```python\n     @router.get(\"/api/system/circuit-breakers\")\n     async def get_circuit_states():\n         # Return states of all circuit breakers\n     \n     @router.post(\"/api/system/circuit-breakers/{service}/reset\")\n     async def reset_circuit(service: str):\n         # Reset specified circuit breaker\n     ```",
        "testStrategy": "1. Unit tests for the CircuitBreaker class:\n   - Test state transitions (closed → open → half-open → closed)\n   - Test configurable thresholds for different services\n   - Test failure counting and reset behavior\n   - Test timeout handling and recovery periods\n   - Test metrics recording\n\n2. Unit tests for fallback mechanisms:\n   - Test fallback registry registration and retrieval\n   - Test default fallbacks for each service\n   - Test custom fallback handlers\n   - Test fallback context preservation\n\n3. Integration tests for each service-specific circuit breaker:\n   - Test AnthropicCircuitBreaker with mocked API responses\n   - Test Neo4jCircuitBreaker with mocked database responses\n   - Test SupabaseCircuitBreaker with mocked storage responses\n   - Test circuit opening on consecutive failures\n   - Test circuit recovery after timeout period\n\n4. Failure scenario testing:\n   - Simulate API timeouts and verify circuit behavior\n   - Simulate connection errors and verify fallback responses\n   - Test partial failures (some endpoints working, others failing)\n   - Verify correct fallback content is returned\n\n5. Performance testing:\n   - Measure overhead of circuit breaker implementation\n   - Test under high concurrency to verify thread safety\n   - Verify no memory leaks during extended operation\n\n6. Monitoring tests:\n   - Verify metrics are correctly recorded and exposed\n   - Test dashboard endpoint for accurate state reporting\n   - Verify logs contain appropriate context for debugging\n\n7. End-to-end tests:\n   - Test complete request flow with circuit breakers in place\n   - Verify application resilience during simulated outages\n   - Test recovery behavior when services come back online",
        "status": "pending",
        "dependencies": [
          "137",
          "101",
          "110"
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-01-15T05:45:32.987Z",
      "taskCount": 59,
      "completedCount": 53,
      "tags": [
        "v7_3_features"
      ]
    }
  },
  "empire_desktop_v75": {
    "tasks": [
      {
        "id": 1,
        "title": "Project Setup and Tauri 2.0 Initialization",
        "description": "Initialize Tauri 2.0 project with React 18, TypeScript, TailwindCSS, and shadcn/ui for macOS native desktop app.",
        "details": "Use Tauri CLI v2.0.3: `npm create tauri-app@latest -- --template react-ts`. Install React 18.3.1, TailwindCSS 3.4.10, shadcn/ui v0.9.0. Configure `tauri.conf.json` for WKWebView, Rust backend with tokio 1.40.0 for async ops, and sqlx 0.8.1 for SQLite. Set up Rust commands for secure keychain access using keyring 2.5.0 crate. Binary target: macOS arm64/x86_64.",
        "testStrategy": "Run `tauri dev` and verify app launches <2s, WKWebView renders React UI, Rust commands invoke via `invoke('check_keychain')`. Test on macOS Sonoma/Ventura.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Local SQLite Database Setup",
        "description": "Implement local SQLite database with exact schema from PRD for projects, conversations, messages, files, and settings.",
        "details": "Use sqlx 0.8.1 with SQLite feature and rusqlite 0.32.1. Create encrypted DB using SQLCipher via tauri-plugin-sql 2.0.0. Execute PRD schema SQL on init. Add migrations with sqlx-cli 0.8.1. Rust function: `init_db(path: PathBuf) -> Result<Pool<Sqlite>>`. Store in `~/.empire/empire.db`. Enable WAL mode for concurrency.",
        "testStrategy": "Unit tests for schema creation, insert/query projects table. Verify encryption with invalid key fails. Integration test: CRUD project record.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Clerk Authentication Integration",
        "description": "Integrate Clerk auth with secure JWT storage in macOS keychain and auto-refresh.",
        "details": "Use @clerk/clerk-react 5.1.2. Rust backend: keyring 2.5.0 for Keychain access (`keyring::Entry::new('empire', 'jwt')`). Implement OAuth flow redirecting to system browser. Auto-refresh using Clerk's token cache. Expose Tauri commands: `login()`, `get_token()`, `logout()`. Biometric unlock via security-framework 2.10.0 crate.",
        "testStrategy": "Mock Clerk API, test token storage/retrieval, refresh flow. Verify keychain isolation per user. E2E: full login cycle.",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Empire Backend API Client",
        "description": "Create TypeScript API client for all specified Empire v7.3 endpoints with WebSocket streaming.",
        "details": "Use axios 1.7.7 for HTTP, @tauri-apps/api 2.0.3 for WS. Implement EmpireAPI interface exactly as PRD. Streaming: `ReadableStream` from WS `/ws/chat`. Auth: Bearer token interceptor. Endpoints: `/api/query/auto`, `/api/query/adaptive`, `/api/documents/upload`, etc. TypeScript types from PRD models. Retry logic with exponential backoff using p-retry 5.0.0.",
        "testStrategy": "Mock server with MSW 2.4.11, test all endpoints, streaming chunks, error handling, auth interceptor.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Main Chat Interface with Streaming",
        "description": "Build core chat UI with multi-line input, attachments, streaming responses, source citations.",
        "details": "React 18 + shadcn/ui Chat components. Use zustand 5.0.0-rc.2 for state. Streaming: use `useEffect` with API client's AsyncGenerator. Markdown rendering: react-markdown 9.0.1 + remark-gfm. Drag-drop files: react-dropzone 14.2.3. Citations: expandable [1][2] popover. Input: TextareaAutosize from shadcn.",
        "testStrategy": "Cypress 13.15.0 E2E: send message, verify streaming animation, expand citations, file drag-drop. Unit: message rendering.",
        "priority": "high",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Projects CRUD Operations",
        "description": "Implement project management: create, list, update, delete with local+remote sync.",
        "details": "React components: ProjectList, ProjectForm using shadcn DataTable. Local ops via sqlx Rust commands. Sync: POST to Empire API, update `remote_id` and `synced_at`. Templates: store as project with `is_template=true`. Department selector: 12 options from PRD. Zustand store: `projectsStore`.",
        "testStrategy": "Unit: CRUD local DB. Integration: create project → API sync → list verifies remote_id. UI: form validation, list search.",
        "priority": "high",
        "dependencies": [
          2,
          4
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Chat History and Global Search",
        "description": "Build sidebar navigation, conversation list, global/project search with filters.",
        "details": "SQLite FTS5 for search: `CREATE VIRTUAL TABLE messages_fts USING fts5(content, tokenize=porter)`. Rust command: `search_messages(query: &str, filters: Json)`. UI: shadcn CommandMenu for Cmd+K. Results: highlight matches with context preview. Filters: date, project, attachments via SQL WHERE.",
        "testStrategy": "Performance: <500ms search 10k messages. Accuracy: insert test data, verify FTS matches. UI: search → jump to message.",
        "priority": "high",
        "dependencies": [
          2,
          5,
          6
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Real-time Data Sync and Offline Mode",
        "description": "Implement bi-directional sync between local SQLite and Supabase with offline history viewing.",
        "details": "Use Supabase JS 2.46.6 client in Rust via tauri-plugin. Conflict resolution: last-write-wins by `updated_at`. Background sync: tokio task polling every 30s. Offline: queue unsynced changes in `pending_sync` table. Sync indicator: badge on sidebar. Queue queries for online.",
        "testStrategy": "Mock network: offline → queue → online → verify sync. Conflict test: edit same record on two devices.",
        "priority": "medium",
        "dependencies": [
          2,
          3,
          4,
          6,
          7
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Keyboard Shortcuts and Native macOS Features",
        "description": "Implement all specified keyboard shortcuts, menu bar, system tray, notifications.",
        "details": "Tauri: tauri-plugin-global-shortcut 2.0.0 for Cmd+N etc. Menu: tauri-plugin-menu 2.0.0. Tray: system_tray 0.7.0 crate. Notifications: notify-rust 4.9.0. Window: tauri-plugin-window-state 2.0.0 for restore position. Dark mode: use-os-theme.",
        "testStrategy": "Manual: test all shortcuts in app. Automated: use tauri-plugin-macos-specific for key event simulation.",
        "priority": "medium",
        "dependencies": [
          1,
          5
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Settings, Quick Actions, and Packaging",
        "description": "Build settings UI, quick action buttons, project instructions/files, DMG packaging.",
        "details": "Settings: shadcn Settings panel, sync to DB. Quick actions: buttons calling API `/api/summarizer`, etc. Project files: upload to `/api/documents/upload`, store metadata. Instructions: rich textarea → project.instructions. Packaging: `tauri build` with codesign, DMG via create-dmg 2.0.0. Auto-updater: tauri-plugin-updater 2.0.0.",
        "testStrategy": "E2E: settings persist after restart, quick actions trigger API, file upload succeeds. Packaging: build → install DMG → launch verifies.",
        "priority": "medium",
        "dependencies": [
          2,
          4,
          5,
          6
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "MCP Client Foundation",
        "description": "Implement MCP client layer for Supabase/Neo4j server management and tool integration.",
        "details": "Rust: tokio for spawning processes per PRD config `~/.empire/mcp_settings.json`. JSON-RPC 2.0 over stdin/stdout using serde_json 1.0.120. Commands: `start_mcp_server(name: String)`, `list_tools()`. UI: settings page to add/remove servers. Cache resources locally.",
        "testStrategy": "Integration: spawn mock MCP server, verify JSON-RPC tool list, invoke tool. Error: invalid config fails gracefully.",
        "priority": "medium",
        "dependencies": [
          1,
          3
        ],
        "status": "done",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2026-01-02T21:40:07.235Z",
      "updated": "2026-01-02T21:40:07.235Z",
      "description": "Tasks for empire_desktop_v75 context"
    }
  },
  "006-markdown-chunking": {
    "tasks": [
      {
        "id": 1,
        "title": "Design MarkdownChunkerStrategy interface and data structures",
        "description": "Define the MarkdownChunkerStrategy class signature, configuration options, and MarkdownSection dataclass in app/services/chunking_service.py, aligned with existing ChunkingStrategy and TextNode abstractions.",
        "details": "Implementation details:\n- Open app/services/chunking_service.py and locate the existing ChunkingStrategy interface and SentenceSplitter integration to mirror patterns (typing, logging, error handling).\n- Define MarkdownSection dataclass near other internal structures:\n  ```python\n  from dataclasses import dataclass\n\n  @dataclass\n  class MarkdownSection:\n      header: str           # e.g., \"## Methods\"\n      level: int            # 1-6\n      content: str          # section content including header\n      start_line: int       # 0-based line index in original document\n      parent_headers: list  # list of (level, header_text)\n  ```\n- Define MarkdownChunkerStrategy class skeleton implementing ChunkingStrategy:\n  ```python\n  class MarkdownChunkerStrategy(ChunkingStrategy):\n      \"\"\"Splits markdown documents by headers while preserving hierarchy.\"\"\"\n\n      def __init__(\n          self,\n          headers_to_split_on: list[tuple[str, str]] | None = None,\n          max_chunk_size: int = 1024,\n          chunk_overlap: int = 200,\n          include_header_in_chunk: bool = True,\n          preserve_hierarchy: bool = True,\n      ) -> None:\n          self.headers_to_split_on = headers_to_split_on or [\n              (\"#\", \"h1\"),\n              (\"##\", \"h2\"),\n              (\"###\", \"h3\"),\n              (\"####\", \"h4\"),\n          ]\n          self.max_chunk_size = max_chunk_size\n          self.chunk_overlap = chunk_overlap\n          self.include_header_in_chunk = include_header_in_chunk\n          self.preserve_hierarchy = preserve_hierarchy\n\n      def chunk(self, text: str, metadata: dict | None = None) -> list[TextNode]:\n          \"\"\"Split markdown text into nodes by headers.\"\"\"\n          raise NotImplementedError\n\n      def _split_by_headers(self, text: str) -> list[MarkdownSection]:\n          raise NotImplementedError\n\n      def _get_header_hierarchy(self, sections: list[MarkdownSection], index: int) -> dict:\n          raise NotImplementedError\n  ```\n- Ensure type hints match the project’s minimum Python version and style (e.g., using from __future__ import annotations if the repo does so).\n- Add constructor defaults exactly as in PRD: max_chunk_size=1024, chunk_overlap=200, include_header_in_chunk=True.\n- Add docstrings matching PRD’s description of attributes and behavior.\n- Confirm TextNode import from LlamaIndex (likely from llama_index.core.schema import TextNode) and ChunkingStrategy base type to ensure compatibility with the rest of the chunking_service module.\n",
        "testStrategy": "- Add or extend unit tests in tests/test_chunking_service.py to:\n  - Validate that MarkdownSection dataclass can be instantiated and its fields are correctly assigned.\n  - Validate that MarkdownChunkerStrategy can be instantiated with default parameters and custom headers_to_split_on.\n  - Check that attributes (max_chunk_size, chunk_overlap, include_header_in_chunk, preserve_hierarchy) are set to expected defaults.\n  - Run mypy or the project’s static type checker (if configured) to ensure no type errors for new interfaces.\n  - Run pytest to confirm no regressions in existing chunking tests.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement markdown header detection and section splitting",
        "description": "Implement _split_by_headers in MarkdownChunkerStrategy to detect markdown headers via regex, build MarkdownSection objects, and preserve header hierarchy and line positions.",
        "details": "Implementation details:\n- In app/services/chunking_service.py implement header detection using the regex specified in the PRD:\n  ```python\n  import re\n\n  HEADER_RE = re.compile(r\"^(#{1,6})\\s+(.+)$\")\n  ```\n- Split the incoming markdown text into lines and iterate with enumerate to track line numbers.\n- For each line, use HEADER_RE.match(line) to detect headers:\n  ```python\n  match = HEADER_RE.match(line)\n  if match:\n      hashes, title = match.groups()\n      level = len(hashes)\n  ```\n- Maintain a list of section start indices; whenever a new header is found, close the previous section by slicing lines from its start_line to current index (exclusive) and create a MarkdownSection.\n- The content of each section should include the header line if include_header_in_chunk is True; otherwise store content without the header in MarkdownSection.content but keep header string separately.\n- Implement parent header tracking to preserve hierarchy:\n  - Maintain a stack of active headers: e.g., list of tuples (level, title).\n  - When a new header of level L is seen, pop from stack until top has level < L, then push (L, title).\n  - For each section, compute parent_headers as a copy of the current stack excluding the section’s own header or including only levels < current level (depending on preference, but align with PRD’s header_hierarchy example where h1 is parent of h2).\n- At the end of the document, finalize the last section from its start_line to the end of lines.\n- Return a list[MarkdownSection] preserving original order; ensure sections without any content beyond the header still have content = header line (if include_header_in_chunk) to avoid empty nodes later.\n- Handle documents with no headers by returning a single MarkdownSection with header=\"\" and level=0 and full content.\n",
        "testStrategy": "- Add unit tests in tests/test_chunking_service.py for _split_by_headers:\n  - Case 1: Document with h1, h2, h3 headers and content lines; assert number of sections, their header strings (e.g., \"# Intro\", \"## Methods\"), levels (1,2,3), and start_line indices.\n  - Case 2: Verify parent_headers: for a section under \"# Chapter 1\", \"## Methods\" header_hierarchy should reflect h1 parent.\n  - Case 3: Document without any headers returns a single MarkdownSection with level=0, header=\"\", content equal to full text.\n  - Case 4: Edge cases: multiple consecutive headers with no body, trailing content without header.\n  - Assert that include_header_in_chunk toggling changes whether the header line appears in MarkdownSection.content.\n  - Run pytest and ensure coverage for header_regex behavior and hierarchy building logic.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Implement token counting, sentence fallback, and chunk() orchestration",
        "description": "Implement chunk() in MarkdownChunkerStrategy to split by headers first, then apply SentenceSplitter for oversized sections while preserving header metadata and overlap configuration.",
        "details": "Implementation details:\n- In app/services/chunking_service.py implement helper methods:\n  - _token_count(text: str) -> int using the same tokenization approach used elsewhere in chunking_service (e.g., via tiktoken or LlamaIndex token counter). If existing utilities exist (e.g., count_tokens), reuse them for consistency.\n  - _sentence_split(text: str) -> list[TextNode] that wraps the existing SentenceSplitter strategy used by the app:\n    ```python\n    def _sentence_split(self, text: str) -> list[TextNode]:\n        splitter = SentenceSplitter(\n            chunk_size=self.max_chunk_size,\n            chunk_overlap=self.chunk_overlap,\n        )\n        return splitter.split_text(text)\n    ```\n    Adjust to existing API of SentenceSplitter in the codebase.\n  - _create_node(section: MarkdownSection, base_metadata: dict | None = None) -> TextNode that builds a TextNode from the section content and attaches metadata.\n- Implement chunk() orchestration roughly as in PRD pseudocode:\n  ```python\n  def chunk(self, text: str, metadata: dict | None = None) -> list[TextNode]:\n      sections = self._split_by_headers(text)\n      nodes: list[TextNode] = []\n\n      for idx, section in enumerate(sections):\n          if self._token_count(section.content) > self.max_chunk_size:\n              sub_nodes = self._sentence_split(section.content)\n              for node in sub_nodes:\n                  self._attach_markdown_metadata(node, section, idx, len(sections), metadata, is_header_split=False)\n              nodes.extend(sub_nodes)\n          else:\n              node = self._create_node(section, metadata)\n              self._attach_markdown_metadata(node, section, idx, len(sections), metadata, is_header_split=True)\n              nodes.append(node)\n\n      return nodes\n  ```\n- Implement _attach_markdown_metadata to enrich each TextNode’s metadata according to the PRD schema:\n  ```python\n  def _attach_markdown_metadata(\n      self,\n      node: TextNode,\n      section: MarkdownSection,\n      index: int,\n      total_sections: int,\n      base_metadata: dict | None,\n      is_header_split: bool,\n  ) -> None:\n      md = dict(base_metadata or {})\n      hierarchy = self._get_header_hierarchy(...)\n      md.update({\n          \"section_header\": section.header,\n          \"header_level\": section.level,\n          \"header_hierarchy\": hierarchy,\n          \"chunk_index\": index,\n          \"total_chunks\": total_sections,\n          \"is_header_split\": is_header_split,\n      })\n      node.metadata.update(md)\n  ```\n- Ensure that any existing metadata (e.g., source_file) is preserved and supplemented, not overwritten.\n- Respect include_header_in_chunk when assembling node.text; for fallback sentence chunks, consider prefixing the header line or leaving text as-is while only using metadata to avoid repeated headers, depending on retrieval expectations.\n- Ensure chunk_overlap semantics are applied only via SentenceSplitter and that header-based sections are not artificially overlapped unless required by downstream consumers.\n",
        "testStrategy": "- Extend tests/test_chunking_service.py:\n  - Test chunk() on a small markdown doc where all sections are below max_chunk_size: ensure number of nodes equals number of sections and that each node.text starts with the header when include_header_in_chunk=True.\n  - Test a document with a very large section exceeding max_chunk_size: verify that chunk() uses SentenceSplitter (more than one node for that section) and that each sub-node has section_header and is_header_split=False.\n  - Verify that metadata fields section_header, header_level, header_hierarchy, chunk_index, total_chunks, is_header_split are correctly populated and stable.\n  - Test that base metadata (e.g., {\"source_file\": \"doc.pdf\"}) is preserved and present on each node.\n  - Test documents without headers: ensure chunk() falls back to a single section which may then be sentence-split if over limit.\n  - Run pytest and evaluate that new tests cover both header-first and fallback behavior.",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Integrate LlamaIndex Markdown parsers where appropriate",
        "description": "Integrate LlamaIndex MarkdownNodeParser or MarkdownElementNodeParser within MarkdownChunkerStrategy or the chunking pipeline for complex markdown documents while maintaining header-based chunk semantics.",
        "details": "Implementation details:\n- Review existing LlamaIndex integration in chunking_service or related services to identify how documents and TextNode objects are currently constructed.\n- Add optional use of LlamaIndex markdown node parsers for cases where structured parsing is beneficial:\n  ```python\n  from llama_index.core.node_parser import MarkdownNodeParser, MarkdownElementNodeParser\n  ```\n- Decide on a simple selection strategy:\n  - Default to MarkdownNodeParser for general cases.\n  - Optionally allow selecting MarkdownElementNodeParser via a constructor flag or configuration (e.g., use_element_parser: bool = False) for docs with complex elements (tables, code blocks).\n- Implement an internal helper in MarkdownChunkerStrategy or a nearby utility:\n  ```python\n  def _parse_markdown_with_llamaindex(self, text: str) -> list[TextNode]:\n      parser = MarkdownNodeParser()\n      docs = [Document(text=text)]  # use correct LlamaIndex Document class\n      return parser.get_nodes_from_documents(docs)\n  ```\n- Evaluate whether to:\n  - Use LlamaIndex only for token counting and TextNode creation while still using header-based splitting; or\n  - Defer fully to LlamaIndex node parsing for specific content types while still attaching header metadata.\n  Follow the PRD’s architecture, which prioritizes header-based MarkdownChunker plus fallback SentenceSplitter; keep LlamaIndex integration minimal and non-breaking.\n- Ensure imports and usage align with the currently installed LlamaIndex version (using llama_index.core.* namespaces as in the PRD).\n- Guard this integration behind feature flags or configuration if necessary to avoid impacting existing behavior unexpectedly.\n",
        "testStrategy": "- Add focused tests (or adjust existing integration tests) to confirm that:\n  - MarkdownNodeParser import paths are valid and do not raise ImportError in the project environment.\n  - The optional parser integration function _parse_markdown_with_llamaindex returns a non-empty list of TextNode objects for sample markdown input.\n  - When the feature flag or configuration to use MarkdownElementNodeParser is enabled, it is instantiated and invoked correctly.\n  - Overall MarkdownChunkerStrategy.chunk() output structure remains the same when LlamaIndex parsing is enabled vs disabled (aside from any additional metadata), ensuring backward compatibility.\n  - Run pytest and ensure that these tests are skipped or guarded appropriately if LlamaIndex version changes are detected (e.g., via hasattr checks).",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Wire MarkdownChunkerStrategy into chunking_service dispatch",
        "description": "Register MarkdownChunkerStrategy within app/services/chunking_service.py and ensure it can be selected alongside existing chunking strategies without breaking current behavior.",
        "details": "Implementation details:\n- Inspect existing strategy selection mechanism in app/services/chunking_service.py (e.g., a factory function, strategy map, or enum-to-class mapping).\n- Add an entry for \"markdown\" or MarkdownChunkerStrategy to the mapping, for example:\n  ```python\n  CHUNKING_STRATEGIES = {\n      \"sentence\": SentenceChunkerStrategy,\n      \"markdown\": MarkdownChunkerStrategy,\n      # other strategies...\n  }\n  ```\n  or adjust to match the project’s existing pattern.\n- Ensure that the default strategy remains SentenceSplitter-based to preserve backward compatibility unless explicitly configured otherwise.\n- Expose MarkdownChunkerStrategy configuration options (e.g., max_chunk_size, chunk_overlap, include_header_in_chunk) through any existing configuration objects or function parameters so callers may override defaults.\n- Confirm that the return type of the factory/dispatcher remains consistent (e.g., returns a ChunkingStrategy instance) and that MarkdownChunkerStrategy adheres to this interface.\n- Add logging at debug level when MarkdownChunkerStrategy is selected to aid in troubleshooting.\n",
        "testStrategy": "- Extend chunking_service tests to cover strategy selection:\n  - Instantiate strategy via the factory/dispatcher with a value corresponding to markdown and assert that the returned instance is MarkdownChunkerStrategy.\n  - Confirm that requesting the default or existing strategies still returns the SentenceSplitter-based implementation.\n  - For a simple markdown input, call the dispatcher-created strategy.chunk() and assert that the results are not empty and reflect header awareness (e.g., first node metadata.section_header is not None).\n  - Run pytest to ensure no regressions in existing strategy selection tests.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Add markdown content detection in document_processor",
        "description": "Enhance app/services/document_processor.py to detect markdown content (by file extension and/or content heuristics) and expose this information to downstream processing.",
        "details": "Implementation details:\n- Open app/services/document_processor.py and locate the logic that normalizes or classifies document content, especially around LlamaParse outputs.\n- Implement markdown detection with a combination of:\n  - File extension checks for .md and .markdown.\n  - Content heuristics: presence of markdown headers (lines matching ^#{1,6}\\s+), bullet lists (-, *, + at line start), or fenced code blocks (``` or ~~~).\n- Add a function like:\n  ```python\n  def is_markdown_content(source_path: str | None, text: str) -> bool:\n      if source_path and source_path.lower().endswith((\".md\", \".markdown\")):\n          return True\n      for line in text.splitlines():\n          if HEADER_RE.match(line):\n              return True\n      return False\n  ```\n  Reuse HEADER_RE from chunking_service if possible or define a shared utility.\n- When processing LlamaParse output (already configured with result_type=\"markdown\" in app/tasks/source_processing.py), annotate the document or metadata with a flag like metadata[\"is_markdown\"] = True.\n- Ensure this flag is included in whatever data structure is passed into the chunking service (e.g., SourceDocument or similar) so that source_processing can choose MarkdownChunkerStrategy when appropriate.\n- Keep behavior for non-markdown sources unchanged, and ensure that unstructured PDF/HTML that coincidentally contains some # signs does not cause misclassification by requiring multiple signals or the explicit LlamaParse markdown flag when available.\n",
        "testStrategy": "- Create unit tests in tests/test_document_processor.py:\n  - Test is_markdown_content() for a .md file with markdown headers; assert True.\n  - Test for a .pdf or .txt file with minimal markdown-like content; adjust heuristics so classification aligns with expectations (likely False unless explicitly flagged).\n  - Test detection for LlamaParse outputs where a metadata flag like result_type=\"markdown\" is present; assert that the is_markdown flag is set accordingly.\n  - End-to-end style test: simulate processing of a markdown file and assert that the resulting document metadata contains is_markdown=True.\n  - Run pytest to validate tests and ensure existing document processing behavior remains intact.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Update source_processing to use MarkdownChunkerStrategy for markdown inputs",
        "description": "Modify app/tasks/source_processing.py so that when the source is markdown (native .md or LlamaParse markdown output), it uses MarkdownChunkerStrategy with SentenceSplitter fallback for large sections, while preserving backward compatibility.",
        "details": "Implementation details:\n- In app/tasks/source_processing.py locate the path where documents are parsed with LlamaParse using result_type=\"markdown\" and then passed to the chunking_service.\n- Inject logic to select the chunking strategy based on:\n  - Source file extension (.md or .markdown).\n  - is_markdown flag set by document_processor.\n  - Explicit configuration (e.g., a per-source chunking_strategy field once Phase 4 is implemented).\n- Example selection logic (pseudocode):\n  ```python\n  if doc.metadata.get(\"is_markdown\") or source_path.endswith(\".md\"):\n      chunker = MarkdownChunkerStrategy(\n          max_chunk_size=settings.chunking.max_tokens,\n          chunk_overlap=settings.chunking.overlap,\n      )\n  else:\n      chunker = SentenceChunkerStrategy(...)\n  nodes = chunker.chunk(doc.text, metadata=doc.metadata)\n  ```\n- Ensure that existing configurations that explicitly request a non-markdown strategy still work and override automatic detection if needed.\n- Confirm that for markdown sources, header metadata produced by MarkdownChunkerStrategy (section_header, header_hierarchy, etc.) is preserved through to the vector store ingestion pipeline.\n- Maintain any existing logging and error handling semantics and add a debug log when markdown chunking is applied for traceability.\n",
        "testStrategy": "- Add tests in tests/test_source_processing.py (create file if it does not exist):\n  - Mock a markdown document (e.g., metadata.is_markdown=True) and assert that source_processing chooses MarkdownChunkerStrategy by checking type of constructed chunker or inspecting resulting nodes’ metadata.\n  - Mock a non-markdown PDF document and confirm that SentenceChunkerStrategy remains in use.\n  - For a simulated LlamaParse markdown output, run through the processing pipeline and assert that chunks include section_header metadata and that chunks do not split mid-section when section size < max_chunk_size.\n  - Run the full test suite to ensure existing behavior is unaffected for non-markdown sources.",
        "priority": "high",
        "dependencies": [
          5,
          6
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Enhance chunk metadata and update vector store schema if needed",
        "description": "Extend chunk metadata to include section header details and hierarchy and update the vector store schema or ingestion logic so these fields are stored and queryable.",
        "details": "Implementation details:\n- Align metadata filled in MarkdownChunkerStrategy with the schema from the PRD:\n  ```json\n  {\n    \"section_header\": \"## Methods\",\n    \"header_level\": 2,\n    \"header_hierarchy\": {\n      \"h1\": \"Chapter 1: Introduction\",\n      \"h2\": \"Methods\"\n    },\n    \"source_file\": \"document.pdf\",\n    \"chunk_index\": 3,\n    \"total_chunks\": 15,\n    \"is_header_split\": true\n  }\n  ```\n- Implement _get_header_hierarchy(sections, index) in MarkdownChunkerStrategy so it returns a dict keyed by header name (e.g., \"h1\", \"h2\") with values from MarkdownSection.parent_headers and current section header.\n- Review the vector store ingestion logic (where TextNode.metadata is stored) to ensure that new fields are persisted; this may be in a separate service or repository layer.\n- If the vector store schema is explicit (e.g., SQL tables or a structured metadata schema), add columns or JSON fields as appropriate to hold the new metadata keys.\n- Ensure backward compatibility:\n  - For non-markdown chunks, either omit these keys or set them to sensible defaults (e.g., section_header=None, is_header_split=False).\n- Consider indexing strategies for improved retrieval (e.g., enabling filtering by section_header or header_level if the underlying vector DB supports metadata filters), but avoid overengineering beyond what the PRD requires.\n",
        "testStrategy": "- Add or update tests in the module responsible for vector store ingestion (e.g., tests/test_vector_store_integration.py):\n  - Create a TextNode with the new metadata fields and persist it; retrieve it from the vector store and assert that metadata fields are round-tripped correctly.\n  - Test that documents without markdown headers still ingest successfully and either omit or set default values for the new fields.\n  - If metadata-based filtering is supported, add a test that queries by section_header or header_level and verify that only relevant chunks are returned.\n  - Run pytest and confirm that schema changes do not break existing tests or migrations.",
        "priority": "medium",
        "dependencies": [
          3,
          7
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Add configuration and API support for chunking strategy selection",
        "description": "Introduce configuration options and API-level controls to select chunking strategies, including a markdown option and markdown-specific settings, updating app/models/sources.py and relevant routes.",
        "details": "Implementation details:\n- In app/models/sources.py add a chunking_strategy field (enum-like) to the Source or equivalent model, with allowed values such as: \"sentence\", \"markdown\", and any existing strategies.\n  - For example, using Pydantic or dataclasses:\n    ```python\n    class ChunkingStrategyEnum(str, Enum):\n        sentence = \"sentence\"\n        markdown = \"markdown\"\n    ```\n- Add optional configuration fields for markdown-specific settings (e.g., markdown_max_chunk_size, markdown_chunk_overlap, include_header_in_chunk) in the project’s config module or settings file.\n- Update API request/response schemas (e.g., FastAPI Pydantic models) to allow clients to specify chunking_strategy and markdown-related options when creating or updating sources.\n- Modify the logic in source_processing and/or chunking_service factory to honor an explicitly configured chunking_strategy over automatic detection, while falling back to detection when the value is unset.\n- Update any OpenAPI or API documentation generation annotations so that the new field appears in API docs.\n- Keep default behavior unchanged for existing clients (e.g., default chunking_strategy=\"sentence\" if not provided).\n",
        "testStrategy": "- Add or update tests for API models and routes (e.g., tests/test_sources_api.py):\n  - Validate that creating a new source with chunking_strategy=\"markdown\" is accepted and stored.\n  - Validate that omitting chunking_strategy defaults to the existing strategy (likely sentence).\n  - Verify that invalid chunking_strategy values result in proper validation errors.\n  - Test that when a source has chunking_strategy=\"markdown\", the downstream processing pipeline uses MarkdownChunkerStrategy even if heuristic detection would not choose it.\n  - Run the API test suite to ensure backward compatibility.",
        "priority": "low",
        "dependencies": [
          5,
          7
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Create comprehensive tests and validation for markdown chunk quality",
        "description": "Add higher-level tests and validation checks to ensure markdown-aware chunking preserves section boundaries, maintains context, and falls back correctly when headers are absent or sections are oversized.",
        "details": "Implementation details:\n- Design end-to-end or integration-style tests that simulate the full pipeline:\n  - Input: sample markdown documents representing common structures (e.g., PRDs, research papers with #, ##, ### headers, and large sections).\n  - Processing: run through document_processor → source_processing → chunking_service → (optional) vector store ingestion.\n- Validate key success metrics qualitatively via tests:\n  - Chunk quality: majority of chunks start with a header or represent a complete section when section size < max_chunk_size.\n  - Context preservation: content under a given header mostly appears in contiguous chunks, especially compared to the old SentenceSplitter-only behavior.\n- Implement concrete assertions, for example:\n  - For a \"Methods\" section under a specific chapter, ensure that at least one chunk’s metadata.section_header contains \"Methods\" and its text starts near that section’s content.\n  - Ensure that chunks created by sentence fallback for large sections still carry section_header metadata and header_hierarchy.\n  - Verify fallback behavior for documents with no headers: ensure the pipeline either uses SentenceSplitter or returns a single large node that is then split by sentences.\n- Optionally, implement a regression test comparing old vs new behavior by invoking SentenceSplitter directly and confirming that MarkdownChunkerStrategy produces fewer cross-section splits on a synthetic document.\n- Ensure tests are deterministic and do not depend on external services by mocking LlamaParse and vector store interactions when necessary.\n",
        "testStrategy": "- Add a new test module (e.g., tests/test_markdown_chunking_integration.py):\n  - Test that for a multi-section markdown doc, len(set(node.metadata[\"section_header\"] for node in nodes)) matches the number of logical sections.\n  - Assert that for each section, the first chunk for that section begins with the header (or at least contains it at the top) when include_header_in_chunk=True.\n  - Verify that when max_chunk_size is artificially small, hybrid behavior triggers: sections are first separated by headers, then sub-chunked via SentenceSplitter, with proper metadata.\n  - Confirm that documents without headers are successfully processed and that is_header_split is False for all nodes.\n  - Run pytest and examine coverage for the full markdown-aware pipeline.\n  - Optionally, measure simple statistics (e.g., percentage of chunks starting with a header) inside tests and assert they exceed a threshold for the sample documents.",
        "priority": "medium",
        "dependencies": [
          2,
          3,
          6,
          7,
          8,
          9
        ],
        "status": "done",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2026-01-12T01:53:19.456Z",
      "updated": "2026-01-12T01:53:19.456Z",
      "description": "Tasks for 006-markdown-chunking context"
    }
  }
}