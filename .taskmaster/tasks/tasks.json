{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Backend Environment Setup (FastAPI, Celery, Supabase, Redis, Neo4j)",
        "description": "Establish the production backend infrastructure using FastAPI, Celery, Supabase PostgreSQL (with pgvector), Redis (Upstash), and Neo4j Community (Docker).",
        "details": "Provision Render services for FastAPI and Celery. Configure Supabase PostgreSQL with pgvector and graph tables. Set up Redis (Upstash) for caching and Celery broker. Deploy Neo4j Community via Docker on Mac Studio for knowledge graph storage. Ensure all services use TLS 1.3 and encrypted environment variables. Recommended versions: FastAPI >=0.110, Celery >=5.3, supabase-py >=2.0, redis-py >=5.0, Neo4j Community 5.x.",
        "testStrategy": "Validate service connectivity, health endpoints, and database schema migrations. Run integration tests for API endpoints and Celery task execution. Confirm Redis and Neo4j connectivity.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Provision FastAPI and Celery Services on Render",
            "description": "Deploy FastAPI and Celery worker services using Render, ensuring production-grade configuration and separation.",
            "dependencies": [],
            "details": "Set up two separate Render services: one for FastAPI (API server) and one for Celery (background worker). Use recommended versions (FastAPI >=0.110, Celery >=5.3). Configure environment variables securely and ensure both services are reachable over the network.",
            "status": "done",
            "testStrategy": "Verify service deployment via Render dashboards. Access FastAPI health endpoint and confirm Celery worker logs show successful startup.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Configure Supabase PostgreSQL with pgvector Extension and Graph Tables",
            "description": "Set up Supabase PostgreSQL instance, enable pgvector, and create required tables for embeddings and graph data.",
            "dependencies": [],
            "details": "In Supabase dashboard, enable the 'vector' extension (pgvector) via Extensions panel or SQL command. Create tables for storing embeddings (e.g., documents with embedding vector columns) and graph structures as needed. Use recommended supabase-py >=2.0 for client access.\n<info added on 2025-11-03T04:12:52.520Z>\nSupabase PostgreSQL is already provisioned at qohsmuevxuetjpuherzo.supabase.co with credentials stored in the .env file. The database is accessible via Supabase Management Console Panel (MCP). \n\nTo complete this subtask:\n\n1. Connect to the Supabase PostgreSQL instance using the MCP or SQL editor.\n\n2. Enable the pgvector extension by executing:\n   ```sql\n   CREATE EXTENSION IF NOT EXISTS vector;\n   ```\n\n3. Create all 37+ required tables from /workflows/database_setup.md, including:\n   - documents\n   - document_chunks\n   - chat_sessions\n   - user_memory_nodes\n   - crewai_agents\n   - crewai_crews\n   - vector tables with embedding columns\n   - graph structure tables\n   - and all other tables specified in the database setup file\n\n4. Verify table creation and ensure proper relationships and constraints are established according to the schema definitions.\n\n5. Test database connectivity using supabase-py >=2.0 client from the application.\n</info added on 2025-11-03T04:12:52.520Z>",
            "status": "done",
            "testStrategy": "Run SQL queries to confirm pgvector is enabled and tables exist. Insert and retrieve sample data, including vector columns.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Set Up Redis (Upstash) for Caching and Celery Broker",
            "description": "Provision a Redis instance on Upstash and configure it for both caching and as the Celery message broker.",
            "dependencies": [],
            "details": "Create a new Redis database on Upstash. Obtain connection URL and credentials. Configure FastAPI and Celery to use this Redis instance for caching and as the Celery broker. Use redis-py >=5.0 for integration.",
            "status": "done",
            "testStrategy": "Connect to Redis from both FastAPI and Celery. Set and retrieve cache keys. Confirm Celery can enqueue and process tasks using Redis broker.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Deploy Neo4j Community Edition via Docker on Mac Studio",
            "description": "Install and run Neo4j Community Edition (5.x) using Docker on the Mac Studio for knowledge graph storage.",
            "dependencies": [],
            "details": "Pull the official Neo4j Community Docker image (version 5.x). Configure Docker container with appropriate ports, volumes for data persistence, and secure environment variables. Ensure Neo4j is accessible from the local network.",
            "status": "done",
            "testStrategy": "Access Neo4j Browser UI, run basic Cypher queries, and verify data persistence after container restart.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Configure TLS 1.3 and Encrypted Environment Variables for All Services",
            "description": "Ensure all backend services (FastAPI, Celery, Supabase, Redis, Neo4j) use TLS 1.3 for secure communication and store environment variables encrypted.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Update service configurations to enforce TLS 1.3 (e.g., Render custom domains with TLS, Upstash Redis with TLS, Supabase with SSL, Neo4j Docker with TLS certificates). Store all secrets and environment variables using encrypted storage mechanisms provided by each platform.",
            "status": "done",
            "testStrategy": "Attempt connections using only TLS 1.3. Inspect certificates and verify environment variables are not exposed in logs or process listings.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Validate Integration and Connectivity Across All Services",
            "description": "Test and confirm that FastAPI, Celery, Supabase, Redis, and Neo4j are correctly integrated and can communicate securely.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Implement health checks and integration tests: FastAPI connects to Supabase and Neo4j, Celery tasks use Redis broker and access Supabase, all over TLS. Run end-to-end tests for API endpoints and background tasks.",
            "status": "done",
            "testStrategy": "Run automated integration tests. Check logs for successful connections. Use tools like curl or Postman to verify TLS and endpoint health.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down the backend environment setup into subtasks for provisioning each service (FastAPI, Celery, Supabase PostgreSQL with pgvector, Redis, Neo4j), configuring secure communication (TLS 1.3), and validating integration and connectivity."
      },
      {
        "id": 2,
        "title": "File Upload Interface & Backblaze B2 Integration",
        "description": "Implement multi-file upload (up to 10 files, 100MB each) with drag-and-drop UI, progress indicators, and direct upload to Backblaze B2 pending/courses/ folder.",
        "details": "Use Gradio or Streamlit for the web UI. Integrate Backblaze B2 via b2sdk (Python >=1.20). Support Mountain Duck polling (30s) and immediate processing for web UI uploads. Enforce file size/type limits and progress feedback. Organize files per B2 folder structure.",
        "testStrategy": "Upload various file types and sizes, verify progress indicators, and confirm files appear in B2 pending/courses/. Test both Mountain Duck and web UI flows.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Implement Multi-File Upload UI with Drag-and-Drop",
            "description": "Create a user interface using Streamlit or Gradio that supports uploading up to 10 files (max 100MB each) via drag-and-drop, with progress indicators and file type/size validation.",
            "dependencies": [],
            "details": "Use Streamlit's st.file_uploader with accept_multiple_files=True or Gradio's file upload component. Implement drag-and-drop functionality, enforce file type and size limits, and display progress indicators for each file. Ensure the UI is intuitive and provides feedback on upload status and errors.",
            "status": "done",
            "testStrategy": "Upload various file types and sizes, verify drag-and-drop works, progress indicators display correctly, and validation prevents unsupported files.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Backblaze B2 Direct Upload via b2sdk",
            "description": "Connect the file upload UI to Backblaze B2 using b2sdk (Python >=1.20), enabling direct upload of files to the pending/courses/ folder and organizing files per B2 folder structure.",
            "dependencies": [
              1
            ],
            "details": "Configure b2sdk for authentication and folder management. Implement logic to upload files directly from the UI to the pending/courses/ folder, ensuring files are organized according to the required B2 structure. Handle upload errors and provide feedback to the user.",
            "status": "done",
            "testStrategy": "Upload files through the UI and confirm they appear in the correct B2 folder. Test error handling and folder organization.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Mountain Duck Polling and Immediate Processing Logic",
            "description": "Support file uploads via Mountain Duck by polling the local folder every 30 seconds and trigger immediate processing for files uploaded via the web UI.",
            "dependencies": [
              2
            ],
            "details": "Set up a polling mechanism to detect new files in the local folder synced by Mountain Duck every 30 seconds. For files uploaded via the web UI, initiate processing immediately after upload. Ensure both flows enforce file limits and integrate with the B2 upload logic.",
            "status": "done",
            "testStrategy": "Simulate uploads via Mountain Duck and web UI, verify polling detects new files, immediate processing works, and all files are uploaded to B2 with correct feedback.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on file upload interface & backblaze b2 integration."
      },
      {
        "id": 3,
        "title": "File Format Validation & Security Scanning",
        "description": "Validate file formats, check integrity, scan for malware, and enforce MIME/extension rules before upload.",
        "details": "Use python-magic for MIME detection, validate extensions, and run integrity checks (e.g., PDF header validation). Integrate ClamAV (clamd) for malware scanning. Reject unsupported formats with clear error messages.",
        "testStrategy": "Attempt uploads of valid, corrupted, and malicious files. Confirm correct rejection and error messaging. Validate security scan logs.",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement File Format and MIME Type Validation",
            "description": "Detect and validate the file's MIME type and extension before upload using python-magic and extension checks.",
            "dependencies": [],
            "details": "Use the python-magic library to inspect the file's magic number and determine its true MIME type. Cross-check this with the file extension to ensure consistency. Reject files with mismatched or unsupported MIME types/extensions, and provide clear error messages. Consider using additional libraries like file-validator for comprehensive checks if needed.",
            "status": "done",
            "testStrategy": "Attempt uploads with valid and invalid file types and extensions. Confirm correct acceptance or rejection and error messaging.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Perform File Integrity and Header Validation",
            "description": "Check file integrity and validate headers for supported formats (e.g., PDF, images) to ensure files are not corrupted or malformed.",
            "dependencies": [
              1
            ],
            "details": "For each supported file type, implement header validation (e.g., check PDF header for '%PDF', image headers for magic numbers). Reject files that fail integrity or header checks. Ensure that only structurally valid files proceed to the next stage.",
            "status": "done",
            "testStrategy": "Upload corrupted or partially valid files and verify that they are rejected with appropriate error messages.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Malware Scanning with ClamAV",
            "description": "Scan validated files for malware using ClamAV (clamd) before final acceptance.",
            "dependencies": [
              2
            ],
            "details": "After passing format and integrity checks, submit files to ClamAV for malware scanning. Reject any files flagged as malicious and log the incident. Ensure the scanning process is efficient and does not introduce significant upload latency.",
            "status": "done",
            "testStrategy": "Upload files containing known malware signatures and verify detection, rejection, and logging. Confirm clean files are accepted.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on file format validation & security scanning."
      },
      {
        "id": 4,
        "title": "Metadata Extraction & Supabase Storage",
        "description": "Extract basic and advanced metadata (filename, size, type, timestamps, EXIF, audio/video info) and store in Supabase documents table.",
        "details": "Use Python libraries: exifread for images, mutagen for audio/video, python-docx for DOCX metadata. Store extracted metadata in Supabase documents table as per schema. Ensure upload triggers metadata extraction.",
        "testStrategy": "Upload files of each supported type, verify metadata extraction accuracy, and confirm correct Supabase storage.",
        "priority": "high",
        "dependencies": [
          "3"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Metadata Extraction for Supported File Types",
            "description": "Develop Python functions to extract basic and advanced metadata from images, audio/video, DOCX, and PDF files using appropriate libraries.",
            "dependencies": [],
            "details": "Use exifread for image EXIF data, mutagen for audio/video metadata, python-docx for DOCX files, and PyPDF2 or pdfminer.six for PDF metadata extraction. Ensure extraction covers filename, size, type, timestamps, and relevant advanced fields (EXIF, audio/video info, document properties). Structure output as per Supabase schema requirements.",
            "status": "done",
            "testStrategy": "Unit test each extractor with sample files of each type. Validate that all required metadata fields are present and accurate.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Metadata Extraction with File Upload Workflow",
            "description": "Ensure that metadata extraction is automatically triggered upon file upload and that extracted data is prepared for storage.",
            "dependencies": [
              1
            ],
            "details": "Modify the upload handler to invoke the correct extraction function based on file type immediately after upload. Collect and format extracted metadata into a dictionary/object matching the Supabase documents table schema.\n<info added on 2025-11-05T22:11:51.333Z>\nImplementation completed for metadata extraction integration with upload workflow:\n\n1. Added metadata_extractor import to upload.py\n2. Modified upload flow to:\n   - Create temp file if not already created (for virus scanning)\n   - Extract metadata from temp file using MetadataExtractor\n   - Include extracted metadata in upload results\n3. Installed required libraries: exifread 3.5.1 and mutagen 1.47.0\n4. Metadata extraction happens after validation and virus scanning, before B2 upload\n5. Graceful error handling - if extraction fails, error is logged but upload continues\n6. Metadata is included in JSON response under \"metadata\" key for each uploaded file\n</info added on 2025-11-05T22:11:51.333Z>",
            "status": "done",
            "testStrategy": "Simulate file uploads via the interface and verify that metadata extraction is triggered and output is correctly formatted for storage.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Store Extracted Metadata in Supabase Documents Table",
            "description": "Insert the extracted metadata into the Supabase documents table, ensuring schema compliance and error handling.",
            "dependencies": [
              2
            ],
            "details": "Use the Supabase Python client to insert metadata records into the documents table. Implement error handling for failed inserts and log issues for debugging. Confirm that all required fields are populated and that the data matches the schema.\n<info added on 2025-11-05T22:21:11.569Z>\nSuccessfully implemented the SupabaseStorage class in app/services/supabase_storage.py with methods for managing document metadata: store_document_metadata(), get_document_by_file_id(), update_document_status(), and list_documents(). The implementation has been integrated into the upload workflow immediately after the B2 upload process. The API response now includes a \"supabase_stored\" boolean flag to indicate successful metadata storage. The system gracefully degrades if Supabase is not configured, allowing the application to function without interruption. Testing confirms that the complete upload workflow functions as expected - metadata extraction works perfectly and Supabase storage attempts are handled gracefully, returning false if not configured.\n</info added on 2025-11-05T22:21:11.569Z>",
            "status": "done",
            "testStrategy": "Upload files of each supported type, then query the Supabase documents table to verify that metadata is stored correctly and completely.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on metadata extraction & supabase storage."
      },
      {
        "id": 5,
        "title": "Duplicate Detection (SHA-256 & Fuzzy Matching)",
        "description": "Detect duplicate and near-duplicate files using SHA-256 hashes and optional fuzzy matching.",
        "details": "Compute SHA-256 hash for each file and check against Supabase documents table. Implement fuzzy matching using Levenshtein distance for filenames and content (rapidfuzz >=2.0). Provide skip/overwrite options.",
        "testStrategy": "Upload duplicate and near-duplicate files, verify detection and user options. Confirm deduplication accuracy.",
        "priority": "high",
        "dependencies": [
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement SHA-256 Hash-Based Duplicate Detection",
            "description": "Compute SHA-256 hashes for each file and compare against existing hashes in the Supabase documents table to identify exact duplicates.",
            "dependencies": [],
            "details": "Use a reliable hashing library to generate SHA-256 hashes for all files. Query the Supabase documents table for existing hashes and flag files with matching hashes as duplicates. Ensure efficient scanning and parallel processing for large file sets.",
            "status": "done",
            "testStrategy": "Upload files with identical content and verify that duplicates are detected solely by hash comparison. Confirm that files with different content are not flagged as duplicates.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Fuzzy Matching for Near-Duplicate Detection",
            "description": "Apply fuzzy matching algorithms (Levenshtein distance via rapidfuzz >=2.0) to filenames and file content to identify near-duplicate files.",
            "dependencies": [
              1
            ],
            "details": "After hash-based filtering, use rapidfuzz to compute similarity scores for filenames and optionally file contents. Set configurable thresholds for similarity to flag near-duplicates. Optimize for performance when comparing large numbers of files.",
            "status": "done",
            "testStrategy": "Upload files with similar but not identical names and/or content. Verify that near-duplicates are detected according to the configured similarity threshold.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement User Options for Duplicate Handling (Skip/Overwrite)",
            "description": "Provide user interface and backend logic for skip or overwrite actions when duplicates or near-duplicates are detected.",
            "dependencies": [
              1,
              2
            ],
            "details": "Design UI prompts and backend logic to allow users to choose whether to skip uploading duplicates, overwrite existing files, or take other actions. Ensure options are clearly presented and actions are reliably executed.",
            "status": "done",
            "testStrategy": "Simulate duplicate and near-duplicate uploads, test all user options (skip, overwrite), and verify correct file handling and user feedback.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on duplicate detection (sha-256 & fuzzy matching)."
      },
      {
        "id": 6,
        "title": "Celery Task Queue Management",
        "description": "Implement priority-based Celery task queue for async document processing, with status tracking, retries, and dead letter queue.",
        "details": "Configure Celery with Redis broker. Use priority queues (urgent, normal, low). Implement status tracking in Supabase file_uploads table. Add retry logic (3 attempts, exponential backoff) and dead letter queue for failed tasks.",
        "testStrategy": "Submit tasks with varying priorities, simulate failures, and verify retry and dead letter queue behavior.",
        "priority": "high",
        "dependencies": [
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Celery with Redis for Priority Queues",
            "description": "Set up Celery to use Redis as the broker and implement priority-based task queues (urgent, normal, low).",
            "dependencies": [],
            "details": "Update Celery configuration to use Redis as the broker. Define separate queues for each priority level (e.g., urgent, normal, low) and configure the broker_transport_options with 'queue_order_strategy': 'priority'. Adjust worker_prefetch_multiplier to 1 for effective prioritization. Ensure workers are started with the correct queue order.",
            "status": "done",
            "testStrategy": "Submit tasks with different priorities and verify that urgent tasks are processed before normal and low priority tasks.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Status Tracking with Supabase",
            "description": "Implement status updates for each task in the Supabase file_uploads table.",
            "dependencies": [
              1
            ],
            "details": "Modify Celery tasks to update the status field in the Supabase file_uploads table at key stages (queued, started, succeeded, failed). Ensure atomic updates and handle race conditions. Use Supabase client libraries for database operations.",
            "status": "done",
            "testStrategy": "Trigger tasks and verify that status changes are accurately reflected in the Supabase file_uploads table throughout the task lifecycle.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Retry Logic with Exponential Backoff",
            "description": "Add retry logic to Celery tasks with up to 3 attempts and exponential backoff on failure.",
            "dependencies": [
              1
            ],
            "details": "Configure Celery task decorators to include retry parameters: max_retries=3 and a backoff strategy (e.g., exponential). Ensure that exceptions trigger retries and that retry attempts are logged or tracked for observability.",
            "status": "done",
            "testStrategy": "Simulate task failures and verify that tasks are retried up to 3 times with increasing delays between attempts.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Set Up Dead Letter Queue for Failed Tasks",
            "description": "Configure a dead letter queue to capture tasks that fail after all retry attempts.",
            "dependencies": [
              3
            ],
            "details": "Create a dedicated dead letter queue in Celery/Redis. Update task failure handlers to route tasks to this queue after exhausting retries. Optionally, log or notify on dead letter events for monitoring.",
            "status": "done",
            "testStrategy": "Force tasks to fail beyond retry limits and verify their presence in the dead letter queue.",
            "updatedAt": "2025-11-05T23:00:18.337Z",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "End-to-End Testing of Priority Queue Management",
            "description": "Test the complete priority queue system, including status tracking, retries, and dead letter handling.",
            "dependencies": [
              2,
              4
            ],
            "details": "Design and execute test cases covering all priority levels, status transitions, retry scenarios, and dead letter queue routing. Validate system behavior under normal and failure conditions.",
            "status": "done",
            "testStrategy": "Run integration tests that submit tasks with various priorities, induce failures, and confirm correct processing, status updates, retries, and dead letter handling.",
            "parentId": "undefined",
            "updatedAt": "2025-11-05T23:00:42.292Z"
          }
        ],
        "complexity": 6.5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Decompose Celery task queue management into subtasks for priority queue configuration, status tracking integration, retry logic implementation, dead letter queue setup, and end-to-end testing.",
        "updatedAt": "2025-11-05T23:00:42.292Z"
      },
      {
        "id": 7,
        "title": "User Notification System (WebSocket & Email)",
        "description": "Provide real-time upload and processing notifications via WebSocket, with optional email alerts for long-running tasks.",
        "details": "Implement FastAPI WebSocket endpoints for progress and completion notifications. Use SMTP or SendGrid for email alerts. Integrate with frontend for actionable error messages.",
        "testStrategy": "Trigger uploads and processing, verify real-time notifications and email delivery for long tasks.",
        "priority": "medium",
        "dependencies": [
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement FastAPI WebSocket Endpoints for Real-Time Notifications",
            "description": "Develop FastAPI WebSocket endpoints to deliver real-time upload and processing progress and completion notifications to connected clients.",
            "dependencies": [],
            "details": "Set up FastAPI WebSocket routes (e.g., /ws/notifications). Manage client connections and broadcast progress/completion events. Ensure endpoints can handle multiple simultaneous connections and send actionable error messages. Integrate with backend processing logic to emit updates as tasks progress or complete.",
            "status": "done",
            "testStrategy": "Simulate uploads and processing tasks; verify clients receive real-time progress and completion notifications via WebSocket.",
            "parentId": "undefined",
            "updatedAt": "2025-11-05T23:01:15.700Z"
          },
          {
            "id": 2,
            "title": "Integrate Email Alert System for Long-Running Tasks",
            "description": "Add optional email notifications for users when uploads or processing tasks exceed a defined duration threshold.",
            "dependencies": [
              1
            ],
            "details": "Configure SMTP or SendGrid integration for sending emails. Implement logic to detect long-running tasks and trigger email alerts with relevant status and error details. Ensure emails are sent only when user opts in or when thresholds are exceeded. Handle email delivery failures gracefully.",
            "status": "done",
            "testStrategy": "Trigger long-running tasks and confirm that email alerts are sent to the correct recipients with accurate information.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Frontend Integration for Real-Time and Email Notifications",
            "description": "Connect frontend application to WebSocket endpoints and display real-time notifications, including actionable error messages. Provide UI for email alert preferences.",
            "dependencies": [
              1,
              2
            ],
            "details": "Update frontend to establish and manage WebSocket connections, display progress/completion notifications, and show errors in a user-friendly manner. Add UI controls for users to opt in/out of email alerts. Ensure seamless user experience for both notification channels.",
            "status": "done",
            "testStrategy": "Test frontend by uploading files and processing tasks; verify real-time updates and error messages appear, and email preferences are respected.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on user notification system (websocket & email).",
        "updatedAt": "2025-11-05T23:01:15.700Z"
      },
      {
        "id": 8,
        "title": "Backblaze B2 Folder Management & Encryption",
        "description": "Automate file movement across B2 folders (pending → processing → processed/failed) and support zero-knowledge encryption for sensitive files.",
        "details": "Use b2sdk for folder operations. Implement file movement logic based on processing status. Integrate PyCryptodome for optional AES encryption before upload.",
        "testStrategy": "Process files through all folder stages, verify correct organization and encryption for flagged files.",
        "priority": "high",
        "dependencies": [
          "7"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate b2sdk and Set Up B2 Folder Interfaces",
            "description": "Initialize b2sdk, authenticate, and set up interfaces for pending, processing, processed, and failed folders in the B2 bucket.",
            "dependencies": [],
            "details": "Use b2sdk's AccountInfo and B2Api to authenticate and connect to the B2 bucket. Instantiate B2Folder objects for each logical folder (pending, processing, processed, failed) to enable file operations between them.",
            "status": "done",
            "testStrategy": "Verify connection and folder listing for each B2 folder using b2sdk methods.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement File Movement Logic Based on Processing Status",
            "description": "Develop logic to move files between B2 folders according to their processing status (pending → processing → processed/failed).",
            "dependencies": [
              1
            ],
            "details": "Create functions to list files in each folder and move them to the next stage based on status. Ensure atomicity and handle errors during move operations using b2sdk's file copy and delete methods.",
            "status": "done",
            "testStrategy": "Simulate status changes and verify files are moved to the correct folders without duplication or loss.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate PyCryptodome for Optional AES Encryption",
            "description": "Add support for zero-knowledge AES encryption of sensitive files before upload to B2.",
            "dependencies": [
              1
            ],
            "details": "Use PyCryptodome to encrypt files with a user-supplied key before uploading to B2. Ensure encryption is optional and only applied to flagged files. Store encrypted files in the appropriate B2 folder.",
            "status": "done",
            "testStrategy": "Upload both encrypted and unencrypted files, then download and verify decryption for encrypted files.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Handle Status-Based Transitions and Error Recovery",
            "description": "Implement robust handling for file status transitions, including retries and error recovery for failed moves or uploads.",
            "dependencies": [
              2,
              3
            ],
            "details": "Add logic to detect and recover from failed moves or uploads. Implement retry mechanisms and ensure files are not lost or duplicated during transitions. Log all status changes and errors for auditability.",
            "status": "done",
            "testStrategy": "Intentionally trigger errors (e.g., network failures) and verify that files are correctly retried or moved to the failed folder.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Develop Comprehensive Tests for Folder Organization and Encryption",
            "description": "Create automated tests to verify correct file organization across all folder stages and validate encryption/decryption for sensitive files.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Write tests that process files through all folder stages, check their presence in the correct folders, and confirm that encryption is correctly applied and reversible for flagged files.",
            "status": "done",
            "testStrategy": "Run end-to-end tests covering all transitions and encryption scenarios, ensuring files are organized and protected as specified.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Divide B2 folder management and encryption into subtasks for implementing folder movement logic, integrating b2sdk, supporting AES encryption with PyCryptodome, handling status-based transitions, and verifying organization/encryption through tests."
      },
      {
        "id": 9,
        "title": "AI Department Classification Workflow (Claude Haiku)",
        "description": "Classify uploaded documents into 10 departments using Claude Haiku API, storing results in Supabase.",
        "details": "Integrate anthropic-py SDK. Implement async auto_classify_course function as per PRD. Store department, confidence, and subdepartment in documents and courses tables.",
        "testStrategy": "Upload sample documents for each department, verify classification accuracy and Supabase updates.",
        "priority": "high",
        "dependencies": [
          "8"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Claude Haiku API and anthropic-py SDK for Document Classification",
            "description": "Set up the Claude Haiku API and anthropic-py SDK to enable classification of uploaded documents into 10 departments.",
            "dependencies": [],
            "details": "Install and configure the anthropic-py SDK. Implement API authentication and error handling (e.g., retries, rate limits). Ensure the async auto_classify_course function is ready to send document content to Claude Haiku and receive department predictions. Tune parameters such as temperature and max_tokens for optimal classification accuracy.",
            "status": "done",
            "testStrategy": "Send sample documents to the API and verify department predictions are returned correctly. Test error handling by simulating API failures.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Async auto_classify_course Function and PRD Logic",
            "description": "Develop the async auto_classify_course function according to the Product Requirements Document (PRD), ensuring it processes documents and extracts department, confidence, and subdepartment.",
            "dependencies": [
              1
            ],
            "details": "Write the async function to handle document input, call the Claude Haiku API, and parse the response for department, confidence score, and subdepartment. Ensure the function supports batch processing and handles edge cases (e.g., ambiguous classifications). Document the function and its parameters for maintainability.",
            "status": "done",
            "testStrategy": "Unit test the function with mock API responses. Validate output structure and accuracy against expected department labels.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Store Classification Results in Supabase Documents and Courses Tables",
            "description": "Persist the classification results (department, confidence, subdepartment) in the Supabase documents and courses tables.",
            "dependencies": [
              2
            ],
            "details": "Map the classification output to the correct schema fields in Supabase. Implement transactional writes to ensure data consistency. Add logging for successful and failed writes. Verify that updates are reflected in both documents and courses tables as required.",
            "status": "done",
            "testStrategy": "Upload test documents, run classification, and confirm Supabase tables are updated with correct department, confidence, and subdepartment values. Check for data integrity and error handling.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on ai department classification workflow (claude haiku)."
      },
      {
        "id": 10,
        "title": "Universal Document Processing Pipeline",
        "description": "Extract text and structured data from all supported document types using specialized services and fallback methods.",
        "details": "Integrate LlamaIndex (REST API) for clean PDFs, Mistral OCR for scanned PDFs, Tesseract OCR for fallback. Use python-docx for DOCX, mutagen for audio/video, Claude Vision API for images. Implement table/image extraction and maintain page/section info.",
        "testStrategy": "Process each file type, verify extraction accuracy, structure preservation, and fallback logic.",
        "priority": "high",
        "dependencies": [
          "9"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Modular Document Ingestion and Classification",
            "description": "Design and build the pipeline's ingestion layer to accept documents from various sources and classify them by type (PDF, DOCX, image, audio/video).",
            "dependencies": [],
            "details": "Set up connectors for file sources (e.g., S3 buckets, local uploads). Integrate document type detection logic to route files to appropriate extraction modules. Log ingestion events and maintain audit trails for each document.",
            "status": "done",
            "testStrategy": "Submit sample files of each supported type, verify correct classification and routing, and check ingestion logs for completeness.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Specialized Extraction Services and Fallbacks",
            "description": "Connect and orchestrate specialized extraction services for each document type, with fallback logic for unsupported or failed cases.",
            "dependencies": [
              1
            ],
            "details": "Integrate LlamaIndex REST API for clean PDFs, Mistral OCR for scanned PDFs, Tesseract OCR as fallback, python-docx for DOCX, mutagen for audio/video, Claude Vision API for images. Implement logic to select extraction method based on classification and handle failures by cascading to fallback services.",
            "status": "done",
            "testStrategy": "Process a diverse set of documents, intentionally trigger extraction failures, and verify fallback mechanisms activate and extract data as expected.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Extract Structured Data and Metadata with Section/Page Tracking",
            "description": "Develop logic to extract tables, images, and maintain page/section metadata for all processed documents, ensuring structured outputs.",
            "dependencies": [
              2
            ],
            "details": "Implement table and image extraction for supported formats. Track and store page/section information alongside extracted text and structured data. Ensure outputs are normalized for downstream consumption.",
            "status": "done",
            "testStrategy": "Validate extracted outputs for structure, completeness, and correct association of metadata (page/section info) using test documents with known layouts.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on universal document processing pipeline."
      },
      {
        "id": 11,
        "title": "Audio & Video Processing (Soniox, Claude Vision)",
        "description": "Transcribe audio, extract speakers/timestamps, and analyze video frames using Soniox and Claude Vision APIs.",
        "details": "Integrate Soniox REST API for transcription and diarization. Use ffmpeg-python for frame/audio extraction from video. Analyze frames with Claude Vision API. Store transcripts and timeline metadata.",
        "testStrategy": "Process audio and video files, verify transcript accuracy, speaker identification, and frame analysis.",
        "priority": "high",
        "dependencies": [
          "10"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Extract Audio and Video Frames from Input Files",
            "description": "Use ffmpeg-python to extract audio tracks and video frames from input video files for downstream processing.",
            "dependencies": [],
            "details": "Implement a Python module using ffmpeg-python to separate audio from video files and extract video frames at configurable intervals. Ensure extracted audio is in a Soniox-compatible format (e.g., 16kHz mono WAV). Store extracted frames and audio in a structured directory or object storage for later processing.",
            "status": "done",
            "testStrategy": "Run extraction on sample video files, verify correct number and quality of frames, and check audio format compatibility with Soniox.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Transcribe Audio and Extract Speaker/Timestamps with Soniox API",
            "description": "Integrate Soniox REST API to transcribe extracted audio, enabling speaker diarization and timestamp extraction.",
            "dependencies": [
              1
            ],
            "details": "Authenticate with Soniox API using a project API key. Send extracted audio files for transcription using the async or streaming endpoints. Enable speaker diarization and timestamp options in the API request. Parse and store the returned transcript, speaker labels, and word-level timestamps in the database or metadata files.",
            "status": "done",
            "testStrategy": "Submit test audio files, verify transcript accuracy, correct speaker segmentation, and presence of timestamps. Compare results with ground truth if available.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Analyze Video Frames with Claude Vision API and Store Metadata",
            "description": "Send extracted video frames to Claude Vision API for analysis and store the resulting metadata alongside transcripts and timeline data.",
            "dependencies": [
              1
            ],
            "details": "Batch or stream video frames to the Claude Vision API, handling authentication and rate limits. Parse the returned analysis (e.g., scene description, object detection) and associate results with corresponding timestamps. Store all metadata in a structured format, linking frame analysis to transcript timeline.",
            "status": "done",
            "testStrategy": "Process sample frames, verify that analysis results are received and correctly mapped to frame timestamps. Check integration with transcript timeline and metadata storage.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on audio & video processing (soniox, claude vision)."
      },
      {
        "id": 12,
        "title": "Structured Data Extraction (LangExtract)",
        "description": "Extract entities, key-value pairs, and course metadata using LangExtract API.",
        "details": "Integrate LangExtract REST API for field/entity extraction. Store results in Supabase courses and document_chunks tables. Implement intelligent filename generation (M01-L02 format).",
        "testStrategy": "Process documents with structured fields, verify entity extraction and metadata accuracy.",
        "priority": "high",
        "dependencies": [
          "11"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Extraction Schema and Example Prompts for LangExtract",
            "description": "Specify the entity types, key-value pairs, and course metadata fields to be extracted. Create example prompts and sample extractions to guide the LangExtract API.",
            "dependencies": [],
            "details": "List all required fields (e.g., course title, module number, lesson number, instructor, date) and define their expected formats. Write natural language prompts and provide high-quality example extractions using LangExtract's ExampleData objects to ensure consistent output schema and accurate extraction.",
            "status": "done",
            "testStrategy": "Review extracted fields from test documents to confirm schema coverage and prompt effectiveness.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate LangExtract REST API for Automated Entity and Metadata Extraction",
            "description": "Connect to the LangExtract REST API and implement logic to process course documents, extracting entities, key-value pairs, and metadata as defined in the schema.",
            "dependencies": [
              1
            ],
            "details": "Set up API authentication and request handling. For each uploaded course document, send the text and extraction instructions/examples to LangExtract. Parse the returned structured data, ensuring source grounding and attribute mapping. Handle errors and edge cases (e.g., missing fields, ambiguous extractions).",
            "status": "done",
            "testStrategy": "Process a variety of course documents and verify that all required entities and metadata are extracted with correct attributes and source positions.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Store Extracted Data in Supabase and Implement Intelligent Filename Generation",
            "description": "Save the extracted entities and metadata into Supabase courses and document_chunks tables. Generate filenames using the M01-L02 format based on extracted module and lesson numbers.",
            "dependencies": [
              2
            ],
            "details": "Map extracted fields to Supabase table schemas, ensuring correct data types and relationships. Implement logic to generate filenames (e.g., M01-L02) from extracted metadata and associate them with stored records. Validate data integrity and handle duplicate or conflicting entries.",
            "status": "done",
            "testStrategy": "Insert extracted data from sample documents into Supabase, verify correct mapping and filename generation, and check for consistency across multiple uploads.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on structured data extraction (langextract)."
      },
      {
        "id": 13,
        "title": "Adaptive Chunking Strategy Implementation",
        "description": "Implement semantic, code, and transcript chunking with configurable size and overlap, preserving context.",
        "details": "Use LlamaIndex chunking for documents, custom logic for code (AST parsing), and time/topic-based chunking for transcripts. Store chunks in document_chunks table with metadata and overlap.",
        "testStrategy": "Chunk various document types, verify chunk boundaries, overlap, and context preservation.",
        "priority": "high",
        "dependencies": [
          "12"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Adaptive Semantic Chunking for Documents",
            "description": "Develop and configure semantic chunking for text documents using LlamaIndex, supporting adjustable chunk size and overlap to preserve context.",
            "dependencies": [],
            "details": "Use LlamaIndex's semantic chunker to split documents into contextually coherent chunks. Expose configuration for chunk size and overlap (e.g., via parameters or settings). Ensure chunk metadata (source_doc_id, chunk boundaries, overlap) is captured for each chunk and stored in the document_chunks table. Validate that semantic boundaries are respected and context is preserved across chunks.",
            "status": "done",
            "testStrategy": "Chunk a variety of document types, verify chunk boundaries align with semantic units, check overlap, and confirm metadata is correctly stored.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop Custom Code Chunking Using AST Parsing",
            "description": "Create a chunking mechanism for code files that leverages AST parsing to split code into logical units with configurable size and overlap.",
            "dependencies": [
              1
            ],
            "details": "Implement code chunking logic that parses source code into AST nodes (e.g., functions, classes) and groups them into chunks based on configurable parameters (lines per chunk, overlap). Support multiple programming languages if required. Store resulting code chunks with relevant metadata (e.g., language, function/class names, overlap) in the document_chunks table.",
            "status": "done",
            "testStrategy": "Process code files in different languages, verify chunking aligns with logical code units, check overlap, and ensure metadata accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Time/Topic-Based Chunking for Transcripts",
            "description": "Design and implement a chunking strategy for transcripts that splits content based on time intervals or topic shifts, with configurable overlap.",
            "dependencies": [
              1
            ],
            "details": "Develop logic to segment transcripts using either fixed time windows or detected topic boundaries. Allow configuration of chunk duration or topic sensitivity, as well as overlap between chunks. Store transcript chunks with metadata (e.g., start/end time, topic label, overlap) in the document_chunks table. Ensure context is preserved across chunk boundaries.",
            "status": "done",
            "testStrategy": "Chunk transcripts with varying lengths and topics, verify chunk boundaries match time/topic criteria, check overlap, and validate metadata storage.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on adaptive chunking strategy implementation."
      },
      {
        "id": 14,
        "title": "Error Handling & Graceful Degradation",
        "description": "Implement robust error handling, retry logic, partial processing, and detailed logging for all pipeline stages.",
        "details": "Use Python exception handling, Celery retry policies, and fallback to simpler methods. Log errors with stack traces in processing_logs table. Move failed files to B2 failed/ folder.",
        "testStrategy": "Simulate service failures, verify retries, partial saves, and error logs.",
        "priority": "high",
        "dependencies": [
          "13"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Robust Exception Handling and Retry Logic in Pipeline Tasks",
            "description": "Integrate structured Python exception handling and Celery retry policies for all pipeline stages to ensure resilience against transient and expected failures.",
            "dependencies": [],
            "details": "Wrap all critical pipeline operations in try/except blocks. Use Celery's retry mechanisms (e.g., autoretry_for, max_retries, retry_backoff) to handle transient errors such as network or service outages. Configure per-task retry parameters and ensure idempotency to avoid side effects on repeated execution. Avoid retrying on non-transient exceptions.",
            "status": "done",
            "testStrategy": "Simulate transient and permanent failures in pipeline tasks. Verify that retries occur as configured, and that non-retriable errors do not trigger retries.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Enable Graceful Degradation and Partial Processing with Fallbacks",
            "description": "Design pipeline stages to degrade gracefully by falling back to simpler or partial processing methods when primary logic fails.",
            "dependencies": [
              1
            ],
            "details": "For each pipeline stage, define fallback logic (e.g., simplified processing, skipping non-critical steps) to be invoked when primary processing fails after retries. Ensure that partial results are saved where possible, and that the system continues processing unaffected files or stages. Move unrecoverable files to the B2 failed/ folder for later inspection.",
            "status": "done",
            "testStrategy": "Force failures in primary processing logic and verify that fallback methods are invoked, partial results are saved, and failed files are moved appropriately.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Detailed Error Logging and Monitoring",
            "description": "Log all errors, stack traces, and processing outcomes in the processing_logs table to support debugging and monitoring.",
            "dependencies": [
              1,
              2
            ],
            "details": "On every exception or failure, capture the full stack trace and relevant context. Insert detailed error records into the processing_logs table, including task identifiers, error types, messages, and timestamps. Ensure logs are structured for easy querying and monitoring. Integrate with monitoring tools if available.",
            "status": "done",
            "testStrategy": "Trigger various error scenarios and verify that all relevant details are logged in the processing_logs table, including stack traces and context.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on error handling & graceful degradation."
      },
      {
        "id": 15,
        "title": "Processing Monitoring & Metrics Collection",
        "description": "Track real-time processing progress, resource usage, and cost per document using Prometheus metrics.",
        "details": "Integrate prometheus_client for FastAPI, Celery, and custom business metrics. Track processing time, resource usage, and cost. Store stage-wise metrics in processing_logs table.",
        "testStrategy": "Process documents, verify Prometheus metrics, Grafana dashboard updates, and Supabase logs.",
        "priority": "high",
        "dependencies": [
          "14"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Prometheus Metrics Collection in FastAPI and Celery",
            "description": "Instrument FastAPI and Celery services to expose Prometheus-compatible metrics endpoints for processing progress, resource usage, and cost tracking.",
            "dependencies": [],
            "details": "Install prometheus_client in both FastAPI and Celery environments. For FastAPI, mount the /metrics endpoint using make_asgi_app and add counters, histograms, and gauges for request counts, processing time, and resource usage. For Celery, use available Prometheus exporters or integrate prometheus_client to expose worker and task metrics. Ensure all relevant business and custom metrics are included.",
            "status": "done",
            "testStrategy": "Verify /metrics endpoints in FastAPI and Celery return expected metrics. Use Prometheus to scrape these endpoints and confirm metrics are ingested.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Track and Store Stage-wise Processing Metrics in Database",
            "description": "Capture and persist detailed stage-wise metrics (processing time, resource usage, cost per document) in the processing_logs table for audit and analysis.",
            "dependencies": [
              1
            ],
            "details": "Extend processing logic to record metrics at each pipeline stage. Store metrics such as start/end timestamps, CPU/memory usage, and cost estimates in the processing_logs table. Ensure schema supports all required fields and that writes are efficient and reliable.",
            "status": "done",
            "testStrategy": "Process sample documents and verify that processing_logs table contains accurate, stage-wise metrics matching Prometheus data.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Validate Metrics Collection and Visualization End-to-End",
            "description": "Test the full monitoring pipeline from metrics emission to visualization and logging, ensuring real-time and historical data is accurate and actionable.",
            "dependencies": [
              1,
              2
            ],
            "details": "Simulate document processing and monitor Prometheus for real-time metrics updates. Confirm that Grafana dashboards reflect current and historical metrics. Cross-check database logs with Prometheus data for consistency. Validate cost calculations and resource usage reporting.",
            "status": "done",
            "testStrategy": "Run end-to-end tests: process documents, check Prometheus and Grafana for live metrics, and verify processing_logs entries. Ensure all metrics are accurate and actionable.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on processing monitoring & metrics collection."
      },
      {
        "id": 16,
        "title": "Embedding Generation Pipeline (BGE-M3, Claude API)",
        "description": "Generate and cache embeddings for document chunks using BGE-M3 (Ollama for dev, Claude API for prod).",
        "details": "Integrate langchain.embeddings.OllamaEmbeddings for dev, Claude API for prod. Batch process 100 chunks, cache embeddings in Supabase pgvector. Regenerate on content updates.",
        "testStrategy": "Generate embeddings for sample chunks, verify latency, caching, and Supabase storage.",
        "priority": "high",
        "dependencies": [
          "15"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate BGE-M3 Embedding Generation for Development (Ollama)",
            "description": "Set up and integrate the BGE-M3 embedding model using Ollama for local development, enabling batch processing of document chunks.",
            "dependencies": [],
            "details": "Install and configure langchain_ollama and OllamaEmbeddings with the BGE-M3 model. Implement batch processing for 100 document chunks at a time. Ensure the pipeline can handle content updates by triggering re-embedding as needed. Optimize for local inference speed and resource usage.",
            "status": "done",
            "testStrategy": "Generate embeddings for a sample batch of 100 chunks, verify output shape and latency, and confirm embeddings are regenerated on content updates.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Claude API for Production Embedding Generation",
            "description": "Implement embedding generation using the Claude API for production, supporting batch processing and seamless switching from development to production.",
            "dependencies": [
              1
            ],
            "details": "Configure the Claude API integration within the embedding pipeline. Ensure batch processing of 100 chunks per request, with error handling and retry logic. Provide a configuration switch to toggle between Ollama (dev) and Claude API (prod). Ensure compatibility of embedding formats and dimensions.",
            "status": "done",
            "testStrategy": "Run embedding generation for a sample batch via Claude API, verify output consistency with dev pipeline, and test failover and error handling.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Embedding Caching and Regeneration Logic in Supabase pgvector",
            "description": "Design and implement caching of generated embeddings in Supabase pgvector, including logic to detect content updates and trigger regeneration.",
            "dependencies": [
              2
            ],
            "details": "Integrate with Supabase pgvector to store and retrieve embeddings. Implement logic to check for content changes and invalidate or update cached embeddings as needed. Ensure efficient batch inserts and retrievals. Maintain metadata for tracking embedding versions and update timestamps.",
            "status": "done",
            "testStrategy": "Insert, retrieve, and update embeddings in Supabase for sample documents. Simulate content updates and verify that embeddings are correctly regenerated and cached.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on embedding generation pipeline (bge-m3, claude api)."
      },
      {
        "id": 17,
        "title": "Vector Storage & Indexing (Supabase pgvector)",
        "description": "Store embeddings in Supabase pgvector, create HNSW index for fast similarity search, and optimize batch inserts.",
        "details": "Enable pgvector extension, create HNSW index, and optimize batch inserts using supabase-py bulk operations. Organize by namespace and support metadata filtering.",
        "testStrategy": "Insert and search embeddings, verify index performance and metadata filtering.",
        "priority": "high",
        "dependencies": [
          "16"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Enable pgvector Extension in Supabase",
            "description": "Activate the pgvector extension in the Supabase PostgreSQL database to support vector data types and similarity search operations.",
            "dependencies": [],
            "details": "Access the Supabase dashboard, navigate to the Extensions section, and enable the 'vector' extension. This step is required before creating tables with vector columns and using vector search features.",
            "status": "done",
            "testStrategy": "Verify that the 'vector' extension is listed as enabled in the Supabase dashboard and that SQL commands using the 'vector' data type execute without errors.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create Embeddings Table and HNSW Index",
            "description": "Design and create a table for storing embeddings, including metadata and namespace columns, and add an HNSW index for fast similarity search.",
            "dependencies": [
              1
            ],
            "details": "Define a table schema with columns for id, embedding (vector), metadata (JSONB), and namespace (text or UUID). Use SQL to create the table and then create an HNSW index on the embedding column for efficient ANN search.",
            "status": "done",
            "testStrategy": "Insert sample embeddings and confirm that the HNSW index exists and is used in EXPLAIN query plans for similarity searches.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Optimize Batch Inserts Using supabase-py Bulk Operations",
            "description": "Implement efficient batch insertion of embeddings and metadata using supabase-py or equivalent bulk insert methods.",
            "dependencies": [
              2
            ],
            "details": "Use supabase-py or another supported client to insert multiple embeddings in a single operation, minimizing transaction overhead and maximizing throughput. Ensure the code handles large batches and error cases.",
            "status": "done",
            "testStrategy": "Benchmark batch insert performance with varying batch sizes and verify that all records are correctly stored in the table.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Organize Embeddings by Namespace",
            "description": "Implement logic to assign and query embeddings by namespace to support multi-tenant or segmented storage.",
            "dependencies": [
              2
            ],
            "details": "Add a namespace column to the embeddings table if not already present. Ensure all insert and query operations include namespace filtering to logically separate data for different use cases or clients.",
            "status": "done",
            "testStrategy": "Insert embeddings with different namespaces and verify that queries scoped to a namespace only return relevant records.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Metadata Filtering in Similarity Search",
            "description": "Enable filtering of similarity search results based on metadata fields stored with each embedding.",
            "dependencies": [
              2
            ],
            "details": "Use PostgreSQL's JSONB operators to filter embeddings by metadata fields in combination with vector similarity queries. Update search queries to support metadata-based filtering (e.g., by document type, tags, or timestamps).",
            "status": "done",
            "testStrategy": "Run similarity searches with and without metadata filters, confirming that results are correctly filtered and performance remains acceptable.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7.5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Expand vector storage and indexing into subtasks for enabling pgvector, creating HNSW index, optimizing batch inserts, organizing by namespace, and implementing metadata filtering."
      },
      {
        "id": 18,
        "title": "Hybrid Search Implementation (Dense, Sparse, Fuzzy, RRF)",
        "description": "Implement hybrid search combining vector similarity, BM25, ILIKE, fuzzy matching, and reciprocal rank fusion.",
        "details": "Use pgvector for dense search, PostgreSQL full-text search for BM25, ILIKE for pattern matching, rapidfuzz for fuzzy search. Implement RRF for result fusion with configurable weights.",
        "testStrategy": "Run hybrid searches, verify result fusion, relevance, and latency targets.",
        "priority": "high",
        "dependencies": [
          "17"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Dense, Sparse, and Fuzzy Search Pipelines",
            "description": "Develop individual search pipelines for dense (vector), sparse (BM25), and fuzzy (ILIKE, rapidfuzz) retrieval methods.",
            "dependencies": [],
            "details": "Set up pgvector for dense search, configure PostgreSQL full-text search for BM25, implement ILIKE for pattern matching, and integrate rapidfuzz for fuzzy matching. Ensure each pipeline can independently retrieve and score results for a given query.",
            "status": "done",
            "testStrategy": "Run isolated queries for each pipeline and verify result relevance, accuracy, and latency.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Design and Implement Reciprocal Rank Fusion (RRF) Algorithm",
            "description": "Create a fusion algorithm to combine ranked results from dense, sparse, and fuzzy pipelines using reciprocal rank fusion.",
            "dependencies": [
              1
            ],
            "details": "Develop RRF logic to merge result lists from all pipelines, applying configurable weights. Ensure the algorithm penalizes lower-ranked results and boosts consensus across methods. Validate with sample queries and edge cases.",
            "status": "done",
            "testStrategy": "Test fusion with controlled input lists, verify ranking consistency, and check that top results reflect combined relevance.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Hybrid Search and Expose Unified API Endpoint",
            "description": "Combine all search pipelines and RRF fusion into a single hybrid search workflow, exposing it via an API endpoint.",
            "dependencies": [
              2
            ],
            "details": "Orchestrate parallel execution of all search methods, collect results, apply RRF fusion, and return unified ranked results. Implement API endpoint with configurable fusion weights and query parameters. Ensure robust error handling and logging.",
            "status": "done",
            "testStrategy": "Run end-to-end hybrid search queries through the API, validate result quality, latency, and error handling.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on hybrid search implementation (dense, sparse, fuzzy, rrf)."
      },
      {
        "id": 19,
        "title": "Query Expansion & Reranking (Claude Haiku, BGE-Reranker-v2)",
        "description": "Expand queries using Claude Haiku, execute parallel searches, and rerank results with BGE-Reranker-v2.",
        "details": "Integrate anthropic-py for query expansion, run parallel searches, and rerank top 20-30 results using Ollama BGE-Reranker-v2 (dev) or Claude API (prod).",
        "testStrategy": "Test query expansion and reranking, verify improved recall and precision.",
        "priority": "high",
        "dependencies": [
          "18"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Claude Haiku for Query Expansion",
            "description": "Implement query expansion using Claude Haiku via anthropic-py, ensuring queries are enriched for improved recall.",
            "dependencies": [],
            "details": "Set up anthropic-py client and configure Claude Haiku 4.5 API parameters (e.g., max_tokens, temperature, top_p). Design prompt templates to expand user queries, leveraging advanced prompt engineering techniques for optimal output. Handle API errors and retries for reliability.",
            "status": "done",
            "testStrategy": "Send sample queries and verify that expanded queries are generated as expected. Compare recall and diversity of results before and after expansion.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Execute Parallel Searches with Expanded Queries",
            "description": "Run parallel searches using the expanded queries to retrieve a broad set of relevant results.",
            "dependencies": [
              1
            ],
            "details": "Implement asynchronous or concurrent search logic to execute multiple queries in parallel. Aggregate results from all searches, ensuring deduplication and efficient handling of large result sets. Optimize for latency and throughput.",
            "status": "done",
            "testStrategy": "Test with multiple expanded queries and measure search latency. Confirm that all relevant results are retrieved and aggregated correctly.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Rerank Search Results Using BGE-Reranker-v2",
            "description": "Apply BGE-Reranker-v2 to rerank the top 20-30 search results for improved relevance and precision.",
            "dependencies": [
              2
            ],
            "details": "Integrate Ollama BGE-Reranker-v2 (dev) or Claude API (prod) to rerank aggregated results. Configure reranker model and permissions, and tune reranking parameters. Validate reranked output for relevance and consistency.",
            "status": "done",
            "testStrategy": "Compare original and reranked result sets using relevance metrics (precision, recall, NDCG). Conduct manual review of top results for quality assurance.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on query expansion & reranking (claude haiku, bge-reranker-v2)."
      },
      {
        "id": 20,
        "title": "Neo4j Graph Integration & Entity Storage",
        "description": "Store entities and relationships in Neo4j, enable graph-based queries and context retrieval.",
        "details": "Use neo4j Python driver (neo4j >=5.10) to create document and entity nodes, relationships, and vector index. Implement Cypher queries for entity-centric and relationship traversal.",
        "testStrategy": "Insert entities/relationships, run Cypher queries, verify graph traversal and context retrieval.",
        "priority": "high",
        "dependencies": [
          "19"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Set Up Neo4j Python Driver and Database Connection",
            "description": "Install the Neo4j Python driver and establish a secure connection to the Neo4j database instance.",
            "dependencies": [],
            "details": "Use pip to install the neo4j Python driver (neo4j >=5.10). Configure connection parameters (URI, username, password) and verify connectivity using GraphDatabase.driver and driver.verify_connectivity(). Ensure the database instance is running and accessible.",
            "status": "done",
            "testStrategy": "Attempt connection and run a simple Cypher query to confirm connectivity.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Entity and Relationship Node Creation",
            "description": "Create Cypher queries and Python functions to insert document and entity nodes, and define relationships between them in Neo4j.",
            "dependencies": [
              1
            ],
            "details": "Define node labels (e.g., Document, Entity) and relationship types. Use MERGE or CREATE Cypher statements to add nodes and relationships. Implement Python functions to batch insert entities and relationships, ensuring idempotency and data integrity.",
            "status": "done",
            "testStrategy": "Insert sample entities and relationships, then query the graph to verify correct node and relationship creation.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Enable Graph-Based Queries and Context Retrieval",
            "description": "Develop Cypher queries and Python interfaces for entity-centric and relationship traversal, including context retrieval and vector index integration.",
            "dependencies": [
              2
            ],
            "details": "Implement Cypher queries for traversing relationships (e.g., MATCH, OPTIONAL MATCH). Integrate vector index for similarity search if required. Provide Python functions to retrieve context around entities and relationships, supporting advanced graph queries.",
            "status": "done",
            "testStrategy": "Run entity-centric and relationship traversal queries, validate context retrieval, and test vector index search if applicable.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on neo4j graph integration & entity storage."
      },
      {
        "id": 21,
        "title": "Caching Strategy (Redis, Tiered Cache)",
        "description": "Implement Redis caching for frequent queries, embeddings, and search results with semantic thresholds and tiered cache.",
        "details": "Use redis-py for L1 cache (Redis), fallback to L2 (PostgreSQL). Implement semantic cache thresholds and 5-minute TTL. Track cache hit rate.",
        "testStrategy": "Run repeated queries, verify cache hits/misses, and cache update logic.",
        "priority": "high",
        "dependencies": [
          "20"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Redis L1 Cache with Semantic Thresholds and TTL",
            "description": "Set up Redis as the primary (L1) cache for frequent queries, embeddings, and search results, applying semantic thresholds and a 5-minute TTL.",
            "dependencies": [],
            "details": "Use redis-py to connect to Redis. Define cache keys for queries, embeddings, and search results. Implement logic to only cache results that meet semantic similarity thresholds. Set a 5-minute expiration (TTL) for all cache entries to ensure freshness. Ensure cache-aside pattern is used for read-heavy workloads, checking Redis first and falling back to the database on cache miss.",
            "status": "done",
            "testStrategy": "Run repeated queries and verify that results are cached in Redis, TTL is respected, and only semantically relevant results are cached.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Tiered Cache Fallback to PostgreSQL (L2)",
            "description": "Implement fallback logic to query PostgreSQL (L2) when Redis (L1) cache misses occur, and repopulate Redis cache as needed.",
            "dependencies": [
              1
            ],
            "details": "On cache miss in Redis, query PostgreSQL for the required data. If found, repopulate Redis with the result, applying the same semantic threshold and TTL logic. Ensure the fallback mechanism is robust and does not introduce significant latency. Use efficient serialization for storing and retrieving data between Redis and PostgreSQL.",
            "status": "done",
            "testStrategy": "Simulate cache misses and verify that data is correctly fetched from PostgreSQL, then cached in Redis for subsequent requests.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Monitor and Track Cache Hit Rate and Effectiveness",
            "description": "Implement monitoring to track cache hit/miss rates and overall cache effectiveness for both Redis and PostgreSQL tiers.",
            "dependencies": [
              1,
              2
            ],
            "details": "Instrument the caching logic to record cache hits, misses, and repopulation events. Aggregate metrics such as hit rate, miss rate, and average response time. Set up dashboards or logs to visualize cache performance and identify optimization opportunities. Use these metrics to tune semantic thresholds and TTL values.",
            "status": "done",
            "testStrategy": "Generate load with a mix of repeated and unique queries, then verify that hit/miss metrics are accurately tracked and reported.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on caching strategy (redis, tiered cache)."
      },
      {
        "id": 22,
        "title": "Query Type Detection & Routing",
        "description": "Classify incoming queries and route to optimal workflow framework (LangGraph, CrewAI, Simple) based on query complexity and requirements.",
        "status": "done",
        "dependencies": [
          "21"
        ],
        "priority": "high",
        "details": "Implemented WorkflowRouter class in app/workflows/workflow_router.py with Claude Haiku-powered classification. The system analyzes queries and routes them to the appropriate processing framework with confidence scoring and reasoning.",
        "testStrategy": "Submit queries of each type, verify correct classification and routing to appropriate workflow frameworks. Validate confidence scoring and fallback mechanisms.",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Query Type Taxonomy and Routing Logic",
            "description": "Define the taxonomy of query types (semantic, relational, hybrid, metadata) and specify routing logic for each type.",
            "dependencies": [],
            "details": "Analyze typical incoming queries and categorize them into clear types. Document routing rules for each category, mapping them to the appropriate search pipeline (vector, graph, metadata). Consider hierarchical classification if the taxonomy is complex, and ensure the design supports future extensibility.",
            "status": "done",
            "testStrategy": "Review taxonomy coverage against a sample set of queries. Validate routing logic with test cases for each query type.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Query Classifier Using Claude Haiku",
            "description": "Develop and deploy a query classifier leveraging Claude Haiku to assign incoming queries to the correct type.",
            "dependencies": [
              1
            ],
            "details": "Use prompt engineering and, if needed, hierarchical classification to maximize accuracy. Integrate Claude Haiku via API, ensuring the classifier outputs only the defined category names. Optimize for speed and reliability, and consider using vector similarity retrieval for highly variable queries.",
            "status": "done",
            "testStrategy": "Submit queries of each type and edge cases to the classifier. Measure classification accuracy and latency. Confirm output matches taxonomy.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Classifier with Search Pipeline Routing",
            "description": "Connect the classifier output to the routing system, ensuring queries are dispatched to the correct search pipeline.",
            "dependencies": [
              2
            ],
            "details": "Implement the routing logic that receives the classified query type and triggers the corresponding search pipeline (vector, graph, metadata, or hybrid). Ensure robust error handling and logging. Validate that each pipeline receives only relevant queries and that fallback logic is in place for unclassified or ambiguous queries.",
            "status": "done",
            "testStrategy": "End-to-end test: submit queries, verify correct classification and routing to the intended pipeline. Check logs and error handling for misrouted or unclassified queries.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Document Workflow Type Taxonomy",
            "description": "Document the implemented workflow type taxonomy (LANGGRAPH, CREWAI, SIMPLE) with detailed characteristics of each type.",
            "dependencies": [
              3
            ],
            "details": "Create comprehensive documentation of the three workflow types: LANGGRAPH for adaptive queries needing iterative refinement and external search; CREWAI for multi-agent tasks with specialized roles and sequential processing; and SIMPLE for straightforward factual lookups. Include examples and decision criteria for each type.",
            "status": "done",
            "testStrategy": "Review documentation with team members to ensure clarity and completeness. Validate with example queries for each workflow type.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Monitor and Optimize Classification Performance",
            "description": "Implement monitoring for classification decisions and optimize performance based on real-world usage patterns.",
            "dependencies": [
              3
            ],
            "details": "Set up analytics to track classification accuracy, confidence scores, and routing decisions in production. Analyze patterns of misclassification or low confidence scores. Refine the classifier based on this data to improve accuracy and reduce fallbacks to SIMPLE workflow.",
            "status": "done",
            "testStrategy": "Analyze classification logs over time. Compare predicted workflow types with actual performance. Measure improvements in classification accuracy after optimization.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on query type detection & routing."
      },
      {
        "id": 23,
        "title": "Query Processing Pipeline & Result Merging",
        "description": "Normalize, expand, execute, deduplicate, and merge query results using RRF and reranking.",
        "status": "done",
        "dependencies": [
          "22"
        ],
        "priority": "high",
        "details": "Implemented comprehensive query processing pipeline with services for query expansion, hybrid search, reranking, and parallel execution. Features include multiple expansion strategies, parallel execution across search methods, deduplication, RRF merging, BGE-Reranker-v2 reranking, logging, metrics tracking, score thresholding, and Top-K selection.",
        "testStrategy": "Process complex queries, verify result quality, deduplication, and latency. All tests passed successfully with the implemented pipeline.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Query Normalization and Expansion",
            "description": "Develop modules to normalize incoming queries and expand them for improved recall and relevance.",
            "dependencies": [],
            "details": "Created query_expansion_service.py using Claude Haiku for query expansion with multiple strategies including synonyms and reformulations. Implemented standardization of query formats (lowercasing, removing stopwords) and comprehensive logging of all normalized and expanded queries for traceability.",
            "status": "done",
            "testStrategy": "Test with diverse query inputs, verify normalization accuracy, and check that expansions improve recall without introducing irrelevant results.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Execute Queries in Parallel and Deduplicate Results",
            "description": "Design and implement parallel query execution across multiple sources, followed by deduplication of retrieved results.",
            "dependencies": [
              1
            ],
            "details": "Implemented parallel_search_service.py to run expanded queries against all relevant data sources concurrently. Applied deduplication algorithms to remove duplicate results based on content similarity and unique identifiers. Added logging for execution times and deduplication statistics.",
            "status": "done",
            "testStrategy": "Simulate concurrent queries, measure execution latency, and verify that deduplication removes all duplicates while retaining unique results.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Merge Results Using RRF and Rerank Final Output",
            "description": "Integrate Reciprocal Rank Fusion (RRF) for merging results and apply reranking models to optimize final result order.",
            "dependencies": [
              2
            ],
            "details": "Created hybrid_search_service.py for RRF merging of results from dense, sparse, and fuzzy search methods. Implemented reranking_service.py using BGE-Reranker-v2 via Ollama (dev) or Claude API (prod). Added score thresholding and Top-K selection for optimal result quality. Ensured comprehensive logging of all queries and merged results for audit and debugging.",
            "status": "done",
            "testStrategy": "Process sample queries, validate that RRF merging and reranking improve relevance, and check that final output matches expected quality benchmarks.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on query processing pipeline & result merging."
      },
      {
        "id": 24,
        "title": "Faceted Search & Result Presentation",
        "description": "Enable faceted filtering (department, type, date, entities) and present results with snippets, highlights, and relevance scores.",
        "status": "done",
        "dependencies": [
          "23"
        ],
        "priority": "medium",
        "details": "Implemented multi-select facets for department, file_type, date_range, and entity filtering. Generated snippets with keyword highlighting using HTML <mark> tags. Displayed relevance scores, source metadata, and B2 URL links. Added pagination support and SQL WHERE clause generation for filters.",
        "testStrategy": "Verified filtered searches functionality, result presentation with snippets and highlights, facet accuracy, and pagination. Confirmed authentication with Clerk JWT tokens works correctly.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Multi-Select Faceted Filtering UI",
            "description": "Develop the frontend components to support multi-select faceted filtering by department, type, date, and entities.",
            "dependencies": [],
            "details": "Design and build user interface elements for each facet (department, type, date, entities) with multi-select capability. Ensure facets are easy to find, mobile-friendly, and update results quickly when filters are applied. Facet values should be ordered logically (alphabetical, numerical, or by relevance) and selected values should be clearly indicated. Only display relevant facets for the current result set.",
            "status": "done",
            "testStrategy": "Test by applying various combinations of facet filters and verifying that the displayed results update accordingly and facet selections persist. Check usability on both desktop and mobile.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Generate and Present Search Result Snippets with Highlights",
            "description": "Create backend and frontend logic to generate result snippets, highlight matched keywords, and display relevant metadata.",
            "dependencies": [
              1
            ],
            "details": "For each search result, extract a relevant snippet containing the matched keywords. Highlight these keywords in the snippet. Display additional metadata such as relevance score, source, and department. Ensure that snippets are concise and informative, and that highlights are visually distinct.",
            "status": "done",
            "testStrategy": "Run searches with various queries and verify that snippets are generated, keywords are highlighted, and metadata is displayed correctly for each result.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Result Presentation with B2 URLs and Relevance Scores",
            "description": "Link each search result to its corresponding Backblaze B2 URL and ensure relevance scores are visible and accurate.",
            "dependencies": [
              2
            ],
            "details": "For each result, provide a clickable link to the B2 URL. Display the relevance score prominently, ensuring it is calculated and presented consistently. Confirm that the source and department fields are shown as specified. Validate that all links are functional and direct users to the correct B2 resource.",
            "status": "done",
            "testStrategy": "Click through result links to verify correct B2 URL redirection. Check that relevance scores match backend calculations and are displayed for all results.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Documentation of Faceted Search Implementation",
            "description": "Document the implementation details of the faceted search service and API endpoint.",
            "dependencies": [
              3
            ],
            "details": "Create comprehensive documentation for the faceted search implementation, including the faceted_search_service.py and the POST /api/query/search/faceted endpoint. Document the parameters accepted by the API (query, departments, file_types, date_from, date_to, entities, page, page_size), authentication requirements, and response format.",
            "status": "done",
            "testStrategy": "Review documentation for completeness and accuracy. Ensure all parameters, response formats, and authentication requirements are clearly explained.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Performance Optimization for Faceted Search",
            "description": "Analyze and optimize the performance of the faceted search implementation.",
            "dependencies": [
              3
            ],
            "details": "Profile the faceted search implementation to identify performance bottlenecks. Optimize SQL queries for facet value extraction and filtering. Implement caching strategies for frequently used facet values. Ensure pagination works efficiently with large result sets.",
            "status": "done",
            "testStrategy": "Benchmark search performance with various query combinations and result set sizes. Verify that response times remain acceptable under load and with complex facet combinations.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on faceted search & result presentation."
      },
      {
        "id": 25,
        "title": "Query Analytics & A/B Testing",
        "description": "Log queries, track latency, CTR, and support A/B testing for ranking algorithms.",
        "details": "Store query logs and result clicks in Supabase. Implement analytics dashboard and A/B test framework for ranking methods.",
        "testStrategy": "Analyze logs, verify CTR tracking, and run A/B tests.",
        "priority": "medium",
        "dependencies": [
          "24"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Query Logging and Metric Tracking in Supabase",
            "description": "Set up infrastructure to log all search queries, track latency, and record click-through rates (CTR) in Supabase.",
            "dependencies": [],
            "details": "Design Supabase tables to store query logs, including query text, timestamps, latency, and user interactions (clicks). Integrate logging into the query execution pipeline to ensure all relevant metrics are captured for each search event.",
            "status": "pending",
            "testStrategy": "Verify that queries, latency, and clicks are correctly logged in Supabase by running test queries and inspecting the stored data for completeness and accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop Analytics Dashboard for Query Metrics",
            "description": "Build a dashboard to visualize query volume, latency, and CTR using data from Supabase.",
            "dependencies": [
              1
            ],
            "details": "Use a dashboarding tool (e.g., Grafana or Streamlit) to connect to Supabase and display real-time and historical analytics for query metrics. Include filters for date ranges and ranking algorithm versions to support analysis.",
            "status": "pending",
            "testStrategy": "Check that the dashboard accurately reflects Supabase data by comparing dashboard metrics with direct database queries. Test responsiveness and filtering capabilities.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement and Run A/B Testing Framework for Ranking Algorithms",
            "description": "Create an A/B testing framework to compare different ranking algorithms by splitting user traffic and measuring impact on CTR and latency.",
            "dependencies": [
              1
            ],
            "details": "Randomly assign users or sessions to control and treatment groups, each using a different ranking algorithm. Log group assignment and outcomes in Supabase. Analyze results using statistical tests (e.g., t-test or Z-test) to determine significance of observed differences in metrics like CTR and latency[1][2][3].",
            "status": "pending",
            "testStrategy": "Simulate A/B tests with test users, verify correct group assignment and metric logging, and validate statistical analysis pipeline with sample data.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on query analytics & a/b testing.",
        "updatedAt": "2025-11-08T18:01:36.843Z"
      },
      {
        "id": 26,
        "title": "Chat UI Implementation (WebSocket, Streaming)",
        "description": "Build a mobile-responsive chat UI with real-time messaging and token-by-token streaming.",
        "status": "done",
        "dependencies": [
          "25"
        ],
        "priority": "high",
        "details": "Implemented with Gradio for frontend. Used HTTP-based streaming with async generators instead of WebSocket for simpler implementation and better reliability with Gradio. Integrated Clerk authentication, comprehensive error handling, retry logic, and mobile-responsive design.",
        "testStrategy": "Tested chat interactions, streaming, and mobile responsiveness across multiple devices. Verified error handling and retry logic.",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop Chat UI Frontend with Gradio or Streamlit",
            "description": "Create a mobile-responsive chat interface using Gradio or Streamlit, supporting user input, message display, and chat history.",
            "dependencies": [],
            "details": "Implemented the chat UI using Gradio's ChatInterface. Created chat_ui.py with mobile-responsive design and app_with_auth.py with Clerk authentication integration. Added custom CSS styling for improved mobile experience.",
            "status": "done",
            "testStrategy": "Manually tested UI on desktop and mobile browsers for responsiveness, usability, and correct message display.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Real-time Messaging for Chat Communication",
            "description": "Set up backend with HTTP-based streaming to handle real-time chat communication between frontend and backend.",
            "dependencies": [
              1
            ],
            "details": "Implemented HTTP-based streaming with async generators instead of WebSocket. This approach proved simpler to implement, more reliable with Gradio ChatInterface, and easier to deploy on Render while providing the same user experience.",
            "status": "done",
            "testStrategy": "Tested streaming implementation to verify real-time message delivery and connection stability.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Token-by-Token Streaming from Claude API",
            "description": "Enable streaming of Claude API responses token-by-token to the frontend for real-time chat experience.",
            "dependencies": [
              2
            ],
            "details": "Modified backend to call Claude API with streaming enabled. Implemented token-by-token streaming to the frontend using HTTP async generators. Updated frontend to append streamed tokens to the chat window in real time.",
            "status": "done",
            "testStrategy": "Sent prompts and verified that responses appeared incrementally in the chat UI, matching Claude API streaming output.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Loading Indicators and Error Handling",
            "description": "Add loading indicators for the assistant and robust error handling for network/API failures.",
            "dependencies": [
              3
            ],
            "details": "Added loading indicators (🔍 Processing your query...) while waiting for Claude API responses. Implemented comprehensive error handling with user-friendly messages. Added retry logic with exponential backoff for improved reliability.",
            "status": "done",
            "testStrategy": "Simulated slow responses and errors; verified loading indicator visibility and user-friendly error messages. Tested retry logic with network interruptions.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Test and Optimize Mobile Responsiveness",
            "description": "Thoroughly test the chat UI on various mobile devices and optimize for touch interaction and layout.",
            "dependencies": [
              4
            ],
            "details": "Used browser dev tools and real devices to test UI scaling, input usability, and scrolling. Applied custom CSS styling for optimal mobile experience. Deployed to production at https://jb-empire-chat.onrender.com with both authenticated (/chat) and non-authenticated versions.",
            "status": "done",
            "testStrategy": "Performed cross-device testing and collected feedback to ensure consistent, responsive behavior on phones and tablets.",
            "parentId": "undefined"
          }
        ],
        "complexity": 6.5,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Expand chat UI implementation into subtasks for frontend development (Gradio/Streamlit), WebSocket endpoint implementation, streaming response handling, typing indicator/error handling, and mobile responsiveness testing."
      },
      {
        "id": 27,
        "title": "Conversation Memory System (Supabase Graph Tables)",
        "description": "Store and retrieve user conversation memory using PostgreSQL graph tables (user_memory_nodes, user_memory_edges).",
        "details": "Implement memory node/edge creation, context window management, and recency/access-weighted retrieval. Enforce RLS policies.",
        "testStrategy": "Simulate conversations, verify memory storage, retrieval, and RLS enforcement.",
        "priority": "high",
        "dependencies": [
          "26"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Implement Graph Tables for Conversation Memory",
            "description": "Create PostgreSQL tables (user_memory_nodes, user_memory_edges) to represent conversation memory as a graph structure, supporting efficient storage and retrieval.",
            "dependencies": [],
            "details": "Define schemas for user_memory_nodes and user_memory_edges, ensuring each node represents a memory item (e.g., message, context) and edges capture relationships (e.g., temporal, reference). Implement table creation scripts and indexes for efficient traversal. Ensure compatibility with Supabase and prepare for RLS enforcement.",
            "status": "done",
            "testStrategy": "Verify table creation, schema correctness, and ability to insert and query nodes/edges representing conversation history.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Memory Node/Edge Management and Context Window Logic",
            "description": "Develop logic for creating, updating, and deleting memory nodes and edges, and manage the context window for conversation retrieval.",
            "dependencies": [
              1
            ],
            "details": "Implement backend functions to add new conversation turns as nodes, link them with edges, and prune or limit history based on a context window (e.g., last N messages). Ensure recency and access-weighted retrieval logic is in place to prioritize relevant memory during retrieval. Integrate with Supabase API for transactional consistency.",
            "status": "done",
            "testStrategy": "Simulate conversations, add and remove nodes/edges, and verify that context window and recency/access-weighted retrieval return expected results.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Enforce Row-Level Security (RLS) and Validate Secure Access",
            "description": "Apply and test RLS policies to ensure users can only access their own conversation memory data in the graph tables.",
            "dependencies": [
              1,
              2
            ],
            "details": "Define and apply RLS policies on user_memory_nodes and user_memory_edges to restrict access by user identity. Test for unauthorized access attempts and verify that only the correct user's data is accessible. Document RLS configuration and integrate with Supabase authentication.",
            "status": "done",
            "testStrategy": "Attempt cross-user access, verify RLS enforcement, and run automated tests to confirm only authorized access to memory nodes and edges.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on conversation memory system (supabase graph tables)."
      },
      {
        "id": 28,
        "title": "Session & Preference Management",
        "description": "Support multiple concurrent sessions, session persistence, user preference learning, and privacy controls.",
        "details": "Implement session tracking, timeout, export, and deletion. Store preferences as memory nodes. Provide opt-out and explicit preference UI.",
        "testStrategy": "Test session persistence, preference learning, and privacy controls.",
        "priority": "medium",
        "dependencies": [
          "27"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Multi-Session Tracking and Persistence",
            "description": "Develop mechanisms to support multiple concurrent user sessions, ensure session data is persistent across server restarts, and enable session export and deletion.",
            "dependencies": [],
            "details": "Design a session management system that assigns unique, secure session IDs, supports concurrent sessions per user, and persists session data using a shared store (e.g., Redis). Implement session timeout, export, and deletion features. Ensure session data is securely stored and can be invalidated or removed on demand.",
            "status": "done",
            "testStrategy": "Simulate multiple concurrent sessions, verify session persistence after server restart, and test session export and deletion functionality.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop User Preference Learning and Storage",
            "description": "Create a system to learn, store, and update user preferences as memory nodes, ensuring preferences are associated with the correct session and user.",
            "dependencies": [
              1
            ],
            "details": "Implement logic to capture user actions and infer preferences, storing them as structured memory nodes linked to user profiles. Ensure updates are atomic and preferences persist across sessions. Provide mechanisms to retrieve and update preferences efficiently.",
            "status": "done",
            "testStrategy": "Test preference capture, retrieval, and update across multiple sessions and users. Validate that preferences persist and are correctly associated with users.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Design Privacy Controls and Explicit Preference UI",
            "description": "Provide user-facing controls for privacy, including opt-out options and an explicit UI for managing preferences and active sessions.",
            "dependencies": [
              1,
              2
            ],
            "details": "Develop UI components that allow users to view and manage their active sessions, export or delete session data, and opt out of preference learning. Ensure privacy controls are clear, accessible, and enforceable at the backend.",
            "status": "done",
            "testStrategy": "Perform UI/UX testing for privacy controls, verify backend enforcement of opt-out and deletion, and ensure users can manage sessions and preferences as intended.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on session & preference management."
      },
      {
        "id": 29,
        "title": "Monitoring & Observability (Prometheus, Grafana, Alertmanager)",
        "description": "Collect metrics, visualize in Grafana, set up alerting, and structured logging for all services.",
        "details": "Integrate prometheus_client for FastAPI, Celery, Redis, Neo4j. Build Grafana dashboards with pre-built panels. Configure Alertmanager for multi-channel alerts. Implement JSON logs and health check endpoints.",
        "testStrategy": "Simulate load, verify metrics, dashboard updates, alert triggers, and log accuracy.",
        "priority": "high",
        "dependencies": [
          "28"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Prometheus Metrics Collection for All Services",
            "description": "Set up Prometheus metrics collection for FastAPI, Celery, Redis, and Neo4j services using prometheus_client.",
            "dependencies": [],
            "details": "Install and configure prometheus_client in each service. Expose /metrics endpoints for FastAPI, Celery, Redis, and Neo4j. Ensure custom business metrics are included where relevant. Validate that metrics are accessible and correctly formatted for Prometheus scraping.",
            "status": "done",
            "testStrategy": "Simulate service activity and verify metrics are exposed and collected by Prometheus. Check for completeness and accuracy of metrics.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Build Grafana Dashboards and Panels for Metrics Visualization",
            "description": "Create Grafana dashboards with pre-built and custom panels to visualize collected metrics from all integrated services.",
            "dependencies": [
              1
            ],
            "details": "Connect Grafana to Prometheus as a data source. Design dashboards for FastAPI, Celery, Redis, and Neo4j, including panels for key metrics (e.g., request rates, error rates, resource usage). Use Grafana's dashboard editor to organize panels and set up useful visualizations for operational monitoring.",
            "status": "done",
            "testStrategy": "Verify dashboards update in real-time with incoming metrics. Confirm panels display accurate and actionable data for each service.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Configure Alertmanager for Multi-Channel Alerting and Notification Routing",
            "description": "Set up Alertmanager to handle alerts from Prometheus and Grafana, routing notifications to multiple channels (e.g., email, Slack).",
            "dependencies": [
              2
            ],
            "details": "Install and configure Alertmanager. Define alert rules in Prometheus and Grafana for critical metrics. Set up Alertmanager contact points for email, Slack, and other channels. Configure notification policies and silences as needed. Integrate Alertmanager with Grafana to manage and route alerts, ensuring unified notification handling[1][3][4][5][6].",
            "status": "done",
            "testStrategy": "Trigger test alerts and verify notifications are sent to all configured channels. Check alert deduplication, grouping, and routing logic.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Deploy Self-Hosted Langfuse on Render",
            "description": "Deploy Langfuse web service on Render using existing Supabase PostgreSQL database for LLM observability and cost tracking.",
            "details": "Deploy Langfuse Docker container to Render as a web service. Configure database connection to existing Supabase PostgreSQL (unified database architecture). Set environment variables: LANGFUSE_DATABASE_URL, NEXTAUTH_SECRET, NEXTAUTH_URL, SALT. Generate API keys after deployment and update .env file. Verify Langfuse UI is accessible and database tables are created. Cost: $7/month (Starter plan). Full deployment guide: .taskmaster/docs/LANGFUSE_INTEGRATION_PLAN.md (Phase 1: Deployment).",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 29,
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on monitoring & observability (prometheus, grafana, alertmanager)."
      },
      {
        "id": 30,
        "title": "Cost Tracking & Optimization",
        "description": "Track API, compute, and storage costs. Generate monthly reports and trigger budget alerts.",
        "details": "Integrate cost tracking for Claude, Soniox, Mistral, LangExtract, Render, Supabase, B2. Implement budget alert logic at 80% threshold.",
        "testStrategy": "Simulate usage, verify cost reports and alert triggers.",
        "priority": "medium",
        "dependencies": [
          "29"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Cost Tracking for All Services",
            "description": "Implement automated cost tracking for Claude, Soniox, Mistral, LangExtract, Render, Supabase, and B2, covering API, compute, and storage expenses.",
            "dependencies": [],
            "details": "Set up data pipelines or use APIs to collect cost and usage data from each provider. Normalize and aggregate costs by service and resource type. Ensure tracking supports multi-cloud and SaaS sources, and enables per-service breakdowns for accurate reporting.",
            "status": "done",
            "testStrategy": "Simulate usage across all services, verify that cost data is collected, normalized, and attributed correctly for each provider.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Generate Monthly Cost Reports",
            "description": "Develop automated monthly reporting that summarizes API, compute, and storage costs for all integrated services.",
            "dependencies": [
              1
            ],
            "details": "Design and implement a reporting system that compiles monthly cost data into clear, actionable reports. Include breakdowns by service, resource type, and time period. Reports should be exportable and support visualization for trend analysis.",
            "status": "done",
            "testStrategy": "Trigger monthly report generation with sample data, verify report accuracy, completeness, and clarity for all tracked services.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Budget Alert Logic at 80% Threshold",
            "description": "Set up automated alerts to notify stakeholders when spending reaches 80% of the defined monthly budget for any tracked service.",
            "dependencies": [
              1
            ],
            "details": "Configure monitoring logic to evaluate cumulative spend against budget thresholds in real time. Integrate with notification channels (e.g., email, Slack) to deliver timely alerts. Ensure alerts are actionable and include relevant cost breakdowns.",
            "status": "done",
            "testStrategy": "Simulate cost increases to exceed 80% of budget, confirm that alerts are triggered promptly and contain accurate, actionable information.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on cost tracking & optimization."
      },
      {
        "id": 31,
        "title": "Role-Based Access Control (RBAC) & API Key Management",
        "description": "Implement RBAC for users, documents, and API keys with audit logging and row-level security.",
        "details": "Use Supabase RLS policies, implement user roles (admin, editor, viewer, guest), API key creation/rotation/revocation, and audit logs. Hash API keys with bcrypt.",
        "testStrategy": "Test role permissions, API key flows, and audit log accuracy.",
        "priority": "high",
        "dependencies": [
          "30"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Implement User Roles and Row-Level Security (RLS) Policies in Supabase",
            "description": "Define user roles (admin, editor, viewer, guest) and implement row-level security (RLS) policies for users, documents, and API keys using Supabase.",
            "dependencies": [],
            "details": "Create a roles table and associate users with roles. Use Supabase's RLS policies to restrict access to tables based on user roles. Ensure that each role has clearly defined permissions for CRUD operations on users, documents, and API keys. Reference Supabase documentation and best practices for RLS and RBAC implementation.\n<info added on 2025-11-11T02:00:19.256Z>\nImplementation completed for User Roles and RLS Policies:\n\n✅ Database Schema Created:\n- Created roles table with 4 default roles (admin, editor, viewer, guest)\n- Created user_roles table for user-to-role mappings\n- Created api_keys table with bcrypt hashing\n- Created rbac_audit_logs table for immutable audit trail\n\n✅ RLS Policies Implemented:\n- Enabled RLS on all RBAC tables\n- roles table: read-only for authenticated users\n- api_keys table: users can only see/manage their own keys\n- user_roles table: users can read own roles, admins can manage all roles\n- rbac_audit_logs table: admin-only access\n\n✅ Default Roles Seeded:\n- admin: Full system access (all permissions)\n- editor: Can read/write documents\n- viewer: Can read documents only\n- guest: Limited read access\n\nFiles created:\n- app/models/rbac.py (Pydantic models)\n- app/core/supabase_client.py (Supabase helper)\n- Supabase migration applied successfully\n\nNext: Testing RLS policies and role permissions.\n</info added on 2025-11-11T02:00:19.256Z>",
            "status": "done",
            "testStrategy": "Test RLS policies by creating users with different roles and verifying access to resources. Attempt unauthorized actions to confirm enforcement of restrictions.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement API Key Lifecycle Management with Secure Storage",
            "description": "Develop endpoints and logic for API key creation, rotation, and revocation. Store API keys securely using bcrypt hashing.",
            "dependencies": [
              1
            ],
            "details": "Create endpoints for generating, rotating, and revoking API keys. Store only hashed versions of API keys using bcrypt in the database. Ensure that API keys are associated with users and roles, and that their permissions align with RBAC policies. Document the API key management process and enforce secure handling throughout the lifecycle.\n<info added on 2025-11-11T02:00:28.704Z>\nAPI Key Lifecycle Management Implementation Complete:\n\nAPI Key Generation:\n- Secure random token generation (64 hex chars)\n- Format: emp_[64-char-token]\n- Bcrypt hashing for secure storage\n- Key prefix extraction for fast lookup (emp_xxxxxxxx)\n\nAPI Key Operations Implemented:\n- create_api_key(): Generate new key with role assignment\n- validate_api_key(): Verify key with bcrypt check\n- list_api_keys(): List user's keys (prefix only, no full keys)\n- rotate_api_key(): Create new key, revoke old one atomically\n- revoke_api_key(): Permanently disable key with reason\n\nSecurity Features:\n- Full key shown ONLY once at creation\n- Automatic expiration checking\n- Usage tracking (last_used_at, usage_count)\n- Rate limiting support (rate_limit_per_hour field)\n- Ownership verification for all operations\n\nFiles Created:\n- app/services/rbac_service.py (Complete service implementation)\n- app/routes/rbac.py (FastAPI endpoints)\n- app/middleware/auth.py (Authentication middleware)\n\nIntegration:\n- RBAC router added to main.py at /api/rbac\n- Supports both API key and JWT authentication (JWT stub for future)\n</info added on 2025-11-11T02:00:28.704Z>",
            "status": "done",
            "testStrategy": "Verify API key creation, rotation, and revocation flows. Confirm that only hashed keys are stored and that revoked keys cannot be used for access.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Audit Logging for Access and Key Management Events",
            "description": "Track and log all access events, permission changes, and API key operations for auditing and compliance.",
            "dependencies": [
              1,
              2
            ],
            "details": "Set up audit logging for all RBAC-related actions, including role assignments, permission changes, and API key lifecycle events. Store logs in a dedicated audit table with relevant metadata (user, action, timestamp, resource). Ensure logs are immutable and accessible for compliance reviews.\n<info added on 2025-11-11T02:00:37.494Z>\nCompleted implementation of Audit Logging:\n\n✅ Audit Log Events Tracked:\n- api_key_created: When new key is generated\n- api_key_used: Every time key is validated/used\n- api_key_rotated: When key is rotated\n- api_key_revoked: When key is revoked\n- role_assigned: When role is granted to user\n- role_revoked: When role is removed from user\n\n✅ Audit Log Fields:\n- event_type: Type of event\n- actor_user_id: Who performed the action\n- target_user_id: Who was affected (for role operations)\n- target_resource_type: Type of resource (api_key, user_role)\n- target_resource_id: UUID of affected resource\n- action: Action performed (create, revoke, assign, etc.)\n- result: Outcome (success, failure, denied)\n- ip_address: IP of the request\n- user_agent: User agent string\n- metadata: Additional context (JSON)\n- error_message: Error details if failed\n- created_at: Immutable timestamp\n\n✅ Audit Features:\n- Immutable logs (insert-only, no updates)\n- Automatic logging in all RBAC operations\n- Admin-only access via RLS policies\n- Query filtering by event_type, user_id\n- Pagination support (limit/offset)\n\n✅ API Endpoint:\n- GET /api/rbac/audit-logs (admin only)\n- Supports filtering and pagination\n\nNext: Testing audit log accuracy and RLS enforcement.\n</info added on 2025-11-11T02:00:37.494Z>",
            "status": "done",
            "testStrategy": "Trigger various RBAC and API key events, then review audit logs to confirm accurate and complete recording of all relevant actions.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on role-based access control (rbac) & api key management."
      },
      {
        "id": 32,
        "title": "Bulk Document Management & Batch Operations",
        "description": "Enable bulk upload, delete, reprocessing, metadata update, versioning, and approval workflow for documents.",
        "details": "Implement batch endpoints for document operations. Track progress and support document versioning and approval states.",
        "testStrategy": "Perform bulk operations, verify throughput, versioning, and approval transitions.",
        "priority": "high",
        "dependencies": [
          "31"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Bulk Document Operations Endpoints",
            "description": "Develop RESTful API endpoints to support bulk upload, delete, reprocessing, and metadata update for documents.",
            "dependencies": [],
            "details": "Design and implement backend endpoints that accept batch requests for document operations. Ensure endpoints handle large payloads efficiently, support progress tracking, and provide clear error reporting for partial failures. Integrate with storage and indexing layers to maintain consistency and performance.\n<info added on 2025-11-11T21:02:25.181Z>\n## Investigation Results\n\n**Already Implemented:**\n1. ✅ All 4 bulk operation REST API endpoints in app/routes/documents.py:\n   - POST /bulk-upload\n   - POST /bulk-delete  \n   - POST /bulk-reprocess\n   - PATCH /bulk-metadata\n   - GET /batch-operations/{operation_id}\n   - GET /batch-operations\n\n2. ✅ All 4 Celery tasks in app/tasks/bulk_operations.py:\n   - bulk_upload_documents\n   - bulk_delete_documents\n   - bulk_reprocess_documents\n   - bulk_update_metadata\n   - Includes progress tracking and error handling\n\n**Missing - Need to Implement:**\nThe Celery tasks reference 4 functions from app.services.document_processor that don't exist yet:\n1. ❌ process_document_upload(file_path, filename, metadata, user_id, auto_process)\n2. ❌ delete_document(document_id, user_id, soft_delete)\n3. ❌ reprocess_document(document_id, user_id, force_reparse, update_embeddings, preserve_metadata)\n4. ❌ update_document_metadata(document_id, metadata, user_id)\n\nThe current document_processor.py only contains text extraction/parsing logic, not document management operations.\n\n**Next Steps:**\nNeed to create these 4 document management functions to complete Task 32.1.\n</info added on 2025-11-11T21:02:25.181Z>",
            "status": "done",
            "testStrategy": "Submit bulk operation requests (upload, delete, reprocess, metadata update) with varying batch sizes. Verify throughput, error handling, and data integrity for all operations.",
            "parentId": "undefined",
            "updatedAt": "2025-11-11T03:42:03.083Z"
          },
          {
            "id": 2,
            "title": "Integrate Document Versioning and Approval Workflow",
            "description": "Enable version control and approval states for documents, supporting batch transitions and rollbacks.",
            "dependencies": [
              1
            ],
            "details": "Extend the document model to support version history and approval status. Implement logic for batch versioning (e.g., uploading new versions in bulk) and batch approval/rejection. Ensure audit trails are maintained for all version and approval changes.",
            "status": "done",
            "testStrategy": "Perform bulk version uploads and approval transitions. Verify correct version history, approval state changes, and audit trail entries for all affected documents.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Progress Tracking and Operation Auditing",
            "description": "Track and expose the progress and audit logs of all batch document operations for transparency and compliance.",
            "dependencies": [
              1,
              2
            ],
            "details": "Develop mechanisms to monitor the status of ongoing batch operations, including per-document success/failure. Provide APIs or dashboards for users to query operation progress and review detailed audit logs. Ensure compliance with organizational and regulatory requirements for traceability.\n<info added on 2025-11-11T21:17:31.729Z>\n## Implementation Status: Complete\n\n**Progress Tracking (✅ Complete):**\n1. Models defined in app/models/documents.py:\n   - BatchOperationResponse (lines 92-106) - operation tracking with progress\n   - BatchOperationStatusResponse (lines 108-123) - detailed status with progress_percentage\n\n2. REST API endpoints in app/routes/documents.py:\n   - GET /api/documents/batch-operations/{operation_id} (lines 402-447) - Get specific operation status\n   - GET /api/documents/batch-operations (lines 450-505) - List all operations with filtering, pagination, and progress calculation\n\n3. Real-time progress updates in app/tasks/bulk_operations.py:\n   - _update_operation_status() helper function (lines 551-600)\n   - Called at start, during processing (per-document), and on completion\n   - Tracks: status, processed_items, successful_items, failed_items, results array\n\n**Operation Auditing (✅ Complete):**\n1. Database table: batch_operations (workflows/database_setup.md lines 609-624)\n   - Stores: operation_type, initiated_by, items counts, status, parameters, results\n   - Timestamps: started_at, completed_at, created_at\n   - JSONB fields for detailed parameters and results\n\n2. Approval workflow audit: approval_audit_log table with ApprovalAuditLogEntry model\n   - Tracks all approval state transitions\n   - Includes: event_type, status changes, user, IP address, user agent, timestamps\n\n3. Detailed result tracking:\n   - DocumentOperationResult model (lines 83-90) - per-document status with success/failure/error\n   - Stored in results JSONB array in batch_operations table\n\n**Compliance & Traceability (✅ Complete):**\n- Full audit trail for all batch operations\n- User tracking (initiated_by field)\n- Timestamp tracking (created_at, started_at, completed_at, updated_at)\n- Error message logging\n- Detailed per-document results\n</info added on 2025-11-11T21:17:31.729Z>",
            "status": "done",
            "testStrategy": "Initiate various batch operations and monitor progress tracking endpoints or dashboards. Validate that audit logs accurately reflect all actions, including errors and rollbacks.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on bulk document management & batch operations.",
        "updatedAt": "2025-11-11T03:42:03.083Z"
      },
      {
        "id": 33,
        "title": "User Management & GDPR Compliance",
        "description": "Support user creation, editing, role assignment, password reset, suspension, activity logs, and GDPR-compliant data export.",
        "details": "Implement admin endpoints for user management. Store activity logs and support data export/deletion per GDPR.",
        "testStrategy": "Test user flows, activity logging, and GDPR export/deletion.",
        "priority": "high",
        "dependencies": [
          "32"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement User Account and Role Management Endpoints",
            "description": "Develop admin endpoints to support user creation, editing, role assignment, password reset, and suspension.",
            "dependencies": [],
            "details": "Create RESTful endpoints for user CRUD operations, role assignment, and password management. Ensure endpoints allow for user suspension/reactivation and support both pre-defined and custom roles. Integrate secure authentication and authorization checks for all admin actions.\n<info added on 2025-11-11T21:36:53.612Z>\n## Implementation Details\n\n**Database Schema (Already Implemented)**\n- admin_users table (username, email, password_hash, role, is_active, etc.)\n- admin_sessions table (session tokens)\n- admin_activity_log table (action logging)\n\n**RBAC System (Already Implemented)**\n- API key lifecycle management\n- Role assignment/revocation functionality\n- Audit logging for RBAC events\n- Authentication middleware using API keys and JWT via Clerk\n- Authorization check middleware\n\n**Required User Management Endpoints**\n1. User CRUD operations:\n   - POST /api/users - Create new admin user\n   - GET /api/users - List all users with pagination/filtering\n   - GET /api/users/{user_id} - Retrieve specific user details\n   - PATCH /api/users/{user_id} - Update user information\n   - DELETE /api/users/{user_id} - Delete user account\n\n2. Password management:\n   - POST /api/users/{user_id}/reset-password - Admin-initiated reset\n   - POST /api/users/change-password - Self-service password change\n\n3. Account status management:\n   - POST /api/users/{user_id}/suspend - Suspend user account\n   - POST /api/users/{user_id}/activate - Reactivate suspended account\n\n**Implementation Plan**\n- Create app/routes/users.py with admin user management endpoints\n- Develop app/services/user_service.py for user operations\n- Define app/models/users.py for Pydantic models\n- Utilize bcrypt for password hashing\n- Integrate with admin_activity_log for comprehensive audit trail\n</info added on 2025-11-11T21:36:53.612Z>",
            "status": "done",
            "testStrategy": "Test user creation, editing, role assignment, password reset, and suspension via API and UI. Verify role-based access control and error handling.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Activity Logging for User Actions",
            "description": "Log all significant user management actions (creation, edits, role changes, suspensions, password resets) for audit and compliance.",
            "dependencies": [
              1
            ],
            "details": "Design and implement a logging mechanism to capture all admin and user actions related to user management. Store logs securely with timestamps, user IDs, action types, and relevant metadata. Ensure logs are immutable and accessible for compliance audits.\n<info added on 2025-11-11T21:42:57.585Z>\n## Investigation Status Update\n\nInitial investigation of logging mechanism reveals:\n\n1. Implementation Status:\n   - _log_activity() function is implemented in user_service.py\n   - Function is called by all user management operations\n   - Logs are written to admin_activity_log table\n\n2. Pending Verification:\n   - Database constraints and RLS policies need to be checked to ensure log immutability\n   - No endpoints currently exist for retrieving user activity logs for compliance audits\n\n3. Action Items:\n   - Implement read-only API endpoints for retrieving filtered activity logs\n   - Add database constraints to prevent modification of existing log entries\n   - Document the logging schema and retention policies\n   - Create test cases to verify logging functionality across all user management actions\n</info added on 2025-11-11T21:42:57.585Z>",
            "status": "done",
            "testStrategy": "Trigger user management actions and verify that logs are created with correct details. Test log retrieval and integrity.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Develop GDPR-Compliant Data Export and Deletion Features",
            "description": "Enable GDPR-compliant export and deletion of user data, including activity logs, upon user or admin request.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement endpoints to export all user-related data in a machine-readable format and to delete user data in accordance with GDPR requirements. Ensure deletion covers user profile, roles, and associated activity logs, and that exports are complete and secure.\n<info added on 2025-11-11T21:46:01.948Z>\n**Requirements Analysis:**\n\n1. Data Export Endpoint (GET /api/users/{user_id}/export):\n   - Export user profile data (username, email, full_name, role, etc.)\n   - Export all activity logs related to user (both as actor and subject)\n   - Export user sessions history\n   - Export API keys (without sensitive key material)\n   - Export user roles and permissions\n   - Format: JSON (machine-readable)\n   - Admin-only access\n\n2. Data Deletion Endpoint (DELETE /api/users/{user_id}/gdpr-delete):\n   - Delete user profile from admin_users table\n   - Delete/anonymize activity logs (preserve audit trail but remove PII)\n   - Delete all user sessions from admin_sessions table\n   - Revoke all user API keys\n   - Delete user role assignments\n   - Cascade deletion with proper foreign key handling\n   - Admin-only access with confirmation required\n\n**Implementation Plan:**\n- Add export_user_data() method to UserService\n- Add gdpr_delete_user() method to UserService\n- Add GDPR export/delete endpoints to users router\n- Add Pydantic models for export response\n- Consider: Activity logs should be anonymized rather than deleted for audit compliance\n</info added on 2025-11-11T21:46:01.948Z>",
            "status": "done",
            "testStrategy": "Request data export and deletion for test users. Verify completeness of exported data and confirm all user data is removed after deletion, including logs.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on user management & gdpr compliance."
      },
      {
        "id": 34,
        "title": "Analytics Dashboard Implementation",
        "description": "Build dashboard for document stats, query metrics, user activity, storage usage, and API endpoint usage.",
        "details": "Use Grafana or Streamlit for dashboard UI. Aggregate metrics from Supabase and Prometheus.",
        "testStrategy": "Verify dashboard accuracy and responsiveness under load.",
        "priority": "medium",
        "dependencies": [
          "33"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop Dashboard UI with Grafana or Streamlit",
            "description": "Set up the dashboard user interface using either Grafana or Streamlit, ensuring a logical layout for document stats, query metrics, user activity, storage usage, and API endpoint usage.",
            "dependencies": [],
            "details": "Install and configure Grafana or Streamlit. Design the dashboard structure, applying best practices such as focusing on key metrics, using consistent layouts, and providing clear panel documentation. Ensure the UI is intuitive and supports dynamic filtering or variable selection as needed.\n<info added on 2025-11-11T21:54:47.058Z>\nBased on the investigation findings, we will implement the analytics dashboard using Grafana since an existing infrastructure pattern is already established. We'll create a comprehensive dashboard with five main panel categories: document statistics, query metrics, user activity, storage usage, and API endpoint usage.\n\nThe implementation will follow this approach:\n1. Create a dedicated metrics service in app/services/metrics_service.py to collect and organize analytics data\n2. Add a Prometheus metrics endpoint in app/routes/monitoring.py to expose metrics for Grafana consumption\n3. Develop a Grafana dashboard JSON configuration at monitoring/grafana/dashboards/empire_analytics.json\n4. Follow the established pattern from the existing ragas_metrics.json dashboard for consistency\n\nThe dashboard will leverage the existing Grafana infrastructure while providing comprehensive visibility into system performance and usage patterns across all key operational areas.\n</info added on 2025-11-11T21:54:47.058Z>",
            "status": "done",
            "testStrategy": "Verify that all required metric categories are represented and the UI is navigable. Check for adherence to dashboard design best practices.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Aggregate Metrics from Supabase and Prometheus",
            "description": "Implement data aggregation logic to collect and preprocess metrics from Supabase and Prometheus for use in the dashboard.",
            "dependencies": [
              1
            ],
            "details": "Develop scripts or queries to extract relevant metrics (document stats, query metrics, user activity, storage usage, API endpoint usage) from Supabase and Prometheus. Transform and aggregate data as needed for efficient dashboard consumption. Ensure data freshness and reliability.",
            "status": "done",
            "testStrategy": "Validate that all required metrics are accurately aggregated and available for the dashboard. Test with sample data and edge cases.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Data Visualization Components",
            "description": "Create and configure visualizations for each metric category, ensuring clarity and actionable insights.",
            "dependencies": [
              2
            ],
            "details": "Select appropriate visualization types (e.g., graphs, tables, gauges) for each metric. Configure panels to highlight key signals and trends. Apply consistent color schemes and labeling. Add annotations or context where relevant to aid interpretation.",
            "status": "done",
            "testStrategy": "Review each visualization for accuracy, clarity, and alignment with dashboard goals. Solicit feedback from stakeholders and iterate as needed.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Test Dashboard Load and Responsiveness",
            "description": "Evaluate dashboard performance under expected and peak loads, optimizing for fast load times and responsive interactions.",
            "dependencies": [
              3
            ],
            "details": "Simulate concurrent users and high data volumes. Monitor dashboard load times, panel refresh rates, and responsiveness. Apply optimizations such as query aggregation, efficient variable usage, and appropriate refresh intervals. Document and address any bottlenecks.",
            "status": "done",
            "testStrategy": "Run load tests and measure key performance indicators (KPIs) such as load time and refresh latency. Confirm dashboard remains usable and responsive under stress.",
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down analytics dashboard implementation into subtasks for dashboard UI development (Grafana/Streamlit), metrics aggregation from Supabase/Prometheus, data visualization, and load/responsiveness testing."
      },
      {
        "id": 35,
        "title": "CrewAI Multi-Agent Integration & Orchestration",
        "description": "Integrate CrewAI service (REST API) for multi-agent workflows, agent management, and orchestration.",
        "details": "Connect to CrewAI REST API (srv-d2n0hh3uibrs73buafo0). Implement agent pool management, dynamic agent creation, lifecycle, and resource allocation. Support async task execution via Celery.",
        "testStrategy": "Run multi-agent workflows, verify orchestration, agent lifecycle, and result aggregation.",
        "priority": "high",
        "dependencies": [
          "34"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate CrewAI REST API for Agent Pool Management and Dynamic Agent Creation",
            "description": "Connect to the CrewAI REST API and implement logic for managing an agent pool, including dynamic creation, configuration, and lifecycle management of agents.",
            "dependencies": [],
            "details": "Establish secure connectivity to the CrewAI REST API (srv-d2n0hh3uibrs73buafo0). Implement endpoints and logic for creating, updating, and deleting agents dynamically. Support agent configuration (roles, goals, tools, memory, etc.) as per CrewAI's agent model. Ensure agents can be instantiated with custom parameters and maintain their lifecycle state.\n<info added on 2025-11-12T02:56:38.121Z>\nBased on the investigation, I'll enhance the CrewAI integration by implementing the following:\n\n1. Extend the existing crewai_service.py with comprehensive agent pool management methods:\n   - Agent CRUD operations: create_agent(), update_agent(), delete_agent(), get_agent(), get_agents()\n   - Crew management functions: create_crew(), update_crew(), delete_crew(), get_crew(), get_crews()\n   - Resource monitoring via get_agent_pool_stats() to track agent utilization and availability\n\n2. Implement Supabase database integration for the existing schema (crewai_agents, crewai_crews, crewai_task_templates, crewai_executions) to ensure persistent storage of agent configurations and execution history.\n\n3. Develop agent lifecycle management functionality including activation, deactivation, and status tracking.\n\n4. Create REST API routes in app/routes/crewai.py exposing agent and crew management endpoints.\n\n5. Connect with the existing CrewAI REST API at https://jb-crewai.onrender.com for agent execution and orchestration.\n</info added on 2025-11-12T02:56:38.121Z>",
            "status": "done",
            "testStrategy": "Create, update, and delete agents via API calls. Verify agent state transitions and configuration persistence.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Multi-Agent Workflow Orchestration and Resource Allocation",
            "description": "Develop orchestration logic to coordinate multi-agent workflows, manage task assignments, and allocate resources efficiently among agents.",
            "dependencies": [
              1
            ],
            "details": "Design and implement orchestration mechanisms using CrewAI's crew-and-flow model. Enable both sequential and parallel task execution modes. Assign tasks to agents based on their roles and goals, and manage dependencies between tasks. Implement resource allocation strategies to optimize agent utilization and prevent overload.",
            "status": "done",
            "testStrategy": "Run sample multi-agent workflows with varying complexity. Verify correct task sequencing, parallelism, and resource allocation.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Enable Asynchronous Task Execution and Monitoring via Celery",
            "description": "Integrate Celery to support asynchronous execution of agent tasks and implement monitoring for workflow progress and agent states.",
            "dependencies": [
              2
            ],
            "details": "Set up Celery workers to handle asynchronous task execution for CrewAI workflows. Ensure tasks can be queued, executed, and monitored independently. Capture logs and state changes for each agent and workflow. Implement error handling and alerting for failed tasks or agent exceptions.",
            "status": "done",
            "testStrategy": "Submit multiple concurrent workflows, monitor execution progress, and verify correct handling of asynchronous tasks and error scenarios.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on crewai multi-agent integration & orchestration."
      },
      {
        "id": 36,
        "title": "CrewAI Asset Generation Agents Implementation",
        "description": "Implement 8 asset generation agents (orchestrator, summarizer, skill, command, agent, prompt, workflow, department classifier) per PRD specs.",
        "details": "Define agent roles, goals, tools, and LLM configs in crewai_agents table. Integrate with CrewAI API for asset generation. Store outputs in B2 processed/ folders.",
        "testStrategy": "Trigger asset generation for sample documents, verify output formats and B2 storage.",
        "priority": "high",
        "dependencies": [
          "35"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define and Configure 8 Asset Generation Agents in crewai_agents Table",
            "description": "Specify roles, goals, tools, and LLM configurations for orchestrator, summarizer, skill, command, agent, prompt, workflow, and department classifier agents as per PRD specifications.",
            "dependencies": [],
            "details": "Draft detailed agent definitions in the crewai_agents table, ensuring each agent's role, goal, toolset, and LLM configuration aligns with PRD requirements. Use YAML or database schema as appropriate. Validate configuration completeness for all 8 agents.",
            "status": "done",
            "testStrategy": "Review crewai_agents table for correct entries and completeness. Validate agent configs load without errors in CrewAI.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate Agents with CrewAI API for Asset Generation Workflows",
            "description": "Connect the configured agents to the CrewAI API, enabling them to generate assets according to workflow requirements.",
            "dependencies": [
              1
            ],
            "details": "Implement integration logic to instantiate and orchestrate the 8 agents using the CrewAI API. Ensure agents can receive tasks, execute asset generation, and interact as needed. Handle API authentication and error management.",
            "status": "done",
            "testStrategy": "Trigger asset generation for sample inputs via CrewAI API and verify that each agent performs its designated function.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Store Generated Assets in B2 Processed Folders",
            "description": "Implement logic to save all outputs from asset generation agents into the appropriate B2 processed/ folders.",
            "dependencies": [
              2
            ],
            "details": "Develop or update storage routines to ensure all generated assets are saved in the correct B2 processed/ directory structure. Confirm metadata and output formats match requirements. Handle storage errors and ensure data integrity.",
            "status": "done",
            "testStrategy": "Generate assets through the workflow and verify their presence, structure, and metadata in B2 processed/ folders.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on crewai asset generation agents implementation."
      },
      {
        "id": 37,
        "title": "CrewAI Document Analysis Agents Implementation",
        "description": "Implement 3 document analysis agents (research analyst, content strategist, fact checker) for structured analysis and verification.",
        "details": "Configure agents in crewai_agents table. Integrate with CrewAI API for analysis workflows. Store analysis outputs in Supabase and B2.",
        "testStrategy": "Run analysis workflows, verify structured outputs and fact verification accuracy.",
        "priority": "high",
        "dependencies": [
          "36"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Document Analysis Agents in crewai_agents Table",
            "description": "Define and register the three specialized agents (research analyst, content strategist, fact checker) in the crewai_agents table with appropriate roles, goals, and capabilities.",
            "dependencies": [],
            "details": "Specify agent roles, goals, and backstories in the crewai_agents table or agents.yaml. Ensure each agent is configured for its analysis specialization and can be referenced by workflows. Use CrewAI's agent configuration standards for compatibility.",
            "status": "done",
            "testStrategy": "Verify agents appear in the crewai_agents table and can be instantiated by CrewAI workflows.",
            "parentId": "undefined",
            "updatedAt": "2025-11-14T18:18:43.346Z"
          },
          {
            "id": 2,
            "title": "Integrate Agents with CrewAI API for Analysis Workflows",
            "description": "Connect the configured agents to the CrewAI API, enabling them to participate in document analysis workflows.",
            "dependencies": [
              1
            ],
            "details": "Implement API integration logic to allow the agents to receive tasks, process documents, and return structured outputs. Ensure agents can be triggered via the CrewAI API and handle input/output formats as required by the workflow.",
            "status": "done",
            "testStrategy": "Trigger sample analysis workflows via the API and confirm agents process and return structured results.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Document Analysis Workflow Execution",
            "description": "Design and execute workflows that coordinate the three agents for structured document analysis and verification.",
            "dependencies": [
              2
            ],
            "details": "Define workflow logic that assigns documents to the appropriate agents, sequences their tasks (e.g., research, content strategy, fact checking), and aggregates their outputs. Use CrewAI's workflow orchestration features to manage task flow.",
            "status": "done",
            "testStrategy": "Run end-to-end workflow executions and verify that each agent performs its designated analysis step.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Store Analysis Outputs in Supabase and B2",
            "description": "Persist the structured outputs from each agent in Supabase for structured data and B2 for file storage.",
            "dependencies": [
              3
            ],
            "details": "Implement logic to map agent outputs to Supabase tables for structured results and upload any relevant files or artifacts to B2. Ensure outputs are linked to the correct document and agent metadata.",
            "status": "done",
            "testStrategy": "Check Supabase and B2 for correct storage of outputs after workflow execution; verify data integrity and retrievability.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Test and Validate Agent Output Accuracy and Fact Verification",
            "description": "Systematically test the accuracy of agent outputs, with a focus on fact-checking reliability and structured result formats.",
            "dependencies": [
              4
            ],
            "details": "Develop test cases with known document inputs and expected outputs. Evaluate the correctness of research, content strategy, and fact-checking results. Measure fact-checker precision and recall, and validate output structure.",
            "status": "done",
            "testStrategy": "Run automated and manual tests comparing outputs to ground truth; review fact-checking results for accuracy and completeness.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down document analysis agent implementation into subtasks for agent configuration, CrewAI API integration, workflow execution, output storage in Supabase/B2, and accuracy testing.",
        "updatedAt": "2025-11-14T18:18:43.346Z"
      },
      {
        "id": 38,
        "title": "CrewAI Multi-Agent Orchestration Agents Implementation",
        "description": "Implement 4 orchestration agents (research, analysis, writing, review) for complex multi-document workflows.",
        "details": "Configure agents and crews in crewai_crews table. Support sequential and parallel execution modes. Integrate with CrewAI API for orchestration.",
        "testStrategy": "Run multi-agent orchestration workflows, verify execution order and output quality.",
        "priority": "high",
        "dependencies": [
          "37"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Orchestration Agents and Crews in crewai_crews Table",
            "description": "Define and register the four orchestration agents (research, analysis, writing, review) and their crew configurations in the crewai_crews table.",
            "dependencies": [],
            "details": "Specify agent roles, goals, backstories, and advanced options (e.g., LLM, delegation, tools) for each agent. Ensure each agent is correctly mapped to its crew and that the crew structure supports both sequential and parallel execution modes.",
            "status": "done",
            "testStrategy": "Verify agents and crews are correctly listed in the crewai_crews table and can be retrieved via API.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Sequential and Parallel Execution Logic for Agent Workflows",
            "description": "Develop logic to support both sequential and parallel execution of agent tasks within a crew for multi-document workflows.",
            "dependencies": [
              1
            ],
            "details": "Design execution engine to trigger agents in order (sequential) or concurrently (parallel) based on workflow configuration. Ensure correct handling of dependencies and data flow between agents.",
            "status": "done",
            "testStrategy": "Run sample workflows in both modes, confirm correct execution order and data handoff.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate CrewAI API for Orchestration and Agent Lifecycle Management",
            "description": "Connect orchestration logic to CrewAI API endpoints for agent invocation, status tracking, and result retrieval.",
            "dependencies": [
              2
            ],
            "details": "Implement API calls for agent task submission, monitor agent progress, and handle callbacks or polling for completion. Ensure robust error handling and retries.",
            "status": "done",
            "testStrategy": "Trigger agent workflows via API, verify correct agent lifecycle events and result collection.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Develop Workflow Management and State Tracking Mechanisms",
            "description": "Create workflow management logic to track the state, progress, and dependencies of multi-agent, multi-document workflows.",
            "dependencies": [
              3
            ],
            "details": "Implement state machine or workflow tracker to monitor each agent's status, handle transitions, and manage workflow metadata. Support resumption and recovery from failures.",
            "status": "done",
            "testStrategy": "Simulate workflow interruptions and restarts, verify accurate state tracking and recovery.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Output Validation and Quality Assurance for Agent Results",
            "description": "Design and apply validation checks to ensure agent outputs meet expected quality, format, and completeness standards.",
            "dependencies": [
              4
            ],
            "details": "Define validation rules for each agent type (e.g., research completeness, analysis accuracy, writing coherence, review thoroughness). Integrate automated and optional human-in-the-loop checks.",
            "status": "done",
            "testStrategy": "Run workflows with known-good and intentionally flawed inputs, verify validation catches errors and approves correct outputs.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Develop and Execute Comprehensive Orchestration Testing Suite",
            "description": "Create automated tests to validate orchestration logic, agent integration, workflow management, and output quality across various scenarios.",
            "dependencies": [
              5
            ],
            "details": "Design test cases for sequential and parallel workflows, error handling, state recovery, and output validation. Use both unit and integration tests to ensure system robustness.",
            "status": "done",
            "testStrategy": "Run full test suite, confirm all orchestration paths and edge cases are covered and pass.",
            "parentId": "undefined"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Expand orchestration agent implementation into subtasks for agent/crew configuration, sequential/parallel execution logic, CrewAI API integration, workflow management, output validation, and orchestration testing."
      },
      {
        "id": 39,
        "title": "CrewAI Inter-Agent Messaging & Collaboration",
        "description": "Enable inter-agent messaging, task delegation, result sharing, and conflict resolution within CrewAI workflows.",
        "details": "Implement agent interactions in crewai_agent_interactions table. Support direct/broadcast messaging, event publication, and state synchronization.",
        "testStrategy": "Simulate collaborative workflows, verify messaging, delegation, and result aggregation.",
        "priority": "high",
        "dependencies": [
          "38"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Inter-Agent Interaction Schema",
            "description": "Define the database schema and data model for agent interactions, supporting messaging, delegation, event publication, and state synchronization.",
            "dependencies": [],
            "details": "Create or update the crewai_agent_interactions table to capture direct/broadcast messages, event logs, delegation records, and state changes. Ensure extensibility for future collaboration features.",
            "status": "done",
            "testStrategy": "Review schema against requirements; validate with sample interaction records.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Direct and Broadcast Messaging Logic",
            "description": "Develop backend logic for agents to send direct and broadcast messages to other agents within a crew.",
            "dependencies": [
              1
            ],
            "details": "Implement API endpoints and internal functions for direct (agent-to-agent) and broadcast (agent-to-crew) messaging. Store messages in the interaction table and trigger notifications as needed.",
            "status": "done",
            "testStrategy": "Unit test message delivery, verify correct routing and storage for both direct and broadcast cases.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Develop Event Publication Mechanism",
            "description": "Enable agents to publish events (e.g., task completion, delegation, errors) for workflow coordination and monitoring.",
            "dependencies": [
              1
            ],
            "details": "Implement event publishing logic, allowing agents to emit structured events to the crewai_agent_interactions table. Support event subscription and notification for relevant agents.",
            "status": "done",
            "testStrategy": "Simulate event publication and subscription; verify event propagation and logging.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement State Synchronization Across Agents",
            "description": "Ensure agents maintain consistent shared state during collaborative workflows, including task progress and result sharing.",
            "dependencies": [
              1
            ],
            "details": "Design and implement mechanisms for agents to synchronize state changes (e.g., task status, shared data) via the interaction table or dedicated state sync service. Handle concurrent updates and conflict scenarios.",
            "status": "done",
            "testStrategy": "Test state updates under concurrent agent actions; verify consistency and conflict handling.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Integrate Conflict Resolution Logic",
            "description": "Develop logic for detecting and resolving conflicts between agents, such as task assignment disputes or inconsistent states.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Implement automated and/or human-in-the-loop conflict resolution workflows. Log conflict events, trigger resolution protocols, and update agent states accordingly.",
            "status": "done",
            "testStrategy": "Simulate conflict scenarios; verify detection, resolution, and state updates.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Simulate and Test Collaborative Workflow Scenarios",
            "description": "Create and execute end-to-end workflow simulations to validate inter-agent messaging, delegation, event handling, state sync, and conflict resolution.",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "Design test scenarios covering typical and edge-case collaborative workflows. Automate simulation runs and verify expected outcomes in the interaction table and agent states.",
            "status": "done",
            "testStrategy": "Run integration tests for full workflows; check messaging, event logs, state consistency, and conflict resolution.",
            "parentId": "undefined"
          }
        ],
        "complexity": 8.5,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down inter-agent messaging and collaboration into subtasks for designing the interaction schema, implementing direct/broadcast messaging, event publication, state synchronization, conflict resolution, and workflow simulation testing."
      },
      {
        "id": 40,
        "title": "CrewAI Asset Storage & Retrieval",
        "description": "Store generated assets in crewai_generated_assets table and B2, enable retrieval by department, type, and confidence.",
        "details": "Implement asset storage logic, organize B2 folders, and support asset retrieval APIs. Track confidence scores and metadata.",
        "testStrategy": "Generate and retrieve assets, verify storage, organization, and retrieval accuracy.",
        "priority": "high",
        "dependencies": [
          "39"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Asset Storage Logic in crewai_generated_assets Table and B2",
            "description": "Design and implement the logic to store generated assets in the crewai_generated_assets database table and organize them in B2 cloud storage.",
            "dependencies": [],
            "details": "Define the schema for asset metadata, including department, type, and confidence score. Integrate asset generation outputs with the database and B2 storage. Ensure assets are stored in organized B2 folders based on department and type, and metadata is consistently tracked in the database.\n<info added on 2025-11-13T20:40:40.237Z>\nIMPLEMENTATION COMPLETE (2025-01-13):\n\n✅ Database Schema:\n- crewai_generated_assets table created in Supabase production\n- All columns implemented: id, execution_id, document_id, department, asset_type, asset_name, content, content_format, b2_path, file_size, mime_type, metadata, confidence_score, created_at\n- Foreign keys configured: execution_id → crewai_executions, document_id → documents\n\n✅ Service Implementation:\n- app/services/crewai_asset_service.py (324 lines)\n- store_asset() method handles both text-based and file-based assets\n- Text assets: stored in DB content column\n- File assets: uploaded to B2, b2_path stored in DB\n- B2 folder organization: crewai/assets/{department}/{asset_type}/{execution_id}/{filename}\n\n✅ Pydantic Models:\n- app/models/crewai_asset.py (173 lines)\n- AssetStorageRequest, AssetResponse, AssetUpdateRequest, AssetListResponse, AssetRetrievalFilters\n- Enums: AssetType, Department, ContentFormat\n\nStatus: READY FOR TESTING\n</info added on 2025-11-13T20:40:40.237Z>",
            "status": "done",
            "testStrategy": "Create sample assets, store them, and verify correct database entries and B2 folder organization.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop Asset Retrieval APIs by Department, Type, and Confidence",
            "description": "Build APIs to enable retrieval of stored assets filtered by department, asset type, and confidence score.",
            "dependencies": [
              1
            ],
            "details": "Design RESTful endpoints for asset retrieval. Implement query logic to filter assets using department, type, and confidence score from the crewai_generated_assets table and B2 storage. Ensure efficient and secure access to asset files and metadata.\n<info added on 2025-11-13T20:41:13.454Z>\nIMPLEMENTATION COMPLETE (2025-01-13):\n\n✅ API Routes Implemented:\n- app/routes/crewai_assets.py (284 lines)\n- Router prefix: /api/crewai/assets\n- Tags: [\"CrewAI Assets\"]\n\n✅ Endpoints:\n1. POST /api/crewai/assets/ - Store asset (text or file-based)\n2. GET /api/crewai/assets/ - Retrieve with filters (department, asset_type, confidence, pagination)\n3. GET /api/crewai/assets/{asset_id} - Get single asset by ID\n4. PATCH /api/crewai/assets/{asset_id} - Update confidence score and metadata\n5. GET /api/crewai/assets/execution/{execution_id} - Get all assets for execution\n\n✅ Filter Implementation:\n- execution_id (UUID)\n- department (enum: marketing, legal, hr, finance, etc.)\n- asset_type (enum: summary, analysis, report, etc.)\n- min_confidence / max_confidence (0-1)\n- limit (max 1000)\n- offset (pagination)\n\n✅ Service Integration:\n- Uses CrewAIAssetService via dependency injection\n- Full error handling (400, 404, 500)\n- Logging for all operations\n\nStatus: IMPLEMENTATION COMPLETE - Need to verify route registration in main.py\n</info added on 2025-11-13T20:41:13.454Z>",
            "status": "done",
            "testStrategy": "Test API endpoints with various filter combinations and validate that correct assets and metadata are returned.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Track and Update Asset Confidence Scores and Metadata",
            "description": "Implement mechanisms to track, update, and manage confidence scores and metadata for each asset throughout its lifecycle.",
            "dependencies": [
              1
            ],
            "details": "Add logic to update confidence scores and metadata in the crewai_generated_assets table as assets are processed or reviewed. Ensure changes are reflected in both the database and B2 storage organization if relevant. Provide audit trails for metadata updates.\n<info added on 2025-11-13T20:41:18.472Z>\nIMPLEMENTATION COMPLETE (2025-01-13):\n\n✅ Confidence Score Tracking:\n- Database column: confidence_score (float, nullable)\n- Validation: 0.0 - 1.0 range enforced in Pydantic models\n- Initial score set during asset creation\n- Update via PATCH /api/crewai/assets/{asset_id}\n\n✅ Metadata Management:\n- Database column: metadata (JSONB, default {})\n- Stored in Supabase as structured JSON\n- Full flexibility for custom metadata fields\n- MERGE behavior: new metadata merged with existing (preserves existing keys)\n- Update via AssetUpdateRequest model\n\n✅ Update Method (app/services/crewai_asset_service.py):\n- update_asset(asset_id, update_request)\n- Fetches existing asset\n- Merges metadata: {**existing.metadata, **update.metadata}\n- Updates confidence_score if provided\n- Returns updated AssetResponse\n\n✅ API Endpoint:\n- PATCH /api/crewai/assets/{asset_id}\n- Request: {confidence_score?: float, metadata?: dict}\n- Response: Updated AssetResponse\n- Errors: 404 (not found), 400 (invalid), 500 (server error)\n\nStatus: READY FOR TESTING\n</info added on 2025-11-13T20:41:18.472Z>",
            "status": "done",
            "testStrategy": "Simulate asset review and update workflows, verify that confidence scores and metadata are correctly updated and tracked.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on crewai asset storage & retrieval."
      },
      {
        "id": 41,
        "title": "Security Hardening & Compliance",
        "description": "Implement JWT authentication, RBAC, encrypted storage, input validation, and compliance features (GDPR, HIPAA, SOC 2).",
        "details": "Use PyJWT for authentication, enforce RBAC, encrypt Supabase volumes and B2 files, validate inputs, and implement audit trails. Support data export/deletion for GDPR.",
        "testStrategy": "Run security tests, penetration testing, and compliance checks.",
        "priority": "high",
        "dependencies": [
          "40"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement JWT Authentication with PyJWT",
            "description": "Set up secure JWT authentication using PyJWT, ensuring best practices for token issuance, validation, and storage.",
            "dependencies": [],
            "details": "Configure PyJWT to use strong signing algorithms (e.g., RS256), set short expiration times, validate all claims (issuer, audience, expiration), and store tokens securely (prefer HttpOnly cookies). Avoid storing sensitive data in JWTs and ensure all token transmission uses HTTPS.\n<info added on 2025-11-14T19:10:23.776Z>\n## Current Status\nJWT authentication implemented via Clerk integration (app/middleware/clerk_auth.py) with session token verification working.\n\n## Required Security Enhancements\n1. Add rate limiting to authentication endpoints using slowapi library\n2. Implement token refresh endpoint with refresh token rotation\n3. Add session timeout middleware with both idle and absolute timeout enforcement\n4. Ensure HTTPS-only transmission in production environment\n\n## Implementation Files\n- Existing: app/middleware/clerk_auth.py (JWT verification)\n- Existing: app/middleware/auth.py (JWT/API key validation)\n- New: app/middleware/rate_limit.py (for API rate limiting)\n- New: app/routes/auth.py (token refresh endpoint)\n\n## Security Assessment\nAuthentication foundation is solid. Focus should be on hardening through rate limiting and robust session management.\n</info added on 2025-11-14T19:10:23.776Z>\n<info added on 2025-11-14T19:38:41.247Z>\n## Implementation Complete\n\nSecurity hardening implementation for JWT authentication has been successfully completed with the following components:\n\n### Rate Limiting\n- Implemented using slowapi>=0.1.9\n- Created app/middleware/rate_limit.py with tiered limits:\n  - Auth endpoints: 5 login attempts/minute, 3 registrations/hour\n  - API key management: 10 creates/hour, 20 revocations/minute\n  - File uploads: 50/hour for single, 10/hour for bulk\n  - Query endpoints: 100/minute for simple, 20/minute for complex\n- Uses Redis in production, in-memory storage in development\n- Per-user and per-IP rate limiting with proper headers\n\n### Security Headers Middleware\n- Created app/middleware/security.py with SecurityHeadersMiddleware\n- Implemented headers: HSTS, X-Content-Type-Options, X-Frame-Options, X-XSS-Protection, Referrer-Policy, Permissions-Policy, and Content-Security-Policy\n- Environment-specific configurations with relaxed settings for documentation endpoints\n\n### CORS Hardening\n- Updated configuration in app/main.py with explicit HTTP methods\n- Environment-based configuration with production warnings\n\n### Testing\n- Created comprehensive test_task41_security.py (320 lines)\n- Tests for headers, rate limiting, CORS, and overall API health\n\n### Files Modified/Created\n- requirements.txt: Added slowapi and redis\n- app/middleware/security.py: NEW (180 lines)\n- app/middleware/rate_limit.py: NEW (260 lines)\n- app/main.py: MODIFIED\n- test_task41_security.py: NEW (320 lines)\n\n### Security Improvements\n- Protection against brute force, clickjacking, MIME sniffing, XSS\n- HTTPS enforcement in production\n- Information disclosure prevention\n- DoS protection through rate limiting\n</info added on 2025-11-14T19:38:41.247Z>",
            "status": "done",
            "testStrategy": "Unit test token issuance and validation, attempt token tampering, and verify rejection of invalid or expired tokens.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Enforce Role-Based Access Control (RBAC)",
            "description": "Integrate RBAC to restrict access to resources based on user roles and permissions.",
            "dependencies": [
              1
            ],
            "details": "Design a roles and permissions schema. Implement middleware to check user roles (from identity, not from JWT claims) on each protected endpoint. Ensure permissions are managed in the authorization layer, not embedded in JWTs.\n<info added on 2025-11-14T19:10:28.200Z>\n## Current Status\nRBAC fully implemented with 4 roles (admin, editor, viewer, guest). Complete lifecycle management in app/services/rbac_service.py. Database tables exist (users, roles, user_roles, api_keys).\n\n## Implementation Details\n- Role permission checking: app/middleware/auth.py:require_admin(), require_role()\n- API key lifecycle: generation, rotation, revocation, expiration\n- Bcrypt hashing for API keys (never stores plaintext)\n- Database schema ready in Supabase\n\n## Additional Work Needed\n- Row-Level Security (RLS) policies on all user-facing tables (CRITICAL)\n- API key scope validation (scopes field exists but not enforced)\n- Permission cache invalidation for role updates\n\n## Focus Areas\n1. Design and implement PostgreSQL RLS policies for data isolation\n2. Add scope validation middleware for API keys\n3. Test user data isolation at database level\n\n## Security Assessment\nAuthorization system is production-ready. Main gap is RLS enforcement.\n</info added on 2025-11-14T19:10:28.200Z>",
            "status": "done",
            "testStrategy": "Test endpoints with users of different roles, verify access is correctly granted or denied according to RBAC rules.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Set Up Encrypted Storage for Supabase and B2 Files",
            "description": "Encrypt all data at rest in Supabase volumes and Backblaze B2 file storage.",
            "dependencies": [],
            "details": "Enable encryption for Supabase storage volumes and configure server-side encryption for B2 buckets. Ensure encryption keys are securely managed and rotated according to policy.\n<info added on 2025-11-14T19:10:32.282Z>\n## Current Status\nFile encryption implementation is EXCELLENT with AES-256-GCM in app/services/encryption.py featuring:\n- 256-bit keys with PBKDF2 (100k iterations)\n- Random salts and nonces per file\n- Authenticated encryption with GCM mode\n- B2 integration ready\n- Test coverage in tests/test_encryption.py\n\n## Additional Work Needed\n- Verify Supabase encryption-at-rest is enabled\n- Confirm TLS for all database connections (Neo4j already using TLS with bolt+ssc://localhost:7687)\n- Add key rotation policies\n- Optional: Integrate with AWS KMS or HashiCorp Vault for key management\n\n## Verification Tasks\n1. Check Supabase project settings for encryption-at-rest\n2. Verify B2 server-side encryption configuration\n3. Document encryption key management procedures\n4. Test file encryption/decryption with B2 upload\n\n## Security Assessment\nEncryption implementation is production-grade. Focus should be on verification and key management procedures.\n</info added on 2025-11-14T19:10:32.282Z>",
            "status": "done",
            "testStrategy": "Verify files and database volumes are encrypted at rest, attempt unauthorized access to raw storage, and confirm data is unreadable without decryption keys.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Comprehensive Input Validation",
            "description": "Validate all user and API inputs to prevent injection and data integrity issues.",
            "dependencies": [],
            "details": "Apply strict input validation on all endpoints using whitelisting and schema validation. Sanitize inputs to prevent SQL injection, XSS, and other common attacks. Use libraries for validation where possible.\n<info added on 2025-11-14T19:10:37.398Z>\n## Current Status\nInput validation implementation is GOOD. Pydantic models are used throughout the codebase (7 model files) with:\n- Type hints and Field() constraints\n- File upload validation (whitelist, 100MB limit, 10 files max)\n- Email validation with EmailStr\n- Custom validators for key fields\n\n## Files With Validation\n- app/models/rbac.py (RBAC validation)\n- app/models/documents.py (document validation)\n- app/models/users.py (user validation)\n- app/api/upload.py (file upload validation)\n\n## Additional Work Needed\n- Request body size limits middleware (prevent DoS)\n- Custom validators for SQL injection prevention\n- Path traversal validation (no ../, null bytes)\n- XSS prevention in metadata fields\n- Rate limiting on all API endpoints\n\n## Hardening Tasks\n1. Add max_body_size middleware\n2. Create security validators for:\n   - Document paths\n   - Query parameters\n   - Metadata values\n3. Audit all database queries for parameterization\n4. Add input sanitization for user-generated content\n\n## Security Assessment\nValidation foundation is solid. Need additional hardening for edge cases.\n</info added on 2025-11-14T19:10:37.398Z>",
            "status": "done",
            "testStrategy": "Fuzz endpoints with invalid and malicious inputs, verify that invalid data is rejected and no vulnerabilities are introduced.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Audit Trail and Logging",
            "description": "Create an audit trail system to log security-relevant events and user actions for compliance and forensic analysis.",
            "dependencies": [
              1,
              2
            ],
            "details": "Log authentication events, access control decisions, data exports/deletions, and administrative actions. Ensure logs are tamper-evident and securely stored. Provide tools for querying and exporting audit logs.\n<info added on 2025-11-14T19:10:44.198Z>\n## Current Status\nAudit logging partially implemented with structlog throughout the application. AuditLogEntry model defined in app/models/rbac.py with all required fields (event_type, actor, target, IP, user_agent, metadata).\n\n## Events Currently Logged\n- Authentication attempts (success/failure)\n- API key creation/rotation/revocation\n- Role assignments\n- Access denials\n\n## Critical Gap\nLogs are only stored in application logs, not persisted to database, preventing querying for compliance or incident investigation purposes.\n\n## Implementation Plan\n1. Create audit_logs table in Supabase with AuditLogEntry schema\n2. Create app/middleware/audit.py to persist all security events\n3. Add audit log query/search endpoints in app/routes/audit.py\n4. Implement log retention policies (90 days active, 7 years archive)\n5. Extract IP address and User-Agent from requests\n6. Make logs tamper-evident (append-only, signed)\n\n## Database Schema\n- Table: audit_logs\n- Columns: id, event_type, actor_user_id, target_user_id, target_resource_type, target_resource_id, action, result, ip_address, user_agent, metadata (JSONB), error_message, created_at\n\n## Security Assessment\nFoundation exists but high priority to persist logs to database for compliance and security investigation capabilities.\n</info added on 2025-11-14T19:10:44.198Z>",
            "status": "done",
            "testStrategy": "Trigger various security events, verify logs are generated, immutable, and contain all required information.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Integrate Compliance Features (GDPR, HIPAA, SOC 2)",
            "description": "Implement features to meet GDPR, HIPAA, and SOC 2 requirements, including data export/deletion and privacy controls.",
            "dependencies": [
              3,
              4,
              5
            ],
            "details": "Support user data export and deletion (GDPR), ensure auditability and access controls (SOC 2), and implement privacy and security safeguards (HIPAA). Document compliance measures and provide user interfaces for data requests.\n<info added on 2025-11-14T19:10:53.454Z>\n## CURRENT STATUS\n- GDPR models exist in app/models/users.py (UserDataExport, GDPRDeleteResponse) but implementation needs verification.\n\n## COMPLIANCE REQUIREMENTS\n- GDPR: User data export, complete deletion, consent tracking, data retention\n- HIPAA: Encryption (✅), access controls (✅), audit trails (⚠️ needs DB persistence)\n- SOC 2: Auditability (⚠️), access controls (✅), security monitoring\n\n## VERIFICATION TASKS\n1. Test GDPR data export endpoint - verify all user PII is included\n2. Test GDPR deletion endpoint - verify complete removal from all tables\n3. Document data retention policies\n4. Add consent tracking for data processing\n5. Create user-facing data request interface\n\n## COMPLIANCE FEATURES TO IMPLEMENT\n- Data export: JSON download of all user data\n- Right to deletion: Remove all PII from documents, embeddings, graphs\n- Data portability: Export in machine-readable format\n- Privacy controls: User-configurable data retention\n- Breach notification: Automated alerts for security incidents\n\n## SOC 2 REQUIREMENTS\n- Access control documentation (✅ via RBAC)\n- Audit trail persistence (⚠️ task 41.5)\n- Security monitoring dashboards\n- Incident response procedures\n\n## HIPAA SAFEGUARDS\n- Technical safeguards: Encryption (✅), access controls (✅)\n- Physical safeguards: Document B2/Supabase security\n- Administrative safeguards: Policies and training documentation\n\n## DEPENDENCIES\nRequires audit logging (41.5) and RLS policies (41.2) to be complete first.\n</info added on 2025-11-14T19:10:53.454Z>",
            "status": "done",
            "testStrategy": "Perform compliance checks, simulate data subject requests, and verify all regulatory requirements are met.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Conduct Security and Compliance Testing",
            "description": "Perform security testing, penetration testing, and compliance verification across all implemented features.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Run automated security scans, manual penetration tests, and compliance audits. Address any vulnerabilities or compliance gaps identified. Document test results and remediation steps.\n<info added on 2025-11-14T19:11:00.118Z>\nCURRENT STATUS: No security testing suite exists yet. This is the final validation phase after all security features are implemented.\n\nTESTING PLAN:\n\n1. AUTOMATED SECURITY SCANS:\n   - OWASP ZAP for penetration testing\n   - Bandit for Python code security analysis\n   - Safety for dependency vulnerability scanning\n   - SQLMap for SQL injection testing\n\n2. MANUAL PENETRATION TESTING:\n   - Auth bypass attempts\n   - Token tampering and replay attacks\n   - RBAC privilege escalation tests\n   - Input fuzzing (SQL injection, XSS, path traversal)\n   - Rate limit bypass attempts\n   - CORS misconfiguration exploits\n\n3. COMPLIANCE VERIFICATION:\n   - GDPR data export/deletion validation\n   - HIPAA audit trail completeness\n   - SOC 2 access control verification\n   - Encryption verification (at-rest, in-transit)\n\n4. SECURITY TEST SUITE:\n   - Create tests/security/ directory\n   - Write pytest tests for:\n     - Authentication flows\n     - RBAC enforcement\n     - Input validation edge cases\n     - Audit log persistence\n     - Rate limiting\n     - Session management\n\n5. DOCUMENTATION:\n   - Security architecture document\n   - Threat model and mitigations\n   - Incident response playbook\n   - Compliance certification evidence\n\nDEPENDENCIES: All subtasks 41.1-41.6 must be complete before testing can begin.\n\nDELIVERABLES:\n- Security test report with findings\n- Remediation plan for any issues\n- Compliance certification readiness assessment\n</info added on 2025-11-14T19:11:00.118Z>",
            "status": "done",
            "testStrategy": "Review test reports, verify all critical issues are resolved, and confirm compliance with GDPR, HIPAA, and SOC 2.",
            "parentId": "undefined"
          }
        ],
        "complexity": 9,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Decompose security hardening and compliance into subtasks for JWT authentication, RBAC enforcement, encrypted storage setup, input validation, audit trail implementation, compliance feature integration (GDPR, HIPAA, SOC 2), and security/compliance testing."
      },
      {
        "id": 42,
        "title": "Reliability & Disaster Recovery Implementation",
        "description": "Set up automated backups, health checks, auto-restart, and disaster recovery procedures.",
        "details": "Configure daily B2 backups, implement health endpoints, auto-restart on failure, and document disaster recovery drills. Use Infrastructure as Code (Terraform/Ansible) for fast rebuild.",
        "testStrategy": "Simulate failures, verify backup/restore, health checks, and recovery procedures.",
        "priority": "high",
        "dependencies": [
          "41"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Automated Backups and Restore Procedures",
            "description": "Set up daily automated B2 backups and validate restore processes to ensure data durability and rapid recovery.",
            "dependencies": [],
            "details": "Configure daily automated backups to Backblaze B2 using Infrastructure as Code (Terraform/Ansible). Regularly test backup integrity and perform restore drills to verify data can be recovered quickly and accurately. Document backup schedules, retention policies, and restoration steps.",
            "status": "done",
            "testStrategy": "Simulate data loss scenarios and perform full and partial restores from backups to verify data integrity and recovery time objectives.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Deploy Health Checks and Auto-Restart Mechanisms",
            "description": "Implement health endpoints and configure automated service restarts on failure to maintain high availability.",
            "dependencies": [
              1
            ],
            "details": "Develop and expose health check endpoints for all critical services. Integrate monitoring tools to continuously check service health. Configure auto-restart policies (e.g., systemd, Kubernetes liveness probes) to automatically recover failed services. Ensure monitoring alerts are in place for failed health checks and restarts.",
            "status": "done",
            "testStrategy": "Induce service failures and verify that health checks detect issues and auto-restart mechanisms restore service availability without manual intervention.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Document and Test Disaster Recovery Procedures",
            "description": "Create, document, and regularly test disaster recovery (DR) drills to ensure readiness for major outages.",
            "dependencies": [
              1,
              2
            ],
            "details": "Develop comprehensive disaster recovery documentation covering failover, rebuild, and recovery steps using Infrastructure as Code. Schedule and execute regular DR drills simulating various failure scenarios (e.g., region outage, data corruption). Update documentation based on drill outcomes and lessons learned.",
            "status": "done",
            "testStrategy": "Conduct scheduled disaster recovery drills, measure recovery time and data loss, and review documentation for completeness and clarity after each drill.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on reliability & disaster recovery implementation."
      },
      {
        "id": 43,
        "title": "Load Testing & Performance Optimization",
        "description": "Conduct load testing for document processing, query execution, and WebSocket connections. Optimize for throughput and latency.",
        "details": "Use locust or k6 for load testing. Profile bottlenecks, optimize Celery worker scaling, database indexes, and caching. Tune API and WebSocket performance.",
        "testStrategy": "Run load tests at 2x expected traffic, verify performance metrics and optimize as needed.",
        "priority": "high",
        "dependencies": [
          "42"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Execute Load Testing Scenarios for Document Processing, Query Execution, and WebSocket Connections",
            "description": "Develop and run comprehensive load tests targeting document processing, query execution, and WebSocket endpoints using tools like Locust or k6.",
            "dependencies": [],
            "details": "Identify key user flows and endpoints for document processing, query execution, and WebSocket communication. Create load test scripts in Locust (Python) or k6 (JavaScript), simulating realistic traffic patterns and scaling up to at least 2x expected peak load. Collect baseline metrics for throughput, latency, and error rates.\n<info added on 2025-11-16T19:08:05.314Z>\nAuthentication setup for load testing completed:\n- Fixed bug in app/middleware/clerk_auth.py by replacing non-existent sessions.verify_token() with proper JWT verification\n- Implemented JWT verification using jwt.decode() with CLERK_SECRET_KEY\n- Created generate_test_token.py script to generate valid JWT tokens for load testing\n- Changes committed (6a67a3b) and pushed to main branch\n- System is now ready for authentication-enabled load testing scenarios\n</info added on 2025-11-16T19:08:05.314Z>",
            "status": "done",
            "testStrategy": "Verify that load tests execute as intended, generate reproducible results, and cover all critical workflows. Ensure metrics are collected for throughput, latency, and error rates under varying load conditions.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Profile System Performance and Identify Bottlenecks",
            "description": "Analyze system performance under load to pinpoint bottlenecks in Celery worker scaling, database indexing, caching, and API/WebSocket layers.",
            "dependencies": [
              1
            ],
            "details": "Use profiling tools and application logs to monitor CPU, memory, database query times, and network utilization during load tests. Focus on Celery worker queues, database slow queries, cache hit/miss ratios, and WebSocket throughput. Document all identified bottlenecks with supporting metrics.",
            "status": "done",
            "testStrategy": "Correlate load test results with profiling data to confirm bottleneck locations. Validate findings by reproducing issues under controlled load and measuring impact of each suspected bottleneck.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Optimize and Re-Test for Throughput and Latency Improvements",
            "description": "Implement targeted optimizations (e.g., Celery scaling, database indexes, caching strategies, API/WebSocket tuning) and validate improvements through iterative load testing.",
            "dependencies": [
              2
            ],
            "details": "Apply optimizations based on profiling results: adjust Celery worker counts, add or tune database indexes, refine caching logic, and optimize API/WebSocket configurations. Re-run load tests to measure improvements in throughput and latency. Iterate as needed until performance targets are met.\n<info added on 2025-11-17T00:16:21.618Z>\n## Progress Update (75% Complete)\n\n### Accomplishments\n1. Created comprehensive load testing framework (query_load_test.py)\n2. Identified and fixed 4 critical bugs:\n   - Langfuse decorator async bug (5e8c9c1) - adaptive endpoint 0% → 100% success\n   - Pydantic cache serialization (f2707f5) - enabled cache infrastructure\n   - LangGraph ToolNode error (7e0972f) - fixed 33% failure rate on complex queries\n   - Redis connection for Upstash (7e0972f) - SSL/TLS support for production\n3. Generated comprehensive documentation:\n   - PERFORMANCE_REPORT_TASK43_3.md (8 sections)\n   - TASK43_3_FINAL_STATUS.md (complete status)\n   - 3 JSON test result files\n4. Re-tested and validated all bug fixes in production\n5. Profiled performance bottlenecks and documented optimizations\n\n### Current Performance Metrics\n- Adaptive endpoint: 100% success (was 0%)\n- Auto-routed endpoint: 100% success\n- Cache hit rate: 0% (embedding service unavailable)\n- Adaptive P95 latency: 14.6s (target: <1s)\n- Auto-routed P95 latency: 7.1s (target: <2s)\n\n### Outstanding Issues\n1. Caching not functional - BGE-M3 via Ollama unavailable from Render\n   - Need OpenAI embeddings fallback\n   - Verify Redis connection in logs\n2. Performance too slow - sequential LLM calls taking 12-14s\n   - Need to combine analyze+plan into single call\n   - Add streaming responses\n   - Implement prompt caching\n\n### Next Steps\n- Add OpenAI embeddings fallback for production caching\n- Optimize LangGraph workflow (combine nodes, add streaming)\n- Final validation with working cache and optimized performance\n- Estimated: 2-4 hours to complete remaining 25%\n</info added on 2025-11-17T00:16:21.618Z>",
            "status": "done",
            "testStrategy": "Compare pre- and post-optimization metrics for throughput, latency, and error rates. Confirm that optimizations resolve identified bottlenecks and that the system meets or exceeds performance goals under 2x expected load.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on load testing & performance optimization."
      },
      {
        "id": 44,
        "title": "Documentation Finalization & User Onboarding",
        "description": "Prepare comprehensive documentation for developers and users. Implement onboarding flows and training materials.",
        "details": "Document API endpoints, workflows, agent configurations, and UI usage. Create onboarding guides and training videos.",
        "testStrategy": "Review documentation for completeness and clarity. Test onboarding flows with new users.",
        "priority": "medium",
        "dependencies": [
          "43"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Comprehensive API & Workflow Documentation",
            "description": "Create detailed, accurate documentation covering all API endpoints, workflows, agent configurations, and UI usage for both developers and end-users.",
            "dependencies": [
              43
            ],
            "details": "Document each API endpoint with request/response examples, authentication details, and error codes. Outline workflows with diagrams and step-by-step instructions. Describe agent configuration options and UI navigation paths. Use clear headings, code samples, and visuals to enhance readability and accessibility[1][2]. Ensure documentation is reviewed by technical stakeholders for accuracy before finalization.",
            "status": "done",
            "testStrategy": "Conduct peer reviews with developers and QA to verify completeness, clarity, and technical accuracy. Test documented workflows against the live system to ensure they match actual behavior.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Onboarding Guide & Training Material Development",
            "description": "Develop onboarding guides and training materials tailored to different user roles, including step-by-step tutorials, FAQs, and best practices.",
            "dependencies": [
              43
            ],
            "details": "Write onboarding guides for new users and developers, focusing on getting started, common tasks, and troubleshooting. Create training videos (e.g., using Loom or similar tools) demonstrating key features and workflows. Include exercises and real-world examples to reinforce learning. Structure content for easy navigation and quick reference, using consistent formatting and visual aids[1][2]. Collaborate with support and training teams to ensure materials address common user pain points.",
            "status": "done",
            "testStrategy": "Pilot onboarding materials with a group of new users and gather feedback on clarity, usefulness, and ease of understanding. Revise materials based on feedback before broad release.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Documentation Maintenance & Continuous Improvement Plan",
            "description": "Establish processes for ongoing documentation review, updates, and user feedback integration to keep materials accurate and relevant.",
            "dependencies": [
              43
            ],
            "details": "Set up a schedule for regular documentation reviews, especially after product updates or releases. Implement a feedback loop where users can report issues or suggest improvements. Use version control to track changes and ensure all stakeholders have access to the latest documentation. Standardize templates and update procedures to maintain consistency across all docs[2][4]. Assign clear ownership for documentation maintenance within the team.",
            "status": "done",
            "testStrategy": "Monitor documentation usage analytics and user feedback channels. Periodically audit docs for outdated information and verify that updates are correctly propagated. Test revised documentation with both new and experienced users to ensure continued effectiveness.",
            "parentId": "undefined"
          }
        ],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down this task with a focus on documentation finalization & user onboarding."
      },
      {
        "id": 45,
        "title": "Integrate RAGAS Metrics Evaluation and Visualization for RAG Pipeline",
        "description": "Implement automated RAG quality evaluation using the RAGAS framework with 4 core metrics (Faithfulness, Answer Relevancy, Context Precision, Context Recall). Store results in Supabase and visualize in Grafana dashboards.",
        "details": "- Set up RAGAS framework integration for automated evaluation of RAG pipeline quality\n- Implement evaluation of 4 core metrics:\n  * Faithfulness (0-1): Measures if the generated answer is factually consistent with the retrieved context\n  * Answer Relevancy (0-1): Measures if the answer addresses the query intent\n  * Context Precision (0-1): Measures the proportion of relevant context chunks\n  * Context Recall (0-1): Measures if all necessary information is present in context\n- Create a test dataset of 30 curated samples from Empire documentation stored in .taskmaster/docs/ragas_test_dataset.json\n- Design and implement Supabase ragas_evaluations table with schema including:\n  * evaluation_id (UUID)\n  * timestamp (TIMESTAMP)\n  * query_text (TEXT)\n  * answer_text (TEXT)\n  * context_chunks (JSONB array)\n  * faithfulness_score (FLOAT)\n  * answer_relevancy_score (FLOAT)\n  * context_precision_score (FLOAT)\n  * context_recall_score (FLOAT)\n  * overall_score (FLOAT)\n  * metadata (JSONB)\n- Develop scripts/ragas_evaluation.py for batch evaluation with:\n  * Command-line interface for running evaluations\n  * Integration with existing RAG pipeline components\n  * Configurable parameters for evaluation settings\n  * Automatic storage of results in Supabase\n- Create Grafana dashboards showing:\n  * Metric trends over time\n  * Comparison between different RAG configurations\n  * Alerts when metrics fall below threshold (0.70)\n  * Drill-down capability to examine specific evaluation runs\n- Document expected baseline performance (0.70-0.85 overall scores)\n- Calculate and document cost estimates (~$0.20 per evaluation run for 30 samples)\n- Integrate with existing observability infrastructure from Task 25",
        "testStrategy": "1. Prepare test environment with sample RAG pipeline and test dataset\n2. Run baseline evaluation on the 30-sample test dataset from .taskmaster/docs/ragas_test_dataset.json\n3. Validate all 4 metrics (Faithfulness, Answer Relevancy, Context Precision, Context Recall) are calculated correctly\n4. Verify scores are within expected ranges (0-1) and reasonable for test data\n5. Confirm results are properly stored in Supabase ragas_evaluations table\n6. Check that all required fields in the schema are populated correctly\n7. Verify Grafana dashboard correctly displays:\n   - Individual metric scores\n   - Overall score trends\n   - Comparison between evaluation runs\n8. Test alert triggers by artificially setting scores below the 0.70 threshold\n9. Validate dashboard filtering and drill-down capabilities\n10. Perform a complete end-to-end test with a new document to ensure the entire evaluation pipeline works\n11. Measure performance and resource usage during evaluation runs\n12. Document baseline scores for the current RAG implementation",
        "status": "done",
        "dependencies": [
          "18",
          "25"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up RAGAS framework integration and test dataset",
            "description": "Integrate the RAGAS framework into the project and prepare the test dataset for evaluation.",
            "dependencies": [],
            "details": "Install RAGAS library and dependencies. Configure the framework to work with the existing RAG pipeline. Prepare and validate the test dataset of 30 curated samples from Empire documentation stored in .taskmaster/docs/ragas_test_dataset.json. Ensure the dataset contains appropriate query-answer-context triplets for evaluation.",
            "status": "done",
            "testStrategy": "Verify RAGAS installation and imports work correctly. Validate test dataset structure and content. Ensure sample queries cover diverse use cases from Empire documentation.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement core metrics evaluation logic",
            "description": "Develop the core functionality to evaluate the 4 RAGAS metrics: Faithfulness, Answer Relevancy, Context Precision, and Context Recall.",
            "dependencies": [
              1
            ],
            "details": "Create evaluation functions for each metric. Implement Faithfulness calculation to measure factual consistency between answers and context. Develop Answer Relevancy evaluation to assess query intent alignment. Build Context Precision measurement for relevant chunk proportion. Implement Context Recall to verify information completeness. Calculate overall combined score from individual metrics.",
            "status": "done",
            "testStrategy": "Run evaluations on sample data and verify each metric produces values between 0-1. Compare results with manual assessments of a subset of examples to validate accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create Supabase ragas_evaluations table and storage logic",
            "description": "Design and implement the Supabase database schema for storing RAGAS evaluation results and develop storage functionality.",
            "dependencies": [
              2
            ],
            "details": "Create ragas_evaluations table with schema including evaluation_id, timestamp, query_text, answer_text, context_chunks, all metric scores (faithfulness, answer_relevancy, context_precision, context_recall), overall_score, and metadata fields. Implement functions to store evaluation results in the database. Add batch processing capabilities for multiple evaluations.",
            "status": "done",
            "testStrategy": "Test database schema creation and data insertion. Verify all fields are properly stored and retrieved. Check batch processing with multiple evaluation records.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Develop scripts/ragas_evaluation.py with CLI interface",
            "description": "Create a command-line script for running RAGAS evaluations with configurable parameters and Supabase integration.",
            "dependencies": [
              3
            ],
            "details": "Develop scripts/ragas_evaluation.py with command-line arguments for evaluation settings. Integrate with existing RAG pipeline components to access retrieval and generation functions. Implement configurable parameters for batch size, metric weights, and thresholds. Add automatic storage of results in Supabase. Include logging and error handling. Document cost estimates (~$0.20 per evaluation run for 30 samples).",
            "status": "done",
            "testStrategy": "Test CLI with various parameter combinations. Verify integration with RAG pipeline components. Confirm results are properly stored in Supabase. Validate error handling for edge cases.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create Grafana dashboards for metrics visualization",
            "description": "Design and implement Grafana dashboards to visualize RAGAS metrics and integrate with existing observability infrastructure.",
            "dependencies": [
              4
            ],
            "details": "Create Grafana dashboards showing metric trends over time. Implement comparison views between different RAG configurations. Set up alerts when metrics fall below threshold (0.70). Add drill-down capability to examine specific evaluation runs. Document expected baseline performance (0.70-0.85 overall scores). Integrate with existing observability infrastructure from Task 25.",
            "status": "done",
            "testStrategy": "Verify dashboard displays all metrics correctly. Test alert functionality with below-threshold values. Confirm drill-down navigation works properly. Validate integration with existing observability infrastructure.",
            "parentId": "undefined"
          }
        ]
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-11-14T18:18:43.347Z",
      "taskCount": 45,
      "completedCount": 39,
      "tags": [
        "master"
      ],
      "created": "2025-11-14T19:10:19.748Z",
      "description": "Tasks for master context",
      "updated": "2025-11-17T18:28:30.661Z"
    }
  },
  "v7_3_features": {
    "tasks": [
      {
        "id": "101",
        "title": "Implement Neo4j HTTP Client",
        "description": "Create a production-optimized Neo4j HTTP client that directly accesses the transaction/commit endpoint for better performance than the driver approach.",
        "details": "Implement the Neo4jHTTPClient class in app/services/neo4j_http_client.py with the following features:\n\n1. Direct HTTP connection to Neo4j's transaction/commit endpoint\n2. Connection pooling for efficient resource usage\n3. Query batching capabilities\n4. Proper error handling and result parsing\n5. Async support for non-blocking operations\n\nImplementation should follow the pattern provided in the PRD:\n```python\nimport httpx\nfrom typing import Dict, Any, List\n\nclass Neo4jHTTPClient:\n    def __init__(self, uri: str, username: str, password: str):\n        self.base_url = uri.replace(\"bolt://\", \"http://\").replace(\"bolt+ssc://\", \"https://\")\n        self.base_url = f\"{self.base_url}/db/neo4j/tx/commit\"\n        self.auth = (username, password)\n        self.client = httpx.AsyncClient(timeout=30.0)\n\n    async def execute_query(\n        self,\n        query: str,\n        parameters: Dict[str, Any] = None\n    ) -> List[Dict[str, Any]]:\n        payload = {\n            \"statements\": [{\n                \"statement\": query,\n                \"parameters\": parameters or {}\n            }]\n        }\n\n        response = await self.client.post(\n            self.base_url,\n            json=payload,\n            auth=self.auth,\n            headers={\"Content-Type\": \"application/json\"}\n        )\n\n        result = response.json()\n        if result.get(\"errors\"):\n            raise Exception(result[\"errors\"][0][\"message\"])\n\n        return self._parse_results(result)\n\n    def _parse_results(self, result: Dict) -> List[Dict]:\n        rows = []\n        for statement_result in result.get(\"results\", []):\n            columns = statement_result.get(\"columns\", [])\n            for row in statement_result.get(\"data\", []):\n                rows.append(dict(zip(columns, row[\"row\"])))\n        return rows\n```\n\nAdd methods for batch query execution and connection management. Include unit tests to verify functionality against a Neo4j instance.",
        "testStrategy": "1. Unit tests with mocked HTTP responses to verify correct parsing\n2. Integration tests against a test Neo4j instance to verify actual connectivity\n3. Performance benchmarks comparing HTTP client vs. driver approach\n4. Test connection pooling under load\n5. Test error handling with malformed queries\n6. Test with various Neo4j query types (READ, WRITE, etc.)",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Core HTTP Client with Connection Management",
            "description": "Implement the base Neo4jHTTPClient class with connection initialization, authentication, and proper connection management.",
            "dependencies": [],
            "details": "Create the Neo4jHTTPClient class in app/services/neo4j_http_client.py with proper initialization, authentication setup, and connection management. Implement the constructor that handles URI transformation from bolt to HTTP format, authentication setup, and httpx client initialization. Include methods for connection lifecycle management (open/close connections) and implement proper resource cleanup. Ensure the client handles connection timeouts and retries appropriately. Write unit tests to verify connection initialization and management functionality.",
            "status": "done",
            "testStrategy": "Unit test connection initialization with various URI formats (bolt://, bolt+ssc://, etc.). Mock HTTP responses to test connection management. Test proper resource cleanup on client shutdown. Test connection timeout handling and retry logic. Integration test with a real Neo4j instance to verify connectivity.",
            "updatedAt": "2026-01-12T01:46:41.108Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Query Execution and Result Parsing",
            "description": "Develop the core query execution functionality and result parsing logic for the Neo4j HTTP client.",
            "dependencies": [
              1
            ],
            "details": "Implement the execute_query method that sends Cypher queries to Neo4j's transaction/commit endpoint. Create the _parse_results method to transform Neo4j's JSON response into a more usable format. Handle different result types (nodes, relationships, paths, etc.) correctly. Implement proper error detection and exception handling for query execution failures. Add support for parameterized queries to prevent injection attacks. Write comprehensive unit tests for query execution and result parsing with various query types and response formats.",
            "status": "done",
            "testStrategy": "Unit test query execution with mocked HTTP responses. Test result parsing with various Neo4j response formats. Test error handling with different error scenarios. Integration test with actual Neo4j instance using various query types. Test parameterized queries for correctness and security.",
            "parentId": "undefined",
            "updatedAt": "2026-01-12T01:47:13.007Z"
          },
          {
            "id": 3,
            "title": "Implement Advanced Features: Batching, Pooling, and Async Support",
            "description": "Add advanced features to the Neo4j HTTP client including query batching, connection pooling, and asynchronous execution support.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement batch_execute_queries method to send multiple queries in a single HTTP request. Configure connection pooling for efficient resource usage under load. Optimize async support for non-blocking operations with proper concurrency handling. Add timeout configuration and circuit breaker patterns for resilience. Implement query result caching for frequently executed queries. Create performance benchmarks comparing the HTTP client against the driver approach. Write comprehensive tests for batching, pooling, and async execution under various load conditions.",
            "status": "done",
            "testStrategy": "Unit test batch query execution with various batch sizes. Test connection pooling under simulated load. Benchmark performance against standard Neo4j driver. Test async execution with concurrent queries. Test timeout handling and circuit breaker functionality. Integration test with Neo4j instance under load to verify pooling benefits.",
            "parentId": "undefined",
            "updatedAt": "2026-01-12T01:47:15.070Z"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down the Neo4j HTTP Client implementation into subtasks covering: 1) Core HTTP client implementation with connection handling, 2) Query execution and result parsing functionality, and 3) Advanced features like connection pooling, batching, and error handling.",
        "updatedAt": "2026-01-12T01:47:15.070Z"
      },
      {
        "id": "102",
        "title": "Extend Neo4j Schema for Graph Agent",
        "description": "Extend the existing Neo4j schema to support the new graph agent capabilities, including Customer 360, Document Structure, and Graph-Enhanced RAG.",
        "details": "Create Cypher scripts to extend the Neo4j schema with new node types and relationships:\n\n1. Customer 360 Nodes:\n   - Customer nodes with properties (id, name, type, industry, etc.)\n   - Ticket nodes for support tickets\n   - Order nodes for customer orders\n   - Interaction nodes for customer interactions\n   - Product nodes for products/services\n\n2. Document Structure Nodes:\n   - Section nodes for document hierarchy\n   - DefinedTerm nodes for document terminology\n   - Citation nodes for external references\n\n3. Relationships:\n   - Customer relationships (HAS_DOCUMENT, HAS_TICKET, etc.)\n   - Document structure relationships (HAS_SECTION, REFERENCES, etc.)\n   - Entity relationships for graph-enhanced RAG\n\n4. Indexes for performance optimization:\n   - Create indexes on frequently queried properties\n   - Create constraints for data integrity\n\nImplement these schema changes in a migration script that can be applied to the existing Neo4j database without data loss.",
        "testStrategy": "1. Create test script to verify schema changes were applied correctly\n2. Test indexes with EXPLAIN/PROFILE on common queries\n3. Verify constraints with intentionally invalid data\n4. Load test data for each node type and verify relationships\n5. Test backward compatibility with existing queries\n6. Verify performance with large datasets",
        "priority": "high",
        "dependencies": [
          "101"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 7,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down the Neo4j schema extension into subtasks covering: 1) Customer 360 schema components, 2) Document Structure schema components, 3) Graph-Enhanced RAG schema components, and 4) Index and constraint creation with migration strategy.",
        "updatedAt": "2026-01-12T01:53:12.016Z"
      },
      {
        "id": "103",
        "title": "Implement Pydantic Models for Graph Agent",
        "description": "Create Pydantic models for request/response objects used by the Graph Agent APIs, including Customer 360, Document Structure, and Graph-Enhanced RAG.",
        "details": "Create a new file app/models/graph_agent.py with Pydantic models as specified in the PRD:\n\n1. Base models:\n   - QueryType enum (CUSTOMER_360, DOCUMENT_STRUCTURE, GRAPH_ENHANCED_RAG)\n   - TraversalDepth enum (SHALLOW, MEDIUM, DEEP)\n\n2. Customer 360 models:\n   - Customer360Request\n   - CustomerNode\n   - Customer360Response\n\n3. Document Structure models:\n   - DocumentStructureRequest\n   - SectionNode\n   - DocumentStructureResponse\n   - SmartRetrievalRequest\n   - SmartRetrievalResponse\n\n4. Graph-Enhanced RAG models:\n   - GraphEnhancedRAGRequest\n   - GraphExpansionResult\n   - GraphEnhancedRAGResponse\n\nImplement all models following the schema provided in the PRD, with proper type hints, field validations, and documentation strings. Ensure models are compatible with FastAPI's automatic request validation and OpenAPI schema generation.",
        "testStrategy": "1. Unit tests for model validation\n2. Test serialization/deserialization\n3. Test with valid and invalid data\n4. Verify OpenAPI schema generation\n5. Test integration with FastAPI endpoints\n6. Verify documentation is generated correctly",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Base Models and Enums for Graph Agent",
            "description": "Create the foundational Pydantic models and enums that will be used across all Graph Agent APIs.",
            "dependencies": [],
            "details": "Create a new file app/models/graph_agent.py and implement the base models including QueryType enum (CUSTOMER_360, DOCUMENT_STRUCTURE, GRAPH_ENHANCED_RAG) and TraversalDepth enum (SHALLOW, MEDIUM, DEEP). Include proper type hints, field validations, and documentation strings. Ensure compatibility with FastAPI's automatic request validation and OpenAPI schema generation.",
            "status": "pending",
            "testStrategy": "Write unit tests to verify enum values are correctly defined. Test serialization/deserialization of the base models. Verify that the models generate correct OpenAPI schema documentation.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Customer 360 Request/Response Models",
            "description": "Create Pydantic models for Customer 360 functionality including request and response objects with proper validation rules.",
            "dependencies": [
              1
            ],
            "details": "In app/models/graph_agent.py, implement Customer360Request model with query parameters, CustomerNode model for representing customer data, and Customer360Response model for returning query results. Include validation rules for required fields, field types, and value constraints. Add comprehensive docstrings explaining each field's purpose and format requirements.",
            "status": "pending",
            "testStrategy": "Test model validation with valid and invalid data. Verify that validation errors are properly raised for invalid inputs. Test serialization/deserialization of complex nested structures. Ensure models work correctly with FastAPI's request parsing.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Document Structure and Graph-Enhanced RAG Models",
            "description": "Create Pydantic models for Document Structure and Graph-Enhanced RAG APIs with appropriate validation logic and relationships.",
            "dependencies": [
              1
            ],
            "details": "In app/models/graph_agent.py, implement DocumentStructureRequest, SectionNode, DocumentStructureResponse, SmartRetrievalRequest, SmartRetrievalResponse models for document structure functionality. Then implement GraphEnhancedRAGRequest, GraphExpansionResult, and GraphEnhancedRAGResponse models for graph-enhanced RAG functionality. Include proper validation rules, nested relationships, and comprehensive documentation for all fields. Ensure models handle complex nested structures and include appropriate default values where needed.",
            "status": "pending",
            "testStrategy": "Test validation of complex nested structures. Verify that models correctly handle optional and required fields. Test with edge cases like empty lists and deeply nested objects. Ensure models generate correct OpenAPI documentation. Test integration with FastAPI endpoints.",
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down the Pydantic models implementation into subtasks covering: 1) Base models and enums, 2) Customer 360 request/response models, 3) Document Structure and Graph-Enhanced RAG models with validation logic.",
        "updatedAt": "2026-01-12T01:56:34.761Z"
      },
      {
        "id": "104",
        "title": "Implement Customer 360 Service",
        "description": "Create the Customer 360 Service that provides unified customer views by traversing the Neo4j graph to consolidate data from multiple sources.",
        "details": "Implement the Customer360Service class in app/services/customer360_service.py with the following features:\n\n1. Query parser for customer-related natural language queries\n2. Multi-hop graph traversal to collect customer data\n3. Result aggregation and formatting\n4. Similar customer detection\n\nThe service should use the Neo4jHTTPClient for efficient graph queries and implement the following methods:\n\n```python\nfrom typing import Dict, List, Optional, Any\nfrom app.services.neo4j_http_client import Neo4jHTTPClient\nfrom app.models.graph_agent import Customer360Request, Customer360Response, CustomerNode\n\nclass Customer360Service:\n    def __init__(self, neo4j_client: Neo4jHTTPClient):\n        self.neo4j_client = neo4j_client\n    \n    async def process_customer_query(self, request: Customer360Request) -> Customer360Response:\n        # Parse natural language query to identify customer and query intent\n        # Execute appropriate graph traversal\n        # Format results into Customer360Response\n        pass\n    \n    async def get_customer_by_id(self, customer_id: str, include_documents: bool = True, \n                               include_tickets: bool = True, include_orders: bool = True,\n                               include_interactions: bool = True) -> Customer360Response:\n        # Direct lookup by customer ID with configurable related data\n        pass\n    \n    async def find_similar_customers(self, customer_id: str, limit: int = 5) -> List[CustomerNode]:\n        # Find customers with similar profiles/behaviors\n        pass\n    \n    async def _execute_customer_traversal(self, cypher_query: str, params: Dict[str, Any]) -> Dict[str, Any]:\n        # Execute graph traversal and process results\n        pass\n```\n\nImplement Cypher queries for customer data retrieval as shown in the PRD's Cypher Query Patterns section.",
        "testStrategy": "1. Unit tests with mocked Neo4j responses\n2. Integration tests with test customer data\n3. Test natural language query parsing with various customer queries\n4. Test multi-hop traversal with different depths\n5. Test result aggregation with complex customer data\n6. Performance testing with large customer datasets",
        "priority": "high",
        "dependencies": [
          "101",
          "102",
          "103"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 8,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down the Customer 360 Service implementation into subtasks covering: 1) Natural language query parsing for customer queries, 2) Graph traversal implementation for customer data retrieval, 3) Customer similarity detection algorithms, 4) Result aggregation and formatting, and 5) Integration with Neo4j HTTP client.",
        "updatedAt": "2026-01-12T02:02:21.283Z"
      },
      {
        "id": "105",
        "title": "Implement Document Structure Service",
        "description": "Create the Document Structure Service that extracts and navigates document hierarchy, clause references, and cross-links from complex documents.",
        "details": "Implement the DocumentStructureService class in app/services/document_structure_service.py with the following features:\n\n1. Structure extraction from documents using LLM\n2. Cross-reference detection and linking\n3. Smart retrieval with context expansion\n4. Definition linking\n\nThe service should implement these key methods:\n\n```python\nfrom typing import Dict, List, Optional, Any\nfrom app.services.neo4j_http_client import Neo4jHTTPClient\nfrom app.models.graph_agent import DocumentStructureRequest, DocumentStructureResponse, SmartRetrievalRequest, SmartRetrievalResponse\n\nclass DocumentStructureService:\n    def __init__(self, neo4j_client: Neo4jHTTPClient, llm_service):\n        self.neo4j_client = neo4j_client\n        self.llm_service = llm_service  # For structure extraction\n    \n    async def extract_document_structure(self, document_id: str, extract_cross_refs: bool = True,\n                                       extract_definitions: bool = True) -> DocumentStructureResponse:\n        # Extract document structure using LLM\n        # Store structure in Neo4j\n        # Return structured representation\n        pass\n    \n    async def get_document_structure(self, document_id: str) -> DocumentStructureResponse:\n        # Retrieve existing document structure from Neo4j\n        pass\n    \n    async def smart_retrieve(self, request: SmartRetrievalRequest) -> SmartRetrievalResponse:\n        # Context-aware retrieval with cross-references\n        pass\n    \n    async def get_cross_references(self, document_id: str, section_id: Optional[str] = None) -> List[Dict[str, Any]]:\n        # Get cross-references for document or specific section\n        pass\n    \n    async def _extract_sections(self, document_text: str) -> List[Dict[str, Any]]:\n        # Use LLM to extract sections and hierarchy\n        pass\n    \n    async def _extract_cross_references(self, document_text: str, sections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        # Identify cross-references between sections\n        pass\n```\n\nImplement the document structure extraction using a combination of LLM prompting and pattern recognition. Use the Neo4j graph to store and query the document structure.",
        "testStrategy": "1. Unit tests for structure extraction with sample documents\n2. Integration tests with Neo4j for storing and retrieving document structure\n3. Test cross-reference detection with complex legal documents\n4. Test smart retrieval with various query types\n5. Test definition linking and resolution\n6. Performance testing with large documents",
        "priority": "medium",
        "dependencies": [
          "101",
          "102",
          "103"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 8,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down the Document Structure Service implementation into subtasks covering: 1) Document structure extraction using LLM, 2) Cross-reference detection and linking, 3) Smart retrieval with context expansion, 4) Definition linking implementation, and 5) Integration with Neo4j for storing and querying document structures.",
        "updatedAt": "2026-01-12T02:08:07.891Z"
      },
      {
        "id": "106",
        "title": "Implement Graph-Enhanced RAG Service",
        "description": "Create the Graph-Enhanced RAG Service that augments vector search results with graph context by traversing relationships to find connected documents and entities.",
        "details": "Implement the GraphEnhancedRAGService class in app/services/graph_enhanced_rag_service.py with the following features:\n\n1. Entity extraction from retrieved chunks\n2. Graph expansion strategies (neighbor expansion, parent context, etc.)\n3. Context enrichment for retrieved results\n4. Result re-ranking based on graph relevance\n\nThe service should implement these key methods:\n\n```python\nfrom typing import Dict, List, Optional, Any\nfrom app.services.neo4j_http_client import Neo4jHTTPClient\nfrom app.models.graph_agent import GraphEnhancedRAGRequest, GraphEnhancedRAGResponse, GraphExpansionResult\n\nclass GraphEnhancedRAGService:\n    def __init__(self, neo4j_client: Neo4jHTTPClient, vector_search_service, entity_extractor):\n        self.neo4j_client = neo4j_client\n        self.vector_search_service = vector_search_service  # Existing RAG service\n        self.entity_extractor = entity_extractor  # For entity extraction\n    \n    async def query(self, request: GraphEnhancedRAGRequest) -> GraphEnhancedRAGResponse:\n        # Perform vector search\n        # Extract entities from results\n        # Expand with graph context\n        # Re-rank and format response\n        pass\n    \n    async def expand_results(self, chunks: List[Dict[str, Any]], \n                           expansion_depth: int = 1) -> GraphExpansionResult:\n        # Expand vector search results with graph context\n        pass\n    \n    async def get_entity_context(self, entity_id: str, depth: int = 1) -> Dict[str, Any]:\n        # Get context for a specific entity\n        pass\n    \n    async def _extract_entities(self, text: str) -> List[Dict[str, Any]]:\n        # Extract entities from text\n        pass\n    \n    async def _expand_by_neighbors(self, entity_ids: List[str], depth: int = 1) -> List[Dict[str, Any]]:\n        # Expand by traversing entity neighbors\n        pass\n    \n    async def _rerank_results(self, original_results: List[Dict[str, Any]], \n                             expanded_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        # Re-rank results based on graph relevance\n        pass\n```\n\nImplement the graph expansion using the Cypher queries outlined in the PRD's Cypher Query Patterns section. Integrate with the existing vector search service to enhance RAG results.",
        "testStrategy": "1. Unit tests for entity extraction\n2. Integration tests with Neo4j for graph expansion\n3. Test with various query types and expansion depths\n4. Test re-ranking with different relevance metrics\n5. Compare results with and without graph enhancement\n6. Performance testing with large result sets",
        "priority": "medium",
        "dependencies": [
          "101",
          "102",
          "103"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 9,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down the Graph-Enhanced RAG Service implementation into subtasks covering: 1) Entity extraction from retrieved chunks, 2) Graph expansion strategies implementation, 3) Context enrichment algorithms, 4) Result re-ranking based on graph relevance, and 5) Integration with existing vector search service.",
        "updatedAt": "2026-01-12T02:13:35.723Z"
      },
      {
        "id": "107",
        "title": "Implement Graph Agent Router",
        "description": "Create the Graph Agent Router that extends existing /api/graph routes with new capabilities for Customer 360, Document Structure, and Graph-Enhanced RAG.",
        "details": "Implement the Graph Agent Router in app/routes/graph_agent.py to handle the new graph agent endpoints:\n\n1. Customer 360 endpoints:\n   - POST /api/graph/customer360/query\n   - GET /api/graph/customer360/{customer_id}\n   - GET /api/graph/customer360/similar/{customer_id}\n\n2. Document Structure endpoints:\n   - POST /api/graph/document-structure/extract\n   - GET /api/graph/document-structure/{doc_id}\n   - POST /api/graph/document-structure/query\n   - GET /api/graph/document-structure/{doc_id}/cross-refs\n   - POST /api/graph/document-structure/smart-retrieve\n\n3. Graph-Enhanced RAG endpoints:\n   - POST /api/graph/enhanced-rag/query\n   - POST /api/graph/enhanced-rag/expand\n   - GET /api/graph/enhanced-rag/entities/{entity_id}/related\n   - POST /api/graph/enhanced-rag/context\n\nImplement the router using FastAPI with dependency injection for services:\n\n```python\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom app.services.customer360_service import Customer360Service\nfrom app.services.document_structure_service import DocumentStructureService\nfrom app.services.graph_enhanced_rag_service import GraphEnhancedRAGService\nfrom app.models.graph_agent import *\n\nrouter = APIRouter(prefix=\"/api/graph\", tags=[\"graph-agent\"])\n\n# Customer 360 endpoints\n@router.post(\"/customer360/query\", response_model=Customer360Response)\nasync def query_customer(request: Customer360Request, \n                       service: Customer360Service = Depends()):\n    return await service.process_customer_query(request)\n\n@router.get(\"/customer360/{customer_id}\", response_model=Customer360Response)\nasync def get_customer(customer_id: str, include_documents: bool = True,\n                     include_tickets: bool = True, include_orders: bool = True,\n                     include_interactions: bool = True,\n                     service: Customer360Service = Depends()):\n    return await service.get_customer_by_id(\n        customer_id, include_documents, include_tickets, \n        include_orders, include_interactions\n    )\n\n# Document Structure endpoints\n# ...\n\n# Graph-Enhanced RAG endpoints\n# ...\n```\n\nIntegrate the router with the main FastAPI application and ensure proper error handling and validation.",
        "testStrategy": "1. Unit tests for each endpoint with mocked service responses\n2. Integration tests with actual services\n3. Test request validation with valid and invalid inputs\n4. Test error handling with various error conditions\n5. Test authentication and authorization if applicable\n6. Load testing for concurrent requests",
        "priority": "high",
        "dependencies": [
          "103",
          "104",
          "105",
          "106"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down the Graph Agent Router implementation into subtasks covering: 1) Customer 360 endpoint implementation, 2) Document Structure endpoint implementation, and 3) Graph-Enhanced RAG endpoint implementation with proper error handling and validation.",
        "updatedAt": "2026-01-12T02:23:05.166Z"
      },
      {
        "id": "108",
        "title": "Implement Query Intent Detection for CKO Chat",
        "description": "Create a query intent detection system that can identify Customer 360, Document Structure, and Graph-Enhanced RAG queries to route them to the appropriate handler.",
        "details": "Implement a QueryIntentDetector class in app/services/query_intent_detector.py that can analyze natural language queries and determine the appropriate graph agent to handle them:\n\n```python\nfrom enum import Enum\nfrom typing import Dict, Any, Tuple\n\nclass QueryIntent(str, Enum):\n    CUSTOMER_360 = \"customer_360\"\n    DOCUMENT_STRUCTURE = \"document_structure\"\n    GRAPH_ENHANCED_RAG = \"graph_enhanced_rag\"\n    STANDARD_RAG = \"standard_rag\"\n\nclass QueryIntentDetector:\n    def __init__(self, llm_service):\n        self.llm_service = llm_service\n    \n    async def detect_intent(self, query: str) -> Tuple[QueryIntent, Dict[str, Any]]:\n        # Use pattern matching and/or LLM to detect query intent\n        # Return intent type and extracted parameters\n        \n        # Example implementation:\n        # 1. Check for customer-related keywords and patterns\n        if any(keyword in query.lower() for keyword in [\"customer\", \"client\", \"account\"]):\n            # Extract customer name/ID if present\n            return QueryIntent.CUSTOMER_360, self._extract_customer_params(query)\n        \n        # 2. Check for document structure patterns\n        if any(pattern in query.lower() for pattern in [\"section\", \"clause\", \"paragraph\", \"document structure\"]):\n            return QueryIntent.DOCUMENT_STRUCTURE, self._extract_document_params(query)\n        \n        # 3. Default to graph-enhanced RAG for general queries\n        return QueryIntent.GRAPH_ENHANCED_RAG, {\"query\": query}\n    \n    def _extract_customer_params(self, query: str) -> Dict[str, Any]:\n        # Extract customer name/ID and other parameters\n        # Use regex or LLM-based extraction\n        pass\n    \n    def _extract_document_params(self, query: str) -> Dict[str, Any]:\n        # Extract document ID, section references, etc.\n        pass\n```\n\nIntegrate this intent detector with the CKO Chat system to route queries to the appropriate graph agent endpoint. The detector should use a combination of pattern matching and LLM-based classification to identify query intents accurately.",
        "testStrategy": "1. Unit tests with sample queries for each intent type\n2. Test with ambiguous queries to verify classification\n3. Test parameter extraction accuracy\n4. Integration tests with CKO Chat\n5. Test with real user queries from logs if available\n6. Measure classification accuracy against human-labeled test set",
        "priority": "medium",
        "dependencies": [
          "104",
          "105",
          "106"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 7,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down the Query Intent Detection implementation into subtasks covering: 1) Pattern-based intent detection for common query types, 2) LLM-based intent classification for complex queries, 3) Parameter extraction from natural language queries, and 4) Integration with CKO Chat routing system.",
        "updatedAt": "2026-01-12T02:26:16.368Z"
      },
      {
        "id": "109",
        "title": "Implement Graph Result Formatter for Chat UI",
        "description": "Create a response formatter that presents graph query results in a user-friendly format for the CKO Chat interface, including expandable sections and graph visualizations.",
        "details": "Implement a GraphResultFormatter class in app/services/graph_result_formatter.py that formats graph query results for display in the chat UI:\n\n```python\nfrom typing import Dict, Any, List\nfrom app.models.graph_agent import Customer360Response, DocumentStructureResponse, GraphEnhancedRAGResponse\n\nclass GraphResultFormatter:\n    def format_customer_360(self, response: Customer360Response) -> Dict[str, Any]:\n        # Format Customer 360 response for chat UI\n        # Create expandable sections for documents, tickets, etc.\n        # Generate summary text\n        return {\n            \"type\": \"customer_360\",\n            \"content\": {\n                \"summary\": self._generate_customer_summary(response),\n                \"sections\": [\n                    {\n                        \"title\": \"Customer Profile\",\n                        \"content\": self._format_customer_profile(response.customer),\n                        \"expanded\": True\n                    },\n                    {\n                        \"title\": f\"Documents ({len(response.documents)})\",\n                        \"content\": self._format_document_list(response.documents),\n                        \"expanded\": False\n                    },\n                    # Additional sections for tickets, orders, etc.\n                ]\n            }\n        }\n    \n    def format_document_structure(self, response: DocumentStructureResponse) -> Dict[str, Any]:\n        # Format Document Structure response for chat UI\n        # Create expandable section tree\n        # Format cross-references as links\n        pass\n    \n    def format_graph_enhanced_rag(self, response: GraphEnhancedRAGResponse) -> Dict[str, Any]:\n        # Format Graph-Enhanced RAG response for chat UI\n        # Show answer with expandable context sections\n        # Include graph visualization data\n        pass\n    \n    def _generate_customer_summary(self, response: Customer360Response) -> str:\n        # Generate natural language summary of customer data\n        pass\n    \n    def _format_customer_profile(self, customer: Dict[str, Any]) -> Dict[str, Any]:\n        # Format customer profile data\n        pass\n    \n    def _format_document_list(self, documents: List[Dict[str, Any]]) -> Dict[str, Any]:\n        # Format document list with links\n        pass\n```\n\nThe formatter should create structured responses that can be rendered in the chat UI with expandable sections, links to source documents, and optional graph visualizations. The output should be compatible with the existing CKO Chat UI components.",
        "testStrategy": "1. Unit tests for each formatter method\n2. Test with various response structures\n3. Verify output format matches UI requirements\n4. Test with edge cases (empty results, large result sets)\n5. Integration tests with actual UI components\n6. User testing for readability and usability",
        "priority": "low",
        "dependencies": [
          "107",
          "108"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 6,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down the Graph Result Formatter implementation into subtasks covering: 1) Customer 360 result formatting with expandable sections, 2) Document Structure result formatting with hierarchical display, and 3) Graph-Enhanced RAG result formatting with answer highlighting and context sections.",
        "updatedAt": "2026-01-12T02:39:18.194Z"
      },
      {
        "id": "110",
        "title": "Implement Redis Caching for Graph Queries",
        "description": "Implement a caching layer using Redis to improve performance of graph queries by storing frequently accessed results.",
        "details": "Create a GraphQueryCache class in app/services/graph_query_cache.py that provides caching for graph query results:\n\n```python\nfrom typing import Dict, Any, Optional, List\nimport json\nimport hashlib\nfrom redis import Redis\n\nclass GraphQueryCache:\n    def __init__(self, redis_client: Redis, ttl: int = 3600):\n        self.redis = redis_client\n        self.default_ttl = ttl  # Default TTL in seconds\n    \n    async def get(self, query_type: str, query_key: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n        # Generate cache key from query type and parameters\n        cache_key = self._generate_cache_key(query_type, query_key)\n        \n        # Try to get from cache\n        cached_result = self.redis.get(cache_key)\n        if cached_result:\n            return json.loads(cached_result)\n        \n        return None\n    \n    async def set(self, query_type: str, query_key: Dict[str, Any], \n                result: Dict[str, Any], ttl: Optional[int] = None) -> None:\n        # Generate cache key and store result\n        cache_key = self._generate_cache_key(query_type, query_key)\n        self.redis.set(\n            cache_key,\n            json.dumps(result),\n            ex=ttl or self.default_ttl\n        )\n    \n    async def invalidate(self, query_type: str, query_key: Dict[str, Any]) -> None:\n        # Invalidate specific cache entry\n        cache_key = self._generate_cache_key(query_type, query_key)\n        self.redis.delete(cache_key)\n    \n    async def invalidate_by_prefix(self, prefix: str) -> None:\n        # Invalidate all cache entries with given prefix\n        keys = self.redis.keys(f\"{prefix}:*\")\n        if keys:\n            self.redis.delete(*keys)\n    \n    def _generate_cache_key(self, query_type: str, query_key: Dict[str, Any]) -> str:\n        # Generate deterministic cache key from query type and parameters\n        key_str = json.dumps(query_key, sort_keys=True)\n        hashed = hashlib.md5(key_str.encode()).hexdigest()\n        return f\"graph:{query_type}:{hashed}\"\n```\n\nIntegrate this cache with the graph agent services (Customer360Service, DocumentStructureService, GraphEnhancedRAGService) to cache query results. Implement cache invalidation strategies for data updates.",
        "testStrategy": "1. Unit tests for cache key generation\n2. Test cache hit/miss scenarios\n3. Test cache invalidation\n4. Integration tests with graph services\n5. Performance tests to measure cache impact\n6. Test with concurrent access patterns\n7. Test cache size growth over time",
        "priority": "medium",
        "dependencies": [
          "104",
          "105",
          "106"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 5,
        "recommendedSubtasks": 3,
        "expansionPrompt": "Break down the Redis Caching implementation into subtasks covering: 1) Cache key generation and result storage, 2) Cache invalidation strategies, and 3) Integration with graph services for transparent caching.",
        "updatedAt": "2026-01-12T02:28:49.111Z"
      },
      {
        "id": "111",
        "title": "Implement Integration Tests for Graph Agent",
        "description": "Create comprehensive integration tests for the Graph Agent components to ensure they work together correctly and meet performance requirements.",
        "details": "Implement integration tests in tests/integration/test_graph_agent.py that verify the correct functioning of the Graph Agent components together:\n\n1. Test setup:\n   - Create test fixtures for Neo4j with sample data\n   - Set up test instances of all services\n   - Configure test client for API endpoints\n\n2. Customer 360 integration tests:\n   - Test end-to-end customer queries\n   - Verify correct data retrieval and formatting\n   - Test performance against SLAs\n\n3. Document Structure integration tests:\n   - Test document structure extraction\n   - Test smart retrieval with cross-references\n   - Verify correct handling of complex documents\n\n4. Graph-Enhanced RAG integration tests:\n   - Test query expansion with graph context\n   - Verify improved results compared to standard RAG\n   - Test performance impact\n\n5. End-to-end CKO Chat integration:\n   - Test query intent detection\n   - Verify correct routing to graph agents\n   - Test response formatting for UI\n\nImplement test utilities for loading test data, measuring performance, and comparing results:\n\n```python\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom app.main import app\nfrom app.services.neo4j_http_client import Neo4jHTTPClient\nfrom app.services.customer360_service import Customer360Service\n# Import other services\n\n@pytest.fixture\ndef test_client():\n    return TestClient(app)\n\n@pytest.fixture\ndef neo4j_client():\n    # Create test Neo4j client with test database\n    client = Neo4jHTTPClient(test_uri, test_user, test_password)\n    # Load test data\n    yield client\n    # Clean up test data\n\n@pytest.fixture\ndef customer360_service(neo4j_client):\n    return Customer360Service(neo4j_client)\n\n# Test cases\ndef test_customer360_query(test_client, neo4j_client):\n    # Test customer query endpoint\n    response = test_client.post(\n        \"/api/graph/customer360/query\",\n        json={\"query\": \"Show me everything about Test Corp\"}\n    )\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"customer\"][\"name\"] == \"Test Corp\"\n    # Additional assertions\n\n# Additional test cases for other components\n```\n\nInclude performance tests that verify the system meets the SLAs defined in the PRD.",
        "testStrategy": "1. Test with realistic test data that mimics production\n2. Measure response times against SLAs\n3. Test with various query patterns\n4. Test error handling and edge cases\n5. Test concurrent access patterns\n6. Compare results with expected outputs\n7. Test integration with existing Empire components",
        "priority": "medium",
        "dependencies": [
          "107",
          "108",
          "109",
          "110"
        ],
        "status": "done",
        "subtasks": [],
        "complexity": 7,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Break down the integration testing implementation into subtasks covering: 1) Test fixtures and data setup, 2) Customer 360 integration tests, 3) Document Structure and Graph-Enhanced RAG integration tests, and 4) Performance and SLA verification tests.",
        "updatedAt": "2026-01-12T03:12:51.429Z"
      },
      {
        "id": "112",
        "title": "Create MarkdownChunkerStrategy class implementing ChunkingStrategy interface",
        "description": "Implement a new chunking strategy that splits documents by markdown headers while preserving header context",
        "details": "Create a new class `MarkdownChunkerStrategy` that implements the existing `ChunkingStrategy` interface. This class will be responsible for splitting documents based on markdown headers.\n\nImplementation details:\n1. Implement the required interface methods from `ChunkingStrategy`\n2. Use regex pattern `^(#{1,6})\\s+(.+)$` to detect markdown headers\n3. Parse the document to identify all headers and their content\n4. Create a hierarchical structure of sections based on header levels\n5. For each section, create a chunk that includes the header and its content\n6. Store header metadata (level, text, hierarchy) with each chunk\n\nPseudo-code:\n```python\nclass MarkdownChunkerStrategy(ChunkingStrategy):\n    def __init__(self, max_chunk_size=1024, chunk_overlap=200):\n        self.max_chunk_size = max_chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.header_pattern = re.compile(r'^(#{1,6})\\s+(.+)$', re.MULTILINE)\n    \n    def split(self, document):\n        # Check if document has markdown headers\n        if not self._has_markdown_headers(document.text):\n            # Fall back to sentence-based chunking\n            return self._fallback_chunking(document)\n        \n        # Parse document into sections by headers\n        sections = self._parse_sections(document.text)\n        \n        # Convert sections to chunks\n        chunks = []\n        for section in sections:\n            if self._get_token_count(section.content) > self.max_chunk_size:\n                # Subdivide large sections\n                sub_chunks = self._subdivide_section(section)\n                chunks.extend(sub_chunks)\n            else:\n                chunks.append(self._create_chunk(section))\n        \n        return chunks\n```",
        "testStrategy": "1. Unit test the `MarkdownChunkerStrategy` class with various markdown documents\n2. Test with documents containing different header levels (h1-h6)\n3. Test with documents having no headers to verify fallback to sentence-based chunking\n4. Test with documents containing very large sections to verify proper subdivision\n5. Verify header metadata is correctly stored with each chunk\n6. Compare chunk quality with existing strategies using sample documents",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create MarkdownSection dataclass",
            "description": "Implement a dataclass to represent markdown sections with header information and content",
            "dependencies": [],
            "details": "Create a MarkdownSection dataclass that will store information about markdown sections including:\n- header_text: The text of the header\n- header_level: Integer representing the header level (1-6)\n- content: The content text under this header\n- parent_headers: List of parent headers for context preservation\n- position: Position in the original document\n\nThis class will be used to represent the hierarchical structure of markdown documents and will be essential for the chunking strategy.",
            "status": "pending",
            "testStrategy": "Unit test the MarkdownSection class to ensure it correctly stores and represents section data. Test initialization with various header levels and content. Verify parent header tracking works correctly.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create MarkdownChunkerConfig dataclass",
            "description": "Implement a configuration class for the markdown chunker with customizable parameters",
            "dependencies": [],
            "details": "Create a MarkdownChunkerConfig dataclass that will store configuration parameters for the markdown chunking strategy including:\n- max_chunk_size: Maximum size of each chunk in tokens\n- chunk_overlap: Number of tokens to overlap between chunks\n- preserve_headers: Boolean flag to determine if headers should be included in each chunk\n- min_chunk_size: Minimum size for a chunk to be considered valid\n- fallback_strategy: Strategy to use when no markdown headers are found\n\nThis configuration class will allow for flexible customization of the chunking behavior.",
            "status": "pending",
            "testStrategy": "Test the MarkdownChunkerConfig class with various parameter combinations. Verify default values are appropriate and parameter validation works correctly. Test serialization/deserialization if applicable.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create MarkdownChunkerStrategy class skeleton",
            "description": "Implement the basic structure of the MarkdownChunkerStrategy class implementing the ChunkingStrategy interface",
            "dependencies": [
              1,
              2
            ],
            "details": "Create the MarkdownChunkerStrategy class that implements the ChunkingStrategy interface. Include:\n- Constructor that accepts a MarkdownChunkerConfig\n- Implementation of required interface methods\n- Placeholder methods for section parsing and chunk creation\n- Basic structure for the split() method that will process documents\n- Method signatures for helper functions\n\nThis will establish the foundation for the chunking implementation while ensuring compliance with the interface contract.",
            "status": "pending",
            "testStrategy": "Test the class instantiation and verify it correctly implements the ChunkingStrategy interface. Test that configuration parameters are properly stored. Mock internal methods to verify the overall structure works as expected.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement header detection and section parsing",
            "description": "Add the regex pattern for header detection and implement the section parsing logic",
            "dependencies": [
              3
            ],
            "details": "Implement the core functionality for header detection and section parsing:\n- Add the regex pattern constant `^(#{1,6})\\s+(.+)$` for markdown header detection\n- Implement the _has_markdown_headers method to check if a document contains markdown headers\n- Create the _parse_sections method to break a document into hierarchical sections\n- Implement logic to track header hierarchy and parent-child relationships\n- Add methods to convert parsed sections into document chunks\n- Ensure proper metadata is attached to each chunk including header context\n\nThis completes the implementation of the markdown chunking strategy.",
            "status": "pending",
            "testStrategy": "Test header detection with various markdown formats. Verify section parsing correctly identifies headers of different levels. Test with complex documents containing nested headers. Ensure chunks maintain proper context and hierarchy information.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-12T02:18:09.066Z"
      },
      {
        "id": "113",
        "title": "Implement markdown header detection and section parsing",
        "description": "Create functionality to detect markdown headers and parse documents into hierarchical sections",
        "details": "Implement helper methods within the MarkdownChunkerStrategy class to detect markdown headers and parse documents into sections based on header hierarchy.\n\nImplementation details:\n1. Create a method to detect if a document contains valid markdown headers\n2. Implement section parsing logic that builds a hierarchical structure\n3. Handle edge cases like inconsistent header levels (e.g., h1 to h4 jumps)\n4. Create a MarkdownSection class to represent sections with header information\n5. Track parent-child relationships between headers\n\nPseudo-code:\n```python\ndef _has_markdown_headers(self, text):\n    return bool(self.header_pattern.search(text))\n\ndef _parse_sections(self, text):\n    lines = text.split('\\n')\n    sections = []\n    current_section = None\n    header_stack = []\n    \n    for line in lines:\n        header_match = self.header_pattern.match(line)\n        if header_match:\n            # New header found\n            level = len(header_match.group(1))  # Number of # symbols\n            header_text = header_match.group(2).strip()\n            \n            # Update header stack based on level\n            while header_stack and header_stack[-1]['level'] >= level:\n                header_stack.pop()\n            \n            # Create header hierarchy\n            hierarchy = [h['text'] for h in header_stack]\n            \n            # Save previous section if exists\n            if current_section:\n                sections.append(current_section)\n            \n            # Create new section\n            current_section = MarkdownSection(\n                header_text=header_text,\n                level=level,\n                content=line,  # Start with header line\n                parent_headers=hierarchy\n            )\n            \n            # Add to header stack\n            header_stack.append({'level': level, 'text': header_text})\n        elif current_section:\n            # Add line to current section content\n            current_section.content += '\\n' + line\n    \n    # Add final section\n    if current_section:\n        sections.append(current_section)\n    \n    return sections\n\nclass MarkdownSection:\n    def __init__(self, header_text, level, content, parent_headers):\n        self.header_text = header_text\n        self.level = level\n        self.content = content\n        self.parent_headers = parent_headers\n```",
        "testStrategy": "1. Unit test the header detection method with various inputs\n2. Test section parsing with documents having different header structures\n3. Verify correct handling of nested headers (h1 > h2 > h3)\n4. Test edge cases like empty sections, inconsistent header levels\n5. Verify parent-child relationships are correctly established\n6. Test with malformed headers to ensure they're treated as regular text",
        "priority": "high",
        "dependencies": [
          "112"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement _split_by_headers() method",
            "description": "Create a method to identify markdown headers and split document content into sections based on headers.",
            "dependencies": [],
            "details": "Implement the _split_by_headers() method in the MarkdownChunkerStrategy class that will: 1) Define a regex pattern to identify markdown headers (e.g., # Header, ## Subheader), 2) Split the document text into sections based on header boundaries, 3) Return a list of raw sections with header information including level and text. Handle edge cases like empty documents or documents without headers.",
            "status": "pending",
            "testStrategy": "Unit test with various markdown documents containing different header patterns. Test edge cases like documents with no headers, empty documents, and documents with only headers but no content.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement _build_header_hierarchy() method",
            "description": "Create a method to build hierarchical relationships between markdown headers of different levels.",
            "dependencies": [
              1
            ],
            "details": "Implement the _build_header_hierarchy() method that takes raw sections from _split_by_headers() and builds parent-child relationships between headers. This method should: 1) Track the current header stack, 2) Handle inconsistent header levels (e.g., h1 to h4 jumps), 3) Assign parent headers to each section, 4) Return a list of MarkdownSection objects with proper hierarchy information.",
            "status": "pending",
            "testStrategy": "Test with documents containing nested headers of various levels. Verify correct parent-child relationships are established. Test edge cases like inconsistent header levels (h1 followed by h3) and ensure proper hierarchy is maintained.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement chunk() main method",
            "description": "Create the main chunking method that uses header detection and hierarchy building to chunk markdown documents.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement the chunk() method in MarkdownChunkerStrategy that orchestrates the markdown chunking process: 1) Check if document contains markdown headers using _has_markdown_headers(), 2) If headers exist, use _split_by_headers() and _build_header_hierarchy() to create hierarchical sections, 3) Convert MarkdownSection objects to the standard chunk format required by the application, 4) Include metadata about section hierarchy in each chunk, 5) Handle fallback to default chunking if no headers are detected.",
            "status": "pending",
            "testStrategy": "Test the complete chunking process with various markdown documents. Verify chunks maintain proper hierarchy information. Test fallback behavior when no headers are detected. Verify chunk metadata correctly represents document structure.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add unit tests for header detection and section extraction",
            "description": "Create comprehensive unit tests for the markdown header detection and section parsing functionality.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Implement unit tests that cover: 1) Header detection with various markdown formats, 2) Section splitting with different document structures, 3) Hierarchy building with nested headers, 4) Edge cases like inconsistent header levels, empty sections, and documents without headers, 5) Integration tests for the complete chunking process, 6) Performance tests with large markdown documents containing many headers and sections.",
            "status": "pending",
            "testStrategy": "Create test fixtures with various markdown document structures. Use parameterized tests to cover multiple scenarios. Include edge cases and boundary conditions. Measure code coverage to ensure all code paths are tested.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-12T02:18:24.157Z"
      },
      {
        "id": "114",
        "title": "Implement large section subdivision with header context preservation",
        "description": "Create functionality to subdivide large sections exceeding token limits while preserving header context",
        "details": "Implement methods to handle sections that exceed the maximum chunk size (1024 tokens) by subdividing them into smaller chunks while preserving the header context.\n\nImplementation details:\n1. Create a method to calculate token count for sections\n2. Implement sentence-aware splitting for large sections\n3. Ensure each sub-chunk retains the original section's header metadata\n4. Apply chunk overlap (200 tokens) between subdivided chunks\n5. Maintain sequential ordering of subdivided chunks\n\nPseudo-code:\n```python\ndef _get_token_count(self, text):\n    # Use tokenizer to count tokens\n    return len(self.tokenizer.encode(text))\n\ndef _subdivide_section(self, section):\n    # Use sentence-aware splitting for large sections\n    sentences = self._split_into_sentences(section.content)\n    chunks = []\n    current_chunk = section.header_text + '\\n'  # Start with header\n    current_chunk_sentences = []\n    \n    for sentence in sentences:\n        # Check if adding this sentence would exceed the limit\n        test_chunk = current_chunk + ' ' + sentence\n        if self._get_token_count(test_chunk) > self.max_chunk_size and current_chunk_sentences:\n            # Create chunk with current content\n            chunk = self._create_chunk_with_header_context(\n                content=current_chunk,\n                section=section,\n                is_subdivision=True,\n                subdivision_index=len(chunks)\n            )\n            chunks.append(chunk)\n            \n            # Start new chunk with overlap\n            overlap_sentences = current_chunk_sentences[-3:]  # Approximate 200 token overlap\n            current_chunk = section.header_text + '\\n' + ' '.join(overlap_sentences) + ' ' + sentence\n            current_chunk_sentences = overlap_sentences + [sentence]\n        else:\n            # Add sentence to current chunk\n            current_chunk = test_chunk\n            current_chunk_sentences.append(sentence)\n    \n    # Add final chunk if there's content\n    if current_chunk_sentences:\n        chunk = self._create_chunk_with_header_context(\n            content=current_chunk,\n            section=section,\n            is_subdivision=True,\n            subdivision_index=len(chunks)\n        )\n        chunks.append(chunk)\n    \n    return chunks\n\ndef _create_chunk_with_header_context(self, content, section, is_subdivision=False, subdivision_index=0):\n    # Create chunk with header metadata\n    return Chunk(\n        text=content,\n        metadata={\n            'header_level': section.level,\n            'section_header': section.header_text,\n            'header_hierarchy': ' > '.join(section.parent_headers + [section.header_text]),\n            'is_header_split': True,\n            'is_subdivision': is_subdivision,\n            'subdivision_index': subdivision_index\n        }\n    )\n```",
        "testStrategy": "1. Test with sections of various sizes to verify subdivision logic\n2. Verify token counting is accurate\n3. Check that subdivided chunks maintain header context\n4. Verify chunk overlap is correctly applied\n5. Test with edge cases like very short sentences and very long sentences\n6. Verify sequential ordering of subdivided chunks",
        "priority": "medium",
        "dependencies": [
          "112",
          "113"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement _count_tokens() method using tiktoken",
            "description": "Create a method to accurately count tokens in text sections using the tiktoken library",
            "dependencies": [],
            "details": "Implement the _count_tokens() method that uses the tiktoken library to accurately count tokens in text sections. This method will be used to determine if a section needs to be subdivided based on the maximum token limit (1024 tokens). The implementation should handle different encoding models and cache the tokenizer for efficiency.",
            "status": "pending",
            "testStrategy": "Write unit tests to verify token counting accuracy with various text inputs. Compare results with OpenAI's tokenizer for validation. Test with edge cases like empty strings, special characters, and multilingual content.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement _chunk_oversized_section() using SentenceSplitter",
            "description": "Create a method to split large sections into smaller chunks while preserving sentence boundaries",
            "dependencies": [
              1
            ],
            "details": "Implement the _chunk_oversized_section() method that uses a SentenceSplitter to divide large sections into smaller chunks without breaking sentences. The method should ensure that each chunk starts with the section header for context preservation and implements sentence-aware splitting logic. It should handle edge cases like very long sentences that might exceed the token limit on their own.",
            "status": "pending",
            "testStrategy": "Test with sections of various sizes to verify subdivision logic. Ensure sentence boundaries are preserved. Verify that very long sections are properly split into multiple chunks. Test edge cases like sections with very few sentences but many tokens.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add chunk_index and total_section_chunks metadata",
            "description": "Enhance chunk metadata with indexing information to maintain sequential ordering of subdivided chunks",
            "dependencies": [
              2
            ],
            "details": "Modify the _create_chunk_with_header_context() method to include additional metadata fields: 'chunk_index' and 'total_section_chunks'. These fields will help maintain the sequential ordering of subdivided chunks and provide information about the total number of chunks a section was split into. Update the chunk creation logic to properly set these values when creating subdivided chunks.",
            "status": "pending",
            "testStrategy": "Test that chunks from subdivided sections have correct sequential index values. Verify that total_section_chunks accurately reflects the number of chunks created. Test with sections that produce different numbers of chunks to ensure consistency.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement chunk_overlap with 200 tokens between subdivided sections",
            "description": "Add functionality to ensure 200 token overlap between adjacent chunks from the same section",
            "dependencies": [
              2,
              3
            ],
            "details": "Enhance the _chunk_oversized_section() method to implement a 200 token overlap between adjacent chunks from the same section. This involves modifying the chunking algorithm to include approximately 200 tokens from the end of the previous chunk at the beginning of the next chunk. The implementation should use the _count_tokens() method to ensure accurate token counting and should handle edge cases where the overlap might be smaller due to section size constraints.",
            "status": "pending",
            "testStrategy": "Verify that adjacent chunks have approximately 200 tokens of overlapping content. Test with various section sizes to ensure consistent overlap behavior. Check that the first and last sentences of adjacent chunks properly overlap. Test edge cases where sections are just slightly larger than the maximum chunk size.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-12T02:18:38.723Z"
      },
      {
        "id": "115",
        "title": "Implement fallback to sentence-based chunking",
        "description": "Create fallback mechanism for documents without markdown headers",
        "details": "Implement a fallback mechanism that uses the existing sentence-based chunking strategy when a document contains no recognizable markdown headers.\n\nImplementation details:\n1. Create a method to delegate to the existing sentence-based chunker\n2. Ensure seamless transition between strategies based on document content\n3. Preserve all existing functionality of sentence-based chunking\n4. Add appropriate logging to indicate fallback was used\n\nPseudo-code:\n```python\ndef _fallback_chunking(self, document):\n    # Log that we're falling back to sentence-based chunking\n    logger.info(f\"No markdown headers found in document {document.id}, falling back to sentence-based chunking\")\n    \n    # Create and use sentence chunker with same parameters\n    sentence_chunker = SentenceChunkerStrategy(\n        max_chunk_size=self.max_chunk_size,\n        chunk_overlap=self.chunk_overlap\n    )\n    \n    # Get chunks from sentence chunker\n    chunks = sentence_chunker.split(document)\n    \n    # Add metadata to indicate fallback was used\n    for chunk in chunks:\n        chunk.metadata['chunking_strategy'] = 'sentence_fallback'\n        chunk.metadata['is_header_split'] = False\n    \n    return chunks\n```",
        "testStrategy": "1. Test with documents containing no markdown headers\n2. Verify fallback to sentence-based chunking works correctly\n3. Compare results with direct use of sentence chunker to ensure equivalence\n4. Verify appropriate metadata is added to chunks\n5. Check logging to ensure fallback is properly recorded\n6. Test with edge cases like documents with malformed headers",
        "priority": "medium",
        "dependencies": [
          "112"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Add is_markdown_content() method with min_headers threshold",
            "description": "Create a method to detect if a document contains sufficient markdown headers to use header-based chunking",
            "dependencies": [],
            "details": "Implement a new method called `is_markdown_content()` that analyzes a document to determine if it contains enough markdown headers to be processed with header-based chunking. The method should accept a parameter for minimum number of headers required (default to 2 or 3). It should use regex patterns to identify markdown headers (e.g., lines starting with #, ##, etc.) and return a boolean indicating if the document meets the threshold. This will be used as the decision point for whether to use header-based chunking or fall back to sentence-based chunking.",
            "status": "pending",
            "testStrategy": "Write unit tests with various document samples: documents with many headers, few headers, no headers, and edge cases like headers with special characters. Verify the method correctly identifies documents that should use header-based chunking.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement fallback to SentenceSplitter when no headers detected",
            "description": "Create the fallback mechanism that delegates to sentence-based chunking when a document lacks sufficient markdown structure",
            "dependencies": [
              1
            ],
            "details": "Implement the `_fallback_chunking()` method as outlined in the pseudo-code. This method should be called when `is_markdown_content()` returns False. It should instantiate a SentenceChunkerStrategy with the same parameters as the current chunker, process the document using this strategy, and add appropriate metadata to each chunk to indicate the fallback strategy was used. Ensure proper logging is implemented to record when fallback occurs. Modify the main chunking method to check for markdown content first and delegate to the appropriate chunking strategy.",
            "status": "pending",
            "testStrategy": "Test with documents containing no markdown headers. Verify chunks are created correctly using sentence-based chunking. Check that metadata is properly added to chunks. Verify logging messages are generated correctly. Compare results with direct use of sentence chunker to ensure equivalence.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add integration test for non-markdown document processing",
            "description": "Create comprehensive integration tests to verify the fallback mechanism works end-to-end",
            "dependencies": [
              2
            ],
            "details": "Develop integration tests that process various document types through the chunking system. Include test cases for documents with no markdown headers, documents with insufficient headers (below threshold), and documents with adequate headers. Verify that the system correctly identifies document types, applies the appropriate chunking strategy, and produces expected results. Test that metadata is correctly applied in all cases. Also test edge cases such as very short documents, documents with unusual formatting, and documents with mixed content types.",
            "status": "pending",
            "testStrategy": "Create a test suite with multiple document fixtures. Compare chunking results against expected outputs for each document type. Verify metadata fields are correctly populated. Check logging output to confirm appropriate strategy selection messages. Test performance to ensure fallback mechanism doesn't introduce significant overhead.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-12T02:18:53.739Z"
      },
      {
        "id": "116",
        "title": "Extend Chunk class with header metadata fields",
        "description": "Extend the Chunk entity to include header-related metadata fields",
        "details": "Extend the existing Chunk class or its metadata structure to include new fields for header information.\n\nImplementation details:\n1. Add the following metadata fields to the Chunk class:\n   - header_level: Integer (1-6) representing the header level\n   - section_header: String containing the section header text\n   - header_hierarchy: String representing the full header hierarchy (e.g., \"Chapter 1 > Introduction > Overview\")\n   - is_header_split: Boolean indicating if the chunk was created by header splitting\n   - is_subdivision: Boolean indicating if the chunk is a subdivision of a larger section\n   - subdivision_index: Integer representing the position in a subdivided section\n\n2. Ensure backward compatibility with existing chunk metadata\n3. Update any relevant documentation or type definitions\n\nPseudo-code:\n```python\n# If Chunk is a dataclass or similar\nclass Chunk:\n    text: str\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    \n    # Helper methods for header metadata\n    @property\n    def header_level(self) -> Optional[int]:\n        return self.metadata.get('header_level')\n    \n    @property\n    def section_header(self) -> Optional[str]:\n        return self.metadata.get('section_header')\n    \n    @property\n    def header_hierarchy(self) -> Optional[str]:\n        return self.metadata.get('header_hierarchy')\n    \n    @property\n    def is_header_split(self) -> bool:\n        return self.metadata.get('is_header_split', False)\n```",
        "testStrategy": "1. Unit test the extended Chunk class\n2. Verify all new metadata fields can be set and retrieved\n3. Test backward compatibility with existing chunks\n4. Verify helper methods work correctly\n5. Test serialization/deserialization of chunks with header metadata\n6. Verify integration with existing systems that use Chunk objects",
        "priority": "medium",
        "dependencies": [
          "112"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-12T02:19:14.286Z"
      },
      {
        "id": "117",
        "title": "Implement automatic markdown detection for LlamaParse output",
        "description": "Create functionality to automatically detect markdown content from LlamaParse output",
        "details": "Implement logic to automatically detect when a document comes from LlamaParse with markdown formatting and apply the appropriate chunking strategy.\n\nImplementation details:\n1. Create a detection method to identify LlamaParse markdown output\n2. Integrate with the document processing pipeline to automatically select the markdown chunker\n3. Look for LlamaParse metadata or specific markdown patterns\n4. Handle edge cases where markdown might be malformed\n\nPseudo-code:\n```python\ndef is_llamaparse_markdown(document):\n    # Check document metadata for LlamaParse origin\n    if document.metadata.get('source') == 'llamaparse' and document.metadata.get('result_type') == 'markdown':\n        return True\n    \n    # Check for markdown header patterns\n    if re.search(r'^(#{1,6})\\s+(.+)$', document.text, re.MULTILINE):\n        # Count headers to ensure it's not just occasional use of # symbols\n        header_count = len(re.findall(r'^(#{1,6})\\s+(.+)$', document.text, re.MULTILINE))\n        if header_count >= 3:  # Arbitrary threshold for a structured document\n            return True\n    \n    return False\n\n# In document processor class\ndef select_chunking_strategy(document):\n    if is_llamaparse_markdown(document):\n        return MarkdownChunkerStrategy()\n    elif is_code_document(document):\n        return CodeChunkerStrategy()\n    elif is_transcript(document):\n        return TranscriptChunkerStrategy()\n    else:\n        return SentenceChunkerStrategy()\n```",
        "testStrategy": "1. Test with various LlamaParse outputs to verify detection\n2. Test with non-LlamaParse markdown documents\n3. Test with documents having different levels of markdown formatting\n4. Verify correct strategy selection based on document type\n5. Test edge cases with minimal or malformed markdown\n6. Verify integration with the document processing pipeline",
        "priority": "medium",
        "dependencies": [
          "112",
          "113"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement is_llamaparse_markdown detection function",
            "description": "Create a function to detect markdown content from LlamaParse output based on metadata and content patterns.",
            "dependencies": [],
            "details": "Implement the is_llamaparse_markdown() function in document_processor.py that analyzes document content and metadata to determine if it's markdown from LlamaParse. The function should check for LlamaParse metadata tags, examine markdown header patterns (using regex to find patterns like '## Header'), count the frequency of markdown elements, and determine if the threshold of markdown elements is met to classify as structured markdown. Include handling for edge cases with malformed markdown.",
            "status": "pending",
            "testStrategy": "Write unit tests with various document samples including: LlamaParse markdown with metadata, markdown without metadata, non-markdown content, edge cases with minimal markdown elements, and malformed markdown. Verify correct detection in each scenario.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Update document processing pipeline for markdown detection",
            "description": "Modify the document processing pipeline to automatically select the markdown chunker when LlamaParse markdown is detected.",
            "dependencies": [
              1
            ],
            "details": "Update the source_processing.py file to integrate the is_llamaparse_markdown detection function into the document processing workflow. Modify the select_chunking_strategy() method to check for markdown content and route documents to the MarkdownChunkerStrategy when appropriate. Ensure the detection happens early in the pipeline to optimize processing. Add logging for strategy selection decisions to aid debugging and monitoring. Handle the transition between detection and chunking strategy selection seamlessly.",
            "status": "pending",
            "testStrategy": "Create integration tests that verify the entire document processing pipeline correctly identifies LlamaParse markdown and selects the appropriate chunking strategy. Test with various document types to ensure other document types still route to their correct chunkers. Measure performance impact of the additional detection step.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add comprehensive tests for LlamaParse markdown flow",
            "description": "Create integration and unit tests to verify the end-to-end markdown detection and processing functionality.",
            "dependencies": [
              1,
              2
            ],
            "details": "Develop a comprehensive test suite in test_llamaparse_markdown.py that validates the entire markdown detection and processing flow. Include tests for various LlamaParse output formats, different markdown structures, and edge cases. Create mock LlamaParse outputs with varying degrees of markdown formatting. Test the integration between detection and chunking strategy selection. Verify that documents are correctly processed with the appropriate chunking strategy based on their content. Include performance tests to ensure the detection doesn't significantly impact processing time.",
            "status": "pending",
            "testStrategy": "Use pytest fixtures to create various test documents. Test the full pipeline from document ingestion through chunking. Verify chunk boundaries respect markdown structure. Test with real-world examples of LlamaParse output. Include regression tests to ensure other document types aren't affected by the changes.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-12T02:20:56.615Z"
      },
      {
        "id": "118",
        "title": "Implement chunking strategy factory and registration",
        "description": "Create a factory pattern for chunking strategies and register the new MarkdownChunkerStrategy",
        "details": "Implement a factory pattern for chunking strategies to allow dynamic selection and registration of strategies, including the new MarkdownChunkerStrategy.\n\nImplementation details:\n1. Create a ChunkerStrategyFactory class\n2. Implement registration mechanism for chunking strategies\n3. Add logic to select the appropriate strategy based on document type\n4. Register all existing strategies (semantic, code, transcript)\n5. Register the new MarkdownChunkerStrategy\n6. Ensure backward compatibility with existing code\n\nPseudo-code:\n```python\nclass ChunkerStrategyFactory:\n    _strategies = {}\n    \n    @classmethod\n    def register_strategy(cls, name, strategy_class):\n        cls._strategies[name] = strategy_class\n    \n    @classmethod\n    def get_strategy(cls, name, **kwargs):\n        strategy_class = cls._strategies.get(name)\n        if not strategy_class:\n            raise ValueError(f\"Unknown chunking strategy: {name}\")\n        return strategy_class(**kwargs)\n    \n    @classmethod\n    def get_strategy_for_document(cls, document, **kwargs):\n        # Select strategy based on document type\n        if is_llamaparse_markdown(document):\n            return cls.get_strategy('markdown', **kwargs)\n        elif is_code_document(document):\n            return cls.get_strategy('code', **kwargs)\n        elif is_transcript(document):\n            return cls.get_strategy('transcript', **kwargs)\n        else:\n            return cls.get_strategy('sentence', **kwargs)\n\n# Register strategies\nChunkerStrategyFactory.register_strategy('sentence', SentenceChunkerStrategy)\nChunkerStrategyFactory.register_strategy('code', CodeChunkerStrategy)\nChunkerStrategyFactory.register_strategy('transcript', TranscriptChunkerStrategy)\nChunkerStrategyFactory.register_strategy('markdown', MarkdownChunkerStrategy)\n```",
        "testStrategy": "1. Unit test the ChunkerStrategyFactory class\n2. Test registration of strategies\n3. Test retrieval of strategies by name\n4. Test automatic strategy selection based on document type\n5. Verify all existing strategies are properly registered\n6. Test with various document types to ensure correct strategy selection",
        "priority": "medium",
        "dependencies": [
          "112",
          "117"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-12T02:22:29.693Z"
      },
      {
        "id": "119",
        "title": "Implement chunk filtering by header metadata",
        "description": "Add functionality to filter chunks by header level or section name",
        "details": "Implement functionality to allow filtering of chunks based on header metadata, enabling advanced search capabilities.\n\nImplementation details:\n1. Extend the search/query interface to accept header filter parameters\n2. Implement filtering logic based on header level, section name, or header hierarchy\n3. Ensure efficient filtering without significant performance impact\n4. Add documentation for the new filtering capabilities\n\nPseudo-code:\n```python\nclass ChunkRepository:\n    # Existing methods...\n    \n    def search_with_filters(self, query, filters=None, **kwargs):\n        # Start with basic search\n        results = self.basic_search(query, **kwargs)\n        \n        # Apply header filters if specified\n        if filters and results:\n            if 'header_level' in filters:\n                results = [r for r in results if r.chunk.metadata.get('header_level') == filters['header_level']]\n            \n            if 'section_header' in filters:\n                section_pattern = re.compile(filters['section_header'], re.IGNORECASE)\n                results = [r for r in results if r.chunk.metadata.get('section_header') and \n                           section_pattern.search(r.chunk.metadata.get('section_header'))]\n            \n            if 'header_hierarchy' in filters:\n                hierarchy_pattern = re.compile(filters['header_hierarchy'], re.IGNORECASE)\n                results = [r for r in results if r.chunk.metadata.get('header_hierarchy') and \n                           hierarchy_pattern.search(r.chunk.metadata.get('header_hierarchy'))]\n        \n        return results\n\n# API endpoint or service method\ndef search_documents(query, header_level=None, section_name=None, header_path=None, **kwargs):\n    filters = {}\n    if header_level is not None:\n        filters['header_level'] = int(header_level)\n    if section_name:\n        filters['section_header'] = section_name\n    if header_path:\n        filters['header_hierarchy'] = header_path\n    \n    return chunk_repository.search_with_filters(query, filters, **kwargs)\n```",
        "testStrategy": "1. Test filtering by header level\n2. Test filtering by section name (exact and partial matches)\n3. Test filtering by header hierarchy\n4. Test combinations of multiple filters\n5. Verify performance impact is minimal\n6. Test with edge cases like non-existent headers or malformed filters",
        "priority": "low",
        "dependencies": [
          "112",
          "116"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-12T03:04:07.631Z"
      },
      {
        "id": "120",
        "title": "Implement logging and observability for markdown chunking",
        "description": "Add comprehensive logging and observability for the markdown chunking process",
        "details": "Implement logging and observability features to track the markdown chunking process, including strategy selection, chunk statistics, and performance metrics.\n\nImplementation details:\n1. Add structured logging throughout the MarkdownChunkerStrategy class\n2. Log key events: strategy selection, header detection, section parsing, chunking decisions\n3. Record metrics: number of chunks created, average chunk size, processing time\n4. Create summary statistics for each processed document\n5. Ensure logs can be used for debugging and performance analysis\n\nPseudo-code:\n```python\nclass MarkdownChunkerStrategy(ChunkingStrategy):\n    # Existing methods...\n    \n    def split(self, document):\n        start_time = time.time()\n        logger.info(f\"Starting markdown chunking for document {document.id}\")\n        \n        # Check if document has markdown headers\n        has_headers = self._has_markdown_headers(document.text)\n        logger.info(f\"Markdown headers detected: {has_headers}\")\n        \n        if not has_headers:\n            logger.info(f\"Falling back to sentence chunking for document {document.id}\")\n            chunks = self._fallback_chunking(document)\n        else:\n            # Parse document into sections by headers\n            sections = self._parse_sections(document.text)\n            logger.info(f\"Parsed {len(sections)} sections from document\")\n            \n            # Log section statistics\n            header_levels = Counter([section.level for section in sections])\n            logger.info(f\"Header level distribution: {dict(header_levels)}\")\n            \n            # Convert sections to chunks\n            chunks = []\n            subdivided_sections = 0\n            \n            for section in sections:\n                token_count = self._get_token_count(section.content)\n                if token_count > self.max_chunk_size:\n                    logger.info(f\"Subdividing section '{section.header_text}' with {token_count} tokens\")\n                    subdivided_sections += 1\n                    sub_chunks = self._subdivide_section(section)\n                    chunks.extend(sub_chunks)\n                else:\n                    chunks.append(self._create_chunk(section))\n            \n            logger.info(f\"Created {len(chunks)} chunks, subdivided {subdivided_sections} sections\")\n        \n        # Calculate and log metrics\n        processing_time = time.time() - start_time\n        avg_chunk_size = sum(len(c.text) for c in chunks) / len(chunks) if chunks else 0\n        avg_token_count = sum(self._get_token_count(c.text) for c in chunks) / len(chunks) if chunks else 0\n        \n        logger.info(f\"Markdown chunking completed in {processing_time:.2f}s\")\n        logger.info(f\"Chunks: {len(chunks)}, Avg size: {avg_chunk_size:.1f} chars, Avg tokens: {avg_token_count:.1f}\")\n        \n        # Add processing metadata to document\n        document.metadata['chunking_stats'] = {\n            'strategy': 'markdown' if has_headers else 'sentence_fallback',\n            'chunk_count': len(chunks),\n            'avg_chunk_size': avg_chunk_size,\n            'avg_token_count': avg_token_count,\n            'processing_time': processing_time,\n            'subdivided_sections': subdivided_sections if has_headers else 0\n        }\n        \n        return chunks\n```",
        "testStrategy": "1. Verify logs are generated for key events in the chunking process\n2. Test that metrics are accurately calculated and recorded\n3. Check that document metadata is properly updated with chunking statistics\n4. Test with various document types to ensure comprehensive logging\n5. Verify log levels are appropriate (info for normal operation, warning/error for issues)\n6. Test integration with existing logging and monitoring systems",
        "priority": "low",
        "dependencies": [
          "112",
          "113",
          "114"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-12T02:21:24.237Z"
      },
      {
        "id": "121",
        "title": "Create comprehensive tests and documentation",
        "description": "Develop comprehensive tests and documentation for the markdown chunking feature",
        "details": "Create a comprehensive test suite and documentation for the markdown chunking feature to ensure quality and usability.\n\nImplementation details:\n1. Create unit tests for all components of the MarkdownChunkerStrategy\n2. Develop integration tests with the full document processing pipeline\n3. Create performance tests to verify compliance with success criteria\n4. Write user documentation explaining the feature and its benefits\n5. Create developer documentation with examples and API references\n6. Update existing documentation to reference the new feature\n\nTest cases to implement:\n- Unit tests for header detection, section parsing, chunk creation\n- Tests for edge cases: empty documents, malformed markdown, inconsistent headers\n- Tests for large documents and sections requiring subdivision\n- Tests for fallback to sentence chunking\n- Integration tests with the full RAG pipeline\n- Performance tests comparing with existing chunking strategies\n\nDocumentation to create:\n- User guide explaining markdown chunking benefits\n- Developer guide for extending or customizing the chunking strategy\n- API reference for the new classes and methods\n- Examples of filtering by header metadata\n- Troubleshooting guide for common issues",
        "testStrategy": "1. Verify all tests pass and provide good coverage\n2. Review documentation for clarity and completeness\n3. Have team members review and provide feedback\n4. Test documentation examples to ensure they work as described\n5. Verify integration with existing documentation systems\n6. Check that all success criteria are verified by tests",
        "priority": "medium",
        "dependencies": [
          "112",
          "113",
          "114",
          "115",
          "116",
          "117",
          "118",
          "119",
          "120"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-12T02:34:20.210Z"
      },
      {
        "id": "122",
        "title": "Create Content Prep Agent Service",
        "description": "Implement the core Content Prep Agent (AGENT-016) service that will validate, order, and prepare content before ingestion into the knowledge base.",
        "details": "Create the `app/services/content_prep_agent.py` file with the following components:\n\n1. Implement the CrewAI agent definition as specified in the PRD\n2. Create the ContentSet, ContentFile, and ProcessingManifest data classes\n3. Implement the three main components:\n   - Set Detector: Pattern matching and grouping logic\n   - Order Resolver: Sequence parsing and gap detection\n   - Manifest Generator: Ordered queue and dependency tracking\n\nPseudo-code:\n```python\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import List, Dict, Optional\nimport re\nimport uuid\n\n# Data models as specified in PRD\n@dataclass\nclass ContentFile:\n    b2_path: str\n    filename: str\n    sequence_number: Optional[int] = None\n    dependencies: List[str] = None\n    estimated_complexity: str = \"medium\"\n    file_type: str = \"\"\n    size_bytes: int = 0\n    metadata: Dict = None\n    \n    def __post_init__(self):\n        if self.dependencies is None:\n            self.dependencies = []\n        if self.metadata is None:\n            self.metadata = {}\n\n@dataclass\nclass ContentSet:\n    id: str\n    name: str\n    detection_method: str\n    files: List[ContentFile]\n    is_complete: bool = False\n    missing_files: List[str] = None\n    processing_status: str = \"pending\"\n    created_at: datetime = None\n    metadata: Dict = None\n    \n    def __post_init__(self):\n        if self.missing_files is None:\n            self.missing_files = []\n        if self.created_at is None:\n            self.created_at = datetime.now()\n        if self.metadata is None:\n            self.metadata = {}\n\n@dataclass\nclass ProcessingManifest:\n    content_set_id: str\n    ordered_files: List[ContentFile]\n    total_files: int\n    estimated_processing_time: int\n    warnings: List[str] = None\n    context: Dict = None\n    \n    def __post_init__(self):\n        if self.warnings is None:\n            self.warnings = []\n        if self.context is None:\n            self.context = {}\n\n# Sequence detection patterns as specified in PRD\nSEQUENCE_PATTERNS = [\n    # Numeric prefix patterns\n    r\"^(\\d{1,3})[-_\\s]\",              # \"01-intro.pdf\", \"1_chapter.pdf\"\n    r\"[-_\\s](\\d{1,3})[-_\\s.]\",        # \"chapter-01-intro.pdf\"\n    r\"module[-_\\s]?(\\d{1,3})\",        # \"module01.pdf\", \"module-1.pdf\"\n    r\"chapter[-_\\s]?(\\d{1,3})\",       # \"chapter1.pdf\", \"chapter-01.pdf\"\n    r\"lesson[-_\\s]?(\\d{1,3})\",        # \"lesson5.pdf\"\n    r\"part[-_\\s]?(\\d{1,3})\",          # \"part-1.pdf\"\n    r\"week[-_\\s]?(\\d{1,3})\",          # \"week-01.pdf\"\n    r\"unit[-_\\s]?(\\d{1,3})\",          # \"unit-1.pdf\"\n\n    # Alpha sequence patterns\n    r\"^([a-z])[-_\\s]\",                # \"a-intro.pdf\", \"b-basics.pdf\"\n\n    # Roman numerals\n    r\"^(i{1,3}|iv|v|vi{0,3}|ix|x)[-_\\s]\",  # \"i-intro.pdf\", \"ii-basics.pdf\"\n]\n\nCONTENT_SET_INDICATORS = [\n    r\"course[-_\\s]?\",\n    r\"tutorial[-_\\s]?\",\n    r\"training[-_\\s]?\",\n    r\"documentation[-_\\s]?\",\n    r\"manual[-_\\s]?\",\n    r\"series[-_\\s]?\",\n    r\"book[-_\\s]?\",\n]\n\nclass ContentPrepAgent:\n    \"\"\"AGENT-016: Content Prep Agent implementation\"\"\"\n    \n    def __init__(self):\n        self.agent_config = {\n            \"id\": \"AGENT-016\",\n            \"name\": \"Content Prep Agent\",\n            \"role\": \"Content Preparation Specialist\",\n            \"goal\": \"Validate, order, and prepare content sets for optimal knowledge base ingestion\",\n            \"backstory\": \"\"\"You are an expert in content organization and curriculum design.\n            You understand that learning materials must be processed in logical sequence\n            to maintain prerequisite relationships. You detect patterns in file naming,\n            identify content sets, and ensure completeness before processing begins.\"\"\",\n            \"model\": \"claude-3-5-haiku-20241022\",  # Fast, cost-effective for ordering\n            \"tools\": [\n                \"file_metadata_reader\",\n                \"sequence_pattern_detector\",\n                \"b2_file_lister\",\n                \"manifest_generator\"\n            ],\n            \"allow_delegation\": False,\n            \"verbose\": True\n        }\n    \n    def detect_content_sets(self, files, detection_mode=\"auto\"):\n        \"\"\"Detect related content sets from a list of files\"\"\"\n        # Implementation of set detection logic\n        # Group by naming patterns, prefixes, etc.\n        pass\n        \n    def validate_completeness(self, content_set):\n        \"\"\"Check for missing files in sequence\"\"\"\n        # Gap detection in sequence numbers\n        # Return list of missing files\n        pass\n        \n    def resolve_order(self, files):\n        \"\"\"Determine the correct processing order\"\"\"\n        # Parse file names for sequence indicators\n        # Sort by detected sequence\n        # Fall back to LLM for ambiguous cases\n        pass\n        \n    def generate_manifest(self, content_set, proceed_incomplete=False):\n        \"\"\"Create processing manifest with ordered files\"\"\"\n        # Generate ordered queue\n        # Add dependencies and context\n        # Include warnings for missing files\n        pass\n        \n    def estimate_processing_time(self, files):\n        \"\"\"Estimate processing time based on file types and sizes\"\"\"\n        # Calculate based on file size, type, and complexity\n        pass\n        \n    def detect_sequence_number(self, filename):\n        \"\"\"Extract sequence number from filename using patterns\"\"\"\n        # Try each pattern in SEQUENCE_PATTERNS\n        # Return extracted number if found\n        pass\n```",
        "testStrategy": "1. Unit tests for each component (Set Detector, Order Resolver, Manifest Generator)\n2. Test with various file naming patterns to ensure correct sequence detection\n3. Test gap detection with incomplete sequences\n4. Test with edge cases (single files, very large sets)\n5. Verify correct data model instantiation\n6. Mock B2 file listing for integration testing",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create data classes for ContentFile, ContentSet, and ProcessingManifest",
            "description": "Implement the data models needed for the Content Prep Agent service as specified in the PRD.",
            "dependencies": [],
            "details": "Create the data classes ContentFile, ContentSet, and ProcessingManifest with all required fields and proper type annotations. Implement the __post_init__ methods to handle default values for lists and dictionaries. Ensure all fields match the specifications in the PRD and pseudo-code.",
            "status": "pending",
            "testStrategy": "Test data class instantiation with various parameters. Verify default values are properly initialized. Test serialization/deserialization of the data classes.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement sequence detection patterns and content set indicators",
            "description": "Define the regex patterns for sequence detection and content set identification.",
            "dependencies": [
              1
            ],
            "details": "Create the SEQUENCE_PATTERNS list containing all regex patterns for detecting sequence numbers in filenames. Implement the CONTENT_SET_INDICATORS list with patterns for identifying related content sets. Ensure patterns cover all specified cases in the PRD including numeric prefixes, alpha sequences, and roman numerals.",
            "status": "pending",
            "testStrategy": "Test each regex pattern against sample filenames to verify correct matching. Create comprehensive test cases covering all pattern variations.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create ContentPrepAgent class skeleton with CrewAI configuration",
            "description": "Implement the base ContentPrepAgent class with CrewAI agent configuration.",
            "dependencies": [
              1,
              2
            ],
            "details": "Create the ContentPrepAgent class with proper initialization and CrewAI agent configuration as specified in the PRD. Include the agent_config dictionary with all required fields (id, name, role, goal, backstory, model, tools, etc.). Define method stubs for all required functionality.",
            "status": "pending",
            "testStrategy": "Verify agent initialization and configuration. Test that the agent_config contains all required fields with correct values.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement sequence detection and extraction methods",
            "description": "Create methods for extracting sequence numbers and prefixes from filenames.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement the detect_sequence_number method to extract sequence numbers from filenames using the defined regex patterns. Create helper methods for extracting prefixes and other metadata from filenames. Ensure proper handling of edge cases and different sequence formats.",
            "status": "pending",
            "testStrategy": "Test sequence detection with various filename formats. Verify correct extraction of sequence numbers, prefixes, and other metadata. Test edge cases like missing sequences or unusual formats.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement content set detection functionality",
            "description": "Create the detect_content_sets method to identify related content sets from file lists.",
            "dependencies": [
              3,
              4
            ],
            "details": "Implement the detect_content_sets method that groups files into related content sets based on naming patterns, prefixes, and other indicators. Include support for different detection modes (auto, pattern-based, prefix-based). Create helper methods for pattern matching and grouping logic.",
            "status": "pending",
            "testStrategy": "Test content set detection with various file collections. Verify correct grouping of related files. Test different detection modes and edge cases like single files or ambiguous groupings.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement order resolution and completeness validation",
            "description": "Create methods for determining processing order and detecting missing files.",
            "dependencies": [
              4,
              5
            ],
            "details": "Implement the resolve_order method to determine the correct processing sequence for files based on detected sequence numbers. Create the validate_completeness method to identify gaps in sequences and generate a list of potentially missing files. Include logic for handling ambiguous cases using LLM fallback.",
            "status": "pending",
            "testStrategy": "Test order resolution with various file sequences. Verify correct ordering based on sequence numbers. Test gap detection with incomplete sequences. Test LLM fallback for ambiguous cases.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Implement manifest generation and processing time estimation",
            "description": "Create methods for generating processing manifests and estimating processing times.",
            "dependencies": [
              5,
              6
            ],
            "details": "Implement the generate_manifest method to create a ProcessingManifest with ordered files, dependencies, and context information. Create the estimate_processing_time method to calculate expected processing duration based on file types, sizes, and complexity. Include handling for warnings about missing files and incomplete sets.",
            "status": "pending",
            "testStrategy": "Test manifest generation with various content sets. Verify correct ordering and dependency tracking. Test processing time estimation with different file types and sizes. Verify warning generation for incomplete sets.",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Implement database storage and retrieval methods",
            "description": "Create methods for persisting content sets and manifests to the database.",
            "dependencies": [
              1,
              7
            ],
            "details": "Implement methods for storing ContentSet and ProcessingManifest objects in the database. Create retrieval methods for loading existing content sets and manifests. Ensure proper error handling and transaction management. Implement status update methods for tracking processing progress.",
            "status": "pending",
            "testStrategy": "Test database storage and retrieval with mock database connections. Verify correct serialization and deserialization of objects. Test error handling and transaction management. Verify status updates work correctly.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-13T21:05:19.213Z"
      },
      {
        "id": "123",
        "title": "Create Content Sets Database Schema",
        "description": "Create the database schema for content sets and content set files in Supabase as specified in the PRD.",
        "details": "Create the migration file `migrations/create_content_sets.sql` with the SQL schema for the content_sets and content_set_files tables. This will store metadata about detected content sets and their files.\n\nThe schema should include:\n1. content_sets table with fields for id, name, detection method, completeness, etc.\n2. content_set_files table with fields for file metadata and sequence information\n3. Appropriate indexes for efficient querying\n4. Foreign key relationships\n\nSQL Schema:\n```sql\nCREATE TABLE content_sets (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    name VARCHAR(255) NOT NULL,\n    detection_method VARCHAR(50) NOT NULL,\n    is_complete BOOLEAN DEFAULT FALSE,\n    missing_files JSONB DEFAULT '[]',\n    file_count INTEGER NOT NULL,\n    processing_status VARCHAR(50) DEFAULT 'pending',\n    created_at TIMESTAMPTZ DEFAULT NOW(),\n    updated_at TIMESTAMPTZ DEFAULT NOW(),\n    metadata JSONB DEFAULT '{}'\n);\n\nCREATE TABLE content_set_files (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    content_set_id UUID REFERENCES content_sets(id) ON DELETE CASCADE,\n    b2_path VARCHAR(500) NOT NULL,\n    filename VARCHAR(255) NOT NULL,\n    sequence_number INTEGER,\n    dependencies JSONB DEFAULT '[]',\n    estimated_complexity VARCHAR(20),\n    file_type VARCHAR(50),\n    size_bytes BIGINT,\n    processing_status VARCHAR(50) DEFAULT 'pending',\n    created_at TIMESTAMPTZ DEFAULT NOW(),\n    metadata JSONB DEFAULT '{}'\n);\n\nCREATE INDEX idx_content_sets_status ON content_sets(processing_status);\nCREATE INDEX idx_content_set_files_set ON content_set_files(content_set_id);\nCREATE INDEX idx_content_set_files_sequence ON content_set_files(content_set_id, sequence_number);\n\n-- Index for retention policy\nCREATE INDEX idx_content_sets_updated ON content_sets(updated_at);\n```\n\nAlso create a Pydantic model file `app/models/content_sets.py` to define the data models for the API:\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Optional\nfrom datetime import datetime\nimport uuid\n\nclass ContentFileBase(BaseModel):\n    b2_path: str\n    filename: str\n    sequence_number: Optional[int] = None\n    dependencies: List[str] = Field(default_factory=list)\n    estimated_complexity: str = \"medium\"\n    file_type: str = \"\"\n    size_bytes: int = 0\n    metadata: Dict = Field(default_factory=dict)\n\nclass ContentFile(ContentFileBase):\n    id: uuid.UUID\n    content_set_id: uuid.UUID\n    processing_status: str = \"pending\"\n    created_at: datetime\n\nclass ContentFileCreate(ContentFileBase):\n    content_set_id: uuid.UUID\n\nclass ContentSetBase(BaseModel):\n    name: str\n    detection_method: str\n    is_complete: bool = False\n    missing_files: List[str] = Field(default_factory=list)\n    file_count: int\n    metadata: Dict = Field(default_factory=dict)\n\nclass ContentSet(ContentSetBase):\n    id: uuid.UUID\n    processing_status: str = \"pending\"\n    created_at: datetime\n    updated_at: datetime\n\nclass ContentSetCreate(ContentSetBase):\n    pass\n\nclass ProcessingManifest(BaseModel):\n    content_set_id: uuid.UUID\n    ordered_files: List[ContentFile]\n    total_files: int\n    estimated_processing_time: int\n    warnings: List[str] = Field(default_factory=list)\n    context: Dict = Field(default_factory=dict)\n```",
        "testStrategy": "1. Verify SQL schema with test database\n2. Test foreign key constraints\n3. Test index performance with large datasets\n4. Validate Pydantic models with sample data\n5. Test serialization/deserialization of JSON fields\n6. Ensure UUID generation works correctly",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T21:01:44.521Z"
      },
      {
        "id": "124",
        "title": "Implement Content Prep API Routes",
        "description": "Create the API routes for the Content Prep Agent as specified in the PRD, including endpoints for analyzing files, validating content sets, and generating processing manifests.",
        "details": "Create the file `app/routes/content_prep.py` with FastAPI routes for the Content Prep Agent. Implement all the endpoints specified in the PRD:\n\n1. POST /api/content-prep/analyze - Analyze pending files, detect sets\n2. POST /api/content-prep/validate - Validate completeness of content set\n3. POST /api/content-prep/order - Generate processing order\n4. GET /api/content-prep/sets - List detected content sets\n5. GET /api/content-prep/sets/{set_id} - Get content set details\n6. POST /api/content-prep/manifest - Generate processing manifest\n7. GET /api/content-prep/health - Service health check\n\nPseudo-code:\n```python\nfrom fastapi import APIRouter, Depends, HTTPException, BackgroundTasks\nfrom typing import List, Optional\nfrom app.models.content_sets import ContentSet, ContentSetCreate, ProcessingManifest\nfrom app.services.content_prep_agent import ContentPrepAgent\nfrom app.db.supabase import get_supabase_client\n\nrouter = APIRouter(prefix=\"/api/content-prep\", tags=[\"content-prep\"])\ncontent_prep_agent = ContentPrepAgent()\n\n@router.post(\"/analyze\", response_model=dict)\nasync def analyze_pending_files(data: dict):\n    \"\"\"Analyze pending files and detect content sets\"\"\"\n    b2_folder = data.get(\"b2_folder\", \"pending/\")\n    detection_mode = data.get(\"detection_mode\", \"auto\")\n    \n    # Get files from B2\n    # Detect content sets\n    # Return results\n    \n    return {\n        \"content_sets\": [],  # List of detected sets\n        \"standalone_files\": []  # List of files not in sets\n    }\n\n@router.post(\"/validate\", response_model=dict)\nasync def validate_content_set(data: dict):\n    \"\"\"Validate completeness of a content set\"\"\"\n    content_set_id = data.get(\"content_set_id\")\n    \n    # Get content set\n    # Validate completeness\n    # Return validation results\n    \n    return {\n        \"is_complete\": True,\n        \"missing_files\": [],\n        \"warnings\": []\n    }\n\n@router.post(\"/order\", response_model=dict)\nasync def generate_processing_order(data: dict):\n    \"\"\"Generate processing order for a content set\"\"\"\n    content_set_id = data.get(\"content_set_id\")\n    \n    # Get content set\n    # Generate order\n    # Return ordered files\n    \n    return {\n        \"ordered_files\": []\n    }\n\n@router.get(\"/sets\", response_model=List[ContentSet])\nasync def list_content_sets(status: Optional[str] = None):\n    \"\"\"List detected content sets\"\"\"\n    # Query database for content sets\n    # Filter by status if provided\n    # Return list\n    \n    return []\n\n@router.get(\"/sets/{set_id}\", response_model=ContentSet)\nasync def get_content_set(set_id: str):\n    \"\"\"Get content set details\"\"\"\n    # Query database for content set\n    # Return details or 404\n    \n    return {}\n\n@router.post(\"/manifest\", response_model=ProcessingManifest)\nasync def generate_processing_manifest(data: dict):\n    \"\"\"Generate processing manifest for a content set\"\"\"\n    content_set_id = data.get(\"content_set_id\")\n    proceed_incomplete = data.get(\"proceed_incomplete\", False)\n    add_context = data.get(\"add_context\", True)\n    \n    # Get content set\n    # Generate manifest\n    # Return manifest\n    \n    return {}\n\n@router.get(\"/health\")\nasync def health_check():\n    \"\"\"Service health check\"\"\"\n    return {\"status\": \"healthy\"}\n```\n\nAlso update the main FastAPI app to include these routes:\n\n```python\n# In app/main.py\nfrom app.routes import content_prep\n\napp.include_router(content_prep.router)\n```",
        "testStrategy": "1. Unit tests for each API endpoint\n2. Test with valid and invalid request data\n3. Test error handling and edge cases\n4. Integration tests with mocked B2 service\n5. Test response formats match API specifications\n6. Load testing with multiple concurrent requests",
        "priority": "high",
        "dependencies": [
          "122",
          "123"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T21:05:19.254Z"
      },
      {
        "id": "125",
        "title": "Implement Set Detection and Ordering Logic",
        "description": "Implement the core algorithms for detecting content sets, validating completeness, and determining the correct processing order.",
        "details": "Enhance the ContentPrepAgent class with detailed implementations of the set detection and ordering algorithms. This includes:\n\n1. Pattern-based content set detection\n2. Sequence number extraction from filenames\n3. Gap detection in sequences\n4. Chronological ordering logic\n\nPseudo-code for key methods:\n\n```python\nclass ContentPrepAgent:\n    # ... existing code ...\n    \n    def detect_content_sets(self, files, detection_mode=\"auto\"):\n        \"\"\"Detect related content sets from a list of files\"\"\"\n        content_sets = []\n        standalone_files = []\n        \n        # Group files by common prefixes\n        prefix_groups = self._group_by_prefix(files)\n        \n        for prefix, group_files in prefix_groups.items():\n            # Skip single files\n            if len(group_files) <= 1:\n                standalone_files.extend(group_files)\n                continue\n                \n            # Check if group matches content set indicators\n            is_content_set = any(re.search(pattern, prefix, re.IGNORECASE) \n                               for pattern in CONTENT_SET_INDICATORS)\n            \n            # If it's a potential content set or has >3 files with sequence numbers\n            if is_content_set or self._has_sequence_numbers(group_files, threshold=0.7):\n                # Create content set\n                content_set = ContentSet(\n                    id=str(uuid.uuid4()),\n                    name=self._generate_set_name(prefix, group_files),\n                    detection_method=detection_mode,\n                    files=[self._create_content_file(f) for f in group_files],\n                )\n                \n                # Validate completeness\n                self.validate_completeness(content_set)\n                \n                content_sets.append(content_set)\n            else:\n                standalone_files.extend(group_files)\n        \n        return content_sets, standalone_files\n    \n    def _group_by_prefix(self, files):\n        \"\"\"Group files by common prefixes\"\"\"\n        groups = {}\n        \n        for file in files:\n            filename = os.path.basename(file['path'])\n            \n            # Try to find a common prefix\n            prefix = self._extract_prefix(filename)\n            if prefix not in groups:\n                groups[prefix] = []\n            \n            groups[prefix].append(file)\n            \n        return groups\n    \n    def _extract_prefix(self, filename):\n        \"\"\"Extract prefix from filename\"\"\"\n        # Remove extension\n        name = os.path.splitext(filename)[0]\n        \n        # Try to match common content set patterns\n        for pattern in CONTENT_SET_INDICATORS:\n            match = re.search(pattern, name, re.IGNORECASE)\n            if match:\n                return name[:match.end()]\n        \n        # Fall back to first part of name (before first number or separator)\n        match = re.search(r'^([^\\d\\-_\\s]+)', name)\n        if match:\n            return match.group(1)\n            \n        return name\n    \n    def _has_sequence_numbers(self, files, threshold=0.7):\n        \"\"\"Check if files have sequence numbers\"\"\"\n        count = sum(1 for f in files if self.detect_sequence_number(os.path.basename(f['path'])) is not None)\n        return count / len(files) >= threshold\n    \n    def detect_sequence_number(self, filename):\n        \"\"\"Extract sequence number from filename using patterns\"\"\"\n        for pattern in SEQUENCE_PATTERNS:\n            match = re.search(pattern, filename, re.IGNORECASE)\n            if match:\n                # Extract the captured group\n                sequence_str = match.group(1)\n                \n                # Convert to integer if numeric\n                if sequence_str.isdigit():\n                    return int(sequence_str)\n                    \n                # Handle roman numerals\n                if re.match(r'^(i{1,3}|iv|v|vi{0,3}|ix|x)$', sequence_str, re.IGNORECASE):\n                    return self._roman_to_int(sequence_str.lower())\n                    \n                # Handle alpha sequence (a, b, c...)\n                if len(sequence_str) == 1 and sequence_str.isalpha():\n                    return ord(sequence_str.lower()) - ord('a') + 1\n        \n        return None\n    \n    def _roman_to_int(self, s):\n        \"\"\"Convert Roman numeral to integer\"\"\"\n        roman_map = {'i': 1, 'v': 5, 'x': 10}\n        result = 0\n        \n        for i in range(len(s)):\n            if i > 0 and roman_map[s[i]] > roman_map[s[i-1]]:\n                result += roman_map[s[i]] - 2 * roman_map[s[i-1]]\n            else:\n                result += roman_map[s[i]]\n                \n        return result\n    \n    def validate_completeness(self, content_set):\n        \"\"\"Check for missing files in sequence\"\"\"\n        # Get sequence numbers\n        sequence_numbers = []\n        for file in content_set.files:\n            if file.sequence_number is not None:\n                sequence_numbers.append(file.sequence_number)\n        \n        if not sequence_numbers:\n            # No sequence numbers detected\n            content_set.is_complete = True\n            return content_set\n        \n        # Find min and max sequence\n        min_seq = min(sequence_numbers)\n        max_seq = max(sequence_numbers)\n        \n        # Check for gaps\n        expected_range = set(range(min_seq, max_seq + 1))\n        actual_set = set(sequence_numbers)\n        missing = expected_range - actual_set\n        \n        if missing:\n            content_set.is_complete = False\n            content_set.missing_files = [f\"Missing sequence {num}\" for num in missing]\n        else:\n            content_set.is_complete = True\n            content_set.missing_files = []\n        \n        return content_set\n    \n    def resolve_order(self, content_set):\n        \"\"\"Determine the correct processing order\"\"\"\n        # Sort files by sequence number if available\n        files_with_sequence = []\n        files_without_sequence = []\n        \n        for file in content_set.files:\n            if file.sequence_number is not None:\n                files_with_sequence.append(file)\n            else:\n                files_without_sequence.append(file)\n        \n        # Sort by sequence number\n        files_with_sequence.sort(key=lambda f: f.sequence_number)\n        \n        # For files without sequence, try to use creation date or name\n        files_without_sequence.sort(key=lambda f: f.filename)\n        \n        # Combine the lists\n        ordered_files = files_with_sequence + files_without_sequence\n        \n        return ordered_files\n```",
        "testStrategy": "1. Unit tests for each algorithm (prefix grouping, sequence detection, gap detection)\n2. Test with various file naming patterns\n3. Test with edge cases (no sequence numbers, mixed formats)\n4. Test completeness validation with different gap scenarios\n5. Benchmark performance with large file sets\n6. Test with real-world examples of course materials",
        "priority": "high",
        "dependencies": [
          "122"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement analyze_folder() method for content set detection",
            "description": "Create the analyze_folder() method that scans a directory and identifies potential content sets based on file patterns.",
            "dependencies": [],
            "details": "Implement the analyze_folder() method in ContentPrepAgent class that takes a folder path as input, scans all files, and calls detect_content_sets() with the file list. Include logic to handle large directories by processing files in batches. The method should return a tuple of (content_sets, standalone_files).",
            "status": "pending",
            "testStrategy": "Unit test with various folder structures, test with empty folders, test with folders containing only standalone files, test with mixed content sets and standalone files.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement _create_content_set() builder method",
            "description": "Create a helper method to build ContentSet objects with proper metadata extraction and initialization.",
            "dependencies": [
              1
            ],
            "details": "Implement the _create_content_set() method that takes a group of files and creates a properly initialized ContentSet object. This includes generating a unique ID, determining an appropriate name based on common prefixes, setting the detection method, and initializing the files list with proper metadata for each file.",
            "status": "pending",
            "testStrategy": "Test with various file naming patterns, verify correct ID generation, test name generation with different prefix patterns, verify all file metadata is correctly extracted.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement sequence number extraction from filenames",
            "description": "Create robust methods to detect and extract sequence numbers from various filename formats.",
            "dependencies": [
              2
            ],
            "details": "Implement the detect_sequence_number() method to extract sequence numbers from filenames using regular expressions. Handle numeric sequences, roman numerals, and alphabetic sequences. Include the _roman_to_int() helper method for converting roman numerals to integers. Ensure the method can handle various delimiter patterns and position of sequence numbers in filenames.",
            "status": "pending",
            "testStrategy": "Test with various filename patterns including numeric sequences (1, 2, 3), roman numerals (i, ii, iii), alphabetic sequences (a, b, c), test with different delimiters, test with sequence numbers in different positions.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement gap detection for missing files in sequences",
            "description": "Create the validate_completeness() method to identify gaps in file sequences and mark content sets as incomplete when files are missing.",
            "dependencies": [
              3
            ],
            "details": "Implement the validate_completeness() method that analyzes a content set's files to detect missing sequence numbers. The method should extract sequence numbers from all files, determine the expected range, identify any gaps, and update the content set's is_complete flag and missing_files list accordingly. Handle edge cases where no sequence numbers are detected.",
            "status": "pending",
            "testStrategy": "Test with complete sequences, test with sequences missing files in the middle, at the beginning, and at the end, test with non-sequential files, test with mixed content types.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement chronological ordering logic for content sets",
            "description": "Create the resolve_order() method to determine the correct processing order for files within a content set.",
            "dependencies": [
              3,
              4
            ],
            "details": "Implement the resolve_order() method that sorts files within a content set based on sequence numbers when available. For files without sequence numbers, implement fallback ordering based on creation date or filename. The method should return an ordered list of files and handle mixed cases where some files have sequence numbers and others don't.",
            "status": "pending",
            "testStrategy": "Test ordering with sequence numbers, test fallback to creation date, test fallback to filename, test with mixed file types, verify correct ordering with various sequence patterns.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement batched analysis for large file sets",
            "description": "Enhance the content set detection to handle large directories by processing files in batches and merging results.",
            "dependencies": [
              1,
              2,
              5
            ],
            "details": "Modify the analyze_folder() method to process large directories (>100 files) in batches to prevent memory issues. Implement a merge_content_sets() helper method that combines results from multiple batches while preserving chronological ordering. Include progress tracking and logging for batch processing. Ensure the final merged result maintains all metadata and relationships between files.",
            "status": "pending",
            "testStrategy": "Test with directories containing >100 files, verify memory usage remains stable, test merging of content sets across batches, verify chronological ordering is preserved after merging, test with edge cases like all files belonging to one content set split across batches.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-13T21:06:53.806Z"
      },
      {
        "id": "126",
        "title": "Implement Processing Manifest Generation",
        "description": "Create the functionality to generate processing manifests that specify the ordered processing queue, dependencies, and context for downstream agents.",
        "details": "Enhance the ContentPrepAgent class with methods to generate processing manifests. This includes:\n\n1. Creating an ordered processing queue\n2. Identifying dependencies between files\n3. Estimating processing complexity and time\n4. Adding context metadata for downstream agents\n\nPseudo-code:\n\n```python\nclass ContentPrepAgent:\n    # ... existing code ...\n    \n    def generate_manifest(self, content_set, proceed_incomplete=False):\n        \"\"\"Create processing manifest with ordered files\"\"\"\n        # Check if content set is complete\n        if not content_set.is_complete and not proceed_incomplete:\n            raise ValueError(\"Content set is incomplete. Set proceed_incomplete=True to generate manifest anyway.\")\n        \n        # Get ordered files\n        ordered_files = self.resolve_order(content_set)\n        \n        # Identify dependencies\n        self._identify_dependencies(ordered_files)\n        \n        # Estimate processing time\n        estimated_time = self.estimate_processing_time(ordered_files)\n        \n        # Create manifest\n        manifest = ProcessingManifest(\n            content_set_id=content_set.id,\n            ordered_files=ordered_files,\n            total_files=len(ordered_files),\n            estimated_processing_time=estimated_time,\n            warnings=content_set.missing_files if not content_set.is_complete else [],\n            context={\n                \"content_set_name\": content_set.name,\n                \"is_complete\": content_set.is_complete,\n                \"detection_method\": content_set.detection_method,\n                \"total_files\": len(ordered_files),\n            }\n        )\n        \n        return manifest\n    \n    def _identify_dependencies(self, ordered_files):\n        \"\"\"Identify dependencies between files based on order\"\"\"\n        # Simple sequential dependencies - each file depends on the previous one\n        for i in range(1, len(ordered_files)):\n            prev_file = ordered_files[i-1]\n            curr_file = ordered_files[i]\n            \n            # Add dependency on previous file\n            curr_file.dependencies.append(prev_file.b2_path)\n    \n    def estimate_processing_time(self, files):\n        \"\"\"Estimate processing time based on file types and sizes\"\"\"\n        total_time = 0\n        \n        for file in files:\n            # Base time by file type\n            if file.file_type.lower() == \"pdf\":\n                base_time = 30  # 30 seconds base for PDF\n            elif file.file_type.lower() in [\"docx\", \"doc\"]:\n                base_time = 25  # 25 seconds base for Word docs\n            elif file.file_type.lower() in [\"txt\", \"md\"]:\n                base_time = 10  # 10 seconds base for text files\n            else:\n                base_time = 20  # Default base time\n            \n            # Adjust by file size (very simple heuristic)\n            size_factor = max(1, file.size_bytes / (1024 * 1024))  # Size in MB, minimum 1\n            \n            # Adjust by complexity\n            complexity_factor = 1.0\n            if file.estimated_complexity == \"high\":\n                complexity_factor = 1.5\n            elif file.estimated_complexity == \"low\":\n                complexity_factor = 0.8\n            \n            # Calculate time for this file\n            file_time = base_time * size_factor * complexity_factor\n            total_time += file_time\n        \n        return int(total_time)  # Return as integer seconds\n    \n    def estimate_file_complexity(self, file):\n        \"\"\"Estimate file complexity based on size, type, and content\"\"\"\n        # Simple heuristic based on file size\n        if file.size_bytes > 5 * 1024 * 1024:  # > 5MB\n            return \"high\"\n        elif file.size_bytes < 100 * 1024:  # < 100KB\n            return \"low\"\n        else:\n            return \"medium\"\n```",
        "testStrategy": "1. Unit tests for manifest generation\n2. Test dependency identification with different file orders\n3. Test processing time estimation with various file types and sizes\n4. Verify manifest structure matches expected schema\n5. Test error handling for incomplete sets\n6. Test with large file sets to ensure performance",
        "priority": "medium",
        "dependencies": [
          "122",
          "125"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T21:06:53.847Z"
      },
      {
        "id": "127",
        "title": "Integrate with B2 Workflow Service",
        "description": "Integrate the Content Prep Agent with the B2 Workflow Service to intercept file uploads and trigger content preparation.",
        "details": "Modify the existing B2 Workflow Service to hook into the Content Prep Agent. This involves:\n\n1. Intercepting new file uploads to the `pending/` folder\n2. Triggering content set detection and analysis\n3. Moving files to `processing/` in the correct order based on the manifest\n\nPseudo-code for the integration:\n\n```python\n# In app/services/b2_workflow.py\n\nfrom app.services.content_prep_agent import ContentPrepAgent\n\nclass B2WorkflowService:\n    # ... existing code ...\n    \n    def __init__(self):\n        # ... existing init ...\n        self.content_prep_agent = ContentPrepAgent()\n    \n    async def process_pending_folder(self):\n        \"\"\"Process files in the pending folder\"\"\"\n        # Get files from pending folder\n        pending_files = await self.list_pending_files()\n        \n        if not pending_files:\n            return\n        \n        # Check if we need content preparation\n        if len(pending_files) > 1:\n            # Detect content sets\n            content_sets, standalone_files = self.content_prep_agent.detect_content_sets(pending_files)\n            \n            # Process standalone files immediately\n            for file in standalone_files:\n                await self.move_to_processing(file['path'])\n                await self.trigger_processing(file['path'])\n            \n            # Process content sets\n            for content_set in content_sets:\n                # Store content set in database\n                await self.store_content_set(content_set)\n                \n                # Check completeness\n                if not content_set.is_complete:\n                    # Log warning about incomplete set\n                    self.logger.warning(f\"Incomplete content set detected: {content_set.name}\")\n                    # Send notification to user (implementation depends on notification system)\n                    await self.notify_incomplete_set(content_set)\n                    # Skip processing until user acknowledges\n                    continue\n                \n                # Generate manifest\n                manifest = self.content_prep_agent.generate_manifest(content_set)\n                \n                # Process files in order\n                for file in manifest.ordered_files:\n                    await self.move_to_processing(file.b2_path)\n                    # Pass content set context to processing task\n                    await self.trigger_processing(file.b2_path, context=manifest.context)\n        else:\n            # Single file - process immediately\n            file = pending_files[0]\n            await self.move_to_processing(file['path'])\n            await self.trigger_processing(file['path'])\n    \n    async def store_content_set(self, content_set):\n        \"\"\"Store content set in database\"\"\"\n        # Implementation depends on database access method\n        pass\n    \n    async def notify_incomplete_set(self, content_set):\n        \"\"\"Notify user about incomplete content set\"\"\"\n        # Implementation depends on notification system\n        pass\n    \n    async def trigger_processing(self, file_path, context=None):\n        \"\"\"Trigger processing task for a file\"\"\"\n        # ... existing code ...\n        \n        # Add content set context if available\n        task_data = {\n            \"file_path\": file_path,\n            # ... existing task data ...\n        }\n        \n        if context:\n            task_data[\"content_set_context\"] = context\n        \n        # Trigger Celery task\n        # ... existing code ...\n```\n\nAlso create a Celery task for content set detection and processing:\n\n```python\n# In app/tasks/content_prep_tasks.py\n\nfrom celery import shared_task\nfrom app.services.content_prep_agent import ContentPrepAgent\n\n@shared_task\ndef detect_content_sets(b2_folder):\n    \"\"\"Detect content sets in a B2 folder\"\"\"\n    agent = ContentPrepAgent()\n    \n    # List files in folder\n    # Detect content sets\n    # Store in database\n    \n    return {\"content_sets\": [], \"standalone_files\": []}\n\n@shared_task\ndef process_content_set(content_set_id, proceed_incomplete=False):\n    \"\"\"Process a content set\"\"\"\n    agent = ContentPrepAgent()\n    \n    # Get content set from database\n    # Generate manifest\n    # Trigger processing for each file in order\n    \n    return {\"status\": \"processing\", \"files_count\": 0}\n```",
        "testStrategy": "1. Integration tests with mocked B2 service\n2. Test single file pass-through\n3. Test multi-file content set detection\n4. Test incomplete set handling and notification\n5. Verify correct processing order\n6. Test with various file naming patterns\n7. Test error handling and recovery",
        "priority": "high",
        "dependencies": [
          "122",
          "125",
          "126"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T21:10:04.660Z"
      },
      {
        "id": "128",
        "title": "Implement Source Processing Task Integration",
        "description": "Modify the existing Source Processing Task (Celery) to accept processing manifests and handle content set context.",
        "details": "Update the Source Processing Task to work with the Content Prep Agent by:\n\n1. Accepting manifest-based processing instructions\n2. Handling content set context metadata\n3. Passing context to downstream services (chunking, embedding)\n\nPseudo-code for the integration:\n\n```python\n# In app/tasks/source_processing.py\n\nfrom celery import shared_task\nfrom app.services.content_prep_agent import ContentPrepAgent\n\n@shared_task\ndef process_source(file_path, **kwargs):\n    \"\"\"Process a source file\"\"\"\n    # ... existing code ...\n    \n    # Check for content set context\n    content_set_context = kwargs.get(\"content_set_context\")\n    \n    # Process the file\n    result = process_file(file_path)\n    \n    # Pass content set context to chunking task\n    if content_set_context:\n        # Add content set context to chunking task\n        chunking_task.delay(result[\"processed_path\"], content_set_context=content_set_context)\n    else:\n        # Normal processing without content set context\n        chunking_task.delay(result[\"processed_path\"])\n    \n    return result\n\n# Update the chunking task to accept content set context\n@shared_task\ndef chunking_task(processed_path, **kwargs):\n    \"\"\"Chunk a processed file\"\"\"\n    # ... existing code ...\n    \n    # Check for content set context\n    content_set_context = kwargs.get(\"content_set_context\")\n    \n    # Process chunks\n    chunks = chunk_document(processed_path)\n    \n    # Add content set metadata to chunks if available\n    if content_set_context:\n        for chunk in chunks:\n            chunk.metadata[\"content_set\"] = content_set_context.get(\"content_set_name\")\n            chunk.metadata[\"content_set_id\"] = content_set_context.get(\"content_set_id\")\n            chunk.metadata[\"is_part_of_sequence\"] = True\n    \n    # Pass to embedding task with context\n    embedding_task.delay(chunks, content_set_context=content_set_context if content_set_context else None)\n    \n    return {\"chunks_count\": len(chunks)}\n```\n\nAlso update the knowledge graph integration to create relationships for content sets:\n\n```python\n# In app/services/knowledge_graph.py\n\nclass KnowledgeGraphService:\n    # ... existing code ...\n    \n    def add_document_node(self, document, **kwargs):\n        \"\"\"Add a document node to the knowledge graph\"\"\"\n        # ... existing code ...\n        \n        # Check for content set context\n        content_set_context = kwargs.get(\"content_set_context\")\n        \n        if content_set_context:\n            # Create content set node if it doesn't exist\n            self.create_content_set_node(content_set_context)\n            \n            # Create PART_OF relationship\n            self.create_part_of_relationship(document.id, content_set_context.get(\"content_set_id\"))\n            \n            # Create PRECEDES/FOLLOWS relationships if this document has dependencies\n            for dependency in document.get(\"dependencies\", []):\n                self.create_dependency_relationship(document.id, dependency)\n    \n    def create_content_set_node(self, content_set_context):\n        \"\"\"Create a ContentSet node in Neo4j\"\"\"\n        query = \"\"\"\n        MERGE (cs:ContentSet {id: $id})\n        ON CREATE SET \n            cs.name = $name,\n            cs.is_complete = $is_complete,\n            cs.total_files = $total_files,\n            cs.created_at = datetime()\n        \"\"\"\n        \n        params = {\n            \"id\": content_set_context.get(\"content_set_id\"),\n            \"name\": content_set_context.get(\"content_set_name\"),\n            \"is_complete\": content_set_context.get(\"is_complete\", False),\n            \"total_files\": content_set_context.get(\"total_files\", 0)\n        }\n        \n        self.graph.run(query, params)\n    \n    def create_part_of_relationship(self, document_id, content_set_id):\n        \"\"\"Create PART_OF relationship between document and content set\"\"\"\n        query = \"\"\"\n        MATCH (d:Document {id: $document_id})\n        MATCH (cs:ContentSet {id: $content_set_id})\n        MERGE (d)-[:PART_OF]->(cs)\n        \"\"\"\n        \n        params = {\n            \"document_id\": document_id,\n            \"content_set_id\": content_set_id\n        }\n        \n        self.graph.run(query, params)\n    \n    def create_dependency_relationship(self, document_id, dependency_id):\n        \"\"\"Create FOLLOWS/PRECEDES relationships between documents\"\"\"\n        query = \"\"\"\n        MATCH (d1:Document {id: $document_id})\n        MATCH (d2:Document {id: $dependency_id})\n        MERGE (d1)-[:FOLLOWS]->(d2)\n        MERGE (d2)-[:PRECEDES]->(d1)\n        \"\"\"\n        \n        params = {\n            \"document_id\": document_id,\n            \"dependency_id\": dependency_id\n        }\n        \n        self.graph.run(query, params)\n```",
        "testStrategy": "1. Integration tests with mocked Celery tasks\n2. Test context propagation through the pipeline\n3. Verify Neo4j relationships are created correctly\n4. Test with various content set scenarios\n5. Test error handling and recovery\n6. Verify metadata is preserved throughout processing",
        "priority": "medium",
        "dependencies": [
          "122",
          "127"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T21:12:14.216Z"
      },
      {
        "id": "129",
        "title": "Implement Chat-Based Ordering Clarification",
        "description": "Implement the chat-based ordering clarification feature that allows the Content Prep Agent to ask users questions via CKO Chat when ordering confidence is below threshold.",
        "details": "Create functionality for the Content Prep Agent to interact with users via CKO Chat when it needs clarification about file ordering. This includes:\n\n1. Detecting when ordering confidence is below threshold\n2. Sending clarification messages to CKO Chat\n3. Parsing user responses\n4. Updating ordering based on user input\n\nPseudo-code:\n\n```python\nclass ContentPrepAgent:\n    # ... existing code ...\n    \n    async def resolve_order_with_clarification(self, content_set, confidence_threshold=0.8):\n        \"\"\"Resolve order with user clarification if needed\"\"\"\n        # Try automatic ordering first\n        ordered_files = self.resolve_order(content_set)\n        \n        # Calculate confidence in ordering\n        confidence = self._calculate_ordering_confidence(ordered_files)\n        \n        if confidence < confidence_threshold:\n            # Need user clarification\n            clarification = await self._request_user_clarification(content_set, ordered_files)\n            \n            if clarification:\n                # Update ordering based on user input\n                ordered_files = self._update_ordering(ordered_files, clarification)\n        \n        return ordered_files\n    \n    def _calculate_ordering_confidence(self, ordered_files):\n        \"\"\"Calculate confidence in the ordering\"\"\"\n        # Count files with explicit sequence numbers\n        files_with_sequence = sum(1 for f in ordered_files if f.sequence_number is not None)\n        total_files = len(ordered_files)\n        \n        # Base confidence on percentage of files with sequence numbers\n        if total_files == 0:\n            return 1.0\n        \n        return files_with_sequence / total_files\n    \n    async def _request_user_clarification(self, content_set, ordered_files):\n        \"\"\"Request clarification from user via CKO Chat\"\"\"\n        from app.services.cko_chat import CKOChatService\n        \n        chat_service = CKOChatService()\n        \n        # Identify ambiguous files (those without sequence numbers)\n        ambiguous_files = [f for f in ordered_files if f.sequence_number is None]\n        \n        if not ambiguous_files:\n            return None\n        \n        # Create clarification message\n        message = f\"I'm processing your content set '{content_set.name}' and need help with the correct order. \"\n        message += \"I've detected these files without clear sequence indicators:\\n\"\n        \n        for i, file in enumerate(ambiguous_files[:5]):  # Limit to 5 files to avoid long messages\n            message += f\"- {file.filename}\\n\"\n        \n        if len(ambiguous_files) > 5:\n            message += f\"...and {len(ambiguous_files) - 5} more.\\n\"\n        \n        message += \"\\nCould you please tell me the correct order for these files? \"\n        message += \"You can respond with file names in the desired order, or with instructions like 'File A comes before File B'.\"\n        \n        # Send message to CKO Chat\n        chat_id = await chat_service.send_agent_message(\n            agent_id=\"AGENT-016\",\n            user_id=content_set.metadata.get(\"user_id\"),\n            message=message\n        )\n        \n        # Wait for user response (with timeout)\n        response = await chat_service.wait_for_user_response(chat_id, timeout=3600)  # 1 hour timeout\n        \n        if not response:\n            # No response within timeout\n            return None\n        \n        # Log the conversation for audit trail\n        await self._log_clarification_conversation(content_set.id, message, response)\n        \n        return response\n    \n    def _update_ordering(self, ordered_files, clarification):\n        \"\"\"Update ordering based on user clarification\"\"\"\n        # This would use LLM to parse the user's response and update the ordering\n        # For simplicity, we'll assume the response is a comma-separated list of filenames\n        \n        # Create a map of filename to file object\n        file_map = {f.filename: f for f in ordered_files}\n        \n        # Try to extract filenames from the response\n        # This is a simplified approach - in reality, you'd use an LLM to parse natural language\n        filenames = [name.strip() for name in clarification.split(',')]\n        \n        # Create new ordered list based on user input\n        new_order = []\n        for filename in filenames:\n            if filename in file_map:\n                new_order.append(file_map[filename])\n                # Remove from map to avoid duplicates\n                del file_map[filename]\n        \n        # Add any remaining files at the end\n        new_order.extend(file_map.values())\n        \n        return new_order\n    \n    async def _log_clarification_conversation(self, content_set_id, question, answer):\n        \"\"\"Log the clarification conversation for audit trail\"\"\"\n        # Implementation depends on logging system\n        pass\n```\n\nCreate a simple CKO Chat service interface:\n\n```python\n# In app/services/cko_chat.py\n\nclass CKOChatService:\n    \"\"\"Interface to the CKO Chat system\"\"\"\n    \n    async def send_agent_message(self, agent_id, user_id, message):\n        \"\"\"Send a message from an agent to a user\"\"\"\n        # Implementation depends on chat system\n        # This would create a new chat or add to existing chat\n        # Return chat ID\n        pass\n    \n    async def wait_for_user_response(self, chat_id, timeout=3600):\n        \"\"\"Wait for user response with timeout\"\"\"\n        # Implementation depends on chat system\n        # This would poll or use websockets to wait for response\n        # Return user message or None if timeout\n        pass\n```",
        "testStrategy": "1. Unit tests for ordering confidence calculation\n2. Test message generation for different ambiguous file scenarios\n3. Test response parsing with various user input formats\n4. Integration tests with mocked CKO Chat service\n5. Test timeout handling\n6. Verify audit trail logging\n7. Test end-to-end flow with simulated user responses",
        "priority": "medium",
        "dependencies": [
          "122",
          "125"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement ordering confidence calculation method",
            "description": "Create the _calculate_ordering_confidence() method that determines when user clarification is needed",
            "dependencies": [],
            "details": "Implement the method that calculates confidence based on the percentage of files with explicit sequence numbers. Set the threshold at 80% as specified. Include edge case handling for empty file sets and ensure the method returns a value between 0 and 1.",
            "status": "pending",
            "testStrategy": "Unit test with various file sets: all files with sequence numbers, mixed sets, empty sets. Verify threshold behavior at boundary conditions (79% vs 81%).",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create CKOChatService interface",
            "description": "Implement the CKOChatService class with methods for agent-user communication",
            "dependencies": [],
            "details": "Create the CKOChatService class in app/services/cko_chat.py with methods for sending agent messages and waiting for user responses. Implement timeout handling for response waiting and proper error handling for communication failures.",
            "status": "pending",
            "testStrategy": "Mock the underlying chat system API. Test successful message sending, response waiting with and without timeouts, and error handling scenarios.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement clarification message generation",
            "description": "Create logic to generate clear, user-friendly clarification messages about ambiguous file ordering",
            "dependencies": [
              1
            ],
            "details": "Implement the first part of _request_user_clarification() that identifies ambiguous files and creates a well-formatted message explaining the issue to the user. Include file listing with truncation for large sets and clear instructions on how to respond.",
            "status": "pending",
            "testStrategy": "Test message generation with various file sets, including edge cases like all files being ambiguous or only one ambiguous file. Verify message formatting and clarity.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement chat integration for sending clarification requests",
            "description": "Connect the Content Prep Agent to CKO Chat for sending clarification requests to users",
            "dependencies": [
              2,
              3
            ],
            "details": "Complete the _request_user_clarification() method to send the generated clarification message to users via the CKOChatService. Implement proper error handling for failed message delivery and configure the agent ID ('AGENT-016') and user ID extraction from content set metadata.",
            "status": "pending",
            "testStrategy": "Integration test with mocked CKOChatService. Verify correct message delivery, proper agent and user ID handling, and error case management.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement natural language response parsing",
            "description": "Create logic to interpret user responses to ordering clarification requests",
            "dependencies": [
              4
            ],
            "details": "Implement the _update_ordering() method that parses user responses and updates file ordering accordingly. Handle various response formats including comma-separated lists, natural language descriptions ('file A before file B'), and numbered lists. Use pattern matching or LLM-based parsing as appropriate.",
            "status": "pending",
            "testStrategy": "Test with various response formats including comma-separated lists, natural language instructions, and edge cases like partial responses or responses with unknown files.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement clarification conversation logging",
            "description": "Create audit trail logging for clarification conversations",
            "dependencies": [
              4
            ],
            "details": "Implement the _log_clarification_conversation() method to record all clarification questions and user responses for audit purposes. Store conversation details including timestamps, content set ID, question text, response text, and outcome (whether ordering was successfully updated).",
            "status": "pending",
            "testStrategy": "Verify log entries are created with correct data. Test with various conversation scenarios including successful clarifications and timeouts.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Integrate all components in resolve_order_with_clarification method",
            "description": "Connect all components in the main workflow method that handles the entire clarification process",
            "dependencies": [
              1,
              5,
              6
            ],
            "details": "Implement the resolve_order_with_clarification() method that orchestrates the entire process: calculating confidence, requesting clarification when needed, waiting for and processing responses, updating ordering, and returning the final ordered files. Include proper state management to pause processing while awaiting responses.",
            "status": "pending",
            "testStrategy": "End-to-end testing with mocked dependencies. Test the full workflow with scenarios above and below confidence threshold. Verify correct handling of timeouts and user responses.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-13T21:19:08.764Z"
      },
      {
        "id": "130",
        "title": "Implement Retention Policy and Cleanup",
        "description": "Implement the 90-day retention policy for content set metadata and create a scheduled cleanup task.",
        "details": "Create a scheduled task to enforce the 90-day retention policy for content set metadata. This includes:\n\n1. Creating a Celery periodic task for cleanup\n2. Implementing the database cleanup logic\n3. Adding logging and monitoring\n\nPseudo-code:\n\n```python\n# In app/tasks/scheduled_tasks.py\n\nfrom celery import shared_task\nfrom celery.schedules import crontab\nfrom app.db.supabase import get_supabase_client\nfrom datetime import datetime, timedelta\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Register as a periodic task to run daily at 3 AM\n@shared_task\ndef cleanup_content_sets():\n    \"\"\"Clean up content sets older than 90 days\"\"\"\n    logger.info(\"Starting content set cleanup task\")\n    \n    try:\n        # Calculate cutoff date (90 days ago)\n        cutoff_date = datetime.now() - timedelta(days=90)\n        \n        # Get Supabase client\n        supabase = get_supabase_client()\n        \n        # Find completed content sets older than 90 days\n        response = supabase.table(\"content_sets\") \\\n            .select(\"id\") \\\n            .eq(\"processing_status\", \"complete\") \\\n            .lt(\"updated_at\", cutoff_date.isoformat()) \\\n            .execute()\n        \n        if response.data:\n            content_set_ids = [item[\"id\"] for item in response.data]\n            count = len(content_set_ids)\n            logger.info(f\"Found {count} content sets to clean up\")\n            \n            # Delete in batches to avoid timeout\n            batch_size = 50\n            for i in range(0, count, batch_size):\n                batch = content_set_ids[i:i+batch_size]\n                \n                # Delete content set files first (due to foreign key constraint)\n                supabase.table(\"content_set_files\") \\\n                    .delete() \\\n                    .in_(\"content_set_id\", batch) \\\n                    .execute()\n                \n                # Delete content sets\n                supabase.table(\"content_sets\") \\\n                    .delete() \\\n                    .in_(\"id\", batch) \\\n                    .execute()\n                \n                logger.info(f\"Deleted batch {i//batch_size + 1} ({len(batch)} content sets)\")\n            \n            return {\"status\": \"success\", \"deleted_count\": count}\n        else:\n            logger.info(\"No content sets to clean up\")\n            return {\"status\": \"success\", \"deleted_count\": 0}\n    \n    except Exception as e:\n        logger.error(f\"Error in content set cleanup: {str(e)}\")\n        return {\"status\": \"error\", \"error\": str(e)}\n```\n\nRegister the periodic task in Celery beat schedule:\n\n```python\n# In app/celery_app.py\n\napp.conf.beat_schedule = {\n    # ... existing scheduled tasks ...\n    \n    'cleanup-content-sets-daily': {\n        'task': 'app.tasks.scheduled_tasks.cleanup_content_sets',\n        'schedule': crontab(hour=3, minute=0),  # Run at 3:00 AM\n    },\n}\n```\n\nAdd monitoring and metrics:\n\n```python\n# In app/monitoring/metrics.py\n\nfrom prometheus_client import Counter, Gauge\n\n# Counters for content set operations\ncontent_sets_created = Counter('content_sets_created', 'Number of content sets created')\ncontent_sets_processed = Counter('content_sets_processed', 'Number of content sets processed')\ncontent_sets_deleted = Counter('content_sets_deleted', 'Number of content sets deleted by retention policy')\n\n# Gauges for current state\ncontent_sets_pending = Gauge('content_sets_pending', 'Number of pending content sets')\ncontent_sets_processing = Gauge('content_sets_processing', 'Number of processing content sets')\ncontent_sets_complete = Gauge('content_sets_complete', 'Number of complete content sets')\n```",
        "testStrategy": "1. Unit tests for cleanup logic\n2. Test with mock database responses\n3. Test date calculation and filtering\n4. Test batch processing\n5. Test error handling\n6. Integration test with test database\n7. Verify metrics are updated correctly",
        "priority": "low",
        "dependencies": [
          "123"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T21:23:08.753Z"
      },
      {
        "id": "131",
        "title": "Create Comprehensive Test Suite",
        "description": "Create a comprehensive test suite for the Content Prep Agent, including unit tests, integration tests, and performance benchmarks.",
        "details": "Create the file `tests/test_content_prep_agent.py` with a comprehensive test suite for the Content Prep Agent. This includes:\n\n1. Unit tests for each component (set detection, ordering, manifest generation)\n2. Integration tests with mocked dependencies\n3. Performance benchmarks for large file sets\n4. Test cases for all user stories\n\nPseudo-code:\n\n```python\n# In tests/test_content_prep_agent.py\n\nimport unittest\nfrom unittest.mock import patch, MagicMock\nimport pytest\nfrom app.services.content_prep_agent import ContentPrepAgent, ContentSet, ContentFile, ProcessingManifest\n\nclass TestContentPrepAgent(unittest.TestCase):\n    def setUp(self):\n        self.agent = ContentPrepAgent()\n        \n        # Sample test files\n        self.test_files = [\n            {\"path\": \"pending/course/01-intro.pdf\", \"size\": 1024 * 1024, \"type\": \"pdf\"},\n            {\"path\": \"pending/course/02-basics.pdf\", \"size\": 2 * 1024 * 1024, \"type\": \"pdf\"},\n            {\"path\": \"pending/course/03-advanced.pdf\", \"size\": 3 * 1024 * 1024, \"type\": \"pdf\"},\n            {\"path\": \"pending/course/10-appendix.pdf\", \"size\": 500 * 1024, \"type\": \"pdf\"},\n            {\"path\": \"pending/random.txt\", \"size\": 10 * 1024, \"type\": \"txt\"},\n        ]\n    \n    def test_detect_sequence_number(self):\n        \"\"\"Test sequence number detection from filenames\"\"\"\n        test_cases = [\n            (\"01-intro.pdf\", 1),\n            (\"chapter-02-basics.pdf\", 2),\n            (\"module03.pdf\", 3),\n            (\"lesson_4_advanced.pdf\", 4),\n            (\"part-v-conclusion.pdf\", 5),  # Roman numeral\n            (\"a-intro.pdf\", 1),  # Alpha sequence\n            (\"random.pdf\", None),  # No sequence\n        ]\n        \n        for filename, expected in test_cases:\n            result = self.agent.detect_sequence_number(filename)\n            self.assertEqual(result, expected, f\"Failed for {filename}\")\n    \n    def test_detect_content_sets(self):\n        \"\"\"Test content set detection\"\"\"\n        content_sets, standalone = self.agent.detect_content_sets(self.test_files)\n        \n        # Should detect one content set with 4 files\n        self.assertEqual(len(content_sets), 1)\n        self.assertEqual(len(content_sets[0].files), 4)\n        \n        # Should have one standalone file\n        self.assertEqual(len(standalone), 1)\n        self.assertEqual(standalone[0][\"path\"], \"pending/random.txt\")\n    \n    def test_validate_completeness(self):\n        \"\"\"Test completeness validation\"\"\"\n        # Create a content set with files 1, 2, 4 (missing 3)\n        files = [\n            ContentFile(b2_path=\"file1.pdf\", filename=\"file1.pdf\", sequence_number=1),\n            ContentFile(b2_path=\"file2.pdf\", filename=\"file2.pdf\", sequence_number=2),\n            ContentFile(b2_path=\"file4.pdf\", filename=\"file4.pdf\", sequence_number=4),\n        ]\n        \n        content_set = ContentSet(\n            id=\"test-id\",\n            name=\"Test Set\",\n            detection_method=\"pattern\",\n            files=files\n        )\n        \n        # Validate completeness\n        self.agent.validate_completeness(content_set)\n        \n        # Should be incomplete with missing file 3\n        self.assertFalse(content_set.is_complete)\n        self.assertEqual(len(content_set.missing_files), 1)\n        \n        # Add the missing file and revalidate\n        content_set.files.append(\n            ContentFile(b2_path=\"file3.pdf\", filename=\"file3.pdf\", sequence_number=3)\n        )\n        \n        self.agent.validate_completeness(content_set)\n        \n        # Should now be complete\n        self.assertTrue(content_set.is_complete)\n        self.assertEqual(len(content_set.missing_files), 0)\n    \n    def test_resolve_order(self):\n        \"\"\"Test ordering resolution\"\"\"\n        # Create a content set with files in random order\n        files = [\n            ContentFile(b2_path=\"file3.pdf\", filename=\"file3.pdf\", sequence_number=3),\n            ContentFile(b2_path=\"file1.pdf\", filename=\"file1.pdf\", sequence_number=1),\n            ContentFile(b2_path=\"file2.pdf\", filename=\"file2.pdf\", sequence_number=2),\n        ]\n        \n        content_set = ContentSet(\n            id=\"test-id\",\n            name=\"Test Set\",\n            detection_method=\"pattern\",\n            files=files\n        )\n        \n        # Resolve order\n        ordered_files = self.agent.resolve_order(content_set)\n        \n        # Should be ordered by sequence number\n        self.assertEqual(ordered_files[0].sequence_number, 1)\n        self.assertEqual(ordered_files[1].sequence_number, 2)\n        self.assertEqual(ordered_files[2].sequence_number, 3)\n    \n    def test_generate_manifest(self):\n        \"\"\"Test manifest generation\"\"\"\n        # Create a complete content set\n        files = [\n            ContentFile(b2_path=\"file1.pdf\", filename=\"file1.pdf\", sequence_number=1),\n            ContentFile(b2_path=\"file2.pdf\", filename=\"file2.pdf\", sequence_number=2),\n            ContentFile(b2_path=\"file3.pdf\", filename=\"file3.pdf\", sequence_number=3),\n        ]\n        \n        content_set = ContentSet(\n            id=\"test-id\",\n            name=\"Test Set\",\n            detection_method=\"pattern\",\n            files=files,\n            is_complete=True\n        )\n        \n        # Generate manifest\n        manifest = self.agent.generate_manifest(content_set)\n        \n        # Verify manifest\n        self.assertEqual(manifest.content_set_id, \"test-id\")\n        self.assertEqual(manifest.total_files, 3)\n        self.assertEqual(len(manifest.ordered_files), 3)\n        self.assertEqual(len(manifest.warnings), 0)\n        \n        # Check dependencies\n        self.assertEqual(len(manifest.ordered_files[0].dependencies), 0)  # First file has no dependencies\n        self.assertEqual(len(manifest.ordered_files[1].dependencies), 1)  # Second depends on first\n        self.assertEqual(len(manifest.ordered_files[2].dependencies), 1)  # Third depends on second\n    \n    @patch('app.services.content_prep_agent.ContentPrepAgent._request_user_clarification')\n    async def test_resolve_order_with_clarification(self, mock_clarification):\n        \"\"\"Test ordering with user clarification\"\"\"\n        # Create a content set with some files missing sequence numbers\n        files = [\n            ContentFile(b2_path=\"file1.pdf\", filename=\"file1.pdf\", sequence_number=1),\n            ContentFile(b2_path=\"fileA.pdf\", filename=\"fileA.pdf\", sequence_number=None),\n            ContentFile(b2_path=\"fileB.pdf\", filename=\"fileB.pdf\", sequence_number=None),\n        ]\n        \n        content_set = ContentSet(\n            id=\"test-id\",\n            name=\"Test Set\",\n            detection_method=\"pattern\",\n            files=files\n        )\n        \n        # Mock user clarification response\n        mock_clarification.return_value = \"fileA.pdf, fileB.pdf\"\n        \n        # Resolve order with clarification\n        ordered_files = await self.agent.resolve_order_with_clarification(content_set, confidence_threshold=0.9)\n        \n        # Should have requested clarification\n        mock_clarification.assert_called_once()\n        \n        # Should be ordered according to user input\n        self.assertEqual(ordered_files[0].filename, \"file1.pdf\")  # Has sequence number, comes first\n        self.assertEqual(ordered_files[1].filename, \"fileA.pdf\")  # From user input\n        self.assertEqual(ordered_files[2].filename, \"fileB.pdf\")  # From user input\n\n# Performance tests\n@pytest.mark.benchmark\ndef test_performance_large_set(benchmark):\n    \"\"\"Benchmark performance with large file sets\"\"\"\n    agent = ContentPrepAgent()\n    \n    # Generate 100 test files\n    large_set = []\n    for i in range(1, 101):\n        large_set.append({\n            \"path\": f\"pending/course/module-{i:02d}.pdf\",\n            \"size\": 1024 * 1024,\n            \"type\": \"pdf\"\n        })\n    \n    # Benchmark content set detection\n    result = benchmark(agent.detect_content_sets, large_set)\n    \n    # Verify result\n    content_sets, standalone = result\n    assert len(content_sets) == 1\n    assert len(content_sets[0].files) == 100\n```\n\nCreate integration tests for the API endpoints:\n\n```python\n# In tests/test_content_prep_api.py\n\nfrom fastapi.testclient import TestClient\nfrom app.main import app\nimport pytest\nfrom unittest.mock import patch\n\nclient = TestClient(app)\n\n@pytest.fixture\ndef mock_content_prep_agent():\n    with patch('app.routes.content_prep.ContentPrepAgent') as mock:\n        yield mock\n\ndef test_analyze_endpoint(mock_content_prep_agent):\n    \"\"\"Test the analyze endpoint\"\"\"\n    # Mock agent response\n    mock_instance = mock_content_prep_agent.return_value\n    mock_instance.detect_content_sets.return_value = ([], [])\n    \n    # Test API\n    response = client.post(\n        \"/api/content-prep/analyze\",\n        json={\"b2_folder\": \"pending/test/\", \"detection_mode\": \"auto\"}\n    )\n    \n    # Verify response\n    assert response.status_code == 200\n    assert \"content_sets\" in response.json()\n    assert \"standalone_files\" in response.json()\n    \n    # Verify agent was called correctly\n    mock_instance.detect_content_sets.assert_called_once()\n\ndef test_manifest_endpoint(mock_content_prep_agent):\n    \"\"\"Test the manifest endpoint\"\"\"\n    # Mock agent response\n    mock_instance = mock_content_prep_agent.return_value\n    mock_instance.generate_manifest.return_value = {\n        \"content_set_id\": \"test-id\",\n        \"ordered_files\": [],\n        \"total_files\": 0,\n        \"estimated_processing_time\": 0,\n        \"warnings\": [],\n        \"context\": {}\n    }\n    \n    # Test API\n    response = client.post(\n        \"/api/content-prep/manifest\",\n        json={\"content_set_id\": \"test-id\", \"proceed_incomplete\": True}\n    )\n    \n    # Verify response\n    assert response.status_code == 200\n    assert \"content_set_id\" in response.json()\n    assert \"ordered_files\" in response.json()\n    \n    # Verify agent was called correctly\n    mock_instance.generate_manifest.assert_called_once()\n```",
        "testStrategy": "1. Run unit tests for each component\n2. Run integration tests with mocked dependencies\n3. Run performance benchmarks with large file sets\n4. Test with various file naming patterns\n5. Test error handling and edge cases\n6. Verify test coverage meets targets (>90%)\n7. Run tests in CI/CD pipeline",
        "priority": "medium",
        "dependencies": [
          "122",
          "124",
          "125",
          "126",
          "127",
          "128",
          "129",
          "130"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-13T21:30:07.791Z"
      },
      {
        "id": "132",
        "title": "Implement Prometheus Metrics for Agent Services",
        "description": "Add standardized Prometheus metrics to all agent services (AGENT-001 through AGENT-015) including counters for success/failure, histograms for duration, and gauges for active executions.",
        "details": "Implement Prometheus metrics across all agent services following the pattern established in monitoring_service.py (AGENT-016). The implementation should include:\n\n1. Add the following metrics to each agent:\n   - Counter: Track success/failure outcomes of agent operations\n   - Histogram: Measure duration of agent operations\n   - Gauge: Monitor active executions in progress\n\n2. Update the following files:\n   - content_summarizer_agent.py\n   - department_classifier_agent.py\n   - document_analysis_agents.py\n   - multi_agent_orchestration.py\n   - asset_generator_agents.py\n   - orchestrator_agent_service.py\n\n3. Implementation pattern for each agent:\n```python\nfrom prometheus_client import Counter, Histogram, Gauge\n\n# Define metrics with appropriate labels\nAGENT_REQUESTS_TOTAL = Counter(\n    'agent_requests_total', \n    'Total number of requests processed by the agent',\n    ['agent_id', 'status']\n)\n\nAGENT_REQUEST_DURATION = Histogram(\n    'agent_request_duration_seconds',\n    'Time spent processing agent requests',\n    ['agent_id', 'operation']\n)\n\nAGENT_ACTIVE_EXECUTIONS = Gauge(\n    'agent_active_executions',\n    'Number of currently active agent executions',\n    ['agent_id']\n)\n\nclass SomeAgent:\n    def __init__(self, agent_id):\n        self.agent_id = agent_id\n        \n    async def process(self, request):\n        # Track active executions\n        AGENT_ACTIVE_EXECUTIONS.labels(agent_id=self.agent_id).inc()\n        \n        # Track request duration\n        with AGENT_REQUEST_DURATION.labels(agent_id=self.agent_id, operation='process').time():\n            try:\n                result = await self._process_implementation(request)\n                # Record success\n                AGENT_REQUESTS_TOTAL.labels(agent_id=self.agent_id, status='success').inc()\n                return result\n            except Exception as e:\n                # Record failure\n                AGENT_REQUESTS_TOTAL.labels(agent_id=self.agent_id, status='failure').inc()\n                raise\n            finally:\n                # Decrement active executions\n                AGENT_ACTIVE_EXECUTIONS.labels(agent_id=self.agent_id).dec()\n```\n\n4. Ensure consistent naming conventions across all agents\n5. Add appropriate labels to distinguish between different agent types and operations\n6. Update any agent factory or registration code to ensure metrics are properly initialized\n7. Ensure thread safety for concurrent agent operations\n8. Add documentation comments explaining the metrics and their usage",
        "testStrategy": "1. Unit tests:\n   - Create test cases for each agent type to verify metrics are incremented/decremented correctly\n   - Test success and failure scenarios to ensure counters are updated appropriately\n   - Test concurrent operations to verify thread safety\n\n2. Integration tests:\n   - Verify metrics are exposed correctly via the Prometheus endpoint\n   - Test that metrics have the correct labels and values\n   - Verify histogram buckets are appropriate for the expected duration ranges\n\n3. Load testing:\n   - Verify metrics behave correctly under concurrent load\n   - Check for any performance impact from metrics collection\n\n4. Manual verification:\n   - Use Prometheus UI to query metrics and verify they appear as expected\n   - Create test Grafana dashboards to visualize the metrics\n   - Verify alerts can be configured based on the new metrics\n\n5. Specific test cases:\n   - Test agent success path: verify counter increments and gauge behaves correctly\n   - Test agent failure path: verify failure counter increments\n   - Test long-running operations: verify histogram captures duration correctly\n   - Test agent restart: verify gauges reset to zero appropriately",
        "status": "done",
        "dependencies": [
          "107",
          "118",
          "128",
          "131"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T05:03:46.813Z"
      },
      {
        "id": "133",
        "title": "Implement Orchestrator API Routes",
        "description": "Create API routes for the Master Orchestrator (AGENT-001) to expose its functionality, including coordination, agent listing, health checks, and statistics endpoints.",
        "details": "Create the file `app/routes/orchestrator.py` with the following FastAPI routes:\n\n1. POST /api/orchestrator/coordinate - Main endpoint for orchestrating agent workflows\n   - Implement request validation using Pydantic models\n   - Parse incoming requests and delegate to the Orchestrator service\n   - Handle response formatting and error cases\n\n2. GET /api/orchestrator/agents - Endpoint to list all available agents\n   - Return metadata about registered agents including capabilities and status\n   - Support filtering by agent type, status, or capability\n\n3. GET /api/orchestrator/health - Health check endpoint\n   - Verify Orchestrator service is running properly\n   - Check connections to dependent services\n   - Return appropriate health status codes\n\n4. GET /api/orchestrator/stats - Statistics endpoint\n   - Collect and return performance metrics\n   - Include agent usage statistics, response times, and success rates\n   - Support time-range filtering\n\nRegister the router in main.py by adding:\n```python\nfrom app.routes import orchestrator\n\napp.include_router(orchestrator.router, tags=[\"orchestrator\"])\n```\n\nCreate Pydantic models in `app/models/orchestrator.py`:\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Optional, Any\nfrom enum import Enum\n\nclass AgentType(str, Enum):\n    CONTENT_PREP = \"content_prep\"\n    GRAPH = \"graph\"\n    RETRIEVAL = \"retrieval\"\n    # Add other agent types\n\nclass AgentStatus(str, Enum):\n    AVAILABLE = \"available\"\n    BUSY = \"busy\"\n    OFFLINE = \"offline\"\n\nclass OrchestrationRequest(BaseModel):\n    workflow_id: Optional[str] = Field(None, description=\"Optional workflow identifier\")\n    task: str = Field(..., description=\"Task description for orchestration\")\n    agents: List[str] = Field(default=[], description=\"Specific agents to include\")\n    parameters: Dict[str, Any] = Field(default={}, description=\"Task parameters\")\n    \nclass AgentInfo(BaseModel):\n    id: str\n    name: str\n    type: AgentType\n    status: AgentStatus\n    capabilities: List[str]\n    \nclass OrchestrationResponse(BaseModel):\n    workflow_id: str\n    status: str\n    result: Optional[Dict[str, Any]] = None\n    \nclass HealthStatus(BaseModel):\n    status: str\n    version: str\n    dependencies: Dict[str, str]\n    \nclass StatsResponse(BaseModel):\n    total_requests: int\n    success_rate: float\n    average_response_time: float\n    agent_usage: Dict[str, int]\n    recent_workflows: List[Dict[str, Any]]\n```\n\nEnsure proper error handling and validation throughout the implementation.",
        "testStrategy": "1. Create unit tests in `tests/routes/test_orchestrator.py` for each endpoint:\n   - Test POST /api/orchestrator/coordinate with valid and invalid payloads\n   - Test GET /api/orchestrator/agents with various filter parameters\n   - Test GET /api/orchestrator/health in normal and degraded states\n   - Test GET /api/orchestrator/stats with different time ranges\n\n2. Create integration tests that verify:\n   - Router registration works correctly in main.py\n   - Endpoints interact properly with the Orchestrator service\n   - Authentication and authorization are enforced correctly\n\n3. Test error handling:\n   - Verify appropriate status codes for various error conditions\n   - Test validation errors with malformed requests\n   - Test service unavailable scenarios\n\n4. Test Pydantic models:\n   - Verify model validation works as expected\n   - Test serialization/deserialization\n   - Ensure OpenAPI schema generation is correct\n\n5. Performance testing:\n   - Test response times under load\n   - Verify endpoints can handle concurrent requests\n   - Check memory usage during extended operation\n\n6. Documentation verification:\n   - Ensure API documentation is generated correctly\n   - Verify examples in documentation match implementation",
        "status": "done",
        "dependencies": [
          "103",
          "122",
          "124"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T03:50:30.220Z"
      },
      {
        "id": "134",
        "title": "Implement Health Endpoint for Asset Generators",
        "description": "Add a health endpoint to Asset Generators (AGENT-003 through AGENT-007) that returns status information and capabilities for all five generators: Skill, Command, Agent, Prompt, and Workflow.",
        "details": "Create a new health endpoint in the asset_generators.py routes file to provide status information for all five Asset Generator agents:\n\n1. Add the following route to app/routes/asset_generators.py:\n```python\n@router.get(\"/health\", response_model=AssetGeneratorsHealthResponse)\nasync def get_health():\n    \"\"\"\n    Get health status for all Asset Generator agents.\n    \n    Returns:\n        AssetGeneratorsHealthResponse: Status and capabilities for all five generators\n    \"\"\"\n    health_service = AssetGeneratorsHealthService()\n    return await health_service.get_health_status()\n```\n\n2. Create a Pydantic model for the health response in app/models/asset_generators.py:\n```python\nclass GeneratorHealth(BaseModel):\n    status: str  # \"healthy\", \"degraded\", or \"offline\"\n    capabilities: List[str]\n    last_check: datetime\n    error_message: Optional[str] = None\n\nclass AssetGeneratorsHealthResponse(BaseModel):\n    skill_generator: GeneratorHealth\n    command_generator: GeneratorHealth\n    agent_generator: GeneratorHealth\n    prompt_generator: GeneratorHealth\n    workflow_generator: GeneratorHealth\n    timestamp: datetime = Field(default_factory=datetime.utcnow)\n```\n\n3. Implement the health service in app/services/asset_generators_health.py:\n```python\nfrom datetime import datetime\nfrom app.models.asset_generators import AssetGeneratorsHealthResponse, GeneratorHealth\nfrom app.services.skill_generator import SkillGeneratorService\nfrom app.services.command_generator import CommandGeneratorService\nfrom app.services.agent_generator import AgentGeneratorService\nfrom app.services.prompt_generator import PromptGeneratorService\nfrom app.services.workflow_generator import WorkflowGeneratorService\n\nclass AssetGeneratorsHealthService:\n    async def get_health_status(self) -> AssetGeneratorsHealthResponse:\n        \"\"\"Get health status for all Asset Generator agents.\"\"\"\n        \n        # Get health status from each generator service\n        skill_health = await self._check_skill_generator()\n        command_health = await self._check_command_generator()\n        agent_health = await self._check_agent_generator()\n        prompt_health = await self._check_prompt_generator()\n        workflow_health = await self._check_workflow_generator()\n        \n        return AssetGeneratorsHealthResponse(\n            skill_generator=skill_health,\n            command_generator=command_health,\n            agent_generator=agent_health,\n            prompt_generator=prompt_health,\n            workflow_generator=workflow_health\n        )\n    \n    async def _check_skill_generator(self) -> GeneratorHealth:\n        try:\n            service = SkillGeneratorService()\n            capabilities = await service.get_capabilities()\n            return GeneratorHealth(\n                status=\"healthy\",\n                capabilities=capabilities,\n                last_check=datetime.utcnow()\n            )\n        except Exception as e:\n            return GeneratorHealth(\n                status=\"offline\",\n                capabilities=[],\n                last_check=datetime.utcnow(),\n                error_message=str(e)\n            )\n    \n    # Implement similar _check methods for other generators\n    # _check_command_generator, _check_agent_generator, etc.\n```\n\n4. Update each generator service to implement a get_capabilities() method that returns a list of capabilities specific to that generator.\n\n5. Ensure proper error handling and timeouts for health checks to prevent cascading failures.\n\n6. Add appropriate logging for health check operations.\n\n7. Update API documentation to include the new health endpoint.",
        "testStrategy": "1. Create unit tests in tests/routes/test_asset_generators.py:\n   ```python\n   async def test_health_endpoint_returns_correct_structure():\n       # Arrange\n       client = TestClient(app)\n       \n       # Act\n       response = client.get(\"/api/asset-generators/health\")\n       \n       # Assert\n       assert response.status_code == 200\n       data = response.json()\n       assert \"skill_generator\" in data\n       assert \"command_generator\" in data\n       assert \"agent_generator\" in data\n       assert \"prompt_generator\" in data\n       assert \"workflow_generator\" in data\n       assert \"timestamp\" in data\n       \n       # Check structure of each generator's health data\n       for generator in [\"skill_generator\", \"command_generator\", \"agent_generator\", \n                         \"prompt_generator\", \"workflow_generator\"]:\n           assert \"status\" in data[generator]\n           assert \"capabilities\" in data[generator]\n           assert \"last_check\" in data[generator]\n   ```\n\n2. Create unit tests for the AssetGeneratorsHealthService:\n   - Test successful health checks for all generators\n   - Test scenarios where one or more generators are offline\n   - Test error handling when exceptions occur\n\n3. Create integration tests that verify the health endpoint with mocked generator services:\n   ```python\n   @patch(\"app.services.skill_generator.SkillGeneratorService\")\n   @patch(\"app.services.command_generator.CommandGeneratorService\")\n   # ... patches for other services\n   async def test_health_endpoint_with_mocked_services(mock_skill, mock_command, ...):\n       # Configure mocks\n       mock_skill.return_value.get_capabilities.return_value = [\"create_skill\", \"update_skill\"]\n       # ... configure other mocks\n       \n       # Make request\n       response = client.get(\"/api/asset-generators/health\")\n       \n       # Verify response\n       # ...\n   ```\n\n4. Test error scenarios:\n   - Test when one generator is down but others are up\n   - Test timeout handling\n   - Test with various error conditions\n\n5. Performance testing:\n   - Measure response time under normal conditions\n   - Test with simulated slow responses from generators\n\n6. Manual testing:\n   - Verify the endpoint in development environment\n   - Check that all capabilities are correctly listed\n   - Verify status values are accurate\n\n7. Update API documentation tests to include the new endpoint.",
        "status": "done",
        "dependencies": [
          "107",
          "132",
          "133"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T05:07:18.068Z"
      },
      {
        "id": "135",
        "title": "Implement Standardized Error Response Model and Handling",
        "description": "Create a standardized error response model and consistent error handling across all agents with a centralized error_handler middleware.",
        "details": "Implement a standardized approach to error handling across all agent routes:\n\n1. Create `app/models/errors.py` with the following structure:\n```python\nfrom pydantic import BaseModel\nfrom typing import Optional, Dict, Any\nfrom datetime import datetime\nfrom enum import Enum\n\nclass ErrorType(str, Enum):\n    RETRIABLE = \"retriable\"\n    PERMANENT = \"permanent\"\n\nclass AgentErrorResponse(BaseModel):\n    error_code: str\n    error_type: ErrorType\n    agent_id: str\n    message: str\n    details: Optional[Dict[str, Any]] = None\n    request_id: Optional[str] = None\n    timestamp: datetime = datetime.utcnow()\n    \n    class Config:\n        schema_extra = {\n            \"example\": {\n                \"error_code\": \"AGENT_PROCESSING_ERROR\",\n                \"error_type\": \"retriable\",\n                \"agent_id\": \"content_prep_agent\",\n                \"message\": \"Failed to process content set\",\n                \"details\": {\"reason\": \"Invalid file format\"},\n                \"request_id\": \"req-123456\",\n                \"timestamp\": \"2023-06-15T10:30:00Z\"\n            }\n        }\n```\n\n2. Create an error handler middleware in `app/middleware/error_handler.py`:\n```python\nfrom fastapi import Request, status\nfrom fastapi.responses import JSONResponse\nfrom app.models.errors import AgentErrorResponse, ErrorType\nimport logging\nimport uuid\nfrom starlette.middleware.base import BaseHTTPMiddleware\n\nlogger = logging.getLogger(__name__)\n\nclass ErrorHandlerMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):\n        try:\n            return await call_next(request)\n        except Exception as e:\n            # Extract agent_id from path if possible\n            path_parts = request.url.path.split(\"/\")\n            agent_id = \"unknown\"\n            for i, part in enumerate(path_parts):\n                if part == \"agent\" and i + 1 < len(path_parts):\n                    agent_id = path_parts[i + 1]\n                    break\n            \n            # Generate request_id if not present\n            request_id = request.headers.get(\"X-Request-ID\", str(uuid.uuid4()))\n            \n            # Log the error\n            logger.error(f\"Request error: {str(e)}\", \n                        extra={\"request_id\": request_id, \"agent_id\": agent_id})\n            \n            # Create standardized error response\n            error_response = AgentErrorResponse(\n                error_code=\"INTERNAL_SERVER_ERROR\",\n                error_type=ErrorType.RETRIABLE,\n                agent_id=agent_id,\n                message=\"An unexpected error occurred\",\n                details={\"exception\": str(e)},\n                request_id=request_id\n            )\n            \n            return JSONResponse(\n                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n                content=error_response.dict()\n            )\n```\n\n3. Update all agent route handlers to use the standardized error format:\n```python\nfrom fastapi import HTTPException\nfrom app.models.errors import AgentErrorResponse, ErrorType\n\n# Example of updated route handler\n@router.post(\"/process\")\nasync def process_content(request: ProcessRequest):\n    try:\n        # Processing logic\n        result = await agent.process(request.content)\n        return result\n    except ValidationError as e:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=AgentErrorResponse(\n                error_code=\"VALIDATION_ERROR\",\n                error_type=ErrorType.PERMANENT,\n                agent_id=\"content_prep_agent\",\n                message=\"Invalid request format\",\n                details={\"errors\": e.errors()}\n            ).dict()\n        )\n    except ResourceNotFoundError as e:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=AgentErrorResponse(\n                error_code=\"RESOURCE_NOT_FOUND\",\n                error_type=ErrorType.PERMANENT,\n                agent_id=\"content_prep_agent\",\n                message=\"Requested resource not found\",\n                details={\"resource_id\": e.resource_id}\n            ).dict()\n        )\n```\n\n4. Register the middleware in the main FastAPI application:\n```python\n# In app/main.py\nfrom app.middleware.error_handler import ErrorHandlerMiddleware\n\napp = FastAPI()\napp.add_middleware(ErrorHandlerMiddleware)\n```\n\n5. Create common error code constants in `app/constants/error_codes.py`:\n```python\n# Common error codes\nVALIDATION_ERROR = \"VALIDATION_ERROR\"\nRESOURCE_NOT_FOUND = \"RESOURCE_NOT_FOUND\"\nUNAUTHORIZED = \"UNAUTHORIZED\"\nFORBIDDEN = \"FORBIDDEN\"\nINTERNAL_SERVER_ERROR = \"INTERNAL_SERVER_ERROR\"\n\n# Agent-specific error codes\nAGENT_PROCESSING_ERROR = \"AGENT_PROCESSING_ERROR\"\nAGENT_TIMEOUT = \"AGENT_TIMEOUT\"\nAGENT_UNAVAILABLE = \"AGENT_UNAVAILABLE\"\n```\n\n6. Update all existing agent implementations to use the new error model and handling approach.",
        "testStrategy": "1. Unit test the AgentErrorResponse model:\n   - Test serialization/deserialization\n   - Test validation of required fields\n   - Test default values (timestamp)\n   - Test enum validation for error_type\n\n2. Unit test the ErrorHandlerMiddleware:\n   - Test with various exception types\n   - Verify correct error response format\n   - Test agent_id extraction from different URL patterns\n   - Test request_id propagation\n   - Test logging behavior\n\n3. Integration tests for error handling:\n   - Test each agent endpoint with invalid inputs\n   - Test with simulated processing errors\n   - Verify consistent error response format across all endpoints\n   - Test with and without X-Request-ID header\n\n4. Test error handling for specific scenarios:\n   - Authentication failures\n   - Authorization failures\n   - Resource not found\n   - Validation errors\n   - Timeout errors\n   - External service failures\n\n5. End-to-end tests:\n   - Verify client applications can properly parse and handle error responses\n   - Test error recovery flows for retriable errors\n   - Test appropriate client behavior for permanent errors\n\n6. Performance testing:\n   - Measure overhead of error handling middleware\n   - Test under high load with frequent errors",
        "status": "done",
        "dependencies": [
          "107",
          "110",
          "125",
          "129"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T05:19:17.976Z"
      },
      {
        "id": "136",
        "title": "Implement X-Request-ID Tracing Across Agent Chains",
        "description": "Create a middleware and utility functions to generate, propagate, and log X-Request-ID headers throughout the entire agent ecosystem, enabling end-to-end request tracing across multi-agent workflows.",
        "details": "Implement a comprehensive request tracing system using X-Request-ID headers:\n\n1. Create middleware in `app/middleware/request_tracing.py`:\n   - Generate a unique UUID for each incoming request if X-Request-ID is not present\n   - Extract existing X-Request-ID from incoming requests if available\n   - Add the request_id to the request context for access throughout the request lifecycle\n\n2. Create utility functions in `app/utils/tracing.py`:\n   - `get_request_id()`: Retrieve the current request_id from context\n   - `with_request_id(headers)`: Add the current request_id to outgoing request headers\n   - `log_with_request_id(message, level)`: Log with request_id included\n\n3. Modify agent service calls:\n   - Update all HTTP client calls to include X-Request-ID header using `with_request_id()`\n   - Ensure all CrewAI agent chains propagate the request_id to downstream agents\n   - Add request_id to agent context objects for access during execution\n\n4. Update logging configuration:\n   - Modify logging formatters to include request_id in all log entries\n   - Create a custom log filter to inject request_id into log records\n\n5. Enhance error responses:\n   - Update error handling middleware to include request_id in all error responses\n   - Add request_id to standardized error response format\n\n6. Add request_id to metrics:\n   - Include request_id as a label in Prometheus metrics\n   - Enable correlation between metrics and logs\n\n7. Create documentation:\n   - Document the request tracing system for developers\n   - Provide examples of how to use the tracing utilities\n\nExample middleware implementation:\n```python\nfrom fastapi import Request\nfrom starlette.middleware.base import BaseHTTPMiddleware\nimport uuid\nfrom contextvars import ContextVar\n\n# Context variable to store request_id\nrequest_id_var = ContextVar(\"request_id\", default=None)\n\nclass RequestTracingMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):\n        # Extract or generate request_id\n        request_id = request.headers.get(\"X-Request-ID\")\n        if not request_id:\n            request_id = str(uuid.uuid4())\n        \n        # Store in context for this request\n        token = request_id_var.set(request_id)\n        \n        # Process request\n        response = await call_next(request)\n        \n        # Add request_id to response headers\n        response.headers[\"X-Request-ID\"] = request_id\n        \n        # Reset context\n        request_id_var.reset(token)\n        \n        return response\n```\n\nExample utility functions:\n```python\nfrom app.middleware.request_tracing import request_id_var\nimport logging\n\ndef get_request_id():\n    \"\"\"Get the current request ID from context.\"\"\"\n    return request_id_var.get()\n\ndef with_request_id(headers=None):\n    \"\"\"Add request_id to headers for outgoing requests.\"\"\"\n    headers = headers or {}\n    request_id = get_request_id()\n    if request_id:\n        headers[\"X-Request-ID\"] = request_id\n    return headers\n\ndef log_with_request_id(message, level=logging.INFO):\n    \"\"\"Log with request_id included.\"\"\"\n    logger = logging.getLogger()\n    request_id = get_request_id()\n    extra = {\"request_id\": request_id} if request_id else {}\n    logger.log(level, message, extra=extra)\n```",
        "testStrategy": "1. Create unit tests in `tests/middleware/test_request_tracing.py`:\n   - Test middleware with no existing X-Request-ID header\n   - Test middleware with existing X-Request-ID header\n   - Verify request_id is properly stored in context\n   - Verify request_id is added to response headers\n\n2. Create unit tests in `tests/utils/test_tracing.py`:\n   - Test `get_request_id()` returns correct value\n   - Test `with_request_id()` adds header correctly\n   - Test `log_with_request_id()` includes request_id in logs\n\n3. Create integration tests in `tests/integration/test_request_tracing.py`:\n   - Test request_id propagation through multiple API calls\n   - Test request_id propagation through agent chains\n   - Verify logs contain consistent request_id across service boundaries\n\n4. Test error scenarios:\n   - Verify error responses include request_id\n   - Test behavior when invalid request_id is provided\n\n5. Performance testing:\n   - Measure overhead of request tracing middleware\n   - Verify no significant performance impact\n\n6. Manual testing:\n   - Use tools like Postman to trace requests through the system\n   - Verify request_id appears in logs and responses\n   - Test multi-agent workflows and verify end-to-end tracing\n\n7. Create a test script that:\n   - Initiates a complex multi-agent workflow\n   - Verifies the same request_id appears in all logs\n   - Checks metrics contain the request_id label\n   - Validates error responses include the request_id",
        "status": "done",
        "dependencies": [
          "133",
          "122",
          "131"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T03:56:29.663Z"
      },
      {
        "id": "137",
        "title": "Implement Circuit Breaker for Anthropic API Calls",
        "description": "Implement a circuit breaker pattern for Anthropic API calls across all agent services to improve resilience and prevent cascading failures during API outages.",
        "details": "Create a robust circuit breaker implementation for Anthropic API calls that will be used across all agent services:\n\n1. Create a new module `app/services/api_resilience.py` to implement the circuit breaker pattern:\n   - Use the `tenacity` or `circuit-breaker` library for implementation\n   - Configure exponential backoff with a maximum of 3 retries\n   - Implement proper circuit state tracking (closed/open/half-open)\n   - Set appropriate thresholds for opening the circuit (e.g., 5 failures in 30 seconds)\n   - Implement half-open state with test requests to check if service is restored\n\n2. Create a wrapper class for Anthropic API calls:\n```python\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom prometheus_client import Counter, Gauge\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Metrics for circuit breaker state\nanthropic_circuit_state = Gauge(\n    'anthropic_circuit_state',\n    'Current state of Anthropic API circuit breaker (0=closed, 1=half-open, 2=open)',\n    ['service']\n)\n\nanthropic_api_failures = Counter(\n    'anthropic_api_failures',\n    'Number of Anthropic API call failures',\n    ['service', 'endpoint']\n)\n\nclass AnthropicCircuitBreaker:\n    CLOSED = 0\n    HALF_OPEN = 1\n    OPEN = 2\n    \n    def __init__(self, service_name, failure_threshold=5, recovery_timeout=60):\n        self.service_name = service_name\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.failure_count = 0\n        self.last_failure_time = None\n        self.state = self.CLOSED\n        self._update_state_metric()\n    \n    def _update_state_metric(self):\n        anthropic_circuit_state.labels(service=self.service_name).set(self.state)\n    \n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=2, max=10),\n        retry_error_callback=lambda retry_state: retry_state.outcome.result()\n    )\n    async def call_api(self, func, *args, **kwargs):\n        \"\"\"Wrapper for Anthropic API calls with circuit breaker pattern\"\"\"\n        # Check if circuit is open\n        if self.state == self.OPEN:\n            # Check if recovery timeout has elapsed to transition to half-open\n            if self.last_failure_time and (time.time() - self.last_failure_time) > self.recovery_timeout:\n                self.state = self.HALF_OPEN\n                self._update_state_metric()\n                logger.info(f\"Circuit for {self.service_name} transitioned to HALF-OPEN state\")\n            else:\n                # Fail fast when circuit is open\n                logger.warning(f\"Circuit for {self.service_name} is OPEN, failing fast\")\n                raise CircuitOpenError(f\"Circuit for {self.service_name} is open\")\n        \n        try:\n            result = await func(*args, **kwargs)\n            \n            # If we're in half-open and call succeeded, close the circuit\n            if self.state == self.HALF_OPEN:\n                self.state = self.CLOSED\n                self.failure_count = 0\n                self._update_state_metric()\n                logger.info(f\"Circuit for {self.service_name} restored to CLOSED state\")\n            \n            return result\n            \n        except Exception as e:\n            endpoint = kwargs.get('endpoint', 'unknown')\n            anthropic_api_failures.labels(service=self.service_name, endpoint=endpoint).inc()\n            \n            self.failure_count += 1\n            self.last_failure_time = time.time()\n            \n            # Check if we need to open the circuit\n            if self.state == self.CLOSED and self.failure_count >= self.failure_threshold:\n                self.state = self.OPEN\n                self._update_state_metric()\n                logger.error(f\"Circuit for {self.service_name} transitioned to OPEN state after {self.failure_count} failures\")\n            \n            # Re-raise the exception for retry handling\n            raise\n```\n\n3. Apply the circuit breaker to all agent services that call Anthropic API:\n   - content_summarizer\n   - department_classifier\n   - document_analysis\n   - multi_agent_orchestration\n   - asset_generators\n\n4. Update each service to use the circuit breaker wrapper:\n```python\n# Example integration in content_summarizer\nfrom app.services.api_resilience import AnthropicCircuitBreaker\n\nclass ContentSummarizerAgent:\n    def __init__(self):\n        # Initialize circuit breaker\n        self.circuit_breaker = AnthropicCircuitBreaker(service_name=\"content_summarizer\")\n        # Other initialization...\n    \n    async def summarize_content(self, content):\n        try:\n            # Use circuit breaker to call Anthropic API\n            summary = await self.circuit_breaker.call_api(\n                self._call_anthropic_api,\n                content=content,\n                endpoint=\"completion\"\n            )\n            return summary\n        except CircuitOpenError:\n            # Handle circuit open case - return cached response or fallback\n            return self._generate_fallback_summary(content)\n```\n\n5. Implement appropriate fallback mechanisms for each service when the circuit is open:\n   - Return cached responses when available\n   - Use simpler models or rules-based approaches as fallbacks\n   - Provide clear error messages to users about temporary service limitations\n\n6. Add comprehensive logging for circuit state changes and API failures to aid in debugging and monitoring.",
        "testStrategy": "1. Unit tests for the circuit breaker implementation:\n   - Test state transitions (closed → open → half-open → closed)\n   - Test failure counting and threshold behavior\n   - Test exponential backoff retry logic\n   - Test timeout handling and recovery\n\n2. Integration tests with mocked Anthropic API responses:\n   - Test successful API calls\n   - Test handling of different error types (rate limits, server errors, etc.)\n   - Test retry behavior with temporary failures\n   - Test circuit opening after threshold failures\n\n3. Test each agent service with the circuit breaker:\n   - Verify circuit breaker is properly integrated in each service\n   - Test fallback mechanisms when circuit is open\n   - Verify metrics are correctly updated for each service\n\n4. Load testing:\n   - Simulate high load scenarios to verify circuit breaker prevents cascading failures\n   - Test recovery behavior under load\n\n5. Monitoring tests:\n   - Verify circuit state metrics are correctly exposed to Prometheus\n   - Test dashboard alerts for circuit open states\n   - Verify logging provides sufficient information for troubleshooting\n\n6. End-to-end tests:\n   - Test the complete flow from API call through circuit breaker to response\n   - Verify correct behavior during simulated Anthropic API outages\n\n7. Test fallback quality:\n   - Evaluate the quality of fallback responses compared to normal operation\n   - Ensure degraded service is still usable when circuit is open",
        "status": "done",
        "dependencies": [
          "110"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-13T22:28:26.078Z"
      },
      {
        "id": "138",
        "title": "Implement Review Agent Revision Loop",
        "description": "Implement a revision loop for AGENT-015 Review Agent that routes content back to AGENT-014 Writing Agent when quality thresholds aren't met, with a maximum of 3 iterations and metrics tracking.",
        "details": "Create a revision loop system between the Review Agent and Writing Agent with the following components:\n\n1. Enhance the Review Agent (AGENT-015) output schema:\n   ```python\n   class ReviewOutput(BaseModel):\n       content_id: str\n       passed: bool\n       quality_score: float  # New field for quantitative assessment\n       feedback: List[FeedbackItem]\n       revision_count: int = 0  # Track number of revisions\n   ```\n\n2. Modify the orchestration workflow to handle revisions:\n   ```python\n   class WorkflowState(BaseModel):\n       # Existing fields\n       revision_requested: bool = False  # New flag to indicate revision needed\n       current_revision: int = 0  # Track current revision number\n   ```\n\n3. Implement the revision loop logic in the workflow orchestrator:\n   ```python\n   async def process_review_result(review_output: ReviewOutput, workflow_state: WorkflowState):\n       # Update metrics\n       metrics.record_quality_score(review_output.content_id, review_output.quality_score)\n       \n       if not review_output.passed and workflow_state.current_revision < 3:\n           # Route back to writing agent with feedback\n           workflow_state.revision_requested = True\n           workflow_state.current_revision += 1\n           \n           # Record metrics for revision request\n           metrics.increment_revision_count(review_output.content_id)\n           \n           # Send to writing agent with feedback\n           return await writing_agent.revise_content(\n               content_id=review_output.content_id,\n               feedback=review_output.feedback,\n               revision_number=workflow_state.current_revision\n           )\n       elif not review_output.passed:\n           # Max revisions reached, flag for human review\n           workflow_state.requires_human_review = True\n           metrics.record_failed_after_max_revisions(review_output.content_id)\n           return workflow_state\n       else:\n           # Content passed review\n           metrics.record_successful_content(\n               content_id=review_output.content_id, \n               revision_count=workflow_state.current_revision\n           )\n           return workflow_state\n   ```\n\n4. Enhance the Writing Agent (AGENT-014) to handle revision requests:\n   ```python\n   async def revise_content(self, content_id: str, feedback: List[FeedbackItem], revision_number: int):\n       # Retrieve original content\n       original_content = await self.content_repository.get_content(content_id)\n       \n       # Format feedback for the LLM\n       formatted_feedback = self.format_feedback_for_revision(feedback)\n       \n       # Create revision prompt with specific focus on addressing feedback\n       revision_prompt = f\"\"\"\n       You are revising content based on review feedback. This is revision #{revision_number}.\n       \n       ORIGINAL CONTENT:\n       {original_content.text}\n       \n       REVIEW FEEDBACK:\n       {formatted_feedback}\n       \n       Please revise the content to address all feedback points while maintaining the original purpose.\n       Focus especially on improving the areas mentioned in the feedback.\n       \"\"\"\n       \n       # Get revised content from LLM\n       revised_content = await self.llm_service.generate_content(revision_prompt)\n       \n       # Store revision history\n       await self.content_repository.save_revision(\n           content_id=content_id,\n           revision_number=revision_number,\n           original_content=original_content.text,\n           revised_content=revised_content,\n           feedback=feedback\n       )\n       \n       # Update content\n       await self.content_repository.update_content(content_id, revised_content)\n       \n       return revised_content\n   ```\n\n5. Implement metrics tracking for revisions:\n   ```python\n   # In app/services/metrics.py\n   \n   def record_quality_score(content_id: str, quality_score: float):\n       # Record quality score in metrics system\n       pass\n       \n   def increment_revision_count(content_id: str):\n       # Increment revision counter for this content\n       pass\n       \n   def record_quality_improvement(content_id: str, original_score: float, new_score: float):\n       # Record delta between original and new quality scores\n       pass\n       \n   def record_failed_after_max_revisions(content_id: str):\n       # Record instances where content failed even after max revisions\n       pass\n       \n   def record_successful_content(content_id: str, revision_count: int):\n       # Record successful content with the number of revisions needed\n       pass\n   ```\n\n6. Create a dashboard view for revision metrics:\n   - Average quality score improvement per revision\n   - Distribution of revision counts\n   - Success rate after revisions\n   - Content requiring max revisions",
        "testStrategy": "1. Unit tests for the revision loop logic:\n   - Test that content with quality_score below threshold gets flagged for revision\n   - Test that revision_count increments correctly\n   - Test that max revisions (3) is enforced\n   - Test that feedback is properly passed to the Writing Agent\n\n2. Integration tests for the Review-Write cycle:\n   - Test end-to-end flow with mock agents\n   - Verify content is properly updated after revision\n   - Test with various quality scores to ensure threshold logic works\n   - Verify revision history is properly stored\n\n3. Metrics validation:\n   - Verify quality scores are recorded correctly\n   - Test that revision counts are accurately tracked\n   - Validate quality improvement calculations\n   - Test dashboard data aggregation\n\n4. Edge case testing:\n   - Test behavior when a revision improves but still doesn't meet threshold\n   - Test when Writing Agent fails during revision\n   - Test concurrent revisions for different content pieces\n   - Test with malformed feedback data\n\n5. Performance testing:\n   - Measure latency impact of revision loops\n   - Test system under load with multiple simultaneous revision cycles\n   - Verify database performance with revision history storage\n\n6. User acceptance testing:\n   - Verify that revision feedback is clear and actionable\n   - Test that quality improvements are meaningful\n   - Validate dashboard metrics against manual calculations",
        "status": "done",
        "dependencies": [
          "107",
          "110",
          "137"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T04:03:47.665Z"
      },
      {
        "id": "139",
        "title": "Create Integration Tests for Agent API Routes",
        "description": "Develop comprehensive integration tests for all agent API routes using FastAPI TestClient to verify the full request-response cycle, including error scenarios and multi-agent workflow chains.",
        "details": "Create a comprehensive integration test suite in the `tests/integration/` directory to test all agent API routes:\n\n1. Set up the test environment:\n   - Create `tests/integration/test_api_routes.py` as the main test file\n   - Implement test fixtures for FastAPI TestClient setup\n   - Configure test database and mock external dependencies\n\n2. Implement test cases for each agent endpoint group:\n   - Content Summarizer endpoints:\n     - Test document summarization with various input types\n     - Test customization parameters (length, style, format)\n   \n   - Department Classifier endpoints:\n     - Test classification with sample documents\n     - Verify confidence scores and multi-label classification\n   \n   - Document Analysis endpoints:\n     - Test document parsing and metadata extraction\n     - Test analysis with different document types (PDF, DOCX, TXT)\n   \n   - Multi-agent Orchestration endpoints:\n     - Test workflow creation and execution\n     - Test agent coordination and task delegation\n   \n   - Asset Generator endpoints:\n     - Test generation of various asset types\n     - Verify output formats and customization options\n   \n   - Content Prep endpoints:\n     - Test content set detection and ordering\n     - Test manifest generation and validation\n\n3. Implement error scenario tests:\n   - Invalid input parameters (wrong format, missing required fields)\n   - Timeout scenarios using mocked delayed responses\n   - API failure handling with simulated service errors\n   - Rate limiting and throttling tests\n   - Authentication and authorization failures\n\n4. Implement end-to-end workflow tests:\n   - Test complete multi-agent workflows from request to final response\n   - Verify correct data passing between agents\n   - Test complex scenarios involving multiple agent interactions\n   - Validate response formats and content\n\n5. Implement performance tests:\n   - Test response times under various load conditions\n   - Verify handling of concurrent requests\n   - Test with large payloads to ensure stability\n\nSample test code structure:\n```python\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom app.main import app\nfrom unittest.mock import patch, MagicMock\n\nclient = TestClient(app)\n\n@pytest.fixture\ndef mock_dependencies():\n    # Set up mocks for external services\n    with patch(\"app.services.content_summarizer.SummarizerService\") as mock_summarizer, \\\n         patch(\"app.services.department_classifier.ClassifierService\") as mock_classifier:\n        yield {\n            \"summarizer\": mock_summarizer,\n            \"classifier\": mock_classifier,\n            # Add other mocked dependencies\n        }\n\ndef test_content_summarizer_endpoint(mock_dependencies):\n    # Arrange\n    mock_dependencies[\"summarizer\"].return_value.summarize.return_value = {\n        \"summary\": \"This is a test summary\",\n        \"confidence\": 0.95\n    }\n    \n    # Act\n    response = client.post(\n        \"/api/content_summarizer/summarize\",\n        json={\"text\": \"This is a test document that needs to be summarized.\", \"max_length\": 100}\n    )\n    \n    # Assert\n    assert response.status_code == 200\n    assert \"summary\" in response.json()\n    assert \"confidence\" in response.json()\n    \ndef test_invalid_input_handling():\n    # Test with missing required field\n    response = client.post(\n        \"/api/content_summarizer/summarize\",\n        json={\"max_length\": 100}  # Missing 'text' field\n    )\n    assert response.status_code == 422\n    \n    # Test with invalid parameter type\n    response = client.post(\n        \"/api/content_summarizer/summarize\",\n        json={\"text\": \"Sample text\", \"max_length\": \"not_a_number\"}\n    )\n    assert response.status_code == 422\n\ndef test_multi_agent_workflow():\n    # Test a complete workflow involving multiple agents\n    # ...\n```",
        "testStrategy": "1. Run individual endpoint tests:\n   - Execute tests for each agent endpoint in isolation\n   - Verify correct HTTP status codes (200 for success, appropriate error codes for failures)\n   - Validate response structure against API specifications\n   - Check that response data matches expected format and content\n\n2. Run error scenario tests:\n   - Verify that invalid inputs return appropriate 400-level status codes\n   - Confirm error messages are descriptive and helpful\n   - Test that timeout scenarios are handled gracefully\n   - Ensure API failures return appropriate 500-level status codes with useful error information\n   - Verify authentication failures return 401/403 status codes\n\n3. Run end-to-end workflow tests:\n   - Execute complete multi-agent workflow tests\n   - Verify data consistency throughout the workflow\n   - Check that final outputs match expected results\n   - Test with various input combinations to ensure robustness\n\n4. Verify test coverage:\n   - Generate test coverage reports using pytest-cov\n   - Ensure all API routes have at least 90% test coverage\n   - Identify and address any gaps in test coverage\n\n5. Run performance tests:\n   - Execute tests with timing measurements\n   - Verify response times are within acceptable limits\n   - Test with concurrent requests to ensure stability\n\n6. Validate in CI/CD pipeline:\n   - Configure tests to run automatically in the CI/CD pipeline\n   - Ensure tests pass consistently in the pipeline environment\n   - Set up test reports for easy review\n\n7. Conduct code review:\n   - Have team members review test code for completeness and correctness\n   - Verify that tests align with API specifications and requirements",
        "status": "done",
        "dependencies": [
          "131",
          "133",
          "122",
          "121",
          "103"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T17:29:02.885Z"
      },
      {
        "id": "140",
        "title": "Enhance Content Prep Agent Health Endpoint",
        "description": "Update the existing GET /api/content-prep/health endpoint to include comprehensive agent status information including agent_id, version, uptime, error metrics, and processing statistics.",
        "details": "Enhance the existing health endpoint in the content_prep.py routes file to provide more detailed status information:\n\n1. Modify the route in app/routes/content_prep.py:\n```python\n@router.get(\"/health\", response_model=ContentPrepHealthResponse)\nasync def get_health():\n    \"\"\"\n    Get health status for the Content Prep Agent (AGENT-016).\n    \n    Returns:\n        ContentPrepHealthResponse: Detailed health status including connectivity and metrics\n    \"\"\"\n    health_service = get_health_service()\n    \n    # Get basic agent information\n    agent_info = {\n        \"agent_id\": \"AGENT-016\",\n        \"name\": \"Content Prep Agent\",\n        \"version\": get_version(),\n        \"uptime\": calculate_uptime()\n    }\n    \n    # Get processing metrics\n    metrics = {\n        \"recent_error_count\": await health_service.get_recent_error_count(),\n        \"pending_content_sets\": await health_service.get_pending_content_sets_count(),\n        \"active_processing_count\": await health_service.get_active_processing_count()\n    }\n    \n    # Check database connectivity\n    connectivity = {\n        \"supabase\": await health_service.check_supabase_connectivity(),\n        \"neo4j\": await health_service.check_neo4j_connectivity()\n    }\n    \n    return ContentPrepHealthResponse(\n        status=\"healthy\" if all(connectivity.values()) else \"degraded\",\n        agent=agent_info,\n        metrics=metrics,\n        connectivity=connectivity\n    )\n```\n\n2. Create or update the response model in app/models/health.py:\n```python\nclass ContentPrepHealthResponse(BaseModel):\n    status: Literal[\"healthy\", \"degraded\", \"unhealthy\"]\n    agent: Dict[str, Any]  # Contains agent_id, name, version, uptime\n    metrics: Dict[str, int]  # Contains error_count, pending_sets, active_processing\n    connectivity: Dict[str, bool]  # Contains status of connected services\n```\n\n3. Implement the health service methods in app/services/health_service.py:\n```python\nclass HealthService:\n    # ... existing code ...\n    \n    async def get_recent_error_count(self) -> int:\n        \"\"\"Get count of errors in the last 24 hours\"\"\"\n        try:\n            # Query error logs from database\n            return await self.db.fetch_error_count(hours=24)\n        except Exception:\n            return -1  # Indicate error retrieving count\n    \n    async def get_pending_content_sets_count(self) -> int:\n        \"\"\"Get count of content sets waiting to be processed\"\"\"\n        try:\n            return await self.db.fetch_content_sets_count(status=\"pending\")\n        except Exception:\n            return -1\n    \n    async def get_active_processing_count(self) -> int:\n        \"\"\"Get count of content sets currently being processed\"\"\"\n        try:\n            return await self.db.fetch_content_sets_count(status=\"processing\")\n        except Exception:\n            return -1\n    \n    async def check_supabase_connectivity(self) -> bool:\n        \"\"\"Check if Supabase connection is working\"\"\"\n        try:\n            # Simple query to verify connection\n            await self.db.execute(\"SELECT 1\")\n            return True\n        except Exception:\n            return False\n    \n    async def check_neo4j_connectivity(self) -> bool:\n        \"\"\"Check if Neo4j connection is working\"\"\"\n        try:\n            # Simple query to verify connection\n            result = await self.neo4j_client.run(\"MATCH (n) RETURN count(n) LIMIT 1\")\n            return result is not None\n        except Exception:\n            return False\n```\n\n4. Implement the utility functions in app/utils/system.py:\n```python\ndef get_version() -> str:\n    \"\"\"Get the current version of the application\"\"\"\n    try:\n        with open(\"VERSION\", \"r\") as f:\n            return f.read().strip()\n    except Exception:\n        return \"unknown\"\n\ndef calculate_uptime() -> int:\n    \"\"\"Calculate uptime in seconds since application start\"\"\"\n    global _start_time\n    return int(time.time() - _start_time)\n```\n\n5. Update the application startup to record start time:\n```python\n# In app/main.py\nimport time\n\n_start_time = time.time()\n\ndef start_application():\n    # ... existing code ...\n```",
        "testStrategy": "1. Create unit tests in tests/routes/test_content_prep.py:\n```python\nasync def test_health_endpoint_returns_correct_structure():\n    # Arrange\n    client = TestClient(app)\n    \n    # Act\n    response = client.get(\"/api/content-prep/health\")\n    \n    # Assert\n    assert response.status_code == 200\n    data = response.json()\n    assert \"status\" in data\n    assert \"agent\" in data\n    assert \"metrics\" in data\n    assert \"connectivity\" in data\n    \n    # Check agent info\n    assert \"agent_id\" in data[\"agent\"]\n    assert data[\"agent\"][\"agent_id\"] == \"AGENT-016\"\n    assert \"version\" in data[\"agent\"]\n    assert \"uptime\" in data[\"agent\"]\n    \n    # Check metrics\n    assert \"recent_error_count\" in data[\"metrics\"]\n    assert \"pending_content_sets\" in data[\"metrics\"]\n    assert \"active_processing_count\" in data[\"metrics\"]\n    \n    # Check connectivity\n    assert \"supabase\" in data[\"connectivity\"]\n    assert \"neo4j\" in data[\"connectivity\"]\n\nasync def test_health_endpoint_with_mocked_services():\n    # Arrange\n    app.dependency_overrides[get_health_service] = lambda: MockHealthService(\n        supabase_healthy=True,\n        neo4j_healthy=True,\n        error_count=5,\n        pending_sets=10,\n        active_processing=3\n    )\n    client = TestClient(app)\n    \n    # Act\n    response = client.get(\"/api/content-prep/health\")\n    \n    # Assert\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"status\"] == \"healthy\"\n    assert data[\"metrics\"][\"recent_error_count\"] == 5\n    assert data[\"metrics\"][\"pending_content_sets\"] == 10\n    assert data[\"metrics\"][\"active_processing_count\"] == 3\n    assert data[\"connectivity\"][\"supabase\"] is True\n    assert data[\"connectivity\"][\"neo4j\"] is True\n    \n    # Clean up\n    app.dependency_overrides.clear()\n\nasync def test_health_endpoint_with_degraded_services():\n    # Arrange\n    app.dependency_overrides[get_health_service] = lambda: MockHealthService(\n        supabase_healthy=True,\n        neo4j_healthy=False,\n        error_count=5,\n        pending_sets=10,\n        active_processing=3\n    )\n    client = TestClient(app)\n    \n    # Act\n    response = client.get(\"/api/content-prep/health\")\n    \n    # Assert\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"status\"] == \"degraded\"\n    assert data[\"connectivity\"][\"supabase\"] is True\n    assert data[\"connectivity\"][\"neo4j\"] is False\n    \n    # Clean up\n    app.dependency_overrides.clear()\n```\n\n2. Create the MockHealthService class in tests/mocks/health_service.py:\n```python\nclass MockHealthService:\n    def __init__(self, supabase_healthy=True, neo4j_healthy=True, \n                 error_count=0, pending_sets=0, active_processing=0):\n        self.supabase_healthy = supabase_healthy\n        self.neo4j_healthy = neo4j_healthy\n        self.error_count = error_count\n        self.pending_sets = pending_sets\n        self.active_processing = active_processing\n    \n    async def get_recent_error_count(self) -> int:\n        return self.error_count\n    \n    async def get_pending_content_sets_count(self) -> int:\n        return self.pending_sets\n    \n    async def get_active_processing_count(self) -> int:\n        return self.active_processing\n    \n    async def check_supabase_connectivity(self) -> bool:\n        return self.supabase_healthy\n    \n    async def check_neo4j_connectivity(self) -> bool:\n        return self.neo4j_healthy\n```\n\n3. Perform integration testing:\n   - Test the endpoint with actual database connections\n   - Verify correct status reporting when services are unavailable\n   - Test with various load conditions to ensure metrics are accurate\n\n4. Verify the endpoint follows the pattern established by other agent health endpoints by comparing the response structure with existing health endpoints.",
        "status": "done",
        "dependencies": [
          "122",
          "124"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-13T22:34:55.721Z"
      },
      {
        "id": "141",
        "title": "Design and Implement RAG Quality Metrics Database Schema",
        "description": "Create the database tables and indexes required to store RAG quality metrics, agent performance history, retrieval parameter configurations, and grounding results.",
        "details": "Implement the following tables in Supabase:\n- rag_quality_metrics (with indexes on intent_type, agent_id, created_at, and quality scores)\n- agent_performance_history (with indexes on agent_id and task_type)\n- retrieval_parameter_configs (with unique constraint on intent_type and query_complexity)\n- grounding_results (with indexes on query_id and grounding_score)\nEnsure all tables use UUID primary keys and appropriate foreign key relationships where needed.",
        "testStrategy": "Validate schema creation with SQL scripts. Confirm indexes are created and test insert/query performance with sample data.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:33:44.923Z"
      },
      {
        "id": "142",
        "title": "Implement Query Intent Analyzer Service",
        "description": "Develop a service that classifies user queries into intent types and extracts relevant metadata.",
        "details": "Create app/services/query_intent_analyzer.py with a class that:\n- Classifies queries as factual, analytical, comparative, procedural, or creative\n- Extracts entities and calculates complexity score (0-1)\n- Suggests optimal retrieval strategy and expected output format\n- Uses lightweight LLM (Claude Haiku) for classification and entity extraction\n- Returns QueryIntent object with all required fields",
        "testStrategy": "Test with labeled query dataset. Validate classification accuracy (>90%) and entity extraction completeness.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:37:04.909Z"
      },
      {
        "id": "143",
        "title": "Implement Retrieval Evaluator Service with RAGAS",
        "description": "Build a service that evaluates retrieval quality using RAGAS metrics.",
        "details": "Create app/services/retrieval_evaluator.py with:\n- Real-time evaluation for high-value queries using Claude 3.5 Haiku\n- Batch evaluation with 10% sampling for high-volume queries using Anthropic Batch API\n- Storage of metrics in rag_quality_metrics table\n- Configurable quality thresholds and retry logic (expanded retrieval, low confidence warning)\n- /api/rag/metrics endpoint for admin access",
        "testStrategy": "Test with various query types. Validate metric calculation, storage, and threshold enforcement. Confirm batch vs real-time evaluation works as specified.",
        "priority": "high",
        "dependencies": [
          "141"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:39:13.312Z"
      },
      {
        "id": "144",
        "title": "Implement Answer Grounding Evaluator Service",
        "description": "Develop a service that verifies answer claims against source documents to prevent hallucinations.",
        "details": "Create app/services/answer_grounding_evaluator.py with:\n- Claim extraction from answers using LLM\n- Claim-to-source alignment scoring (0-1)\n- Calculation of overall grounding score and confidence level\n- Flagging of ungrounded claims\n- Storage of results in grounding_results table\n- Blocking of answers below critical grounding threshold",
        "testStrategy": "Test with answers containing known hallucinations. Validate claim extraction, grounding scoring, and blocking logic.",
        "priority": "high",
        "dependencies": [
          "141"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:41:24.087Z"
      },
      {
        "id": "145",
        "title": "Implement Adaptive Retrieval Service",
        "description": "Build a service that dynamically adjusts retrieval parameters based on query characteristics and historical performance.",
        "details": "Create app/services/adaptive_retrieval_service.py with:\n- Retrieval parameter configuration per intent type and complexity\n- Adjustment of weights (dense, sparse, fuzzy), top_k, rerank_threshold, graph_expansion_depth\n- Feedback recording and parameter optimization\n- Manual override capability\n- Audit logging of parameter decisions",
        "testStrategy": "Test with different query types and feedback scenarios. Validate parameter adjustment and optimization logic.",
        "priority": "medium",
        "dependencies": [
          "141",
          "142"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:44:12.061Z"
      },
      {
        "id": "146",
        "title": "Implement Agent Selector Service",
        "description": "Develop a service that intelligently routes tasks to optimal agents based on capabilities and performance history.",
        "details": "Create app/services/agent_selector_service.py with:\n- Agent capability mapping for all 17 agents\n- Historical performance tracking per agent per task type\n- Optimal agent selection based on performance history\n- Exploration factor for underutilized agents\n- Selection explanation for transparency\n- Outcome recording for future optimization",
        "testStrategy": "Test with various task types and agent performance scenarios. Validate selection logic and exploration factor.",
        "priority": "medium",
        "dependencies": [
          "141",
          "142"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:46:27.280Z"
      },
      {
        "id": "147",
        "title": "Implement Output Validator Service",
        "description": "Build a service that validates and corrects agent outputs before delivery.",
        "details": "Create app/services/output_validator_service.py with:\n- Format compliance checking (JSON, markdown, etc.)\n- Completeness validation (required sections present)\n- Consistency checking (no internal contradictions)\n- Style guideline enforcement\n- Auto-correction of formatting issues\n- Flagging of uncorrectable issues for human review",
        "testStrategy": "Test with outputs containing various issues. Validate detection, correction, and flagging logic.",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:48:43.869Z"
      },
      {
        "id": "148",
        "title": "Integrate Services into RAG Pipeline",
        "description": "Connect all enhancement services into the existing RAG pipeline.",
        "details": "Update the RAG pipeline to:\n- Route queries through Query Intent Analyzer\n- Use Adaptive Retrieval Service for parameter adjustment\n- Evaluate retrieval quality with Retrieval Evaluator\n- Select agent with Agent Selector Service\n- Validate answer grounding with Answer Grounding Evaluator\n- Validate output with Output Validator Service\n- Ensure proper error handling and fallback mechanisms",
        "testStrategy": "Test end-to-end pipeline with various query types. Validate service integration and error handling.",
        "priority": "high",
        "dependencies": [
          "142",
          "143",
          "144",
          "145",
          "146",
          "147"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:50:59.604Z"
      },
      {
        "id": "149",
        "title": "Develop Metrics Dashboard",
        "description": "Create a dashboard for administrators to monitor RAG quality metrics.",
        "details": "Build a web interface that:\n- Displays real-time RAGAS scores (Context Relevance, Answer Relevance, Faithfulness, Coverage)\n- Allows drill-down by query type, time period, and agent\n- Shows trends and optimization opportunities\n- Loads within 2 seconds with 30-day data",
        "testStrategy": "Test dashboard performance and data accuracy with sample metrics data.",
        "priority": "medium",
        "dependencies": [
          "143"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:54:03.894Z"
      },
      {
        "id": "150",
        "title": "Implement Quality Gate and Fallback Logic",
        "description": "Add logic to handle quality gate failures and provide fallback options.",
        "details": "Implement:\n- Retry with expanded retrieval (increased top_k, graph expansion) when quality threshold breached\n- Visible 'low confidence' warning if still below threshold\n- Logging of quality gate failures and fallback actions\n- Alert system for persistent quality issues",
        "testStrategy": "Test with queries that trigger quality gate failures. Validate retry logic and warning display.",
        "priority": "medium",
        "dependencies": [
          "143",
          "144"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-14T18:56:19.646Z"
      },
      {
        "id": "151",
        "title": "Implement PostgreSQL Session Variables for Row-Level Security",
        "description": "Create PostgreSQL functions for managing row-level security context, including set_rls_context, get_rls_context, clear_rls_context, current_user_id, and session_has_role functions. Update middleware to call these functions via Supabase RPC.",
        "details": "Implement the following PostgreSQL functions in a new migration file `migrations/rls_context_functions.sql`:\n\n1. `set_rls_context(key text, value text)`: Sets a session variable for RLS context\n   ```sql\n   CREATE OR REPLACE FUNCTION set_rls_context(key text, value text)\n   RETURNS void AS $$\n   BEGIN\n     PERFORM set_config('app.rls_' || key, value, false);\n   EXCEPTION\n     WHEN OTHERS THEN\n       RAISE EXCEPTION 'Failed to set RLS context: %', SQLERRM;\n   END;\n   $$ LANGUAGE plpgsql SECURITY DEFINER;\n   ```\n\n2. `get_rls_context(key text)`: Retrieves a session variable\n   ```sql\n   CREATE OR REPLACE FUNCTION get_rls_context(key text)\n   RETURNS text AS $$\n   BEGIN\n     RETURN current_setting('app.rls_' || key, true);\n   EXCEPTION\n     WHEN OTHERS THEN\n       RETURN NULL;\n   END;\n   $$ LANGUAGE plpgsql SECURITY DEFINER;\n   ```\n\n3. `clear_rls_context()`: Clears all RLS context variables\n   ```sql\n   CREATE OR REPLACE FUNCTION clear_rls_context()\n   RETURNS void AS $$\n   DECLARE\n     setting_name text;\n   BEGIN\n     FOR setting_name IN \n       SELECT name FROM pg_settings WHERE name LIKE 'app.rls_%'\n     LOOP\n       PERFORM set_config(setting_name, NULL, false);\n     END LOOP;\n   EXCEPTION\n     WHEN OTHERS THEN\n       RAISE EXCEPTION 'Failed to clear RLS context: %', SQLERRM;\n   END;\n   $$ LANGUAGE plpgsql SECURITY DEFINER;\n   ```\n\n4. `current_user_id()`: Returns the current user ID from context\n   ```sql\n   CREATE OR REPLACE FUNCTION current_user_id()\n   RETURNS uuid AS $$\n   BEGIN\n     RETURN get_rls_context('user_id')::uuid;\n   EXCEPTION\n     WHEN OTHERS THEN\n       RETURN NULL;\n   END;\n   $$ LANGUAGE plpgsql SECURITY DEFINER;\n   ```\n\n5. `session_has_role(role_name text)`: Checks if the current session has a specific role\n   ```sql\n   CREATE OR REPLACE FUNCTION session_has_role(role_name text)\n   RETURNS boolean AS $$\n   DECLARE\n     roles text;\n   BEGIN\n     roles := get_rls_context('roles');\n     RETURN roles LIKE '%' || role_name || '%';\n   EXCEPTION\n     WHEN OTHERS THEN\n       RETURN false;\n   END;\n   $$ LANGUAGE plpgsql SECURITY DEFINER;\n   ```\n\nNext, update the middleware in `app/middleware/auth.py` to set these context variables via Supabase RPC calls:\n\n```python\nfrom fastapi import Request, Response\nfrom app.db.supabase import get_supabase_client\n\nasync def rls_middleware(request: Request, call_next):\n    # Extract user information from request (JWT token)\n    user_id = extract_user_id(request)\n    user_roles = extract_user_roles(request)\n    \n    try:\n        # Get Supabase client\n        supabase = get_supabase_client()\n        \n        # Set RLS context via RPC\n        if user_id:\n            await supabase.rpc('set_rls_context', {'key': 'user_id', 'value': str(user_id)})\n            \n        if user_roles:\n            roles_str = ','.join(user_roles)\n            await supabase.rpc('set_rls_context', {'key': 'roles', 'value': roles_str})\n        \n        # Process the request\n        response = await call_next(request)\n        \n        # Clear RLS context after request is processed\n        await supabase.rpc('clear_rls_context')\n        \n        return response\n    except Exception as e:\n        # Security-first error handling - return 503 on failure\n        return Response(\n            content={\"error\": \"Service temporarily unavailable\"},\n            status_code=503,\n            media_type=\"application/json\"\n        )\n```\n\nFinally, register the middleware in the main FastAPI application:\n\n```python\n# In app/main.py\nfrom app.middleware.auth import rls_middleware\n\napp = FastAPI()\napp.middleware(\"http\")(rls_middleware)\n```",
        "testStrategy": "1. **Unit Tests for PostgreSQL Functions**:\n   - Create a test file `tests/db/test_rls_functions.sql` to test each function individually\n   - Test `set_rls_context` with various key-value pairs\n   - Test `get_rls_context` retrieves the correct values\n   - Test `clear_rls_context` properly removes all context variables\n   - Test `current_user_id` returns the expected UUID\n   - Test `session_has_role` correctly identifies roles in the context\n\n2. **Integration Tests for Middleware**:\n   - Create a test file `tests/middleware/test_rls_middleware.py`\n   - Test middleware correctly extracts user information from requests\n   - Test middleware makes appropriate RPC calls to Supabase\n   - Test middleware clears context after request processing\n   - Test error handling returns 503 status code on failure\n\n3. **Security Tests**:\n   - Verify RLS policies correctly use the context variables\n   - Test that unauthorized access is properly prevented\n   - Test that context variables are properly isolated between concurrent requests\n   - Verify SQL injection prevention in the functions\n\n4. **Performance Tests**:\n   - Measure overhead of setting and clearing context variables\n   - Test with concurrent requests to ensure no context leakage\n   - Benchmark database queries with RLS enabled vs. disabled\n\n5. **End-to-End Tests**:\n   - Create test scenarios that use RLS to restrict data access\n   - Verify different user roles see appropriate data\n   - Test API endpoints that rely on RLS for data filtering",
        "status": "done",
        "dependencies": [
          "101",
          "123"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-14T20:19:52.791Z"
      },
      {
        "id": "152",
        "title": "WebSocket Authentication with JWT Validation",
        "description": "Implement JWT validation for WebSocket connections, add token refresh mechanism for long-running connections, implement connection timeout and heartbeat functionality, and create integration tests.",
        "details": "Implement secure WebSocket authentication with the following components:\n\n1. JWT Validation for WebSocket Connections:\n   - Create a middleware for validating JWT tokens during WebSocket handshake\n   - Implement token extraction from the connection request headers or query parameters\n   - Verify token signature, expiration, and claims\n   - Store authenticated user information in the WebSocket connection context\n\n```python\n# In app/middleware/websocket_auth.py\nfrom fastapi import WebSocket, status\nfrom jose import jwt, JWTError\nfrom app.core.config import settings\nfrom app.core.security import ALGORITHM\n\nasync def websocket_auth_middleware(websocket: WebSocket):\n    try:\n        # Extract token from query params or headers\n        token = websocket.query_params.get(\"token\") or websocket.headers.get(\"Authorization\", \"\").replace(\"Bearer \", \"\")\n        \n        if not token:\n            await websocket.close(code=status.WS_1008_POLICY_VIOLATION, reason=\"Missing authentication token\")\n            return None\n            \n        # Validate token\n        payload = jwt.decode(token, settings.SECRET_KEY, algorithms=[ALGORITHM])\n        user_id = payload.get(\"sub\")\n        \n        if not user_id:\n            await websocket.close(code=status.WS_1008_POLICY_VIOLATION, reason=\"Invalid token payload\")\n            return None\n            \n        # Store user info in connection state\n        websocket.state.user_id = user_id\n        return user_id\n    except JWTError:\n        await websocket.close(code=status.WS_1008_POLICY_VIOLATION, reason=\"Invalid authentication token\")\n        return None\n```\n\n2. Token Refresh Mechanism:\n   - Implement a token refresh protocol for WebSocket connections\n   - Create a refresh event type that clients can send before token expiration\n   - Implement server-side refresh logic that validates the current token and issues a new one\n   - Send the new token back to the client through the WebSocket connection\n\n```python\n# In app/websockets/handlers.py\nfrom fastapi import WebSocket\nfrom app.core.security import create_access_token\nfrom app.models.user import User\nfrom datetime import datetime, timedelta\nimport json\n\nasync def handle_token_refresh(websocket: WebSocket, data: dict):\n    user_id = websocket.state.user_id\n    \n    # Get user from database to ensure they still exist and are active\n    user = await User.get(user_id)\n    if not user or not user.is_active:\n        await websocket.close(code=status.WS_1008_POLICY_VIOLATION, reason=\"User no longer valid\")\n        return\n        \n    # Create new token\n    new_token = create_access_token(\n        data={\"sub\": user_id},\n        expires_delta=timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)\n    )\n    \n    # Send new token to client\n    await websocket.send_text(json.dumps({\n        \"event\": \"token_refresh\",\n        \"data\": {\"token\": new_token}\n    }))\n```\n\n3. Connection Timeout and Heartbeat:\n   - Implement a heartbeat mechanism to detect stale connections\n   - Set up a configurable connection timeout (default: 60 seconds)\n   - Create a background task for each connection to monitor heartbeat status\n   - Implement client-side ping messages and server-side pong responses\n\n```python\n# In app/websockets/connection_manager.py\nimport asyncio\nfrom fastapi import WebSocket\nfrom typing import Dict, Set\nimport time\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: Dict[str, WebSocket] = {}\n        self.last_heartbeat: Dict[str, float] = {}\n        self.heartbeat_interval = 30  # seconds\n        self.connection_timeout = 60  # seconds\n        \n    async def connect(self, websocket: WebSocket, user_id: str):\n        await websocket.accept()\n        self.active_connections[user_id] = websocket\n        self.last_heartbeat[user_id] = time.time()\n        \n        # Start heartbeat monitor\n        asyncio.create_task(self._heartbeat_monitor(user_id))\n        \n    async def disconnect(self, user_id: str):\n        if user_id in self.active_connections:\n            del self.active_connections[user_id]\n        if user_id in self.last_heartbeat:\n            del self.last_heartbeat[user_id]\n            \n    async def handle_heartbeat(self, user_id: str):\n        self.last_heartbeat[user_id] = time.time()\n        if user_id in self.active_connections:\n            await self.active_connections[user_id].send_text(json.dumps({\n                \"event\": \"pong\",\n                \"timestamp\": time.time()\n            }))\n            \n    async def _heartbeat_monitor(self, user_id: str):\n        while user_id in self.active_connections:\n            await asyncio.sleep(5)  # Check every 5 seconds\n            \n            if user_id not in self.last_heartbeat:\n                break\n                \n            elapsed = time.time() - self.last_heartbeat[user_id]\n            if elapsed > self.connection_timeout:\n                # Connection timed out\n                if user_id in self.active_connections:\n                    await self.active_connections[user_id].close(\n                        code=status.WS_1008_POLICY_VIOLATION,\n                        reason=\"Connection timeout\"\n                    )\n                await self.disconnect(user_id)\n                break\n```\n\n4. WebSocket Route Implementation:\n   - Create WebSocket endpoint with authentication middleware\n   - Implement message handling with event routing\n   - Add support for the token refresh and heartbeat events\n\n```python\n# In app/routes/websocket.py\nfrom fastapi import APIRouter, WebSocket, Depends\nfrom app.middleware.websocket_auth import websocket_auth_middleware\nfrom app.websockets.connection_manager import ConnectionManager\nimport json\n\nrouter = APIRouter()\nconnection_manager = ConnectionManager()\n\n@router.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    user_id = await websocket_auth_middleware(websocket)\n    if not user_id:\n        return  # Connection was closed by middleware\n        \n    await connection_manager.connect(websocket, user_id)\n    \n    try:\n        while True:\n            data = await websocket.receive_text()\n            message = json.loads(data)\n            \n            event_type = message.get(\"event\")\n            event_data = message.get(\"data\", {})\n            \n            if event_type == \"heartbeat\":\n                await connection_manager.handle_heartbeat(user_id)\n            elif event_type == \"refresh_token\":\n                await handle_token_refresh(websocket, event_data)\n            else:\n                # Handle other message types\n                pass\n                \n    except Exception as e:\n        # Log the error\n        pass\n    finally:\n        await connection_manager.disconnect(user_id)\n```\n\n5. Client-Side Implementation Guidance:\n   - Provide example client code for handling token refresh and heartbeat\n   - Document the WebSocket protocol for authentication and token refresh\n   - Include error handling recommendations",
        "testStrategy": "1. Unit Tests:\n   - Test JWT validation with valid and invalid tokens\n   - Test token extraction from different sources (headers, query params)\n   - Test token refresh logic with various token states (valid, near-expiry, expired)\n   - Test heartbeat mechanism with simulated timeouts\n   - Test connection manager with multiple concurrent connections\n\n2. Integration Tests:\n   - Create a test WebSocket client that connects to the server\n   - Test the full authentication flow from connection to token refresh\n   - Test heartbeat mechanism with real timing\n   - Test connection timeout by deliberately missing heartbeats\n   - Test reconnection scenarios after disconnection\n\n3. Security Tests:\n   - Test with tampered JWT tokens to ensure proper validation\n   - Test with expired tokens to verify rejection\n   - Test token refresh with invalid refresh attempts\n   - Verify that unauthenticated connections are properly rejected\n   - Test rate limiting for connection attempts and token refreshes\n\n4. Load Tests:\n   - Test with multiple concurrent WebSocket connections\n   - Measure performance under load for token validation and refresh\n   - Test heartbeat mechanism with many connections\n   - Verify resource cleanup after disconnections\n\n5. End-to-End Tests:\n   - Create a test client application that uses the WebSocket connection\n   - Test the complete user flow including authentication, heartbeat, and token refresh\n   - Verify proper handling of network interruptions\n   - Test long-running connections with multiple token refreshes\n\n6. Test Automation:\n   - Create automated tests that can be run in CI/CD pipeline\n   - Implement test fixtures for WebSocket connections\n   - Create mock authentication services for testing\n   - Document test coverage and results",
        "status": "done",
        "dependencies": [
          "137",
          "110"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-15T05:23:50.170Z"
      },
      {
        "id": "153",
        "title": "Neo4j Graph Sync Error Handling",
        "description": "Implement robust error handling for Neo4j graph synchronization operations including retry logic with exponential backoff, circuit breaker pattern, and a dead letter queue for failed operations.",
        "details": "Enhance the Neo4j synchronization system with comprehensive error handling:\n\n1. Implement retry logic with exponential backoff:\n   ```python\n   from tenacity import retry, stop_after_attempt, wait_exponential\n   \n   @retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=2, max=60))\n   async def perform_graph_sync_operation(operation_data):\n       # Existing sync operation code\n       try:\n           result = await neo4j_client.execute_query(operation_data.query, operation_data.params)\n           return result\n       except Exception as e:\n           logger.error(f\"Graph sync operation failed: {str(e)}\")\n           raise  # Let tenacity handle the retry\n   ```\n\n2. Implement circuit breaker pattern:\n   ```python\n   from circuitbreaker import circuit\n   \n   # Configure the circuit breaker\n   @circuit(failure_threshold=5, recovery_timeout=60, expected_exception=Neo4jConnectionError)\n   async def protected_graph_operation(operation_data):\n       # Existing operation code\n       return await neo4j_client.execute_query(operation_data.query, operation_data.params)\n   ```\n\n3. Create a dead letter queue for failed operations:\n   ```python\n   class DeadLetterQueue:\n       def __init__(self, redis_client):\n           self.redis = redis_client\n           self.queue_key = \"neo4j:sync:dead_letter_queue\"\n           \n       async def add_failed_operation(self, operation_data, error_info):\n           entry = {\n               \"operation_id\": str(uuid.uuid4()),\n               \"timestamp\": datetime.utcnow().isoformat(),\n               \"operation_data\": operation_data,\n               \"error_info\": error_info,\n               \"retry_count\": 0\n           }\n           await self.redis.lpush(self.queue_key, json.dumps(entry))\n           \n       async def get_failed_operations(self, limit=100):\n           # Retrieve operations for manual inspection or retry\n           operations = await self.redis.lrange(self.queue_key, 0, limit-1)\n           return [json.loads(op) for op in operations]\n           \n       async def remove_operation(self, operation_id):\n           # Remove after successful manual processing\n           operations = await self.get_failed_operations()\n           for i, op in enumerate(operations):\n               if op[\"operation_id\"] == operation_id:\n                   await self.redis.lrem(self.queue_key, 1, json.dumps(op))\n                   return True\n           return False\n   ```\n\n4. Implement monitoring alerts:\n   ```python\n   class GraphSyncMonitor:\n       def __init__(self, alert_service):\n           self.alert_service = alert_service\n           self.failure_counts = {}\n           \n       async def record_operation_result(self, operation_type, success):\n           # Track success/failure rates\n           if operation_type not in self.failure_counts:\n               self.failure_counts[operation_type] = {\"success\": 0, \"failure\": 0}\n               \n           if success:\n               self.failure_counts[operation_type][\"success\"] += 1\n           else:\n               self.failure_counts[operation_type][\"failure\"] += 1\n               \n           # Check if alert threshold is reached\n           self._check_alert_threshold(operation_type)\n           \n       def _check_alert_threshold(self, operation_type):\n           stats = self.failure_counts[operation_type]\n           total = stats[\"success\"] + stats[\"failure\"]\n           \n           if total >= 10:  # Minimum sample size\n               failure_rate = stats[\"failure\"] / total\n               \n               if failure_rate > 0.2:  # 20% failure rate threshold\n                   self.alert_service.send_alert(\n                       level=\"warning\",\n                       title=f\"High Neo4j sync failure rate for {operation_type}\",\n                       message=f\"Failure rate of {failure_rate:.1%} detected for {operation_type} operations\",\n                       metadata={\"operation_type\": operation_type, \"stats\": stats}\n                   )\n   ```\n\n5. Integration with existing Neo4j client:\n   ```python\n   class EnhancedNeo4jClient:\n       def __init__(self, base_client, dead_letter_queue, monitor):\n           self.base_client = base_client\n           self.dlq = dead_letter_queue\n           self.monitor = monitor\n           \n       @circuit(failure_threshold=5, recovery_timeout=60)\n       @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=30))\n       async def execute_sync_operation(self, operation_type, query, params):\n           try:\n               result = await self.base_client.execute_query(query, params)\n               await self.monitor.record_operation_result(operation_type, success=True)\n               return result\n           except Exception as e:\n               await self.monitor.record_operation_result(operation_type, success=False)\n               await self.dlq.add_failed_operation(\n                   {\"type\": operation_type, \"query\": query, \"params\": params},\n                   {\"error\": str(e), \"traceback\": traceback.format_exc()}\n               )\n               raise\n   ```\n\n6. Create a background worker for processing the dead letter queue:\n   ```python\n   @shared_task\n   async def process_dead_letter_queue():\n       dlq = DeadLetterQueue(get_redis_client())\n       client = Neo4jHTTPClient()\n       \n       failed_ops = await dlq.get_failed_operations(limit=50)\n       for op in failed_ops:\n           if op[\"retry_count\"] < 5:  # Max retry attempts\n               try:\n                   # Attempt to reprocess\n                   await client.execute_query(op[\"operation_data\"][\"query\"], op[\"operation_data\"][\"params\"])\n                   # If successful, remove from queue\n                   await dlq.remove_operation(op[\"operation_id\"])\n               except Exception as e:\n                   # Update retry count and push back to queue\n                   op[\"retry_count\"] += 1\n                   op[\"last_error\"] = str(e)\n                   await dlq.remove_operation(op[\"operation_id\"])\n                   await dlq.add_failed_operation(op[\"operation_data\"], \n                                                {\"error\": str(e), \"retry_count\": op[\"retry_count\"]})\n   ```",
        "testStrategy": "1. Unit tests for retry logic:\n   - Test successful operation after temporary failures\n   - Test that retry count respects maximum attempts\n   - Verify exponential backoff timing between retries\n   - Test behavior when max retries are exhausted\n\n2. Unit tests for circuit breaker:\n   - Test circuit transitions from closed to open state after threshold failures\n   - Test that requests fail fast when circuit is open\n   - Test half-open state behavior after recovery timeout\n   - Test circuit closing after successful operations in half-open state\n\n3. Unit tests for dead letter queue:\n   - Test adding failed operations to the queue\n   - Test retrieving operations from the queue\n   - Test removing operations from the queue\n   - Test queue persistence across application restarts\n\n4. Integration tests:\n   - Test end-to-end flow with simulated Neo4j failures\n   - Verify operations eventually succeed after temporary failures\n   - Verify operations go to dead letter queue after permanent failures\n   - Test background worker processing of dead letter queue\n\n5. Monitoring and alerting tests:\n   - Test alert generation when failure thresholds are reached\n   - Test alert suppression during circuit open state\n   - Verify metrics are correctly updated for success/failure counts\n   - Test alert resolution when failure rates return to normal\n\n6. Performance tests:\n   - Measure impact of retry logic on system performance\n   - Test system behavior under high load with partial Neo4j outages\n   - Verify dead letter queue performance with large numbers of failed operations\n\n7. Chaos testing:\n   - Simulate Neo4j service outages and verify system resilience\n   - Test recovery behavior after Neo4j service restoration\n   - Verify no data loss during outage scenarios",
        "status": "done",
        "dependencies": [
          "101",
          "110",
          "137"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-15T05:44:15.804Z"
      },
      {
        "id": "154",
        "title": "Implement Standardized Exception Handling Framework",
        "description": "Create a comprehensive exception handling framework with a custom exception hierarchy, global middleware, structured error responses with request_id tracking, and contextual error logging.",
        "details": "Implement a standardized exception handling framework to improve error management across the application:\n\n1. Create a custom exception hierarchy in `app/exceptions/`:\n   ```python\n   # app/exceptions/base.py\n   from typing import Optional, Dict, Any\n   \n   class BaseAppException(Exception):\n       \"\"\"Base exception class for all application exceptions\"\"\"\n       def __init__(\n           self, \n           message: str, \n           error_code: str = \"INTERNAL_ERROR\",\n           status_code: int = 500,\n           details: Optional[Dict[str, Any]] = None\n       ):\n           self.message = message\n           self.error_code = error_code\n           self.status_code = status_code\n           self.details = details or {}\n           super().__init__(self.message)\n   \n   # app/exceptions/client_errors.py\n   from .base import BaseAppException\n   \n   class BadRequestException(BaseAppException):\n       \"\"\"Exception for invalid request data\"\"\"\n       def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):\n           super().__init__(\n               message=message,\n               error_code=\"BAD_REQUEST\",\n               status_code=400,\n               details=details\n           )\n   \n   class NotFoundException(BaseAppException):\n       \"\"\"Exception for resource not found\"\"\"\n       def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):\n           super().__init__(\n               message=message,\n               error_code=\"NOT_FOUND\",\n               status_code=404,\n               details=details\n           )\n   \n   # app/exceptions/server_errors.py\n   from .base import BaseAppException\n   \n   class DatabaseException(BaseAppException):\n       \"\"\"Exception for database errors\"\"\"\n       def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):\n           super().__init__(\n               message=message,\n               error_code=\"DATABASE_ERROR\",\n               status_code=500,\n               details=details\n           )\n   ```\n\n2. Implement a global exception handler middleware in `app/middleware/exception_handler.py`:\n   ```python\n   import uuid\n   import logging\n   import traceback\n   from fastapi import Request, Response\n   from fastapi.responses import JSONResponse\n   from starlette.middleware.base import BaseHTTPMiddleware\n   \n   from app.exceptions.base import BaseAppException\n   from app.models.errors import AgentErrorResponse, ErrorType\n   \n   logger = logging.getLogger(__name__)\n   \n   class ExceptionHandlerMiddleware(BaseHTTPMiddleware):\n       async def dispatch(self, request: Request, call_next):\n           # Generate unique request ID for tracking\n           request_id = str(uuid.uuid4())\n           request.state.request_id = request_id\n           \n           try:\n               response = await call_next(request)\n               return response\n           except BaseAppException as exc:\n               # Handle custom application exceptions\n               return self._handle_app_exception(exc, request)\n           except Exception as exc:\n               # Handle unexpected exceptions\n               return self._handle_unexpected_exception(exc, request)\n       \n       def _handle_app_exception(self, exc: BaseAppException, request: Request) -> JSONResponse:\n           logger.error(\n               f\"Application exception: {exc.error_code} - {exc.message}\",\n               extra={\n                   \"request_id\": request.state.request_id,\n                   \"path\": request.url.path,\n                   \"method\": request.method,\n                   \"details\": exc.details,\n                   \"error_code\": exc.error_code\n               }\n           )\n           \n           return JSONResponse(\n               status_code=exc.status_code,\n               content=AgentErrorResponse(\n                   error_code=exc.error_code,\n                   error_type=ErrorType.PERMANENT if exc.status_code >= 400 and exc.status_code < 500 else ErrorType.RETRIABLE,\n                   agent_id=request.app.state.agent_id,\n                   message=exc.message,\n                   details=exc.details,\n                   request_id=request.state.request_id\n               ).dict()\n           )\n       \n       def _handle_unexpected_exception(self, exc: Exception, request: Request) -> JSONResponse:\n           # Log full traceback for unexpected exceptions\n           error_details = {\n               \"traceback\": traceback.format_exc(),\n               \"exception_type\": exc.__class__.__name__\n           }\n           \n           logger.error(\n               f\"Unexpected exception: {str(exc)}\",\n               extra={\n                   \"request_id\": request.state.request_id,\n                   \"path\": request.url.path,\n                   \"method\": request.method,\n                   \"details\": error_details\n               },\n               exc_info=True\n           )\n           \n           return JSONResponse(\n               status_code=500,\n               content=AgentErrorResponse(\n                   error_code=\"INTERNAL_SERVER_ERROR\",\n                   error_type=ErrorType.RETRIABLE,\n                   agent_id=request.app.state.agent_id,\n                   message=\"An unexpected error occurred\",\n                   details={\"request_id\": request.state.request_id},\n                   request_id=request.state.request_id\n               ).dict()\n           )\n   ```\n\n3. Enhance the existing error response model in `app/models/errors.py` to include request_id:\n   ```python\n   from pydantic import BaseModel, Field\n   from typing import Optional, Dict, Any\n   from datetime import datetime\n   from enum import Enum\n   \n   class ErrorType(str, Enum):\n       RETRIABLE = \"retriable\"\n       PERMANENT = \"permanent\"\n   \n   class AgentErrorResponse(BaseModel):\n       error_code: str\n       error_type: ErrorType\n       agent_id: str\n       message: str\n       details: Optional[Dict[str, Any]] = None\n       timestamp: datetime = Field(default_factory=datetime.utcnow)\n       request_id: str\n   ```\n\n4. Create a contextual error logging utility in `app/utils/error_logging.py`:\n   ```python\n   import logging\n   import inspect\n   import uuid\n   from typing import Dict, Any, Optional\n   from fastapi import Request\n   \n   logger = logging.getLogger(__name__)\n   \n   class ErrorLogger:\n       @staticmethod\n       def log_error(\n           error: Exception,\n           request: Optional[Request] = None,\n           context: Optional[Dict[str, Any]] = None,\n           level: int = logging.ERROR\n       ):\n           # Get calling function and module\n           frame = inspect.currentframe().f_back\n           func_name = frame.f_code.co_name\n           module_name = frame.f_globals['__name__']\n           \n           # Build error context\n           error_context = {\n               \"exception_type\": error.__class__.__name__,\n               \"function\": func_name,\n               \"module\": module_name\n           }\n           \n           # Add request context if available\n           if request:\n               request_id = getattr(request.state, \"request_id\", str(uuid.uuid4()))\n               error_context.update({\n                   \"request_id\": request_id,\n                   \"path\": request.url.path,\n                   \"method\": request.method,\n                   \"client_ip\": request.client.host\n               })\n           \n           # Add custom context if provided\n           if context:\n               error_context.update(context)\n           \n           # Log the error with context\n           logger.log(\n               level,\n               f\"{error.__class__.__name__} in {module_name}.{func_name}: {str(error)}\",\n               extra=error_context,\n               exc_info=True\n           )\n   ```\n\n5. Register the middleware in the FastAPI application setup:\n   ```python\n   # app/main.py\n   from fastapi import FastAPI\n   from app.middleware.exception_handler import ExceptionHandlerMiddleware\n   \n   app = FastAPI()\n   app.add_middleware(ExceptionHandlerMiddleware)\n   ```\n\n6. Create usage examples in `app/docs/exception_handling.md` to demonstrate proper usage:\n   ```markdown\n   # Exception Handling Guidelines\n   \n   ## Raising Custom Exceptions\n   \n   ```python\n   from app.exceptions.client_errors import BadRequestException\n   \n   def validate_user_input(data):\n       if not data.get(\"username\"):\n           raise BadRequestException(\n               message=\"Username is required\",\n               details={\"field\": \"username\", \"provided\": data.get(\"username\")}\n           )\n   ```\n   \n   ## Using the Error Logger\n   \n   ```python\n   from fastapi import APIRouter, Request, Depends\n   from app.utils.error_logging import ErrorLogger\n   \n   router = APIRouter()\n   \n   @router.get(\"/items/{item_id}\")\n   async def get_item(item_id: int, request: Request):\n       try:\n           # Attempt to get item\n           item = await db.get_item(item_id)\n           if not item:\n               raise ItemNotFoundException(f\"Item with ID {item_id} not found\")\n           return item\n       except Exception as e:\n           # Log error with context\n           ErrorLogger.log_error(\n               error=e,\n               request=request,\n               context={\"item_id\": item_id}\n           )\n           # Let middleware handle the exception\n           raise\n   ```\n   ```",
        "testStrategy": "1. Unit test the custom exception hierarchy:\n   - Test each exception class to ensure it properly inherits from BaseAppException\n   - Verify that status codes, error codes, and messages are correctly set\n   - Test serialization of exception details\n\n2. Test the ExceptionHandlerMiddleware:\n   - Create test cases for each type of custom exception\n   - Verify correct status codes are returned in responses\n   - Ensure request_id is properly generated and included in responses\n   - Test handling of unexpected exceptions\n   - Verify error response format matches the AgentErrorResponse model\n\n3. Test the enhanced AgentErrorResponse model:\n   - Verify the model correctly validates with and without optional fields\n   - Test serialization/deserialization with request_id included\n   - Ensure timestamp is automatically generated\n\n4. Test the ErrorLogger utility:\n   - Mock the logging system to capture log output\n   - Test logging with and without request context\n   - Verify all context fields are properly included in log entries\n   - Test different logging levels\n\n5. Integration tests:\n   - Create test endpoints that raise different types of exceptions\n   - Verify middleware correctly intercepts and formats all exceptions\n   - Test request_id propagation through the request lifecycle\n   - Verify log entries contain the same request_id as the response\n\n6. Performance tests:\n   - Measure overhead of exception handling middleware\n   - Test with high concurrency to ensure no performance degradation\n\n7. Documentation verification:\n   - Review exception handling documentation for clarity\n   - Ensure all examples work as described\n   - Verify documentation covers all exception types and usage patterns",
        "status": "done",
        "dependencies": [
          "135",
          "153"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-15T17:44:47.337Z"
      },
      {
        "id": "155",
        "title": "Implement Claude Haiku-based Entity Extraction for Research Tasks",
        "description": "Develop a system that uses Claude Haiku to extract entities, topics, and facts from research tasks with structured output and store the extracted information in Neo4j graph database.",
        "details": "Implement a Claude Haiku-based entity extraction system for research tasks:\n\n1. Create a new service class `app/services/entity_extraction_service.py`:\n   ```python\n   from typing import Dict, List, Any, Optional\n   from app.clients.claude_client import ClaudeClient\n   from app.models.research_task import ResearchTask\n   from app.repositories.neo4j_repository import Neo4jRepository\n   \n   class EntityExtractionService:\n       def __init__(self, claude_client: ClaudeClient, neo4j_repo: Neo4jRepository):\n           self.claude_client = claude_client\n           self.neo4j_repo = neo4j_repo\n           \n       async def extract_entities(self, research_task: ResearchTask) -> Dict[str, Any]:\n           \"\"\"Extract entities from a research task using Claude Haiku\"\"\"\n           # Prepare prompt for Claude\n           prompt = self._build_extraction_prompt(research_task)\n           \n           # Call Claude Haiku\n           extraction_result = await self.claude_client.generate(\n               prompt=prompt,\n               model=\"claude-3-haiku-20240307\",\n               max_tokens=2000,\n               temperature=0.2,\n               response_format={\"type\": \"json\"}\n           )\n           \n           # Validate and process the extraction result\n           validated_result = self._validate_extraction_result(extraction_result)\n           \n           # Store entities in Neo4j\n           await self._store_entities_in_graph(research_task.id, validated_result)\n           \n           return validated_result\n       \n       def _build_extraction_prompt(self, research_task: ResearchTask) -> str:\n           \"\"\"Build a prompt for Claude to extract entities\"\"\"\n           return f\"\"\"\n           Extract the following types of information from this research task:\n           \n           1. Main topics\n           2. Key entities (people, organizations, technologies, concepts)\n           3. Important facts and findings\n           4. Relationships between entities\n           \n           Research task:\n           Title: {research_task.title}\n           Description: {research_task.description}\n           Content: {research_task.content}\n           \n           Return the extracted information in the following JSON format:\n           {{\n               \"topics\": [\n                   {{ \"name\": \"topic name\", \"relevance_score\": 0.95 }}\n               ],\n               \"entities\": [\n                   {{ \n                       \"name\": \"entity name\", \n                       \"type\": \"PERSON|ORGANIZATION|TECHNOLOGY|CONCEPT|LOCATION|OTHER\",\n                       \"mentions\": [\"text mention 1\", \"text mention 2\"],\n                       \"relevance_score\": 0.85\n                   }}\n               ],\n               \"facts\": [\n                   {{ \n                       \"statement\": \"factual statement\",\n                       \"confidence_score\": 0.75,\n                       \"source_text\": \"original text from which fact was derived\"\n                   }}\n               ],\n               \"relationships\": [\n                   {{\n                       \"source\": \"entity or topic name\",\n                       \"target\": \"entity or topic name\",\n                       \"type\": \"RELATED_TO|PART_OF|CREATED_BY|USED_BY|etc\",\n                       \"description\": \"description of relationship\"\n                   }}\n               ]\n           }}\n           \"\"\"\n       \n       def _validate_extraction_result(self, extraction_result: Dict[str, Any]) -> Dict[str, Any]:\n           \"\"\"Validate and clean up the extraction result\"\"\"\n           # Implement validation logic using the OutputValidatorService\n           # Ensure all required fields are present and properly formatted\n           # Return the validated result\n           # TODO: Implement validation logic\n           return extraction_result\n       \n       async def _store_entities_in_graph(self, task_id: str, extraction_result: Dict[str, Any]) -> None:\n           \"\"\"Store extracted entities and relationships in Neo4j\"\"\"\n           # Create transaction for batch operations\n           tx = await self.neo4j_repo.begin_transaction()\n           \n           try:\n               # Store topics\n               for topic in extraction_result.get(\"topics\", []):\n                   await self.neo4j_repo.execute_query(\n                       \"\"\"\n                       MERGE (t:Topic {name: $name})\n                       SET t.relevance_score = $relevance_score\n                       WITH t\n                       MATCH (rt:ResearchTask {id: $task_id})\n                       MERGE (rt)-[:HAS_TOPIC]->(t)\n                       \"\"\",\n                       {\"name\": topic[\"name\"], \"relevance_score\": topic[\"relevance_score\"], \"task_id\": task_id},\n                       tx=tx\n                   )\n               \n               # Store entities\n               for entity in extraction_result.get(\"entities\", []):\n                   await self.neo4j_repo.execute_query(\n                       \"\"\"\n                       MERGE (e:Entity {name: $name})\n                       SET e.type = $type,\n                           e.mentions = $mentions,\n                           e.relevance_score = $relevance_score\n                       WITH e\n                       MATCH (rt:ResearchTask {id: $task_id})\n                       MERGE (rt)-[:MENTIONS]->(e)\n                       \"\"\",\n                       {\n                           \"name\": entity[\"name\"],\n                           \"type\": entity[\"type\"],\n                           \"mentions\": entity[\"mentions\"],\n                           \"relevance_score\": entity[\"relevance_score\"],\n                           \"task_id\": task_id\n                       },\n                       tx=tx\n                   )\n               \n               # Store facts\n               for i, fact in enumerate(extraction_result.get(\"facts\", [])):\n                   fact_id = f\"{task_id}_fact_{i}\"\n                   await self.neo4j_repo.execute_query(\n                       \"\"\"\n                       CREATE (f:Fact {id: $fact_id, statement: $statement, confidence_score: $confidence_score, source_text: $source_text})\n                       WITH f\n                       MATCH (rt:ResearchTask {id: $task_id})\n                       CREATE (rt)-[:CONTAINS_FACT]->(f)\n                       \"\"\",\n                       {\n                           \"fact_id\": fact_id,\n                           \"statement\": fact[\"statement\"],\n                           \"confidence_score\": fact[\"confidence_score\"],\n                           \"source_text\": fact[\"source_text\"],\n                           \"task_id\": task_id\n                       },\n                       tx=tx\n                   )\n               \n               # Store relationships\n               for rel in extraction_result.get(\"relationships\", []):\n                   await self.neo4j_repo.execute_query(\n                       \"\"\"\n                       MATCH (source {name: $source_name})\n                       MATCH (target {name: $target_name})\n                       WHERE source:Topic OR source:Entity\n                       AND target:Topic OR target:Entity\n                       MERGE (source)-[r:RELATED {type: $rel_type}]->(target)\n                       SET r.description = $description\n                       \"\"\",\n                       {\n                           \"source_name\": rel[\"source\"],\n                           \"target_name\": rel[\"target\"],\n                           \"rel_type\": rel[\"type\"],\n                           \"description\": rel[\"description\"]\n                       },\n                       tx=tx\n                   )\n               \n               # Commit transaction\n               await self.neo4j_repo.commit_transaction(tx)\n           except Exception as e:\n               # Rollback transaction on error\n               await self.neo4j_repo.rollback_transaction(tx)\n               raise e\n   ```\n\n2. Register the service in the dependency injection container:\n   ```python\n   # In app/di/container.py\n   from app.services.entity_extraction_service import EntityExtractionService\n   \n   # Add to the container setup\n   container.register(EntityExtractionService)\n   ```\n\n3. Create an API endpoint to trigger entity extraction for a research task:\n   ```python\n   # In app/api/routes/research_tasks.py\n   from fastapi import APIRouter, Depends, HTTPException\n   from app.services.entity_extraction_service import EntityExtractionService\n   from app.repositories.research_task_repository import ResearchTaskRepository\n   \n   router = APIRouter()\n   \n   @router.post(\"/{task_id}/extract-entities\", response_model=Dict[str, Any])\n   async def extract_entities(\n       task_id: str,\n       entity_extraction_service: EntityExtractionService = Depends(),\n       research_task_repository: ResearchTaskRepository = Depends()\n   ):\n       \"\"\"Extract entities from a research task and store them in Neo4j\"\"\"\n       # Get the research task\n       task = await research_task_repository.get_by_id(task_id)\n       if not task:\n           raise HTTPException(status_code=404, detail=\"Research task not found\")\n       \n       # Extract entities\n       try:\n           extraction_result = await entity_extraction_service.extract_entities(task)\n           return extraction_result\n       except Exception as e:\n           # Use the standardized exception handling\n           raise HTTPException(status_code=500, detail=f\"Entity extraction failed: {str(e)}\")\n   ```\n\n4. Implement a background task processor for asynchronous entity extraction:\n   ```python\n   # In app/tasks/entity_extraction_task.py\n   from app.services.entity_extraction_service import EntityExtractionService\n   from app.repositories.research_task_repository import ResearchTaskRepository\n   \n   async def process_entity_extraction(task_id: str):\n       \"\"\"Background task to extract entities from a research task\"\"\"\n       # Get dependencies\n       from app.di.container import container\n       entity_extraction_service = container.resolve(EntityExtractionService)\n       research_task_repository = container.resolve(ResearchTaskRepository)\n       \n       # Get the research task\n       task = await research_task_repository.get_by_id(task_id)\n       if not task:\n           # Log error and return\n           print(f\"Research task {task_id} not found\")\n           return\n       \n       # Extract entities\n       try:\n           await entity_extraction_service.extract_entities(task)\n           print(f\"Entity extraction completed for task {task_id}\")\n       except Exception as e:\n           # Log error\n           print(f\"Entity extraction failed for task {task_id}: {str(e)}\")\n   ```\n\n5. Add a trigger to automatically extract entities when a research task is created or updated:\n   ```python\n   # In app/services/research_task_service.py\n   from app.tasks.task_queue import enqueue_task\n   \n   async def create_research_task(self, task_data: Dict[str, Any]) -> ResearchTask:\n       # Existing code to create a research task\n       task = await self.repository.create(task_data)\n       \n       # Enqueue entity extraction task\n       await enqueue_task(\"process_entity_extraction\", {\"task_id\": task.id})\n       \n       return task\n   \n   async def update_research_task(self, task_id: str, task_data: Dict[str, Any]) -> ResearchTask:\n       # Existing code to update a research task\n       task = await self.repository.update(task_id, task_data)\n       \n       # Enqueue entity extraction task\n       await enqueue_task(\"process_entity_extraction\", {\"task_id\": task.id})\n       \n       return task\n   ```\n\n6. Create a utility to query the extracted entities from Neo4j:\n   ```python\n   # In app/services/entity_extraction_service.py (additional method)\n   async def get_entities_for_task(self, task_id: str) -> Dict[str, Any]:\n       \"\"\"Get all extracted entities for a research task\"\"\"\n       # Get topics\n       topics_result = await self.neo4j_repo.execute_query(\n           \"\"\"\n           MATCH (rt:ResearchTask {id: $task_id})-[:HAS_TOPIC]->(t:Topic)\n           RETURN t.name as name, t.relevance_score as relevance_score\n           ORDER BY t.relevance_score DESC\n           \"\"\",\n           {\"task_id\": task_id}\n       )\n       \n       # Get entities\n       entities_result = await self.neo4j_repo.execute_query(\n           \"\"\"\n           MATCH (rt:ResearchTask {id: $task_id})-[:MENTIONS]->(e:Entity)\n           RETURN e.name as name, e.type as type, e.mentions as mentions, e.relevance_score as relevance_score\n           ORDER BY e.relevance_score DESC\n           \"\"\",\n           {\"task_id\": task_id}\n       )\n       \n       # Get facts\n       facts_result = await self.neo4j_repo.execute_query(\n           \"\"\"\n           MATCH (rt:ResearchTask {id: $task_id})-[:CONTAINS_FACT]->(f:Fact)\n           RETURN f.statement as statement, f.confidence_score as confidence_score, f.source_text as source_text\n           ORDER BY f.confidence_score DESC\n           \"\"\",\n           {\"task_id\": task_id}\n       )\n       \n       # Get relationships\n       relationships_result = await self.neo4j_repo.execute_query(\n           \"\"\"\n           MATCH (rt:ResearchTask {id: $task_id})-[:MENTIONS|HAS_TOPIC]->(source)\n           MATCH (source)-[r:RELATED]->(target)\n           RETURN source.name as source, target.name as target, r.type as type, r.description as description\n           \"\"\",\n           {\"task_id\": task_id}\n       )\n       \n       return {\n           \"topics\": topics_result,\n           \"entities\": entities_result,\n           \"facts\": facts_result,\n           \"relationships\": relationships_result\n       }\n   ```\n\n7. Integrate with the OutputValidatorService to ensure proper formatting of Claude's responses:\n   ```python\n   # In app/services/entity_extraction_service.py (update _validate_extraction_result method)\n   from app.services.output_validator_service import OutputValidatorService\n   \n   def __init__(self, claude_client: ClaudeClient, neo4j_repo: Neo4jRepository, output_validator: OutputValidatorService):\n       self.claude_client = claude_client\n       self.neo4j_repo = neo4j_repo\n       self.output_validator = output_validator\n   \n   def _validate_extraction_result(self, extraction_result: Dict[str, Any]) -> Dict[str, Any]:\n       \"\"\"Validate and clean up the extraction result\"\"\"\n       # Define the expected schema\n       schema = {\n           \"type\": \"object\",\n           \"required\": [\"topics\", \"entities\", \"facts\", \"relationships\"],\n           \"properties\": {\n               \"topics\": {\n                   \"type\": \"array\",\n                   \"items\": {\n                       \"type\": \"object\",\n                       \"required\": [\"name\", \"relevance_score\"],\n                       \"properties\": {\n                           \"name\": {\"type\": \"string\"},\n                           \"relevance_score\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1}\n                       }\n                   }\n               },\n               \"entities\": {\n                   \"type\": \"array\",\n                   \"items\": {\n                       \"type\": \"object\",\n                       \"required\": [\"name\", \"type\", \"mentions\", \"relevance_score\"],\n                       \"properties\": {\n                           \"name\": {\"type\": \"string\"},\n                           \"type\": {\"type\": \"string\", \"enum\": [\"PERSON\", \"ORGANIZATION\", \"TECHNOLOGY\", \"CONCEPT\", \"LOCATION\", \"OTHER\"]},\n                           \"mentions\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                           \"relevance_score\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1}\n                       }\n                   }\n               },\n               \"facts\": {\n                   \"type\": \"array\",\n                   \"items\": {\n                       \"type\": \"object\",\n                       \"required\": [\"statement\", \"confidence_score\", \"source_text\"],\n                       \"properties\": {\n                           \"statement\": {\"type\": \"string\"},\n                           \"confidence_score\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n                           \"source_text\": {\"type\": \"string\"}\n                       }\n                   }\n               },\n               \"relationships\": {\n                   \"type\": \"array\",\n                   \"items\": {\n                       \"type\": \"object\",\n                       \"required\": [\"source\", \"target\", \"type\", \"description\"],\n                       \"properties\": {\n                           \"source\": {\"type\": \"string\"},\n                           \"target\": {\"type\": \"string\"},\n                           \"type\": {\"type\": \"string\"},\n                           \"description\": {\"type\": \"string\"}\n                       }\n                   }\n               }\n           }\n       }\n       \n       # Validate against schema\n       validated_result = self.output_validator.validate_json(extraction_result, schema)\n       return validated_result\n   ```\n\n8. Implement error handling using the standardized exception framework:\n   ```python\n   # In app/exceptions/entity_extraction_exceptions.py\n   from app.exceptions.base import BaseAppException\n   \n   class EntityExtractionError(BaseAppException):\n       \"\"\"Base exception for entity extraction errors\"\"\"\n       def __init__(self, message: str, error_code: str = \"ENTITY_EXTRACTION_ERROR\", status_code: int = 500):\n           super().__init__(message, error_code, status_code)\n   \n   class InvalidExtractionResultError(EntityExtractionError):\n       \"\"\"Exception for invalid extraction results\"\"\"\n       def __init__(self, message: str):\n           super().__init__(message, \"INVALID_EXTRACTION_RESULT\", 422)\n   \n   class GraphStorageError(EntityExtractionError):\n       \"\"\"Exception for errors storing entities in Neo4j\"\"\"\n       def __init__(self, message: str):\n           super().__init__(message, \"GRAPH_STORAGE_ERROR\", 500)\n   ```\n\n9. Update the entity extraction service to use these custom exceptions:\n   ```python\n   # In app/services/entity_extraction_service.py\n   from app.exceptions.entity_extraction_exceptions import InvalidExtractionResultError, GraphStorageError\n   \n   # In _validate_extraction_result method\n   try:\n       validated_result = self.output_validator.validate_json(extraction_result, schema)\n       return validated_result\n   except Exception as e:\n       raise InvalidExtractionResultError(f\"Invalid extraction result: {str(e)}\")\n   \n   # In _store_entities_in_graph method\n   try:\n       # Existing code\n   except Exception as e:\n       await self.neo4j_repo.rollback_transaction(tx)\n       raise GraphStorageError(f\"Failed to store entities in graph: {str(e)}\")\n   ```",
        "testStrategy": "1. Unit Tests for EntityExtractionService:\n   - Create test file `tests/services/test_entity_extraction_service.py`\n   - Test `_build_extraction_prompt` method to ensure it generates correct prompts\n   - Test `_validate_extraction_result` with valid and invalid extraction results\n   - Mock Claude client responses and test the full extraction process\n   - Test error handling with various failure scenarios\n\n2. Integration Tests for Neo4j Storage:\n   - Create test file `tests/integration/test_entity_extraction_neo4j.py`\n   - Set up a test Neo4j instance or use a mock\n   - Test storing different types of entities and relationships\n   - Verify that all data is correctly persisted in the graph\n   - Test transaction rollback on errors\n   - Test querying stored entities and relationships\n\n3. API Endpoint Tests:\n   - Create test file `tests/api/test_research_task_entity_extraction.py`\n   - Test the entity extraction endpoint with valid research tasks\n   - Test error handling for non-existent tasks\n   - Test handling of malformed responses from Claude\n   - Test authentication and authorization for the endpoint\n\n4. End-to-End Tests:\n   - Create test file `tests/e2e/test_entity_extraction_workflow.py`\n   - Test the complete workflow from research task creation to entity extraction\n   - Verify that entities are correctly extracted and stored\n   - Test the background task processing\n   - Verify that entity extraction is triggered on task updates\n\n5. Performance Tests:\n   - Test extraction performance with research tasks of varying sizes\n   - Measure and optimize Neo4j query performance\n   - Test concurrent extraction requests\n   - Verify system behavior under load\n\n6. Validation Tests:\n   - Test the integration with OutputValidatorService\n   - Verify that malformed Claude responses are properly handled\n   - Test schema validation with various edge cases\n   - Ensure all required fields are properly validated\n\n7. Manual Testing:\n   - Create a sample research task with rich content\n   - Trigger entity extraction and verify the results\n   - Examine the extracted entities in Neo4j using the Neo4j Browser\n   - Verify the relationships between entities\n   - Test the quality of extracted entities and facts",
        "status": "done",
        "dependencies": [
          "147",
          "153"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-15T18:27:47.223Z"
      },
      {
        "id": "156",
        "title": "LlamaIndex Integration Hardening",
        "description": "Implement reliability improvements for the LlamaIndex service including connection pooling, request timeout handling, retry logic for transient failures, and a health check endpoint.",
        "details": "Enhance the LlamaIndex integration with the following reliability improvements:\n\n1. Connection Pooling:\n   - Implement a connection pool for LlamaIndex service to efficiently manage and reuse connections\n   - Create a `LlamaIndexConnectionPool` class in `app/services/llama_index/connection_pool.py`\n   - Configure pool size based on expected load (default: min=5, max=20)\n   - Implement connection lifecycle management (creation, validation, recycling)\n   - Add monitoring for pool statistics\n\n2. Request Timeout Handling:\n   - Add configurable timeout parameters for all LlamaIndex operations\n   - Implement graceful timeout handling with appropriate error messages\n   - Create a timeout configuration in `app/config/llama_index_config.py`\n   - Add logging for timeout events with context information\n\n3. Retry Logic:\n   - Implement exponential backoff retry mechanism for transient failures\n   - Define retry policies for different types of operations (queries, indexing, etc.)\n   - Create a `RetryHandler` class in `app/services/llama_index/retry_handler.py`\n   - Configure max retry attempts, backoff factor, and jitter\n   - Add detailed logging of retry attempts and outcomes\n\n4. Health Check Endpoint:\n   - Create a `/api/llama-index/health` endpoint in `app/api/routes/llama_index.py`\n   - Implement comprehensive health checks that verify:\n     - Connection to LlamaIndex service\n     - Index availability and status\n     - Query functionality with a simple test query\n     - Resource availability (memory, storage)\n   - Return detailed health status with component-level information\n   - Add integration with the application's overall health monitoring system\n\n5. Error Handling Improvements:\n   - Create standardized error responses for different failure scenarios\n   - Implement detailed logging with context information\n   - Add error classification to distinguish between transient and permanent failures\n\nSample code for connection pool implementation:\n```python\n# app/services/llama_index/connection_pool.py\nfrom typing import List, Optional\nimport time\nimport threading\nfrom llama_index import ServiceContext, StorageContext, load_index_from_storage\n\nclass LlamaIndexConnection:\n    def __init__(self, index_id: str):\n        self.index_id = index_id\n        self.last_used = time.time()\n        self.created_at = time.time()\n        self.service_context = ServiceContext.from_defaults()\n        self.storage_context = StorageContext.from_defaults()\n        self.index = load_index_from_storage(self.storage_context)\n        \n    def is_valid(self) -> bool:\n        # Implement validation logic\n        return True\n        \n    def refresh(self) -> None:\n        # Implement refresh logic\n        self.last_used = time.time()\n\nclass LlamaIndexConnectionPool:\n    def __init__(self, min_connections: int = 5, max_connections: int = 20):\n        self.min_connections = min_connections\n        self.max_connections = max_connections\n        self.connections: List[LlamaIndexConnection] = []\n        self.lock = threading.RLock()\n        self.initialize_pool()\n        \n    def initialize_pool(self) -> None:\n        with self.lock:\n            for _ in range(self.min_connections):\n                self.connections.append(LlamaIndexConnection(\"default\"))\n                \n    def get_connection(self) -> LlamaIndexConnection:\n        with self.lock:\n            if not self.connections:\n                if len(self.connections) < self.max_connections:\n                    return LlamaIndexConnection(\"default\")\n                else:\n                    # Wait for a connection to become available\n                    # Implement waiting logic\n                    pass\n            \n            connection = self.connections.pop(0)\n            if not connection.is_valid():\n                connection = LlamaIndexConnection(\"default\")\n            \n            return connection\n            \n    def release_connection(self, connection: LlamaIndexConnection) -> None:\n        with self.lock:\n            connection.refresh()\n            self.connections.append(connection)\n```",
        "testStrategy": "1. Connection Pool Testing:\n   - Unit test the `LlamaIndexConnectionPool` class with various pool sizes\n   - Verify connections are properly created, managed, and recycled\n   - Test concurrent access patterns with multiple threads\n   - Validate connection validation logic works correctly\n   - Measure performance improvements with and without connection pooling\n\n2. Timeout Handling Testing:\n   - Create test cases with deliberately slow operations\n   - Verify timeout configuration is correctly applied\n   - Test different timeout values and their effects\n   - Ensure proper error messages are returned on timeout\n   - Validate that resources are properly cleaned up after timeout\n\n3. Retry Logic Testing:\n   - Create mock LlamaIndex service that fails intermittently\n   - Test retry behavior with various failure patterns\n   - Verify exponential backoff works as expected\n   - Test different retry configurations\n   - Ensure maximum retry limit is respected\n   - Validate that permanent failures are handled correctly\n\n4. Health Check Endpoint Testing:\n   - Test the endpoint returns correct status when all systems are operational\n   - Simulate various failure conditions and verify correct reporting\n   - Test response format and content\n   - Verify integration with monitoring systems\n   - Test performance impact of health checks\n\n5. Integration Testing:\n   - Create end-to-end tests that verify all components work together\n   - Test under load to ensure stability\n   - Verify error propagation and handling\n   - Test recovery scenarios after simulated failures\n   - Validate logging and monitoring integration\n\n6. Performance Testing:\n   - Measure throughput with and without the improvements\n   - Test under various load conditions\n   - Measure resource utilization (CPU, memory)\n   - Identify and address any bottlenecks",
        "status": "done",
        "dependencies": [
          "143",
          "145",
          "147"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-15T21:39:43.983Z"
      },
      {
        "id": "157",
        "title": "B2 Storage Error Handling",
        "description": "Implement robust error handling for B2 storage operations including retry logic for uploads/downloads, checksum verification, dead letter handling for failed operations, and storage metrics collection.",
        "details": "Enhance the B2 storage service with comprehensive error handling mechanisms:\n\n1. Implement retry logic with exponential backoff for upload/download operations:\n   ```python\n   from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n   from b2sdk.exception import B2ConnectionError, B2RequestTimeout\n   \n   @retry(\n       stop=stop_after_attempt(5),\n       wait=wait_exponential(multiplier=1, min=2, max=60),\n       retry=retry_if_exception_type((B2ConnectionError, B2RequestTimeout))\n   )\n   async def resilient_b2_upload(file_path, destination_path):\n       try:\n           return await b2_client.upload_file(file_path, destination_path)\n       except Exception as e:\n           logger.error(f\"B2 upload error: {str(e)}\")\n           raise\n   ```\n\n2. Implement checksum verification for data integrity:\n   ```python\n   import hashlib\n   \n   def calculate_sha1(file_path):\n       sha1 = hashlib.sha1()\n       with open(file_path, 'rb') as f:\n           while chunk := f.read(8192):\n               sha1.update(chunk)\n       return sha1.hexdigest()\n   \n   async def verify_upload_integrity(local_file_path, b2_file_info):\n       local_checksum = calculate_sha1(local_file_path)\n       remote_checksum = b2_file_info.content_sha1\n       \n       if local_checksum != remote_checksum:\n           logger.error(f\"Checksum mismatch for {local_file_path}\")\n           raise IntegrityError(f\"Upload verification failed: checksums don't match\")\n       \n       return True\n   ```\n\n3. Implement dead letter queue for failed uploads:\n   ```python\n   class B2DeadLetterQueue:\n       def __init__(self, redis_client, queue_name=\"b2_dead_letter_queue\"):\n           self.redis = redis_client\n           self.queue_name = queue_name\n       \n       async def add_failed_operation(self, operation_type, file_path, error_details, retry_count):\n           entry = {\n               \"operation_type\": operation_type,\n               \"file_path\": file_path,\n               \"error_details\": str(error_details),\n               \"retry_count\": retry_count,\n               \"timestamp\": datetime.utcnow().isoformat()\n           }\n           await self.redis.lpush(self.queue_name, json.dumps(entry))\n       \n       async def get_failed_operations(self, limit=100):\n           entries = await self.redis.lrange(self.queue_name, 0, limit-1)\n           return [json.loads(entry) for entry in entries]\n       \n       async def retry_operation(self, entry_index):\n           entry = json.loads(await self.redis.lindex(self.queue_name, entry_index))\n           # Logic to retry the operation\n           # If successful, remove from queue\n           await self.redis.lrem(self.queue_name, 1, json.dumps(entry))\n           return True\n   ```\n\n4. Implement storage metrics collection:\n   ```python\n   from prometheus_client import Counter, Histogram, Gauge\n   \n   # Define metrics\n   b2_operation_counter = Counter(\n       'b2_operations_total',\n       'Total number of B2 operations',\n       ['operation_type', 'status']\n   )\n   \n   b2_operation_latency = Histogram(\n       'b2_operation_latency_seconds',\n       'Latency of B2 operations in seconds',\n       ['operation_type']\n   )\n   \n   b2_storage_usage = Gauge(\n       'b2_storage_usage_bytes',\n       'Current B2 storage usage in bytes'\n   )\n   \n   # Usage in code\n   async def track_b2_operation(operation_type, start_time, status):\n       duration = time.time() - start_time\n       b2_operation_counter.labels(operation_type=operation_type, status=status).inc()\n       b2_operation_latency.labels(operation_type=operation_type).observe(duration)\n   \n   async def update_storage_metrics():\n       usage = await b2_client.get_bucket_usage()\n       b2_storage_usage.set(usage)\n   ```\n\n5. Create a B2StorageService class that integrates all these components:\n   ```python\n   class B2StorageService:\n       def __init__(self, b2_client, redis_client):\n           self.b2_client = b2_client\n           self.dead_letter_queue = B2DeadLetterQueue(redis_client)\n       \n       async def upload_file(self, local_path, remote_path, retry_count=0):\n           start_time = time.time()\n           try:\n               result = await resilient_b2_upload(local_path, remote_path)\n               await verify_upload_integrity(local_path, result)\n               await track_b2_operation('upload', start_time, 'success')\n               return result\n           except Exception as e:\n               await track_b2_operation('upload', start_time, 'failure')\n               if retry_count >= 5:\n                   await self.dead_letter_queue.add_failed_operation(\n                       'upload', local_path, str(e), retry_count\n                   )\n               raise\n       \n       async def download_file(self, remote_path, local_path, retry_count=0):\n           start_time = time.time()\n           try:\n               result = await resilient_b2_download(remote_path, local_path)\n               await track_b2_operation('download', start_time, 'success')\n               return result\n           except Exception as e:\n               await track_b2_operation('download', start_time, 'failure')\n               if retry_count >= 5:\n                   await self.dead_letter_queue.add_failed_operation(\n                       'download', remote_path, str(e), retry_count\n                   )\n               raise\n       \n       async def process_dead_letter_queue(self, batch_size=10):\n           failed_operations = await self.dead_letter_queue.get_failed_operations(batch_size)\n           for i, operation in enumerate(failed_operations):\n               try:\n                   if operation['operation_type'] == 'upload':\n                       await self.upload_file(\n                           operation['file_path'],\n                           operation['destination_path'],\n                           retry_count=operation['retry_count'] + 1\n                       )\n                   elif operation['operation_type'] == 'download':\n                       await self.download_file(\n                           operation['remote_path'],\n                           operation['local_path'],\n                           retry_count=operation['retry_count'] + 1\n                       )\n                   await self.dead_letter_queue.retry_operation(i)\n               except Exception as e:\n                   logger.error(f\"Failed to process dead letter queue item: {str(e)}\")\n   ```\n\n6. Create a scheduled task to periodically process the dead letter queue and update metrics:\n   ```python\n   @shared_task\n   async def process_b2_maintenance():\n       storage_service = B2StorageService(get_b2_client(), get_redis_client())\n       await storage_service.process_dead_letter_queue()\n       await update_storage_metrics()\n   ```",
        "testStrategy": "1. Unit tests for retry logic:\n   - Test successful upload/download after temporary failures\n   - Test that retry count respects maximum attempts\n   - Verify exponential backoff timing between retries\n   - Test behavior when max retries are exhausted\n\n2. Unit tests for checksum verification:\n   - Test successful verification with matching checksums\n   - Test failure detection with mismatched checksums\n   - Test handling of corrupted files\n   - Test with various file sizes (small, medium, large)\n\n3. Unit tests for dead letter queue:\n   - Test adding failed operations to the queue\n   - Test retrieving operations from the queue\n   - Test retrying operations from the queue\n   - Test queue persistence across service restarts\n\n4. Unit tests for metrics collection:\n   - Test counter increments for successful operations\n   - Test counter increments for failed operations\n   - Test latency histogram recording\n   - Test storage usage gauge updates\n\n5. Integration tests:\n   - Test end-to-end upload with simulated network failures\n   - Test end-to-end download with simulated network failures\n   - Test dead letter queue processing with real Redis instance\n   - Test metrics reporting to Prometheus endpoint\n\n6. Performance tests:\n   - Benchmark upload/download speeds with retry logic enabled\n   - Test system under high concurrency\n   - Measure impact of checksum verification on throughput\n   - Test dead letter queue processing with large backlogs\n\n7. Chaos testing:\n   - Test behavior during B2 service outages\n   - Test behavior during Redis outages\n   - Test behavior with network packet loss and latency\n   - Test recovery after system restarts",
        "status": "done",
        "dependencies": [
          "137",
          "153"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-15T18:40:48.534Z"
      },
      {
        "id": "158",
        "title": "CrewAI Workflow Improvements",
        "description": "Enhance CrewAI workflow management with state persistence, graceful shutdown handling, task cancellation support, and comprehensive metrics and logging.",
        "details": "Implement the following workflow improvements to the CrewAI system:\n\n1. **Workflow State Persistence**:\n   - Create a `WorkflowStateManager` class that serializes and persists workflow state\n   - Implement checkpointing at critical workflow stages\n   - Store state in a configurable backend (file system, database, or Redis)\n   - Add state recovery mechanisms for workflow resumption\n   - Include versioning for backward compatibility\n   - Example implementation:\n   ```python\n   class WorkflowStateManager:\n       def __init__(self, storage_backend=\"file\", storage_path=\"./workflow_states\"):\n           self.storage_backend = storage_backend\n           self.storage_path = storage_path\n           \n       def save_state(self, workflow_id, state_data):\n           \"\"\"Serialize and save workflow state\"\"\"\n           serialized_state = self._serialize_state(state_data)\n           if self.storage_backend == \"file\":\n               self._save_to_file(workflow_id, serialized_state)\n           elif self.storage_backend == \"redis\":\n               self._save_to_redis(workflow_id, serialized_state)\n           # Add other backends as needed\n           \n       def load_state(self, workflow_id):\n           \"\"\"Load and deserialize workflow state\"\"\"\n           # Implementation for loading state\n   ```\n\n2. **Graceful Shutdown Handling**:\n   - Implement signal handlers for SIGTERM and SIGINT\n   - Add workflow pause functionality to safely stop at checkpoints\n   - Create cleanup procedures for resources (connections, temp files)\n   - Implement state saving before shutdown\n   - Add logging for shutdown events\n   - Example implementation:\n   ```python\n   import signal\n   import sys\n   \n   class GracefulShutdownHandler:\n       def __init__(self, workflow_manager):\n           self.workflow_manager = workflow_manager\n           signal.signal(signal.SIGTERM, self.handle_shutdown)\n           signal.signal(signal.SIGINT, self.handle_shutdown)\n           \n       def handle_shutdown(self, signum, frame):\n           \"\"\"Handle shutdown signals gracefully\"\"\"\n           logger.info(f\"Received signal {signum}, initiating graceful shutdown\")\n           self.workflow_manager.pause_workflows()\n           self.workflow_manager.save_all_states()\n           self.workflow_manager.cleanup_resources()\n           sys.exit(0)\n   ```\n\n3. **Task Cancellation Support**:\n   - Implement a cancellation token system for tasks\n   - Add API endpoints for cancelling specific tasks or workflows\n   - Create cleanup procedures for cancelled tasks\n   - Implement notification system for cancellation events\n   - Handle dependent task cancellation logic\n   - Example implementation:\n   ```python\n   class CancellationToken:\n       def __init__(self):\n           self.cancelled = False\n           self._callbacks = []\n           \n       def cancel(self):\n           \"\"\"Mark as cancelled and execute callbacks\"\"\"\n           self.cancelled = True\n           for callback in self._callbacks:\n               callback()\n               \n       def register_callback(self, callback):\n           \"\"\"Register a callback to be executed on cancellation\"\"\"\n           self._callbacks.append(callback)\n           \n       def is_cancelled(self):\n           \"\"\"Check if cancellation has been requested\"\"\"\n           return self.cancelled\n   ```\n\n4. **Workflow Metrics and Logging**:\n   - Implement structured logging with contextual information\n   - Create metrics collection for workflow performance (duration, resource usage)\n   - Add task-level timing and status metrics\n   - Implement error rate and failure tracking\n   - Create dashboard-ready metrics output (Prometheus format)\n   - Example implementation:\n   ```python\n   class WorkflowMetricsCollector:\n       def __init__(self):\n           self.metrics = {}\n           \n       def record_task_start(self, task_id, metadata=None):\n           \"\"\"Record the start of a task\"\"\"\n           self.metrics[task_id] = {\n               \"start_time\": time.time(),\n               \"status\": \"running\",\n               \"metadata\": metadata or {}\n           }\n           \n       def record_task_completion(self, task_id, success=True, error=None):\n           \"\"\"Record the completion of a task\"\"\"\n           if task_id in self.metrics:\n               self.metrics[task_id].update({\n                   \"end_time\": time.time(),\n                   \"duration\": time.time() - self.metrics[task_id][\"start_time\"],\n                   \"status\": \"completed\" if success else \"failed\",\n                   \"error\": error\n               })\n   ```\n\nIntegration points:\n- Update the main CrewAI workflow controller to use these new components\n- Modify the API layer to expose cancellation and metrics endpoints\n- Update documentation to reflect new capabilities\n- Create migration path for existing workflows",
        "testStrategy": "1. **Workflow State Persistence Testing**:\n   - Unit test the `WorkflowStateManager` class with different storage backends\n   - Test serialization/deserialization with various workflow states\n   - Verify state recovery after simulated crashes\n   - Test with corrupted state files to ensure proper error handling\n   - Benchmark performance impact of state persistence\n   - Test concurrent access patterns\n\n2. **Graceful Shutdown Testing**:\n   - Create test harness that sends shutdown signals to running workflows\n   - Verify all resources are properly cleaned up after shutdown\n   - Test shutdown during different workflow stages\n   - Measure shutdown completion time under various loads\n   - Verify state is correctly saved during shutdown\n   - Test with multiple concurrent workflows\n\n3. **Task Cancellation Testing**:\n   - Unit test the cancellation token implementation\n   - Test API endpoints for task and workflow cancellation\n   - Verify dependent tasks are properly handled during cancellation\n   - Test cancellation at different stages of task execution\n   - Verify resource cleanup after cancellation\n   - Test cancellation propagation in complex workflow graphs\n\n4. **Metrics and Logging Testing**:\n   - Verify metrics are correctly collected for all workflow events\n   - Test structured logging format and content\n   - Validate metrics accuracy with controlled workflow executions\n   - Test integration with monitoring systems (Prometheus, Grafana)\n   - Verify performance impact of metrics collection\n   - Test with high-volume workflows to ensure scalability\n\n5. **Integration Testing**:\n   - End-to-end tests with all components integrated\n   - Test recovery from various failure scenarios\n   - Verify backward compatibility with existing workflows\n   - Performance testing under production-like conditions\n   - Stress testing with concurrent workflows and cancellations",
        "status": "done",
        "dependencies": [
          "127",
          "131",
          "150",
          "156",
          "157"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-16T04:30:34.820Z"
      },
      {
        "id": "159",
        "title": "Implement Circuit Breaker Pattern for External API Services",
        "description": "Implement a comprehensive circuit breaker pattern for all external API calls including Anthropic, Neo4j, and Supabase with configurable thresholds, fallback responses, and monitoring capabilities.",
        "details": "Extend the existing circuit breaker implementation to cover all external API services with enhanced functionality:\n\n1. Refactor the existing `app/services/api_resilience.py` module to support multiple API services:\n   - Create a configurable CircuitBreaker class that can be instantiated for different services\n   - Implement service-specific configuration profiles for Anthropic, Neo4j, and Supabase\n   - Add support for per-service configurable thresholds:\n     ```python\n     class CircuitBreakerConfig:\n         def __init__(self, \n                     failure_threshold: int = 5,\n                     recovery_timeout: int = 30,\n                     retry_max_attempts: int = 3,\n                     retry_backoff_factor: float = 2.0,\n                     timeout: int = 10):\n             self.failure_threshold = failure_threshold\n             self.recovery_timeout = recovery_timeout\n             self.retry_max_attempts = retry_max_attempts\n             self.retry_backoff_factor = retry_backoff_factor\n             self.timeout = timeout\n     \n     class CircuitBreaker:\n         def __init__(self, service_name: str, config: CircuitBreakerConfig = None):\n             self.service_name = service_name\n             self.config = config or CircuitBreakerConfig()\n             self.state = \"CLOSED\"  # CLOSED, OPEN, HALF-OPEN\n             self.failure_count = 0\n             self.last_failure_time = None\n             # Additional state tracking\n     ```\n\n2. Implement fallback response mechanisms:\n   - Create a FallbackRegistry to store and retrieve fallback handlers\n   - Implement default fallback responses for each service type\n   - Allow custom fallback handlers to be registered per operation\n   - Example implementation:\n     ```python\n     class FallbackRegistry:\n         def __init__(self):\n             self.fallbacks = {}\n         \n         def register(self, service_name: str, operation: str, handler: Callable):\n             if service_name not in self.fallbacks:\n                 self.fallbacks[service_name] = {}\n             self.fallbacks[service_name][operation] = handler\n         \n         def get_fallback(self, service_name: str, operation: str) -> Optional[Callable]:\n             return self.fallbacks.get(service_name, {}).get(operation)\n     ```\n\n3. Add circuit state monitoring and metrics:\n   - Implement Prometheus metrics for circuit state changes\n   - Track success/failure rates, response times, and circuit open duration\n   - Create a dashboard endpoint for current circuit states\n   - Log all circuit state transitions with context\n   - Example metrics:\n     ```python\n     # In app/monitoring/metrics.py\n     from prometheus_client import Counter, Gauge, Histogram\n     \n     # Counters\n     circuit_state_changes = Counter('circuit_breaker_state_changes_total', \n                                    'Circuit breaker state transitions',\n                                    ['service', 'from_state', 'to_state'])\n     circuit_requests = Counter('circuit_breaker_requests_total',\n                               'Requests through circuit breaker',\n                               ['service', 'result'])\n     \n     # Gauges\n     circuit_state = Gauge('circuit_breaker_state',\n                          'Current circuit breaker state (0=closed, 1=half-open, 2=open)',\n                          ['service'])\n     \n     # Histograms\n     circuit_response_time = Histogram('circuit_breaker_response_time_seconds',\n                                      'Response time for requests through circuit breaker',\n                                      ['service'])\n     ```\n\n4. Create service-specific circuit breaker wrappers:\n   - Implement AnthropicCircuitBreaker (extending existing implementation)\n   - Implement Neo4jCircuitBreaker for database operations\n   - Implement SupabaseCircuitBreaker for storage operations\n   - Example wrapper:\n     ```python\n     class Neo4jCircuitBreaker:\n         def __init__(self, client, config=None):\n             self.client = client\n             self.circuit = CircuitBreaker(\"neo4j\", config)\n             self.fallback_registry = FallbackRegistry()\n             \n             # Register default fallbacks\n             self.fallback_registry.register(\"neo4j\", \"query\", self._default_query_fallback)\n         \n         async def query(self, cypher, params=None):\n             try:\n                 return await self.circuit.execute(\n                     lambda: self.client.query(cypher, params),\n                     operation=\"query\"\n                 )\n             except CircuitOpenError:\n                 fallback = self.fallback_registry.get_fallback(\"neo4j\", \"query\")\n                 return await fallback(cypher, params)\n         \n         async def _default_query_fallback(self, cypher, params=None):\n             # Return cached results or empty response\n             return {\"results\": [], \"from_fallback\": True}\n     ```\n\n5. Update service initialization to use circuit breakers:\n   - Modify service factory methods to wrap clients with circuit breakers\n   - Update dependency injection to provide circuit-protected clients\n   - Add configuration loading from environment variables\n\n6. Implement a circuit breaker management API:\n   - Create endpoints to view circuit states\n   - Allow manual reset of circuits\n   - Provide configuration update capabilities\n   - Example API routes:\n     ```python\n     @router.get(\"/api/system/circuit-breakers\")\n     async def get_circuit_states():\n         # Return states of all circuit breakers\n     \n     @router.post(\"/api/system/circuit-breakers/{service}/reset\")\n     async def reset_circuit(service: str):\n         # Reset specified circuit breaker\n     ```",
        "testStrategy": "1. Unit tests for the CircuitBreaker class:\n   - Test state transitions (closed → open → half-open → closed)\n   - Test configurable thresholds for different services\n   - Test failure counting and reset behavior\n   - Test timeout handling and recovery periods\n   - Test metrics recording\n\n2. Unit tests for fallback mechanisms:\n   - Test fallback registry registration and retrieval\n   - Test default fallbacks for each service\n   - Test custom fallback handlers\n   - Test fallback context preservation\n\n3. Integration tests for each service-specific circuit breaker:\n   - Test AnthropicCircuitBreaker with mocked API responses\n   - Test Neo4jCircuitBreaker with mocked database responses\n   - Test SupabaseCircuitBreaker with mocked storage responses\n   - Test circuit opening on consecutive failures\n   - Test circuit recovery after timeout period\n\n4. Failure scenario testing:\n   - Simulate API timeouts and verify circuit behavior\n   - Simulate connection errors and verify fallback responses\n   - Test partial failures (some endpoints working, others failing)\n   - Verify correct fallback content is returned\n\n5. Performance testing:\n   - Measure overhead of circuit breaker implementation\n   - Test under high concurrency to verify thread safety\n   - Verify no memory leaks during extended operation\n\n6. Monitoring tests:\n   - Verify metrics are correctly recorded and exposed\n   - Test dashboard endpoint for accurate state reporting\n   - Verify logs contain appropriate context for debugging\n\n7. End-to-end tests:\n   - Test complete request flow with circuit breakers in place\n   - Verify application resilience during simulated outages\n   - Test recovery behavior when services come back online",
        "status": "done",
        "dependencies": [
          "137",
          "101",
          "110"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-15T22:06:17.589Z"
      },
      {
        "id": "160",
        "title": "Implement Environment Variable Validation",
        "description": "Create a startup validation module that fails fast if required environment variables are missing, ensuring the application doesn't start in an invalid state.",
        "details": "Create a new file `app/core/startup_validation.py` that validates critical and recommended environment variables. The module should:\n1. Define two categories of variables: critical (must be present) and recommended\n2. Check for presence of all variables at startup\n3. Raise RuntimeError if any critical variables are missing\n4. Log warnings for missing recommended variables\n5. Return a dictionary with missing variables by category\n\nImplement the validation function as specified in the PRD, with proper error messages and logging.\n\nIntegrate this validation in `app/main.py` by calling it during application startup before any services are initialized:\n```python\nfrom app.core.startup_validation import validate_environment\n\n# At the beginning of FastAPI app initialization\ndef startup_event():\n    validate_environment()\n    # Other startup tasks\n\napp = FastAPI()\napp.add_event_handler(\"startup\", startup_event)\n```",
        "testStrategy": "1. Create unit tests in `tests/test_startup_validation.py` that:\n   - Test successful validation when all critical variables are present\n   - Test failure when critical variables are missing\n   - Test warning generation for missing recommended variables\n   - Verify correct error messages and return values\n2. Create integration tests that verify the application fails to start when critical variables are missing\n3. Test with various combinations of missing variables to ensure all validation paths work correctly",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-16T05:27:16.877Z"
      },
      {
        "id": "161",
        "title": "Implement CORS Production Hardening",
        "description": "Enhance CORS configuration to fail in production if wildcard origins are detected, preventing potential security vulnerabilities.",
        "details": "Modify `app/main.py` to enforce strict CORS rules in production environments:\n\n1. Extract the current CORS configuration logic\n2. Add environment-aware validation that:\n   - Allows wildcard origins (`*`) in development only\n   - Requires explicit origin list in production\n   - Raises RuntimeError with clear error message if wildcard is detected in production\n\nImplementation should follow the pattern provided in the PRD:\n```python\ncors_origins = os.getenv(\"CORS_ORIGINS\", \"\").split(\",\")\nif not cors_origins or cors_origins == [\"\"]:\n    if os.getenv(\"ENVIRONMENT\") == \"production\":\n        raise RuntimeError(\n            \"CORS_ORIGINS must be explicitly set in production. \"\n            \"Set to specific origins like 'https://app.example.com,https://admin.example.com'\"\n        )\n    cors_origins = [\"*\"]  # Allow in development only\n\nif \"*\" in cors_origins and os.getenv(\"ENVIRONMENT\") == \"production\":\n    raise RuntimeError(\n        \"CORS_ORIGINS cannot be '*' in production. \"\n        \"Set specific allowed origins for security.\"\n    )\n```\n\nEnsure this validation happens before the FastAPI CORSMiddleware is initialized.",
        "testStrategy": "1. Create unit tests that verify:\n   - Wildcard origins are allowed in development\n   - Empty CORS settings default to wildcard in development\n   - Empty CORS settings raise error in production\n   - Wildcard origins raise error in production\n   - Valid origin lists are accepted in all environments\n2. Create integration tests that verify application startup behavior with different CORS configurations\n3. Test with various environment configurations to ensure correct behavior in all scenarios",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-16T05:27:16.943Z"
      },
      {
        "id": "162",
        "title": "Implement Tiered Rate Limiting for Sensitive Endpoints",
        "description": "Apply stricter rate limits to authentication and upload endpoints to prevent brute force attacks, spam, and resource abuse.",
        "details": "Create a new file `app/middleware/rate_limit_tiers.py` to define endpoint-specific rate limits and modify the existing rate limiting middleware:\n\n1. Define rate limit tiers as specified in the PRD:\n```python\nRATE_LIMIT_TIERS = {\n    \"/api/users/login\": {\"limit\": 5, \"period\": 60},  # 5/minute\n    \"/api/users/register\": {\"limit\": 3, \"period\": 60},  # 3/minute\n    \"/api/documents/upload\": {\"limit\": 10, \"period\": 60},  # 10/minute\n    \"/api/query/*\": {\"limit\": 60, \"period\": 60},  # 60/minute\n    \"/api/orchestration/*\": {\"limit\": 30, \"period\": 60},  # 30/minute\n    \"default\": {\"limit\": 200, \"period\": 60}  # 200/minute\n}\n```\n\n2. Modify `app/middleware/rate_limit.py` to use these tiers:\n   - Add function to match request path against patterns (supporting wildcards)\n   - Determine applicable rate limit based on endpoint\n   - Apply the appropriate limit\n   - Return standardized 429 error when limit exceeded\n\n3. Update Redis key generation to include the endpoint pattern to ensure separate rate limit buckets per endpoint type\n\n4. Add proper error responses with retry-after headers when limits are exceeded",
        "testStrategy": "1. Create unit tests in `tests/test_rate_limiting.py` that verify:\n   - Correct limit is applied to each endpoint type\n   - Pattern matching works correctly with wildcards\n   - Default limit is applied to unmatched endpoints\n   - Rate limit counters increment correctly\n\n2. Create integration tests that:\n   - Verify rate limiting behavior for each endpoint type\n   - Test rate limit reset after period expiration\n   - Confirm correct error responses and status codes\n   - Verify retry-after headers are present and accurate",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-16T05:27:17.015Z"
      },
      {
        "id": "163",
        "title": "Add Configurable Timeouts to External Service Calls",
        "description": "Implement configurable timeouts for all external HTTP calls to prevent indefinite hanging when external services are slow or unresponsive.",
        "details": "Create a new file `app/core/service_timeouts.py` to define timeout constants and modify service client implementations:\n\n1. Define timeout constants for each service:\n```python\nSERVICE_TIMEOUTS = {\n    \"llama_index\": 60.0,  # 60 seconds for document parsing\n    \"crewai\": 120.0,     # 120 seconds for multi-agent workflows\n    \"ollama\": 30.0,      # 30 seconds for embeddings\n    \"neo4j\": 15.0,       # 15 seconds for graph database\n    \"supabase\": 10.0,    # 10 seconds for database operations\n    \"default\": 30.0      # Default timeout\n}\n```\n\n2. Create a base ExternalServiceClient class as specified in the PRD:\n```python\nimport httpx\nfrom app.services.circuit_breaker import circuit_breaker\nfrom app.core.service_timeouts import SERVICE_TIMEOUTS\n\nclass ExternalServiceClient:\n    def __init__(self, service_name: str, base_url: str, timeout: float = None):\n        self.service_name = service_name\n        timeout = timeout or SERVICE_TIMEOUTS.get(service_name, SERVICE_TIMEOUTS[\"default\"])\n        self.client = httpx.AsyncClient(\n            base_url=base_url,\n            timeout=httpx.Timeout(timeout, connect=5.0),\n            limits=httpx.Limits(max_connections=100, max_keepalive_connections=20)\n        )\n\n    @circuit_breaker()\n    async def call(self, endpoint: str, **kwargs):\n        return await self.client.post(endpoint, **kwargs)\n```\n\n3. Modify each service client to use this base class or implement similar timeout logic:\n   - `app/services/llama_index_service.py`\n   - `app/services/crewai_service.py`\n   - `app/services/embedding_service.py`\n   - `app/services/neo4j_service.py`\n   - Any other services making external calls",
        "testStrategy": "1. Create unit tests that verify:\n   - Timeout values are correctly applied from configuration\n   - Default timeout is used for undefined services\n   - Custom timeout overrides work correctly\n\n2. Create integration tests that:\n   - Mock slow responses and verify timeout behavior\n   - Test timeout exceptions are properly handled\n   - Verify timeout settings for each service type\n\n3. Create a test utility that simulates slow responses to verify timeout behavior across all service clients",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-16T05:27:17.078Z"
      },
      {
        "id": "164",
        "title": "Implement Circuit Breaker Pattern for All External Services",
        "description": "Apply circuit breakers to all external service calls to prevent cascading failures when external services are down or degraded.",
        "details": "Extend the existing circuit breaker implementation to cover all external services:\n\n1. Define circuit breaker configuration in a central location:\n```python\n# app/core/circuit_breaker_config.py\nCIRCUIT_BREAKER_CONFIG = {\n    \"llama_index\": {\"failure_threshold\": 5, \"recovery_timeout\": 30},\n    \"crewai\": {\"failure_threshold\": 3, \"recovery_timeout\": 60},\n    \"ollama\": {\"failure_threshold\": 5, \"recovery_timeout\": 15},\n    \"neo4j\": {\"failure_threshold\": 3, \"recovery_timeout\": 30},\n    \"b2\": {\"failure_threshold\": 5, \"recovery_timeout\": 60},\n    \"default\": {\"failure_threshold\": 3, \"recovery_timeout\": 30},\n}\n```\n\n2. Enhance the circuit breaker decorator to use this configuration:\n```python\n# app/services/circuit_breaker.py\nimport functools\nimport time\nfrom typing import Dict, Any, Callable, Optional\nimport structlog\nfrom app.core.circuit_breaker_config import CIRCUIT_BREAKER_CONFIG\n\nlogger = structlog.get_logger(__name__)\n\nclass CircuitBreaker:\n    _instances: Dict[str, \"CircuitBreaker\"] = {}\n    \n    @classmethod\n    def get_instance(cls, service_name: str) -> \"CircuitBreaker\":\n        if service_name not in cls._instances:\n            config = CIRCUIT_BREAKER_CONFIG.get(\n                service_name, CIRCUIT_BREAKER_CONFIG[\"default\"]\n            )\n            cls._instances[service_name] = CircuitBreaker(\n                service_name=service_name,\n                failure_threshold=config[\"failure_threshold\"],\n                recovery_timeout=config[\"recovery_timeout\"]\n            )\n        return cls._instances[service_name]\n    \n    def __init__(self, service_name: str, failure_threshold: int, recovery_timeout: int):\n        self.service_name = service_name\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.failure_count = 0\n        self.last_failure_time = 0\n        self.open = False\n    \n    def record_failure(self):\n        self.failure_count += 1\n        self.last_failure_time = time.time()\n        if self.failure_count >= self.failure_threshold:\n            self.open = True\n            logger.warning(\n                \"Circuit breaker opened\",\n                service=self.service_name,\n                failures=self.failure_count\n            )\n    \n    def record_success(self):\n        self.failure_count = 0\n        if self.open:\n            self.open = False\n            logger.info(\n                \"Circuit breaker closed\",\n                service=self.service_name\n            )\n    \n    def is_open(self) -> bool:\n        if not self.open:\n            return False\n            \n        # Check if recovery timeout has elapsed\n        if time.time() - self.last_failure_time > self.recovery_timeout:\n            logger.info(\n                \"Circuit breaker recovery timeout elapsed, allowing request\",\n                service=self.service_name\n            )\n            return False\n            \n        return True\n\ndef circuit_breaker(service_name: Optional[str] = None):\n    def decorator(func: Callable):\n        @functools.wraps(func)\n        async def wrapper(self, *args, **kwargs):\n            # Get service name from instance or parameter\n            svc_name = service_name or getattr(self, \"service_name\", \"default\")\n            breaker = CircuitBreaker.get_instance(svc_name)\n            \n            if breaker.is_open():\n                logger.warning(\n                    \"Circuit breaker open, rejecting request\",\n                    service=svc_name\n                )\n                raise ServiceUnavailableError(f\"{svc_name} service is currently unavailable\")\n                \n            try:\n                result = await func(self, *args, **kwargs)\n                breaker.record_success()\n                return result\n            except Exception as e:\n                breaker.record_failure()\n                logger.exception(\n                    \"External service call failed\",\n                    service=svc_name,\n                    error=str(e)\n                )\n                raise\n                \n        return wrapper\n    return decorator\n```\n\n3. Apply the circuit breaker decorator to all external service calls in:\n   - `app/services/llama_index_service.py`\n   - `app/services/crewai_service.py`\n   - `app/services/embedding_service.py`\n   - `app/services/arcade_service.py`\n   - `app/services/neo4j_service.py`\n   - `app/services/b2_storage.py`\n\n4. Create a ServiceUnavailableError class that returns appropriate 503 responses",
        "testStrategy": "1. Create unit tests that verify:\n   - Circuit breaker opens after threshold failures\n   - Circuit breaker rejects requests when open\n   - Circuit breaker allows requests after recovery timeout\n   - Circuit breaker resets on successful calls\n\n2. Create integration tests that:\n   - Simulate service failures to trigger circuit breaker\n   - Verify correct error responses when circuit is open\n   - Test recovery behavior after timeout\n   - Confirm circuit breaker state is maintained between requests\n\n3. Test with various failure scenarios to ensure all services are properly protected",
        "priority": "high",
        "dependencies": [
          "163"
        ],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-16T05:27:17.477Z"
      },
      {
        "id": "165",
        "title": "Standardize Error Response Format",
        "description": "Ensure all endpoints use a standardized error response format to provide consistent error handling across the API.",
        "details": "Create or update the error models and handlers to ensure standardized error responses:\n\n1. Update `app/models/errors.py` to define standard error response models:\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, Any, Optional\nfrom datetime import datetime\nimport uuid\n\nclass ErrorDetails(BaseModel):\n    code: str\n    message: str\n    details: Optional[Dict[str, Any]] = None\n    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    timestamp: datetime = Field(default_factory=datetime.utcnow)\n\nclass ErrorResponse(BaseModel):\n    error: ErrorDetails\n\n# Define standard error codes\nclass ErrorCode:\n    VALIDATION_ERROR = \"VALIDATION_ERROR\"  # 400\n    AUTHENTICATION_ERROR = \"AUTHENTICATION_ERROR\"  # 401\n    AUTHORIZATION_ERROR = \"AUTHORIZATION_ERROR\"  # 403\n    NOT_FOUND = \"NOT_FOUND\"  # 404\n    RATE_LIMITED = \"RATE_LIMITED\"  # 429\n    EXTERNAL_SERVICE_ERROR = \"EXTERNAL_SERVICE_ERROR\"  # 502\n    SERVICE_UNAVAILABLE = \"SERVICE_UNAVAILABLE\"  # 503\n    INTERNAL_ERROR = \"INTERNAL_ERROR\"  # 500\n```\n\n2. Create error factory functions:\n```python\ndef create_error_response(code: str, message: str, details: Dict[str, Any] = None, request_id: str = None) -> ErrorResponse:\n    return ErrorResponse(\n        error=ErrorDetails(\n            code=code,\n            message=message,\n            details=details,\n            request_id=request_id or str(uuid.uuid4()),\n            timestamp=datetime.utcnow()\n        )\n    )\n```\n\n3. Update `app/middleware/error_handler.py` to use these standardized formats:\n```python\nfrom fastapi import Request, status\nfrom fastapi.responses import JSONResponse\nfrom fastapi.exceptions import RequestValidationError\nfrom app.models.errors import create_error_response, ErrorCode\nimport structlog\n\nlogger = structlog.get_logger(__name__)\n\nasync def validation_exception_handler(request: Request, exc: RequestValidationError):\n    details = {}\n    for error in exc.errors():\n        loc = \"_\".join([str(l) for l in error[\"loc\"]])\n        details[loc] = error[\"msg\"]\n        \n    return JSONResponse(\n        status_code=status.HTTP_400_BAD_REQUEST,\n        content=create_error_response(\n            code=ErrorCode.VALIDATION_ERROR,\n            message=\"Validation error\",\n            details=details,\n            request_id=request.headers.get(\"X-Request-ID\")\n        ).dict()\n    )\n\n# Add similar handlers for other error types\n```\n\n4. Update all custom exception handlers to use the standardized format\n\n5. Register the exception handlers in `app/main.py`:\n```python\nfrom app.middleware.error_handler import validation_exception_handler\nfrom fastapi.exceptions import RequestValidationError\n\napp = FastAPI()\napp.add_exception_handler(RequestValidationError, validation_exception_handler)\n# Register other exception handlers\n```",
        "testStrategy": "1. Create unit tests that verify:\n   - Error response format matches the specification\n   - All error codes produce correct status codes\n   - Error details are properly included\n   - Request ID and timestamp are present\n\n2. Create integration tests that:\n   - Trigger various error conditions (validation, auth, not found, etc.)\n   - Verify response format is consistent across all error types\n   - Check that status codes match the error type\n   - Confirm all required fields are present in responses\n\n3. Create a test utility that validates error response structure against the schema",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-16T05:27:17.837Z"
      },
      {
        "id": "166",
        "title": "Enhance Exception Logging",
        "description": "Improve exception handling to include detailed context information for better debugging and observability in production.",
        "details": "Enhance the exception logging in error handlers to include more context:\n\n1. Update `app/middleware/error_handler.py` to include comprehensive logging:\n```python\nimport traceback\nimport structlog\nfrom fastapi import Request, status\nfrom fastapi.responses import JSONResponse\n\nlogger = structlog.get_logger(__name__)\n\nasync def generic_exception_handler(request: Request, exc: Exception):\n    # Extract user ID if available\n    user_id = getattr(request.state, \"user_id\", None) if hasattr(request, \"state\") else None\n    \n    # Get request ID from header or generate one\n    request_id = request.headers.get(\"X-Request-ID\") or str(uuid.uuid4())\n    \n    # Log the exception with context\n    logger.exception(\n        \"Unhandled exception in request\",\n        error_type=type(exc).__name__,\n        error_message=str(exc),\n        endpoint=request.url.path,\n        method=request.method,\n        request_id=request_id,\n        user_id=user_id,\n        traceback=traceback.format_exc()\n    )\n    \n    # Create standardized error response\n    return JSONResponse(\n        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n        content=create_error_response(\n            code=ErrorCode.INTERNAL_ERROR,\n            message=\"An unexpected error occurred\",\n            request_id=request_id\n        ).dict()\n    )\n```\n\n2. Add try-except blocks with enhanced logging in all service methods:\n```python\ntry:\n    # Service method implementation\n    return result\nexcept Exception as e:\n    logger.exception(\n        \"Error in service method\",\n        error_type=type(e).__name__,\n        error_message=str(e),\n        method=\"method_name\",\n        params={\"param1\": value1, \"param2\": value2},  # Include relevant parameters\n        user_id=user_id,\n        request_id=request_id\n    )\n    raise\n```\n\n3. Update all custom exception handlers to include similar detailed logging\n\n4. Ensure all logs include the request ID for correlation\n\n5. Register the enhanced exception handler in `app/main.py`:\n```python\nfrom app.middleware.error_handler import generic_exception_handler\n\napp = FastAPI()\napp.add_exception_handler(Exception, generic_exception_handler)\n```",
        "testStrategy": "1. Create unit tests that verify:\n   - Exception logs include all required context fields\n   - User ID is correctly extracted and included\n   - Request ID is properly propagated\n   - Stack traces are included for 500 errors\n\n2. Create integration tests that:\n   - Trigger various exception types\n   - Verify log output contains all required information\n   - Check that request context is properly captured\n   - Confirm correlation between logs and error responses\n\n3. Test with various request scenarios to ensure all context is properly captured",
        "priority": "medium",
        "dependencies": [
          "165"
        ],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-16T05:27:18.209Z"
      },
      {
        "id": "167",
        "title": "Implement Query Parameter Validation",
        "description": "Add Pydantic models for all query parameters to ensure proper validation and sanitization of user input.",
        "details": "Implement Pydantic models for query parameter validation across all endpoints:\n\n1. Create base validation models in `app/models/validators.py`:\n```python\nfrom pydantic import BaseModel, Field, validator\nfrom typing import Dict, Any, Optional, List\n\nclass QueryRequest(BaseModel):\n    query: str = Field(..., min_length=1, max_length=10000)\n    top_k: int = Field(default=10, ge=1, le=100)\n    filters: Optional[Dict[str, Any]] = Field(default=None)\n\n    @validator(\"query\")\n    def sanitize_query(cls, v):\n        # Remove potential injection patterns\n        dangerous_patterns = [\"--\", \";\", \"DROP\", \"DELETE\", \"INSERT\"]\n        for pattern in dangerous_patterns:\n            if pattern.upper() in v.upper():\n                raise ValueError(f\"Query contains disallowed pattern: {pattern}\")\n        return v.strip()\n\nclass PaginationParams(BaseModel):\n    page: int = Field(default=1, ge=1)\n    page_size: int = Field(default=20, ge=1, le=100)\n\nclass DocumentQueryParams(PaginationParams):\n    sort_by: str = Field(default=\"created_at\")\n    sort_order: str = Field(default=\"desc\")\n    filter_tag: Optional[str] = None\n    \n    @validator(\"sort_by\")\n    def validate_sort_field(cls, v):\n        allowed_fields = [\"created_at\", \"updated_at\", \"title\", \"size\"]\n        if v not in allowed_fields:\n            raise ValueError(f\"Invalid sort field. Must be one of: {', '.join(allowed_fields)}\")\n        return v\n        \n    @validator(\"sort_order\")\n    def validate_sort_order(cls, v):\n        if v.lower() not in [\"asc\", \"desc\"]:\n            raise ValueError(\"Sort order must be 'asc' or 'desc'\")\n        return v.lower()\n\nclass GraphQueryParams(BaseModel):\n    entity_id: str = Field(..., min_length=1, max_length=100)\n    depth: int = Field(default=2, ge=1, le=5)\n    relationship_types: Optional[List[str]] = None\n    \n    @validator(\"entity_id\")\n    def validate_entity_id(cls, v):\n        if not v.isalnum() and not \"-\" in v:\n            raise ValueError(\"Entity ID must contain only alphanumeric characters and hyphens\")\n        return v\n\nclass SearchParams(BaseModel):\n    term: str = Field(..., min_length=1, max_length=200)\n    category: Optional[str] = None\n    date_from: Optional[str] = None\n    date_to: Optional[str] = None\n    \n    @validator(\"term\")\n    def sanitize_term(cls, v):\n        return v.strip()\n```\n\n2. Apply these validators in route handlers:\n```python\nfrom fastapi import Depends\nfrom app.models.validators import QueryRequest, DocumentQueryParams\n\n@router.post(\"/query\")\nasync def query_endpoint(query_params: QueryRequest):\n    # Use validated parameters\n    result = await query_service.process_query(\n        query=query_params.query,\n        top_k=query_params.top_k,\n        filters=query_params.filters\n    )\n    return result\n\n@router.get(\"/documents\")\nasync def list_documents(params: DocumentQueryParams = Depends()):\n    # Use validated parameters\n    documents = await document_service.list_documents(\n        page=params.page,\n        page_size=params.page_size,\n        sort_by=params.sort_by,\n        sort_order=params.sort_order,\n        filter_tag=params.filter_tag\n    )\n    return documents\n```\n\n3. Update all API endpoints to use appropriate validators:\n   - `/api/query/*` endpoints\n   - `/api/documents/*` endpoints\n   - `/api/graph/*` endpoints\n   - `/api/search/*` endpoints",
        "testStrategy": "1. Create unit tests that verify:\n   - Valid parameters are accepted\n   - Invalid parameters are rejected with appropriate errors\n   - Validators correctly sanitize and transform input\n   - Default values are applied correctly\n\n2. Create integration tests that:\n   - Test each endpoint with valid and invalid parameters\n   - Verify error responses for validation failures\n   - Check that sanitized values are used correctly\n   - Test edge cases for each parameter type\n\n3. Create security tests that attempt to inject malicious patterns and verify they are caught by validators",
        "priority": "medium",
        "dependencies": [
          "165"
        ],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-16T05:27:18.589Z"
      },
      {
        "id": "168",
        "title": "Create Integration Tests for Production Readiness",
        "description": "Develop comprehensive integration tests to verify all production readiness improvements function correctly together.",
        "details": "Create integration tests that verify the combined functionality of all production readiness improvements:\n\n1. Create a test suite in `tests/integration/test_production_readiness.py`:\n```python\nimport pytest\nimport os\nfrom fastapi.testclient import TestClient\nfrom unittest.mock import patch, MagicMock\nfrom app.main import app\n\n@pytest.fixture\ndef client():\n    return TestClient(app)\n\n# Environment Variable Validation Tests\ndef test_startup_validation_fails_with_missing_critical_vars():\n    with patch.dict(os.environ, {\"SUPABASE_URL\": \"\", \"REDIS_URL\": \"\"}, clear=True):\n        with pytest.raises(RuntimeError, match=\"Missing critical env vars\"):\n            from app.core.startup_validation import validate_environment\n            validate_environment()\n\n# CORS Tests\ndef test_cors_rejects_wildcard_in_production():\n    with patch.dict(os.environ, {\"ENVIRONMENT\": \"production\", \"CORS_ORIGINS\": \"*\"}, clear=False):\n        with pytest.raises(RuntimeError, match=\"CORS_ORIGINS cannot be '*' in production\"):\n            # Import here to trigger CORS validation\n            from app.main import setup_cors\n            setup_cors()\n\n# Rate Limiting Tests\ndef test_login_endpoint_rate_limiting(client):\n    # Make 6 requests to login endpoint (limit is 5/minute)\n    for i in range(6):\n        response = client.post(\"/api/users/login\", json={\"username\": \"test\", \"password\": \"test\"})\n        if i < 5:\n            assert response.status_code != 429\n        else:\n            assert response.status_code == 429\n            assert \"error\" in response.json()\n            assert response.json()[\"error\"][\"code\"] == \"RATE_LIMITED\"\n\n# Timeout Tests\n@patch(\"httpx.AsyncClient.post\")\nasync def test_external_service_timeout(mock_post):\n    # Simulate timeout\n    mock_post.side_effect = httpx.TimeoutException(\"Timeout\")\n    \n    from app.services.llama_index_service import LlamaIndexService\n    service = LlamaIndexService()\n    \n    with pytest.raises(httpx.TimeoutException):\n        await service.process_document(\"test.pdf\")\n\n# Circuit Breaker Tests\n@patch(\"httpx.AsyncClient.post\")\nasync def test_circuit_breaker_opens_after_failures(mock_post):\n    # Simulate multiple failures\n    mock_post.side_effect = Exception(\"Service error\")\n    \n    from app.services.llama_index_service import LlamaIndexService\n    service = LlamaIndexService()\n    \n    # Cause failures to trigger circuit breaker\n    for i in range(6):  # Threshold is 5\n        try:\n            await service.process_document(\"test.pdf\")\n        except Exception:\n            pass\n    \n    # Next call should be rejected by circuit breaker\n    with pytest.raises(ServiceUnavailableError):\n        await service.process_document(\"test.pdf\")\n\n# Error Response Format Tests\ndef test_error_response_format(client):\n    # Test validation error\n    response = client.post(\"/api/query\", json={})\n    assert response.status_code == 400\n    \n    error_data = response.json()\n    assert \"error\" in error_data\n    assert \"code\" in error_data[\"error\"]\n    assert \"message\" in error_data[\"error\"]\n    assert \"details\" in error_data[\"error\"]\n    assert \"request_id\" in error_data[\"error\"]\n    assert \"timestamp\" in error_data[\"error\"]\n    \n    assert error_data[\"error\"][\"code\"] == \"VALIDATION_ERROR\"\n```\n\n2. Create a test fixture that simulates production environment:\n```python\n@pytest.fixture\ndef production_environment():\n    original_env = os.environ.copy()\n    os.environ.update({\n        \"ENVIRONMENT\": \"production\",\n        \"CORS_ORIGINS\": \"https://app.example.com,https://admin.example.com\",\n        \"SUPABASE_URL\": \"https://example.supabase.co\",\n        \"SUPABASE_SERVICE_KEY\": \"dummy-key\",\n        \"REDIS_URL\": \"redis://localhost:6379/0\",\n        \"NEO4J_URI\": \"bolt://localhost:7687\",\n        \"NEO4J_PASSWORD\": \"password\",\n        \"ANTHROPIC_API_KEY\": \"dummy-key\"\n    })\n    yield\n    os.environ.clear()\n    os.environ.update(original_env)\n```\n\n3. Create tests that verify all components work together in a production-like environment",
        "testStrategy": "1. Run the integration tests in a CI/CD pipeline to verify production readiness\n2. Test with various environment configurations to ensure all validation works correctly\n3. Verify that all components (validation, CORS, rate limiting, timeouts, circuit breakers, error handling) work together correctly\n4. Create a test matrix that covers all possible combinations of failure scenarios\n5. Verify that the tests accurately reflect the production environment",
        "priority": "medium",
        "dependencies": [
          "160",
          "161",
          "162",
          "163",
          "164",
          "165",
          "166",
          "167"
        ],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-16T05:27:19.063Z"
      },
      {
        "id": "169",
        "title": "Implement Feature Flags for Gradual Rollout",
        "description": "Create a feature flag system to enable gradual rollout of production readiness improvements and provide a rollback mechanism.",
        "details": "Implement a feature flag system to control the rollout of production readiness features:\n\n1. Create a feature flag module in `app/core/feature_flags.py`:\n```python\nimport os\nimport redis\nfrom typing import Dict, Any, Optional\nimport structlog\n\nlogger = structlog.get_logger(__name__)\n\nclass FeatureFlags:\n    _instance = None\n    \n    @classmethod\n    def get_instance(cls) -> \"FeatureFlags\":\n        if cls._instance is None:\n            cls._instance = FeatureFlags()\n        return cls._instance\n    \n    def __init__(self):\n        self.redis_client = None\n        self.local_flags = {\n            \"strict_env_validation\": True,\n            \"strict_cors\": True,\n            \"tiered_rate_limiting\": True,\n            \"service_timeouts\": True,\n            \"circuit_breakers\": True,\n            \"standardized_errors\": True,\n            \"enhanced_logging\": True,\n            \"query_validation\": True\n        }\n        \n        # Try to connect to Redis for distributed flags\n        redis_url = os.getenv(\"REDIS_URL\")\n        if redis_url:\n            try:\n                self.redis_client = redis.from_url(redis_url)\n                logger.info(\"Connected to Redis for feature flags\")\n            except Exception as e:\n                logger.warning(\n                    \"Failed to connect to Redis for feature flags, using local flags\",\n                    error=str(e)\n                )\n    \n    def is_enabled(self, flag_name: str, default: bool = False) -> bool:\n        # Check environment variable override first\n        env_override = os.getenv(f\"FEATURE_{flag_name.upper()}\")\n        if env_override is not None:\n            return env_override.lower() in (\"1\", \"true\", \"yes\")\n        \n        # Check Redis if available\n        if self.redis_client:\n            try:\n                value = self.redis_client.get(f\"feature:{flag_name}\")\n                if value is not None:\n                    return value.decode() in (\"1\", \"true\", \"yes\")\n            except Exception as e:\n                logger.warning(\n                    \"Error reading feature flag from Redis\",\n                    flag=flag_name,\n                    error=str(e)\n                )\n        \n        # Fall back to local flags\n        return self.local_flags.get(flag_name, default)\n\n# Convenience function\ndef is_feature_enabled(flag_name: str, default: bool = False) -> bool:\n    return FeatureFlags.get_instance().is_enabled(flag_name, default)\n```\n\n2. Wrap each production readiness feature with feature flag checks:\n\n```python\n# In app/core/startup_validation.py\nfrom app.core.feature_flags import is_feature_enabled\n\ndef validate_environment():\n    if not is_feature_enabled(\"strict_env_validation\"):\n        logger.info(\"Strict environment validation disabled by feature flag\")\n        return {\"critical\": [], \"recommended\": []}\n    \n    # Proceed with validation as normal\n    # ...\n```\n\n3. Create an admin endpoint to control feature flags in production:\n```python\n@router.post(\"/admin/features/{feature_name}\", status_code=200)\nasync def set_feature_flag(\n    feature_name: str,\n    enabled: bool,\n    request: Request,\n    current_user: User = Depends(get_current_user)\n):\n    # Check admin permissions\n    if not current_user.is_admin:\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    \n    # Set the flag in Redis\n    redis_client = get_redis_client()\n    redis_client.set(f\"feature:{feature_name}\", \"1\" if enabled else \"0\")\n    \n    logger.info(\n        \"Feature flag updated\",\n        feature=feature_name,\n        enabled=enabled,\n        user_id=current_user.id\n    )\n    \n    return {\"status\": \"success\", \"feature\": feature_name, \"enabled\": enabled}\n```\n\n4. Add feature flag documentation in `docs/feature_flags.md` explaining each flag and its purpose",
        "testStrategy": "1. Create unit tests that verify:\n   - Feature flags can be enabled/disabled\n   - Environment variable overrides work correctly\n   - Redis-based flags take precedence over local flags\n   - Default values are used when flags are not defined\n\n2. Create integration tests that:\n   - Test each feature with flags enabled and disabled\n   - Verify admin endpoints correctly update flags\n   - Test fallback behavior when Redis is unavailable\n\n3. Create documentation tests that verify all features have corresponding flags and documentation",
        "priority": "medium",
        "dependencies": [
          "160",
          "161",
          "162",
          "163",
          "164",
          "165",
          "166",
          "167"
        ],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-16T05:27:19.585Z"
      },
      {
        "id": "170",
        "title": "Implement Environment Variable Validation",
        "description": "Create a startup validation module that fails fast if required environment variables are missing, ensuring the application doesn't start in an invalid state.",
        "details": "Create a new module `app/core/startup_validation.py` that validates critical and recommended environment variables. The module should:\n1. Define lists of critical variables (`SUPABASE_URL`, `SUPABASE_SERVICE_KEY`, `REDIS_URL`, `ANTHROPIC_API_KEY`, `ENVIRONMENT`) and recommended variables (`NEO4J_URI`, `NEO4J_PASSWORD`, `LLAMAINDEX_SERVICE_URL`, `CREWAI_SERVICE_URL`)\n2. Implement a `validate_environment()` function that checks for missing variables\n3. Raise a `RuntimeError` with a clear message if any critical variables are missing\n4. Log warnings for missing recommended variables\n5. Return a dictionary with lists of missing variables by category\n\nModify `app/main.py` to call this validation function during application startup before any services are initialized.",
        "testStrategy": "1. Create unit tests in `tests/test_startup_validation.py` that:\n   - Test successful validation when all critical variables are present\n   - Test failure when critical variables are missing\n   - Test warning generation for missing recommended variables\n   - Test the returned dictionary structure\n2. Create integration tests that verify the application fails to start when critical variables are missing\n3. Test with various combinations of missing variables to ensure proper error messages",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create test_startup_validation.py with test fixtures",
            "description": "Create a test file with fixtures for environment variable validation tests",
            "dependencies": [],
            "details": "Create tests/test_startup_validation.py with pytest fixtures that can mock environment variables for testing. Include fixtures for setting up different environment scenarios (all vars present, missing critical vars, missing recommended vars, empty string vars).",
            "status": "pending",
            "testStrategy": "Use pytest monkeypatch fixture to temporarily modify environment variables during tests. Create parametrized tests for different scenarios.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Test missing critical env var causes startup failure",
            "description": "Implement test case to verify that missing critical environment variables cause application startup to fail",
            "dependencies": [
              1
            ],
            "details": "Create a test function that mocks environment with one or more missing critical variables and verifies that validate_environment() raises a RuntimeError with appropriate error message listing the missing variables.",
            "status": "pending",
            "testStrategy": "Use pytest.raises to verify RuntimeError is raised with the expected error message containing the names of missing critical variables.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Test missing recommended env var logs warning but continues",
            "description": "Implement test case to verify that missing recommended variables generate warnings but don't prevent startup",
            "dependencies": [
              1
            ],
            "details": "Create a test function that mocks environment with all critical variables but missing recommended variables, and verifies that validate_environment() logs warnings but returns successfully with a dictionary containing the missing recommended variables.",
            "status": "pending",
            "testStrategy": "Use caplog fixture to capture log output and verify warning messages. Check the returned dictionary structure contains the missing recommended variables.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Test empty string env var treated as missing",
            "description": "Implement test case to verify that environment variables with empty string values are treated as missing",
            "dependencies": [
              1
            ],
            "details": "Create a test function that sets critical and recommended environment variables to empty strings and verifies they are treated the same as missing variables (raising errors for critical vars, warnings for recommended vars).",
            "status": "pending",
            "testStrategy": "Set environment variables to empty strings and verify the same behavior as when they're completely missing.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Test all env vars present logs success message",
            "description": "Implement test case to verify that when all environment variables are present, a success message is logged",
            "dependencies": [
              1
            ],
            "details": "Create a test function that mocks environment with all critical and recommended variables present and verifies that validate_environment() logs a success message and returns an empty dictionary (no missing variables).",
            "status": "pending",
            "testStrategy": "Use caplog fixture to capture log output and verify success message. Check the returned dictionary structure shows no missing variables.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Create startup_validation.py with environment variable constants",
            "description": "Create the startup validation module with constants for critical and recommended environment variables",
            "dependencies": [],
            "details": "Create app/core/startup_validation.py module and define CRITICAL_ENV_VARS and RECOMMENDED_ENV_VARS lists containing the required environment variables as specified in the task description. CRITICAL_ENV_VARS should include SUPABASE_URL, SUPABASE_SERVICE_KEY, REDIS_URL, ANTHROPIC_API_KEY, and ENVIRONMENT. RECOMMENDED_ENV_VARS should include NEO4J_URI, NEO4J_PASSWORD, LLAMAINDEX_SERVICE_URL, and CREWAI_SERVICE_URL.",
            "status": "pending",
            "testStrategy": "Verify constants are correctly defined through the tests implemented in subtasks 2-5.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Implement validate_environment() function",
            "description": "Implement the core validation function that checks for missing environment variables",
            "dependencies": [
              6
            ],
            "details": "Implement validate_environment() function in app/core/startup_validation.py that checks for missing critical and recommended variables, raises RuntimeError for missing critical variables, logs warnings for missing recommended variables, and returns a dictionary with lists of missing variables by category.",
            "status": "pending",
            "testStrategy": "Function will be tested by the test cases implemented in subtasks 2-5.",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Add structlog logging for validation results",
            "description": "Implement structured logging for environment variable validation results",
            "dependencies": [
              7
            ],
            "details": "Add structlog logging to the validate_environment() function to log validation results with appropriate log levels (error for missing critical vars, warning for missing recommended vars, info for successful validation). Include structured data in logs for easier parsing and monitoring.",
            "status": "pending",
            "testStrategy": "Verify log messages and structure through the tests implemented in subtasks 2-5 using caplog fixture.",
            "parentId": "undefined"
          },
          {
            "id": 9,
            "title": "Add startup hook in app/main.py",
            "description": "Modify app/main.py to call validate_environment() during application startup",
            "dependencies": [
              7,
              8
            ],
            "details": "Modify app/main.py to import and call the validate_environment() function during application startup, before any services are initialized. Ensure the application fails to start if validate_environment() raises a RuntimeError due to missing critical environment variables.",
            "status": "pending",
            "testStrategy": "Create an integration test that verifies the application startup sequence calls validate_environment() and handles errors appropriately.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-16T17:43:23.112Z"
      },
      {
        "id": "171",
        "title": "Implement CORS Production Hardening",
        "description": "Enhance CORS configuration to fail in production if wildcard origins are detected, preventing potential security vulnerabilities.",
        "details": "Modify the CORS configuration in `app/main.py` to:\n1. Parse the `CORS_ORIGINS` environment variable into a list of allowed origins\n2. Check if the environment is production (`ENVIRONMENT=production`)\n3. If in production, raise a `RuntimeError` if:\n   - `CORS_ORIGINS` is not set or empty\n   - `CORS_ORIGINS` contains a wildcard (`*`)\n4. Allow wildcard origins only in non-production environments\n5. Add clear error messages explaining the security requirement\n\nExample implementation:\n```python\ncors_origins = os.getenv(\"CORS_ORIGINS\", \"\").split(\",\")\nif not cors_origins or cors_origins == [\"\"]:\n    if os.getenv(\"ENVIRONMENT\") == \"production\":\n        raise RuntimeError(\n            \"CORS_ORIGINS must be explicitly set in production. \"\n            \"Set to specific origins like 'https://app.example.com,https://admin.example.com'\"\n        )\n    cors_origins = [\"*\"]  # Allow in development only\n\nif \"*\" in cors_origins and os.getenv(\"ENVIRONMENT\") == \"production\":\n    raise RuntimeError(\n        \"CORS_ORIGINS cannot be '*' in production. \"\n        \"Set specific allowed origins for security.\"\n    )\n```",
        "testStrategy": "1. Create unit tests that verify:\n   - Application starts with explicit origins in production\n   - Application fails with wildcard origins in production\n   - Application fails with empty origins in production\n   - Application allows wildcard in development/staging\n2. Create integration tests that verify CORS headers are properly set\n3. Test with various origin configurations to ensure proper behavior",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create test fixtures for CORS hardening tests",
            "description": "Create a new test file tests/test_cors_hardening.py with necessary test fixtures and setup for testing CORS validation in different environments.",
            "dependencies": [],
            "details": "Create tests/test_cors_hardening.py with pytest fixtures that simulate different environment configurations (production vs development). Include fixtures for mocking environment variables like ENVIRONMENT and CORS_ORIGINS with various values. Set up the test structure with appropriate imports and mock application initialization.",
            "status": "pending",
            "testStrategy": "Verify that test fixtures correctly simulate different environment configurations and CORS settings. Ensure mocks are properly set up to isolate tests from actual environment variables.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Test wildcard CORS in production causes startup failure",
            "description": "Implement a test case that verifies the application raises a RuntimeError when wildcard CORS origins are detected in production environment.",
            "dependencies": [
              1
            ],
            "details": "Add a test function in tests/test_cors_hardening.py that sets ENVIRONMENT='production' and CORS_ORIGINS='*' and verifies that application initialization raises a RuntimeError with an appropriate error message about wildcard CORS not being allowed in production.",
            "status": "pending",
            "testStrategy": "Use pytest's raises context manager to verify the correct exception type is raised with the expected error message about wildcard CORS being insecure in production.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Test missing CORS_ORIGINS in production causes startup failure",
            "description": "Implement a test case that verifies the application raises a RuntimeError when CORS_ORIGINS is not set or empty in production environment.",
            "dependencies": [
              1
            ],
            "details": "Add a test function in tests/test_cors_hardening.py that sets ENVIRONMENT='production' and either doesn't set CORS_ORIGINS or sets it to an empty string. Verify that application initialization raises a RuntimeError with an appropriate error message about explicit CORS origins being required in production.",
            "status": "pending",
            "testStrategy": "Test both scenarios: completely unset CORS_ORIGINS and empty string CORS_ORIGINS. Verify the correct exception type and error message in both cases.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Test wildcard CORS in development is allowed with warning",
            "description": "Implement a test case that verifies the application allows wildcard CORS origins in development environment but logs a warning.",
            "dependencies": [
              1
            ],
            "details": "Add a test function in tests/test_cors_hardening.py that sets ENVIRONMENT='development' and CORS_ORIGINS='*' or empty. Verify that application initialization succeeds without errors but logs an appropriate warning message about using wildcard CORS in development.",
            "status": "pending",
            "testStrategy": "Use a log capture fixture to verify that the appropriate warning is logged. Test both wildcard and empty CORS_ORIGINS scenarios in development environment.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement validate_cors_origins() function",
            "description": "Create a new function validate_cors_origins() in app/core/startup_validation.py that handles CORS validation logic.",
            "dependencies": [],
            "details": "Create app/core/startup_validation.py if it doesn't exist. Implement validate_cors_origins() function that takes cors_origins list and environment as parameters. The function should validate that cors_origins is not empty and doesn't contain wildcards in production, raising appropriate RuntimeErrors with clear error messages. In non-production environments, it should return a default ['*'] if cors_origins is empty.",
            "status": "pending",
            "testStrategy": "Unit test the function directly with various input combinations to verify it correctly validates CORS origins based on environment and returns appropriate values or raises expected exceptions.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Update CORS configuration in app/main.py",
            "description": "Modify the CORS configuration in app/main.py to use the new validate_cors_origins() function for validating CORS settings.",
            "dependencies": [
              5
            ],
            "details": "Update app/main.py to import and use the validate_cors_origins() function. Parse the CORS_ORIGINS environment variable into a list, get the current environment, and pass both to validate_cors_origins(). Use the validated origins list for configuring the CORS middleware. Remove any existing CORS validation logic from main.py that's now handled by the validation function.",
            "status": "pending",
            "testStrategy": "Integration test to verify the application correctly initializes with valid CORS settings and fails with invalid settings. Test both production and development environments.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Add structlog logging for CORS validation",
            "description": "Implement structured logging using structlog for CORS validation events in app/main.py.",
            "dependencies": [
              5,
              6
            ],
            "details": "Import structlog in app/main.py and add logging statements for CORS validation events. Log when CORS origins are validated successfully, including the list of allowed origins. In development, log a warning when wildcard origins are used. Ensure logs include relevant context such as environment name and validation status.",
            "status": "pending",
            "testStrategy": "Verify logs are correctly generated with appropriate log levels and context information. Test both success and warning scenarios to ensure proper logging in each case.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-16T17:46:34.811Z"
      },
      {
        "id": "172",
        "title": "Implement Tiered Rate Limiting for Sensitive Endpoints",
        "description": "Apply stricter rate limits to authentication and upload endpoints to prevent brute force attacks, spam, and resource abuse.",
        "details": "1. Create a new file `app/middleware/rate_limit_tiers.py` to define rate limit configurations for different endpoint patterns:\n```python\nRATE_LIMIT_TIERS = {\n    \"/api/users/login\": {\"limit\": 5, \"period\": 60},  # 5 per minute\n    \"/api/users/register\": {\"limit\": 3, \"period\": 60},  # 3 per minute\n    \"/api/documents/upload\": {\"limit\": 10, \"period\": 60},  # 10 per minute\n    \"/api/query/*\": {\"limit\": 60, \"period\": 60},  # 60 per minute\n    \"/api/orchestration/*\": {\"limit\": 30, \"period\": 60},  # 30 per minute\n    \"default\": {\"limit\": 200, \"period\": 60}  # 200 per minute\n}\n```\n\n2. Modify `app/middleware/rate_limit.py` to use these tiered configurations:\n   - Implement pattern matching for endpoints\n   - Apply the most specific matching rate limit\n   - Fall back to default limit if no pattern matches\n   - Use Redis to track request counts\n   - Return standardized 429 error responses when limits are exceeded\n\n3. Add proper headers to responses:\n   - `X-RateLimit-Limit`: The rate limit ceiling\n   - `X-RateLimit-Remaining`: The number of requests left\n   - `X-RateLimit-Reset`: The time when the limit resets",
        "testStrategy": "1. Create unit tests in `tests/test_rate_limiting.py` that verify:\n   - Correct rate limit is applied for each endpoint pattern\n   - Pattern matching works correctly for wildcard patterns\n   - Default limit is applied when no pattern matches\n\n2. Create integration tests that:\n   - Verify rate limiting actually blocks requests after limit is reached\n   - Test rate limit headers are correctly set\n   - Verify different clients get separate rate limits\n   - Test rate limit reset behavior\n\n3. Load test to ensure rate limiting doesn't add significant overhead",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create test_rate_limiting.py with Redis mock fixtures",
            "description": "Set up the test file with necessary fixtures to mock Redis for rate limiting tests",
            "dependencies": [],
            "details": "Create tests/test_rate_limiting.py with Redis mock fixtures that simulate the rate limiting behavior without requiring a real Redis instance. Include setup and teardown methods to initialize and clean up the mock Redis environment for each test.",
            "status": "pending",
            "testStrategy": "Verify that the Redis mock correctly simulates increment operations and expiry times. Ensure fixtures can be reused across multiple test cases.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement test for login endpoint rate limiting",
            "description": "Create test case to verify login endpoint blocks after 5 requests per minute",
            "dependencies": [
              1
            ],
            "details": "Add a test function in tests/test_rate_limiting.py that sends 6 consecutive requests to the login endpoint and verifies that the 6th request returns a 429 status code. Ensure the test checks that the first 5 requests succeed with 200 status codes.",
            "status": "pending",
            "testStrategy": "Use pytest to simulate 6 rapid requests to the login endpoint and assert the expected response status codes.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement test for registration endpoint rate limiting",
            "description": "Create test case to verify registration endpoint blocks after 3 requests per minute",
            "dependencies": [
              1
            ],
            "details": "Add a test function in tests/test_rate_limiting.py that sends 4 consecutive requests to the registration endpoint and verifies that the 4th request returns a 429 status code. Ensure the test checks that the first 3 requests succeed with 200 status codes.",
            "status": "pending",
            "testStrategy": "Use pytest to simulate 4 rapid requests to the registration endpoint and assert the expected response status codes.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement test for upload endpoint rate limiting",
            "description": "Create test case to verify upload endpoint blocks after 10 requests per minute",
            "dependencies": [
              1
            ],
            "details": "Add a test function in tests/test_rate_limiting.py that sends 11 consecutive requests to the document upload endpoint and verifies that the 11th request returns a 429 status code. Ensure the test checks that the first 10 requests succeed with 200 status codes.",
            "status": "pending",
            "testStrategy": "Use pytest to simulate 11 rapid requests to the upload endpoint and assert the expected response status codes.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement test for Retry-After header in rate limit responses",
            "description": "Create test case to verify rate limit responses include the Retry-After header",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Add a test function in tests/test_rate_limiting.py that triggers a rate limit response and verifies that the response includes the Retry-After header with an appropriate value indicating when the client can retry the request.",
            "status": "pending",
            "testStrategy": "Verify the Retry-After header is present and contains a valid integer value representing seconds until the rate limit resets.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Create rate_limit_tiers.py with RateLimitTier model",
            "description": "Implement the rate limit tiers configuration file with appropriate models and constants",
            "dependencies": [],
            "details": "Create app/middleware/rate_limit_tiers.py that defines a RateLimitTier model class and the RATE_LIMIT_TIERS constant dictionary mapping endpoint patterns to their respective rate limits. Include pattern matching utility functions to determine the appropriate tier for a given endpoint.",
            "status": "pending",
            "testStrategy": "Unit test the pattern matching functionality to ensure it correctly identifies the most specific matching pattern for various endpoint URLs.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Update rate_limit.py to support tiered rate limiting",
            "description": "Modify the existing rate limiting middleware to use the tiered configuration",
            "dependencies": [
              6
            ],
            "details": "Update app/middleware/rate_limit.py to use the RATE_LIMIT_TIERS configuration. Implement pattern matching for endpoints to apply the most specific matching rate limit, falling back to the default limit if no pattern matches. Ensure Redis is used to track request counts and implement standardized 429 error responses.",
            "status": "pending",
            "testStrategy": "Test the middleware with various endpoint patterns to ensure it applies the correct rate limit tier and properly tracks request counts in Redis.",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Add rate limit decorators to login endpoint",
            "description": "Apply rate limiting to the login endpoint with a limit of 5 requests per minute",
            "dependencies": [
              7
            ],
            "details": "Update app/routes/users.py to apply the rate limiting decorator to the login endpoint. Ensure the decorator is configured to use the '/api/users/login' tier which allows 5 requests per minute. Verify that the rate limit is applied before any authentication logic.",
            "status": "pending",
            "testStrategy": "Test that the login endpoint correctly applies the rate limit by sending multiple requests and verifying the behavior.",
            "parentId": "undefined"
          },
          {
            "id": 9,
            "title": "Add rate limit decorators to register endpoint",
            "description": "Apply rate limiting to the registration endpoint with a limit of 3 requests per minute",
            "dependencies": [
              7
            ],
            "details": "Update app/routes/users.py to apply the rate limiting decorator to the register endpoint. Ensure the decorator is configured to use the '/api/users/register' tier which allows 3 requests per minute. Verify that the rate limit is applied before any registration logic.",
            "status": "pending",
            "testStrategy": "Test that the register endpoint correctly applies the rate limit by sending multiple requests and verifying the behavior.",
            "parentId": "undefined"
          },
          {
            "id": 10,
            "title": "Add rate limit decorators to upload endpoint",
            "description": "Apply rate limiting to the document upload endpoint with a limit of 10 requests per minute",
            "dependencies": [
              7
            ],
            "details": "Update app/routes/documents.py to apply the rate limiting decorator to the upload endpoint. Ensure the decorator is configured to use the '/api/documents/upload' tier which allows 10 requests per minute. Verify that the rate limit is applied before any file processing logic.",
            "status": "pending",
            "testStrategy": "Test that the upload endpoint correctly applies the rate limit by sending multiple requests and verifying the behavior.",
            "parentId": "undefined"
          },
          {
            "id": 11,
            "title": "Add rate limit decorators to query endpoints",
            "description": "Apply rate limiting to all query endpoints with a limit of 60 requests per minute",
            "dependencies": [
              7
            ],
            "details": "Update app/routes/query.py to apply the rate limiting decorator to all query endpoints. Ensure the decorator is configured to use the '/api/query/*' tier which allows 60 requests per minute. Verify that the wildcard pattern correctly matches all endpoints in the query module.",
            "status": "pending",
            "testStrategy": "Test that all query endpoints correctly apply the rate limit by sending multiple requests to different query endpoints and verifying the behavior.",
            "parentId": "undefined"
          },
          {
            "id": 12,
            "title": "Update rate limit response with standardized ErrorResponse",
            "description": "Enhance rate limit responses to use a standardized error format with Retry-After header",
            "dependencies": [
              7
            ],
            "details": "Modify the rate limiting middleware to return a standardized ErrorResponse object when rate limits are exceeded. Include the Retry-After header in the response with the time (in seconds) until the rate limit resets. Ensure the response includes appropriate HTTP headers: X-RateLimit-Limit, X-RateLimit-Remaining, and X-RateLimit-Reset.",
            "status": "pending",
            "testStrategy": "Test that rate limit responses contain the correct ErrorResponse format and all required headers with appropriate values.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-16T17:58:41.226Z"
      },
      {
        "id": "173",
        "title": "Implement External Service Timeouts",
        "description": "Add configurable timeouts to all external HTTP calls to prevent indefinite hanging when external services are slow or unresponsive.",
        "details": "1. Create a new file `app/core/service_timeouts.py` to define timeout constants:\n```python\nSERVICE_TIMEOUTS = {\n    \"llama_index\": 60.0,  # 60 seconds (document parsing can be slow)\n    \"crewai\": 120.0,     # 120 seconds (multi-agent workflows)\n    \"ollama\": 30.0,      # 30 seconds (embeddings)\n    \"neo4j\": 15.0,       # 15 seconds\n    \"supabase\": 10.0,    # 10 seconds\n    \"default\": 30.0      # Default timeout\n}\n```\n\n2. Create a base client class that implements timeouts:\n```python\nclass ExternalServiceClient:\n    def __init__(self, base_url: str, service_name: str):\n        timeout = SERVICE_TIMEOUTS.get(service_name, SERVICE_TIMEOUTS[\"default\"])\n        self.client = httpx.AsyncClient(\n            base_url=base_url,\n            timeout=httpx.Timeout(timeout, connect=5.0),\n            limits=httpx.Limits(max_connections=100, max_keepalive_connections=20)\n        )\n        self.service_name = service_name\n\n    async def call(self, endpoint: str, **kwargs):\n        try:\n            return await self.client.post(endpoint, **kwargs)\n        except httpx.TimeoutException:\n            logger.error(f\"{self.service_name} request timed out\", endpoint=endpoint)\n            raise ServiceTimeoutError(f\"{self.service_name} request timed out\")\n```\n\n3. Modify the following service files to use the timeout-enabled client:\n   - `app/services/llama_index_service.py`\n   - `app/services/crewai_service.py`\n   - `app/services/embedding_service.py`\n   - `app/services/neo4j_service.py`\n   - `app/services/b2_storage.py`",
        "testStrategy": "1. Create unit tests that verify:\n   - Timeout values are correctly applied from configuration\n   - TimeoutException is properly caught and converted to ServiceTimeoutError\n\n2. Create integration tests that:\n   - Use a mock server that delays responses beyond timeout threshold\n   - Verify requests actually timeout after the configured duration\n   - Test different timeout values for different services\n\n3. Create tests for timeout error handling and reporting",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create test_service_timeouts.py with httpx mock fixtures",
            "description": "Set up the test file with necessary fixtures for mocking httpx responses and timeouts",
            "dependencies": [],
            "details": "Create tests/test_service_timeouts.py with httpx mock fixtures for simulating timeouts. Include setup for mocked responses, delayed responses, and timeout exceptions. Set up pytest fixtures that can be reused across all timeout tests.",
            "status": "pending",
            "testStrategy": "Verify that the mock fixtures correctly simulate timeouts and can be used in subsequent tests. Ensure the mocks can be configured with different timeout durations.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Test LlamaIndex service timeout after 60s",
            "description": "Implement test case to verify LlamaIndex service times out correctly after 60 seconds",
            "dependencies": [
              1
            ],
            "details": "Create a test that verifies the LlamaIndex service correctly applies a 60-second timeout. Use the httpx mock to simulate a response that takes longer than 60 seconds and verify that a ServiceTimeoutError is raised with the appropriate message.",
            "status": "pending",
            "testStrategy": "Use a mock server that delays responses beyond 60 seconds and verify the timeout exception is properly caught and converted to ServiceTimeoutError.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Test CrewAI service timeout after 120s",
            "description": "Implement test case to verify CrewAI service times out correctly after 120 seconds",
            "dependencies": [
              1
            ],
            "details": "Create a test that verifies the CrewAI service correctly applies a 120-second timeout. Use the httpx mock to simulate a response that takes longer than 120 seconds and verify that a ServiceTimeoutError is raised with the appropriate message.",
            "status": "pending",
            "testStrategy": "Use a mock server that delays responses beyond 120 seconds and verify the timeout exception is properly caught and converted to ServiceTimeoutError.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Test Ollama service timeout after 30s",
            "description": "Implement test case to verify Ollama service times out correctly after 30 seconds",
            "dependencies": [
              1
            ],
            "details": "Create a test that verifies the Ollama service correctly applies a 30-second timeout. Use the httpx mock to simulate a response that takes longer than 30 seconds and verify that a ServiceTimeoutError is raised with the appropriate message.",
            "status": "pending",
            "testStrategy": "Use a mock server that delays responses beyond 30 seconds and verify the timeout exception is properly caught and converted to ServiceTimeoutError.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Test Neo4j service timeout after 15s",
            "description": "Implement test case to verify Neo4j service times out correctly after 15 seconds",
            "dependencies": [
              1
            ],
            "details": "Create a test that verifies the Neo4j service correctly applies a 15-second timeout. Use the httpx mock to simulate a response that takes longer than 15 seconds and verify that a ServiceTimeoutError is raised with the appropriate message.",
            "status": "pending",
            "testStrategy": "Use a mock server that delays responses beyond 15 seconds and verify the timeout exception is properly caught and converted to ServiceTimeoutError.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Create service_timeouts.py with timeout constants",
            "description": "Implement the core timeout configuration file with constants for all services",
            "dependencies": [],
            "details": "Create app/core/service_timeouts.py with the SERVICE_TIMEOUTS dictionary containing timeout values for all external services. Implement the ExternalServiceClient class that uses these timeout values to create httpx clients with appropriate timeout settings.",
            "status": "pending",
            "testStrategy": "Verify that the SERVICE_TIMEOUTS dictionary contains all required service entries with correct timeout values. Test that the ExternalServiceClient correctly applies the timeout values from the configuration.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Add timeout configuration to llama_index_service.py",
            "description": "Modify the LlamaIndex service to use the timeout-enabled client",
            "dependencies": [
              6
            ],
            "details": "Update app/services/llama_index_service.py to use the ExternalServiceClient with the 'llama_index' service name. Replace direct httpx calls with the timeout-enabled client. Ensure proper error handling for timeout exceptions.",
            "status": "pending",
            "testStrategy": "Verify that the LlamaIndex service correctly initializes the ExternalServiceClient with the 'llama_index' service name and that all HTTP calls use the timeout-enabled client.",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Add timeout configuration to crewai_service.py",
            "description": "Modify the CrewAI service to use the timeout-enabled client",
            "dependencies": [
              6
            ],
            "details": "Update app/services/crewai_service.py to use the ExternalServiceClient with the 'crewai' service name. Replace direct httpx calls with the timeout-enabled client. Ensure proper error handling for timeout exceptions.",
            "status": "pending",
            "testStrategy": "Verify that the CrewAI service correctly initializes the ExternalServiceClient with the 'crewai' service name and that all HTTP calls use the timeout-enabled client.",
            "parentId": "undefined"
          },
          {
            "id": 9,
            "title": "Add timeout configuration to embedding_service.py",
            "description": "Modify the embedding service to use the timeout-enabled client for Ollama",
            "dependencies": [
              6
            ],
            "details": "Update app/services/embedding_service.py to use the ExternalServiceClient with the 'ollama' service name. Replace direct httpx calls with the timeout-enabled client. Ensure proper error handling for timeout exceptions.",
            "status": "pending",
            "testStrategy": "Verify that the embedding service correctly initializes the ExternalServiceClient with the 'ollama' service name and that all HTTP calls use the timeout-enabled client.",
            "parentId": "undefined"
          },
          {
            "id": 10,
            "title": "Add timeout configuration to neo4j_service.py",
            "description": "Modify the Neo4j service to use the timeout-enabled client",
            "dependencies": [
              6
            ],
            "details": "Update app/services/neo4j_service.py to use the ExternalServiceClient with the 'neo4j' service name. Replace direct httpx calls with the timeout-enabled client. Ensure proper error handling for timeout exceptions.",
            "status": "pending",
            "testStrategy": "Verify that the Neo4j service correctly initializes the ExternalServiceClient with the 'neo4j' service name and that all HTTP calls use the timeout-enabled client.",
            "parentId": "undefined"
          },
          {
            "id": 11,
            "title": "Add timeout error handling with EXTERNAL_SERVICE_ERROR response",
            "description": "Implement standardized error handling for service timeouts across the application",
            "dependencies": [
              7,
              8,
              9,
              10
            ],
            "details": "Create a ServiceTimeoutError exception class and implement error handling that returns a standardized EXTERNAL_SERVICE_ERROR response when timeouts occur. Update all service files to catch timeout exceptions and convert them to ServiceTimeoutError with appropriate logging.",
            "status": "pending",
            "testStrategy": "Test that timeout exceptions are properly caught and converted to ServiceTimeoutError. Verify that the error response includes appropriate status codes and error messages for client handling.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-16T18:47:22.672Z"
      },
      {
        "id": "174",
        "title": "Implement Circuit Breaker Pattern for External Services",
        "description": "Apply circuit breakers to all external service calls to prevent cascading failures when external services are down or degraded.",
        "details": "1. Create a circuit breaker implementation in `app/services/circuit_breaker.py`:\n```python\nimport functools\nimport time\nfrom typing import Dict, Any, Callable, TypeVar, Awaitable\nimport structlog\n\nlogger = structlog.get_logger(__name__)\n\nT = TypeVar('T')\n\nCIRCUIT_BREAKER_CONFIG = {\n    \"llama_index\": {\"failure_threshold\": 5, \"recovery_timeout\": 30},\n    \"crewai\": {\"failure_threshold\": 3, \"recovery_timeout\": 60},\n    \"ollama\": {\"failure_threshold\": 5, \"recovery_timeout\": 15},\n    \"neo4j\": {\"failure_threshold\": 3, \"recovery_timeout\": 30},\n    \"b2\": {\"failure_threshold\": 5, \"recovery_timeout\": 60},\n    \"default\": {\"failure_threshold\": 3, \"recovery_timeout\": 30},\n}\n\nclass CircuitBreaker:\n    _instances: Dict[str, 'CircuitBreaker'] = {}\n    \n    @classmethod\n    def get_instance(cls, service_name: str) -> 'CircuitBreaker':\n        if service_name not in cls._instances:\n            config = CIRCUIT_BREAKER_CONFIG.get(\n                service_name, CIRCUIT_BREAKER_CONFIG[\"default\"]\n            )\n            cls._instances[service_name] = CircuitBreaker(\n                service_name=service_name,\n                failure_threshold=config[\"failure_threshold\"],\n                recovery_timeout=config[\"recovery_timeout\"]\n            )\n        return cls._instances[service_name]\n    \n    def __init__(self, service_name: str, failure_threshold: int, recovery_timeout: int):\n        self.service_name = service_name\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.failure_count = 0\n        self.last_failure_time = 0\n        self.open = False\n        \n    def record_failure(self):\n        self.failure_count += 1\n        self.last_failure_time = time.time()\n        \n        if self.failure_count >= self.failure_threshold:\n            if not self.open:\n                logger.warning(\n                    f\"Circuit breaker opened for {self.service_name}\",\n                    service=self.service_name,\n                    failures=self.failure_count,\n                    threshold=self.failure_threshold\n                )\n            self.open = True\n    \n    def record_success(self):\n        self.failure_count = 0\n        if self.open:\n            logger.info(\n                f\"Circuit breaker closed for {self.service_name}\",\n                service=self.service_name\n            )\n            self.open = False\n    \n    def is_open(self) -> bool:\n        if not self.open:\n            return False\n            \n        # Check if recovery timeout has elapsed\n        if time.time() - self.last_failure_time > self.recovery_timeout:\n            logger.info(\n                f\"Circuit breaker recovery timeout elapsed for {self.service_name}\",\n                service=self.service_name\n            )\n            return False\n            \n        return True\n\ndef circuit_breaker(service_name: str):\n    def decorator(func: Callable[..., Awaitable[T]]) -> Callable[..., Awaitable[T]]:\n        @functools.wraps(func)\n        async def wrapper(*args: Any, **kwargs: Any) -> T:\n            breaker = CircuitBreaker.get_instance(service_name)\n            \n            if breaker.is_open():\n                raise ServiceUnavailableError(\n                    f\"{service_name} circuit breaker is open\"\n                )\n                \n            try:\n                result = await func(*args, **kwargs)\n                breaker.record_success()\n                return result\n            except Exception as e:\n                breaker.record_failure()\n                logger.error(\n                    f\"Circuit breaker recorded failure for {service_name}\",\n                    service=service_name,\n                    error=str(e),\n                    error_type=type(e).__name__\n                )\n                raise\n                \n        return wrapper\n    return decorator\n```\n\n2. Apply the circuit breaker decorator to all external service methods in:\n   - `app/services/llama_index_service.py`\n   - `app/services/crewai_service.py`\n   - `app/services/embedding_service.py`\n   - `app/services/neo4j_service.py`\n   - `app/services/b2_storage.py`\n\n3. Create a `ServiceUnavailableError` class in `app/models/errors.py` that returns a 503 status code",
        "testStrategy": "1. Create unit tests that verify:\n   - Circuit breaker opens after threshold failures\n   - Circuit breaker stays open during recovery timeout\n   - Circuit breaker closes after recovery timeout\n   - Circuit breaker closes after successful call\n\n2. Create integration tests that:\n   - Test circuit breaker with mock services that fail\n   - Verify circuit breaker prevents calls to failed services\n   - Test recovery behavior\n\n3. Test different configuration values for different services",
        "priority": "high",
        "dependencies": [
          "173"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create tests/test_circuit_breakers.py with mock service fixtures",
            "description": "Set up a test file with mock service fixtures to test the circuit breaker functionality",
            "dependencies": [],
            "details": "Create a new test file at tests/test_circuit_breakers.py that includes pytest fixtures for mocking external services. Include setup for simulating failures and successes, and configure the test environment with appropriate timeouts for testing recovery periods.",
            "status": "pending",
            "testStrategy": "Use pytest fixtures to create isolated test environments. Mock external service calls to simulate failures and successes.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Test LlamaIndex circuit opens after 5 failures",
            "description": "Implement a test case to verify that the circuit breaker for LlamaIndex opens after 5 consecutive failures",
            "dependencies": [
              1
            ],
            "details": "Create a test function that simulates 5 consecutive failures when calling LlamaIndex service. Verify that after the 5th failure, the circuit breaker transitions to the open state and logs the appropriate warning message.",
            "status": "pending",
            "testStrategy": "Use mocks to simulate service failures. Assert that the circuit breaker state changes correctly and verify log messages.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Test circuit in open state fails immediately",
            "description": "Verify that when a circuit is in the open state, subsequent requests fail immediately without attempting to call the service",
            "dependencies": [
              1,
              2
            ],
            "details": "Create a test function that first puts a circuit breaker into the open state, then attempts to make additional service calls. Verify that these calls immediately raise a ServiceUnavailableError without attempting to call the underlying service.",
            "status": "pending",
            "testStrategy": "Use spy objects to verify that the underlying service is not called when the circuit is open.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Test circuit transitions to half-open after recovery timeout",
            "description": "Implement a test to verify that a circuit breaker transitions from open to half-open state after the recovery timeout period",
            "dependencies": [
              1,
              3
            ],
            "details": "Create a test function that puts a circuit breaker into the open state, then advances the mock time beyond the recovery timeout period. Verify that the next call attempt is allowed through (half-open state) rather than failing immediately.",
            "status": "pending",
            "testStrategy": "Use time mocking to simulate the passage of time without waiting for actual timeouts.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Test successful request in half-open closes circuit",
            "description": "Verify that a successful service call when the circuit is in half-open state causes it to transition back to closed",
            "dependencies": [
              1,
              4
            ],
            "details": "Create a test function that puts a circuit breaker into the half-open state, then simulates a successful service call. Verify that the circuit breaker transitions back to the closed state and resets its failure counter.",
            "status": "pending",
            "testStrategy": "Mock a successful service response and verify the circuit breaker state changes correctly.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Apply circuit breaker decorator to app/services/llama_index_service.py",
            "description": "Apply the circuit breaker decorator to all external service methods in the LlamaIndex service",
            "dependencies": [
              1,
              5
            ],
            "details": "Modify app/services/llama_index_service.py to apply the @circuit_breaker('llama_index') decorator to all methods that make external API calls. Ensure the configuration uses 5 failures threshold and 30s recovery timeout as specified in the CIRCUIT_BREAKER_CONFIG.",
            "status": "pending",
            "testStrategy": "Create integration tests that verify the circuit breaker behavior with the LlamaIndex service.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Apply circuit breaker decorator to app/services/crewai_service.py",
            "description": "Apply the circuit breaker decorator to all external service methods in the CrewAI service",
            "dependencies": [
              1,
              5
            ],
            "details": "Modify app/services/crewai_service.py to apply the @circuit_breaker('crewai') decorator to all methods that make external API calls. Ensure the configuration uses 3 failures threshold and 60s recovery timeout as specified in the CIRCUIT_BREAKER_CONFIG.",
            "status": "pending",
            "testStrategy": "Create integration tests that verify the circuit breaker behavior with the CrewAI service.",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Apply circuit breaker decorator to app/services/embedding_service.py",
            "description": "Apply the circuit breaker decorator to all external service methods in the Embedding service",
            "dependencies": [
              1,
              5
            ],
            "details": "Modify app/services/embedding_service.py to apply the @circuit_breaker('ollama') decorator to all methods that make external API calls. Ensure the configuration uses 5 failures threshold and 15s recovery timeout as specified in the CIRCUIT_BREAKER_CONFIG.",
            "status": "pending",
            "testStrategy": "Create integration tests that verify the circuit breaker behavior with the Embedding service.",
            "parentId": "undefined"
          },
          {
            "id": 9,
            "title": "Apply circuit breaker decorator to app/services/neo4j_service.py",
            "description": "Apply the circuit breaker decorator to all external service methods in the Neo4j service",
            "dependencies": [
              1,
              5
            ],
            "details": "Modify app/services/neo4j_service.py to apply the @circuit_breaker('neo4j') decorator to all methods that make external API calls. Ensure the configuration uses 3 failures threshold and 30s recovery timeout as specified in the CIRCUIT_BREAKER_CONFIG.",
            "status": "pending",
            "testStrategy": "Create integration tests that verify the circuit breaker behavior with the Neo4j service.",
            "parentId": "undefined"
          },
          {
            "id": 10,
            "title": "Apply circuit breaker decorator to app/services/b2_storage.py",
            "description": "Apply the circuit breaker decorator to all external service methods in the B2 Storage service",
            "dependencies": [
              1,
              5
            ],
            "details": "Modify app/services/b2_storage.py to apply the @circuit_breaker('b2') decorator to all methods that make external API calls. Ensure the configuration uses 5 failures threshold and 60s recovery timeout as specified in the CIRCUIT_BREAKER_CONFIG.",
            "status": "pending",
            "testStrategy": "Create integration tests that verify the circuit breaker behavior with the B2 Storage service.",
            "parentId": "undefined"
          },
          {
            "id": 11,
            "title": "Implement /api/system/circuit-breakers endpoint",
            "description": "Create an API endpoint that exposes the current state of all circuit breakers in the system",
            "dependencies": [
              6,
              7,
              8,
              9,
              10
            ],
            "details": "Create a new endpoint at /api/system/circuit-breakers that returns the current state of all circuit breakers, including their service name, current state (open/closed), failure count, and last failure time. This will be used for monitoring and debugging purposes.",
            "status": "pending",
            "testStrategy": "Create API tests that verify the endpoint returns the correct information for all circuit breakers in various states.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-16T19:29:42.662Z"
      },
      {
        "id": "175",
        "title": "Implement Standardized Error Responses",
        "description": "Ensure all endpoints use a standardized error format from app/models/errors.py to provide consistent error responses across the API.",
        "details": "1. Enhance `app/models/errors.py` to define a standard error response model:\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, Any, Optional\nfrom datetime import datetime\nimport uuid\n\nclass ErrorResponse(BaseModel):\n    code: str\n    message: str\n    details: Optional[Dict[str, Any]] = Field(default=None)\n    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    timestamp: datetime = Field(default_factory=datetime.utcnow)\n    \n    class Config:\n        schema_extra = {\n            \"example\": {\n                \"error\": {\n                    \"code\": \"VALIDATION_ERROR\",\n                    \"message\": \"Invalid input parameters\",\n                    \"details\": {\"field\": \"description of issue\"},\n                    \"request_id\": \"123e4567-e89b-12d3-a456-426614174000\",\n                    \"timestamp\": \"2025-01-15T12:00:00Z\"\n                }\n            }\n        }\n\nclass APIError(Exception):\n    def __init__(self, \n                 code: str, \n                 message: str, \n                 status_code: int = 500, \n                 details: Optional[Dict[str, Any]] = None):\n        self.code = code\n        self.message = message\n        self.status_code = status_code\n        self.details = details\n        super().__init__(self.message)\n        \n    def to_response(self, request_id: Optional[str] = None) -> Dict[str, Any]:\n        return {\n            \"error\": ErrorResponse(\n                code=self.code,\n                message=self.message,\n                details=self.details,\n                request_id=request_id or str(uuid.uuid4())\n            ).dict()\n        }\n\n# Define standard error types\nclass ValidationError(APIError):\n    def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):\n        super().__init__(\"VALIDATION_ERROR\", message, 400, details)\n\nclass AuthenticationError(APIError):\n    def __init__(self, message: str = \"Authentication required\"):\n        super().__init__(\"AUTHENTICATION_ERROR\", message, 401)\n\nclass AuthorizationError(APIError):\n    def __init__(self, message: str = \"Not authorized\"):\n        super().__init__(\"AUTHORIZATION_ERROR\", message, 403)\n\nclass NotFoundError(APIError):\n    def __init__(self, message: str = \"Resource not found\"):\n        super().__init__(\"NOT_FOUND\", message, 404)\n\nclass RateLimitedError(APIError):\n    def __init__(self, message: str = \"Rate limit exceeded\"):\n        super().__init__(\"RATE_LIMITED\", message, 429)\n\nclass ExternalServiceError(APIError):\n    def __init__(self, message: str = \"External service error\"):\n        super().__init__(\"EXTERNAL_SERVICE_ERROR\", message, 502)\n\nclass ServiceUnavailableError(APIError):\n    def __init__(self, message: str = \"Service temporarily unavailable\"):\n        super().__init__(\"SERVICE_UNAVAILABLE\", message, 503)\n\nclass InternalError(APIError):\n    def __init__(self, message: str = \"Internal server error\"):\n        super().__init__(\"INTERNAL_ERROR\", message, 500)\n```\n\n2. Modify `app/middleware/error_handler.py` to use these standardized errors:\n```python\nfrom fastapi import Request\nfrom fastapi.responses import JSONResponse\nfrom app.models.errors import APIError, InternalError\nimport structlog\n\nlogger = structlog.get_logger(__name__)\n\nasync def error_handler(request: Request, exc: Exception):\n    request_id = request.headers.get(\"X-Request-ID\")\n    \n    if isinstance(exc, APIError):\n        return JSONResponse(\n            status_code=exc.status_code,\n            content=exc.to_response(request_id)\n        )\n    \n    # Handle unexpected errors\n    logger.exception(\n        \"Unhandled exception in request\",\n        error_type=type(exc).__name__,\n        error_message=str(exc),\n        endpoint=request.url.path,\n        method=request.method,\n        request_id=request_id,\n        user_id=request.state.user_id if hasattr(request.state, \"user_id\") else None\n    )\n    \n    internal_error = InternalError()\n    return JSONResponse(\n        status_code=500,\n        content=internal_error.to_response(request_id)\n    )\n```\n\n3. Update all route handlers to use these error classes instead of raising raw exceptions or returning inconsistent error formats",
        "testStrategy": "1. Create unit tests that verify:\n   - Each error type produces the correct status code\n   - Error responses have the correct structure\n   - Request IDs are properly propagated\n\n2. Create integration tests that:\n   - Trigger various error conditions\n   - Verify the response format matches the standard\n   - Check that all required fields are present\n\n3. Test error handling for different HTTP methods and endpoints",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create test fixtures for error response testing",
            "description": "Create tests/test_error_responses.py with endpoint fixtures to test various error scenarios",
            "dependencies": [],
            "details": "Create a new test file with fixtures that can trigger different error conditions. Include setup for validation errors, authentication errors, not found errors, and internal server errors. Set up test client and necessary mocks for external services.",
            "status": "pending",
            "testStrategy": "Verify that test fixtures correctly trigger the intended error conditions and return appropriate status codes.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement validation error response tests",
            "description": "Create tests to verify validation errors return VALIDATION_ERROR code with 400 status",
            "dependencies": [
              1
            ],
            "details": "Add test cases that trigger validation errors by sending invalid data to endpoints. Verify the response contains the correct error code 'VALIDATION_ERROR', status code 400, and includes details about which fields failed validation.",
            "status": "pending",
            "testStrategy": "Test with multiple invalid input scenarios to ensure consistent error format and appropriate field-level validation details.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement external service error response tests",
            "description": "Create tests to verify external service errors return EXTERNAL_SERVICE_ERROR code with 502 status",
            "dependencies": [
              1
            ],
            "details": "Add test cases that simulate failures in external service calls. Mock external service dependencies to throw exceptions and verify the API returns the correct error code 'EXTERNAL_SERVICE_ERROR' with status code 502.",
            "status": "pending",
            "testStrategy": "Test with different external service failure scenarios including timeouts, connection errors, and invalid responses.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement request metadata tests for error responses",
            "description": "Create tests to verify all errors include request_id and timestamp fields",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Add assertions to all error response tests to verify that request_id and timestamp fields are always present and properly formatted. Test with both system-generated request IDs and those provided via X-Request-ID header.",
            "status": "pending",
            "testStrategy": "Verify UUID format for request_id and ISO-8601 format for timestamp across all error types.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement error logging correlation tests",
            "description": "Create tests to verify error logs include request_id for correlation with responses",
            "dependencies": [
              4
            ],
            "details": "Create test cases that capture log output during error conditions and verify that the request_id in the logs matches the request_id in the error response. Test with both system-generated and client-provided request IDs.",
            "status": "pending",
            "testStrategy": "Use log capture fixtures to verify log entries contain the same request_id as the response.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Extend error models with StandardError and ErrorCode enum",
            "description": "Enhance app/models/errors.py with StandardError model and ErrorCode enum for consistent error typing",
            "dependencies": [],
            "details": "Refactor the error models to use an ErrorCode enum for all error types. Create a StandardError model that extends the existing ErrorResponse with additional fields as needed. Update all error classes to use the enum values for consistent error codes.",
            "status": "pending",
            "testStrategy": "Unit test the error models to ensure they generate the expected JSON structure and validate required fields.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Update error handler middleware to use StandardError",
            "description": "Modify app/middleware/error_handler.py to use StandardError for all exception types",
            "dependencies": [
              6
            ],
            "details": "Update the error handler middleware to convert all exceptions to StandardError instances. Implement specific mapping logic for common exceptions like ValidationError, NotFoundError, etc. Ensure all unhandled exceptions are converted to InternalError with appropriate logging.",
            "status": "pending",
            "testStrategy": "Test the middleware with various exception types to verify correct conversion to StandardError format.",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Implement request_id extraction and propagation",
            "description": "Add middleware to extract X-Request-ID header or generate a new request_id for each request",
            "dependencies": [
              7
            ],
            "details": "Create or update middleware to extract the X-Request-ID header from incoming requests. If not present, generate a new UUID. Store the request_id in request.state for use throughout the request lifecycle. Ensure it's propagated to error responses and logs.",
            "status": "pending",
            "testStrategy": "Test with and without X-Request-ID header to verify extraction and generation behavior.",
            "parentId": "undefined"
          },
          {
            "id": 9,
            "title": "Update structlog configuration for request_id inclusion",
            "description": "Modify logging configuration to include request_id in all error logs for correlation",
            "dependencies": [
              8
            ],
            "details": "Update the structlog configuration to include request_id in the log context. Create a middleware or processor that adds the request_id from request.state to the log context. Ensure all log entries include the request_id field for correlation with API responses.",
            "status": "pending",
            "testStrategy": "Verify log entries contain request_id field and that it matches the ID used in the corresponding request/response.",
            "parentId": "undefined"
          },
          {
            "id": 10,
            "title": "Implement stack trace logging for 500 errors",
            "description": "Add detailed stack trace logging only for 500-level internal server errors",
            "dependencies": [
              7,
              9
            ],
            "details": "Enhance the error handler to capture and log full stack traces only for 500-level errors (InternalError). For other error types, log only essential information. Configure structlog to format stack traces appropriately and ensure they're included in logs for debugging.",
            "status": "pending",
            "testStrategy": "Trigger various error types and verify stack traces are only included for 500 errors, while other errors log only essential information.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-16T19:16:35.628Z"
      },
      {
        "id": "176",
        "title": "Enhance Exception Logging",
        "description": "Improve exception logging to include error type, message, request context, stack trace, and request ID for better debugging and correlation.",
        "details": "1. Enhance the exception logging in `app/middleware/error_handler.py` to include comprehensive information:\n```python\nimport traceback\nfrom fastapi import Request\nfrom fastapi.responses import JSONResponse\nfrom app.models.errors import APIError, InternalError\nimport structlog\n\nlogger = structlog.get_logger(__name__)\n\nasync def error_handler(request: Request, exc: Exception):\n    request_id = request.headers.get(\"X-Request-ID\")\n    user_id = getattr(request.state, \"user_id\", None) if hasattr(request.state, \"user_id\") else None\n    \n    # Collect context information\n    context = {\n        \"error_type\": type(exc).__name__,\n        \"error_message\": str(exc),\n        \"endpoint\": request.url.path,\n        \"method\": request.method,\n        \"request_id\": request_id,\n        \"user_id\": user_id,\n        \"client_ip\": request.client.host if request.client else None,\n        \"user_agent\": request.headers.get(\"User-Agent\")\n    }\n    \n    if isinstance(exc, APIError):\n        # For known API errors, log at appropriate level\n        if exc.status_code >= 500:\n            logger.error(f\"Server error: {exc.message}\", **context)\n        else:\n            logger.info(f\"Client error: {exc.message}\", **context)\n            \n        return JSONResponse(\n            status_code=exc.status_code,\n            content=exc.to_response(request_id)\n        )\n    \n    # For unexpected errors, include stack trace\n    stack_trace = traceback.format_exc()\n    context[\"stack_trace\"] = stack_trace\n    \n    logger.exception(\n        \"Unhandled exception in request\",\n        **context\n    )\n    \n    # Return standardized 500 error\n    internal_error = InternalError()\n    return JSONResponse(\n        status_code=500,\n        content=internal_error.to_response(request_id)\n    )\n```\n\n2. Add a middleware to ensure request IDs are always available:\n```python\n# app/middleware/request_id.py\nimport uuid\nfrom fastapi import Request\nfrom starlette.middleware.base import BaseHTTPMiddleware\n\nclass RequestIDMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):\n        request_id = request.headers.get(\"X-Request-ID\")\n        if not request_id:\n            request_id = str(uuid.uuid4())\n            request.headers.__dict__[\"_list\"].append(\n                (b\"x-request-id\", request_id.encode())\n            )\n        \n        response = await call_next(request)\n        response.headers[\"X-Request-ID\"] = request_id\n        return response\n```\n\n3. Register the middleware in `app/main.py`:\n```python\nfrom app.middleware.request_id import RequestIDMiddleware\n\napp = FastAPI()\napp.add_middleware(RequestIDMiddleware)\n```",
        "testStrategy": "1. Create unit tests that verify:\n   - All context fields are properly captured\n   - Different error types are logged at appropriate levels\n   - Stack traces are included for unexpected errors\n\n2. Create integration tests that:\n   - Trigger various error conditions\n   - Verify logs contain all required information\n   - Check that request IDs are properly propagated\n\n3. Test with different types of requests and error scenarios",
        "priority": "medium",
        "dependencies": [
          "175"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-16T19:57:06.845Z"
      },
      {
        "id": "177",
        "title": "Implement Query Parameter Validation",
        "description": "Add Pydantic models for all query parameters to ensure proper validation and sanitization of user input.",
        "details": "1. Create Pydantic models for query parameters in appropriate modules:\n\n```python\n# app/models/query.py\nfrom pydantic import BaseModel, Field, validator\nfrom typing import Dict, Any, Optional, List\n\nclass QueryRequest(BaseModel):\n    query: str = Field(..., min_length=1, max_length=10000)\n    top_k: int = Field(default=10, ge=1, le=100)\n    filters: Optional[Dict[str, Any]] = Field(default=None)\n\n    @validator(\"query\")\n    def sanitize_query(cls, v):\n        # Remove potential injection patterns\n        dangerous_patterns = [\"--\", \";\", \"DROP\", \"DELETE\", \"INSERT\"]\n        for pattern in dangerous_patterns:\n            if pattern.upper() in v.upper():\n                raise ValueError(f\"Query contains disallowed pattern: {pattern}\")\n        return v.strip()\n\n# app/models/document.py\nclass DocumentListParams(BaseModel):\n    page: int = Field(default=1, ge=1)\n    page_size: int = Field(default=20, ge=1, le=100)\n    sort_by: str = Field(default=\"created_at\")\n    sort_order: str = Field(default=\"desc\")\n    filter_tag: Optional[str] = None\n\n    @validator(\"sort_by\")\n    def validate_sort_field(cls, v):\n        allowed_fields = [\"created_at\", \"updated_at\", \"title\", \"size\"]\n        if v not in allowed_fields:\n            raise ValueError(f\"Invalid sort field. Must be one of: {', '.join(allowed_fields)}\")\n        return v\n\n    @validator(\"sort_order\")\n    def validate_sort_order(cls, v):\n        if v.lower() not in [\"asc\", \"desc\"]:\n            raise ValueError(\"Sort order must be 'asc' or 'desc'\")\n        return v.lower()\n\n# app/models/graph.py\nclass GraphQueryParams(BaseModel):\n    entity_id: str = Field(..., min_length=1, max_length=100)\n    traversal_depth: int = Field(default=2, ge=1, le=5)\n    relationship_types: Optional[List[str]] = None\n    limit: int = Field(default=50, ge=1, le=200)\n\n# app/models/search.py\nclass SearchParams(BaseModel):\n    query: str = Field(..., min_length=1, max_length=1000)\n    page: int = Field(default=1, ge=1)\n    page_size: int = Field(default=20, ge=1, le=100)\n    content_type: Optional[List[str]] = None\n    date_from: Optional[str] = None\n    date_to: Optional[str] = None\n\n    @validator(\"query\")\n    def sanitize_search_query(cls, v):\n        return v.strip()\n\n    @validator(\"content_type\")\n    def validate_content_type(cls, v):\n        if v is not None:\n            allowed_types = [\"document\", \"image\", \"audio\", \"video\"]\n            for t in v:\n                if t not in allowed_types:\n                    raise ValueError(f\"Invalid content type: {t}\")\n        return v\n```\n\n2. Update route handlers to use these models for parameter validation:\n\n```python\n# Example for query endpoint\n@router.post(\"/api/query\")\nasync def query_documents(query_params: QueryRequest):\n    # Access validated parameters\n    query = query_params.query\n    top_k = query_params.top_k\n    filters = query_params.filters or {}\n    \n    # Process query...\n    \n# Example for document listing\n@router.get(\"/api/documents\")\nasync def list_documents(params: DocumentListParams = Depends()):\n    # Access validated parameters\n    page = params.page\n    page_size = params.page_size\n    sort_by = params.sort_by\n    sort_order = params.sort_order\n    \n    # Process request...\n```",
        "testStrategy": "1. Create unit tests that verify:\n   - Valid parameters are accepted\n   - Invalid parameters are rejected with appropriate error messages\n   - Validators correctly sanitize and transform input\n\n2. Create integration tests that:\n   - Test endpoints with various parameter combinations\n   - Verify error responses for invalid parameters\n   - Check that sanitization is working correctly\n\n3. Test edge cases like minimum/maximum values and special characters",
        "priority": "medium",
        "dependencies": [
          "175"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-16T20:04:51.719Z"
      },
      {
        "id": "178",
        "title": "Create Service Timeout Configuration Module",
        "description": "Create a central configuration module for service timeouts to ensure consistent timeout settings across the application.",
        "details": "1. Create a new file `app/core/service_timeouts.py` to define timeout constants and configuration:\n\n```python\nfrom typing import Dict, Any\nimport os\n\n# Default timeout values in seconds\nDEFAULT_TIMEOUTS = {\n    \"llama_index\": 60.0,  # 60 seconds (document parsing can be slow)\n    \"crewai\": 120.0,     # 120 seconds (multi-agent workflows)\n    \"ollama\": 30.0,      # 30 seconds (embeddings)\n    \"neo4j\": 15.0,       # 15 seconds\n    \"supabase\": 10.0,    # 10 seconds\n    \"arcade\": 30.0,      # 30 seconds\n    \"b2\": 45.0,          # 45 seconds\n    \"default\": 30.0      # Default timeout\n}\n\n# Environment variable prefix for overriding timeouts\nENV_PREFIX = \"TIMEOUT_\"\n\ndef get_timeout(service_name: str) -> float:\n    \"\"\"Get timeout for a service, allowing environment variable override.\"\"\"\n    # Check for environment variable override (e.g., TIMEOUT_LLAMA_INDEX=90)\n    env_var = f\"{ENV_PREFIX}{service_name.upper()}\"\n    env_value = os.getenv(env_var)\n    \n    if env_value:\n        try:\n            return float(env_value)\n        except ValueError:\n            # Log warning about invalid timeout value\n            pass\n    \n    # Fall back to default timeout\n    return DEFAULT_TIMEOUTS.get(service_name, DEFAULT_TIMEOUTS[\"default\"])\n\ndef get_connect_timeout(service_name: str) -> float:\n    \"\"\"Get connection timeout, which is typically shorter than request timeout.\"\"\"\n    # Connection timeouts are usually shorter\n    return min(5.0, get_timeout(service_name) / 4)\n\ndef get_all_timeouts() -> Dict[str, Any]:\n    \"\"\"Get all timeout configurations for monitoring/logging.\"\"\"\n    result = {}\n    \n    # Include defaults\n    for service, timeout in DEFAULT_TIMEOUTS.items():\n        result[service] = {\n            \"default\": timeout,\n            \"current\": get_timeout(service),\n            \"connect\": get_connect_timeout(service)\n        }\n    \n    return result\n```\n\n2. Create a utility function to configure httpx clients with proper timeouts:\n\n```python\n# app/utils/http_client.py\nimport httpx\nfrom app.core.service_timeouts import get_timeout, get_connect_timeout\n\ndef create_http_client(service_name: str, base_url: str = None) -> httpx.AsyncClient:\n    \"\"\"Create an httpx client with appropriate timeouts for the service.\"\"\"\n    timeout = get_timeout(service_name)\n    connect_timeout = get_connect_timeout(service_name)\n    \n    return httpx.AsyncClient(\n        base_url=base_url,\n        timeout=httpx.Timeout(timeout, connect=connect_timeout),\n        limits=httpx.Limits(max_connections=100, max_keepalive_connections=20)\n    )\n```\n\n3. Add a startup event handler to log all timeout configurations:\n\n```python\n# In app/main.py\nfrom app.core.service_timeouts import get_all_timeouts\nimport structlog\n\nlogger = structlog.get_logger(__name__)\n\n@app.on_event(\"startup\")\nasync def log_timeout_configuration():\n    timeouts = get_all_timeouts()\n    logger.info(\"Service timeout configuration\", timeouts=timeouts)\n```",
        "testStrategy": "1. Create unit tests in `tests/test_service_timeouts.py` that verify:\n   - Default timeouts are correctly defined\n   - Environment variable overrides work correctly\n   - Invalid environment values are handled gracefully\n   - Connection timeouts are calculated correctly\n\n2. Create integration tests that:\n   - Verify HTTP clients are created with correct timeout values\n   - Test that timeouts actually trigger after the specified duration\n   - Check that timeout configurations are properly logged at startup\n\n3. Test with various environment variable configurations",
        "priority": "medium",
        "dependencies": [
          "173"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-16T20:05:40.362Z"
      },
      {
        "id": "179",
        "title": "Create Comprehensive Test Suite for Production Readiness",
        "description": "Develop a comprehensive test suite to verify all production readiness improvements, including startup validation, rate limiting, circuit breakers, and error handling.",
        "details": "1. Create a test directory structure:\n```\ntests/\n  test_startup_validation.py\n  test_cors_security.py\n  test_rate_limiting.py\n  test_circuit_breaker.py\n  test_error_handling.py\n  test_timeouts.py\n  test_input_validation.py\n  integration/\n    test_production_readiness.py\n```\n\n2. Implement unit tests for each component:\n\n```python\n# tests/test_startup_validation.py\nimport os\nimport pytest\nfrom unittest.mock import patch\nfrom app.core.startup_validation import validate_environment, REQUIRED_ENV_VARS\n\ndef test_validate_environment_success():\n    # Mock all required env vars\n    with patch.dict(os.environ, {\n        var: \"test_value\" for var in REQUIRED_ENV_VARS[\"critical\"]\n    }):\n        result = validate_environment()\n        assert result[\"critical\"] == []\n\ndef test_validate_environment_missing_critical():\n    # Mock with missing critical vars\n    with patch.dict(os.environ, {}, clear=True):\n        with pytest.raises(RuntimeError) as excinfo:\n            validate_environment()\n        # Check error message contains missing vars\n        for var in REQUIRED_ENV_VARS[\"critical\"]:\n            assert var in str(excinfo.value)\n\n# tests/test_cors_security.py\n# Similar tests for CORS configuration\n\n# tests/test_rate_limiting.py\n# Tests for rate limiting tiers and behavior\n\n# tests/test_circuit_breaker.py\n# Tests for circuit breaker behavior\n```\n\n3. Implement integration tests that verify the entire system:\n\n```python\n# tests/integration/test_production_readiness.py\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom app.main import app\n\n@pytest.fixture\ndef client():\n    return TestClient(app)\n\ndef test_error_response_format(client):\n    # Test invalid input to trigger validation error\n    response = client.post(\"/api/query\", json={\"query\": \"\"})\n    assert response.status_code == 400\n    data = response.json()\n    \n    # Verify error structure\n    assert \"error\" in data\n    assert \"code\" in data[\"error\"]\n    assert \"message\" in data[\"error\"]\n    assert \"request_id\" in data[\"error\"]\n    assert \"timestamp\" in data[\"error\"]\n    \n    # Verify specific error code\n    assert data[\"error\"][\"code\"] == \"VALIDATION_ERROR\"\n\ndef test_rate_limiting(client):\n    # Make multiple requests to trigger rate limit\n    endpoint = \"/api/users/login\"\n    for _ in range(10):  # Limit is 5/minute\n        client.post(endpoint, json={\"username\": \"test\", \"password\": \"test\"})\n    \n    # This should be rate limited\n    response = client.post(endpoint, json={\"username\": \"test\", \"password\": \"test\"})\n    assert response.status_code == 429\n    \n    # Verify rate limit headers\n    assert \"X-RateLimit-Limit\" in response.headers\n    assert \"X-RateLimit-Remaining\" in response.headers\n    assert \"X-RateLimit-Reset\" in response.headers\n    \n    # Verify error response format\n    data = response.json()\n    assert data[\"error\"][\"code\"] == \"RATE_LIMITED\"\n```\n\n4. Create a test for the overall production readiness score:\n\n```python\ndef test_production_readiness_score():\n    # This test verifies all production readiness criteria\n    # Each check contributes to the overall score\n    \n    # Check environment validation\n    from app.core.startup_validation import validate_environment\n    with patch.dict(os.environ, {var: \"test\" for var in REQUIRED_ENV_VARS[\"critical\"]}):\n        result = validate_environment()\n        assert result[\"critical\"] == []\n    \n    # Check CORS configuration\n    # ...\n    \n    # Check rate limiting configuration\n    # ...\n    \n    # Check circuit breaker configuration\n    # ...\n    \n    # Check error handling\n    # ...\n    \n    # Calculate final score based on passing checks\n    # assert score == 100\n```",
        "testStrategy": "1. Run unit tests for each component:\n   - Use pytest for all tests\n   - Use mocking to isolate components\n   - Test both success and failure cases\n\n2. Run integration tests:\n   - Use TestClient to make actual HTTP requests\n   - Test end-to-end behavior\n   - Verify all components work together\n\n3. Run performance tests:\n   - Measure overhead of new features\n   - Ensure rate limiting and circuit breakers work under load\n\n4. Run security tests:\n   - Verify CORS protection\n   - Test input validation against injection attacks",
        "priority": "medium",
        "dependencies": [
          "170",
          "171",
          "172",
          "173",
          "174",
          "175",
          "176",
          "177",
          "178"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-16T20:15:07.155Z"
      },
      {
        "id": "180",
        "title": "Implement B2 Integration for Document Management Service",
        "description": "Complete the Document Management Service integration with B2 Storage by implementing upload, deletion, text re-extraction, and embedding regeneration functionality.",
        "details": "In `app/services/document_management.py`, implement the following TODOs:\n\n1. Implement B2 upload in `upload_document()` method:\n```python\ndef upload_document(self, file_path, project_id, document_type, metadata=None):\n    # Generate a unique B2 path using project_id and filename\n    filename = os.path.basename(file_path)\n    b2_path = f\"documents/{project_id}/{document_type}/{filename}\"\n    \n    # Use existing B2StorageService to upload the file\n    b2_url = self.b2_service.upload_file(file_path, b2_path)\n    \n    # Store metadata in Supabase\n    document_record = {\n        \"project_id\": project_id,\n        \"document_type\": document_type,\n        \"filename\": filename,\n        \"b2_path\": b2_path,\n        \"b2_url\": b2_url,\n        \"metadata\": metadata or {},\n        \"status\": \"uploaded\"\n    }\n    \n    # Insert into documents table\n    document_id = self.db.insert(\"documents\", document_record).get(\"id\")\n    \n    # Return document record with ID\n    return {**document_record, \"id\": document_id}\n```\n\n2. Implement B2 deletion in `delete_document()`:\n```python\ndef delete_document(self, document_id):\n    # Get document record from Supabase\n    document = self.db.get(\"documents\", document_id)\n    if not document:\n        raise DocumentNotFoundError(f\"Document {document_id} not found\")\n    \n    # Delete from B2\n    self.b2_service.delete_file(document[\"b2_path\"])\n    \n    # Delete from Supabase\n    self.db.delete(\"documents\", document_id)\n    \n    # Delete associated chunks and embeddings\n    self.db.delete_where(\"document_chunks\", {\"document_id\": document_id})\n    \n    return {\"success\": True, \"message\": f\"Document {document_id} deleted\"}\n```\n\n3. Implement text re-extraction in `reprocess_document()`:\n```python\ndef reprocess_document(self, document_id, force_reparse=False, update_embeddings=False):\n    # Get document record\n    document = self.db.get(\"documents\", document_id)\n    if not document:\n        raise DocumentNotFoundError(f\"Document {document_id} not found\")\n    \n    # If force_reparse is True, re-extract text using LlamaIndex\n    if force_reparse:\n        # Download file from B2 to temp location\n        temp_path = f\"/tmp/{document['filename']}\"\n        self.b2_service.download_file(document[\"b2_path\"], temp_path)\n        \n        # Use LlamaIndex to extract text\n        extracted_text = self.llamaindex_service.extract_text(temp_path)\n        \n        # Update document record\n        self.db.update(\"documents\", document_id, {\"extracted_text\": extracted_text})\n        \n        # Delete old chunks\n        self.db.delete_where(\"document_chunks\", {\"document_id\": document_id})\n        \n        # Create new chunks\n        chunks = self.text_chunker.chunk(extracted_text)\n        \n        # Store new chunks\n        for chunk in chunks:\n            self.db.insert(\"document_chunks\", {\n                \"document_id\": document_id,\n                \"content\": chunk,\n                \"metadata\": {}\n            })\n    \n    # If update_embeddings is True, regenerate embeddings\n    if update_embeddings:\n        # Get all chunks for this document\n        chunks = self.db.query(\"document_chunks\", {\"document_id\": document_id})\n        \n        # Generate embeddings for each chunk\n        for chunk in chunks:\n            embedding = self.embedding_service.generate_embedding(chunk[\"content\"])\n            \n            # Update chunk with embedding\n            self.db.update(\"document_chunks\", chunk[\"id\"], {\"embedding\": embedding})\n    \n    return {\"success\": True, \"message\": f\"Document {document_id} reprocessed\"}\n```",
        "testStrategy": "1. Unit tests for each method in the DocumentManagementService:\n   - Test upload_document with various file types and verify B2 paths\n   - Test delete_document and verify both B2 and database records are removed\n   - Test reprocess_document with force_reparse=True and verify text extraction\n   - Test reprocess_document with update_embeddings=True and verify embedding updates\n\n2. Integration tests:\n   - Test the full document lifecycle (upload, process, reprocess, delete)\n   - Test with actual B2 service in a staging environment\n   - Verify proper error handling for missing files or failed uploads\n\n3. Mock tests:\n   - Create mock B2StorageService to test failure scenarios\n   - Test timeout handling and retry logic",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement B2 upload in upload_document() method",
            "description": "Complete the implementation of the upload_document() method to handle file uploads to B2 Storage.",
            "dependencies": [],
            "details": "In app/services/document_management.py, implement the upload_document() method to: 1) Generate a unique B2 path using project_id and filename, 2) Use the existing B2StorageService to upload the file, 3) Store metadata in Supabase, and 4) Return the document record with ID.",
            "status": "pending",
            "testStrategy": "Write unit tests to verify: 1) Correct B2 path generation, 2) Successful file upload to B2, 3) Proper metadata storage in Supabase, 4) Correct document record creation and return.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement B2 deletion in delete_document() method",
            "description": "Complete the implementation of the delete_document() method to handle file deletion from B2 Storage.",
            "dependencies": [
              1
            ],
            "details": "In app/services/document_management.py, implement the delete_document() method to: 1) Retrieve the document record from Supabase, 2) Delete the file from B2 using B2StorageService, 3) Delete the document record from Supabase, 4) Delete associated chunks and embeddings, and 5) Return a success message.",
            "status": "pending",
            "testStrategy": "Write unit tests to verify: 1) Proper error handling for non-existent documents, 2) Successful file deletion from B2, 3) Complete removal of document record from Supabase, 4) Removal of associated chunks and embeddings.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement text re-extraction in reprocess_document() method",
            "description": "Implement the text re-extraction functionality in the reprocess_document() method when force_reparse is True.",
            "dependencies": [
              1,
              2
            ],
            "details": "In app/services/document_management.py, implement the text re-extraction part of reprocess_document() to: 1) Download the file from B2 to a temporary location, 2) Use LlamaIndex to extract text, 3) Update the document record with the extracted text, 4) Delete old chunks, 5) Create new chunks using the text chunker, and 6) Store the new chunks in the database.",
            "status": "pending",
            "testStrategy": "Write unit tests to verify: 1) Successful file download from B2, 2) Proper text extraction using LlamaIndex, 3) Correct document record update, 4) Removal of old chunks, 5) Creation and storage of new chunks.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement embedding regeneration in reprocess_document() method",
            "description": "Implement the embedding regeneration functionality in the reprocess_document() method when update_embeddings is True.",
            "dependencies": [
              3
            ],
            "details": "In app/services/document_management.py, implement the embedding regeneration part of reprocess_document() to: 1) Retrieve all chunks for the document, 2) Generate embeddings for each chunk using the embedding service, and 3) Update each chunk with its corresponding embedding in the database.",
            "status": "pending",
            "testStrategy": "Write unit tests to verify: 1) Proper retrieval of document chunks, 2) Successful embedding generation for each chunk, 3) Correct update of chunks with embeddings in the database.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Integrate and test the complete B2 document management workflow",
            "description": "Integrate all implemented methods and test the complete document management workflow with B2 integration.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Perform integration testing of the complete document management workflow: 1) Upload a document to B2, 2) Verify the document is properly stored and indexed, 3) Reprocess the document with text re-extraction, 4) Regenerate embeddings, 5) Delete the document and verify all associated data is removed. Fix any issues that arise during integration testing.",
            "status": "pending",
            "testStrategy": "Create integration tests that verify the end-to-end workflow: 1) Test the complete lifecycle of document upload, processing, and deletion, 2) Test error handling and edge cases, 3) Verify database consistency after each operation, 4) Test with different document types and sizes.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-17T20:35:49.608Z"
      },
      {
        "id": "181",
        "title": "Implement Research Tasks Report Generation",
        "description": "Complete the report generation functionality in the research tasks module by implementing the ReportExecutor integration and B2 storage for reports.",
        "details": "In `app/tasks/research_tasks.py`, implement the following TODOs:\n\n1. Implement report generation using ReportExecutor:\n```python\ndef generate_research_report(project_id, task_ids=None):\n    \"\"\"Generate a comprehensive research report for a project.\"\"\"\n    # Get project details\n    project = db.get(\"research_projects\", project_id)\n    if not project:\n        raise ValueError(f\"Project {project_id} not found\")\n    \n    # Get tasks for this project\n    if task_ids:\n        tasks = [db.get(\"research_tasks\", task_id) for task_id in task_ids]\n        tasks = [t for t in tasks if t]  # Filter out None values\n    else:\n        tasks = db.query(\"research_tasks\", {\"project_id\": project_id})\n    \n    # Get task results\n    task_results = []\n    for task in tasks:\n        results = db.query(\"task_results\", {\"task_id\": task[\"id\"]})\n        task_results.append({\n            \"task\": task,\n            \"results\": results\n        })\n    \n    # Initialize ReportExecutor\n    executor = ReportExecutor(\n        model=\"claude-3-opus-20240229\",\n        temperature=0.2,\n        max_tokens=12000\n    )\n    \n    # Generate report content\n    report_content = executor.generate_report(\n        project_name=project[\"name\"],\n        project_description=project[\"description\"],\n        task_results=task_results,\n        format=\"markdown\"\n    )\n    \n    # Generate a PDF version\n    pdf_content = executor.convert_to_pdf(report_content)\n    \n    # Save report to B2\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    report_filename = f\"report_{project_id}_{timestamp}.md\"\n    pdf_filename = f\"report_{project_id}_{timestamp}.pdf\"\n    \n    b2_path_md = f\"reports/{project_id}/{report_filename}\"\n    b2_path_pdf = f\"reports/{project_id}/{pdf_filename}\"\n    \n    # Save markdown to temp file\n    md_temp_path = f\"/tmp/{report_filename}\"\n    with open(md_temp_path, \"w\") as f:\n        f.write(report_content)\n    \n    # Save PDF to temp file\n    pdf_temp_path = f\"/tmp/{pdf_filename}\"\n    with open(pdf_temp_path, \"wb\") as f:\n        f.write(pdf_content)\n    \n    # Upload to B2\n    b2_service = B2StorageService()\n    md_url = b2_service.upload_file(md_temp_path, b2_path_md)\n    pdf_url = b2_service.upload_file(pdf_temp_path, b2_path_pdf)\n    \n    # Store report metadata in database\n    report_record = {\n        \"project_id\": project_id,\n        \"title\": f\"Research Report: {project['name']}\",\n        \"description\": f\"Generated report for project {project['name']}\",\n        \"md_path\": b2_path_md,\n        \"md_url\": md_url,\n        \"pdf_path\": b2_path_pdf,\n        \"pdf_url\": pdf_url,\n        \"task_ids\": task_ids or [t[\"id\"] for t in tasks],\n        \"created_at\": datetime.now().isoformat()\n    }\n    \n    report_id = db.insert(\"research_reports\", report_record).get(\"id\")\n    \n    # Clean up temp files\n    os.remove(md_temp_path)\n    os.remove(pdf_temp_path)\n    \n    return {\"report_id\": report_id, \"md_url\": md_url, \"pdf_url\": pdf_url}\n```",
        "testStrategy": "1. Unit tests:\n   - Test report generation with mock task data\n   - Test PDF conversion functionality\n   - Test B2 upload with mock B2StorageService\n   - Test database record creation\n\n2. Integration tests:\n   - Test end-to-end report generation with sample project data\n   - Verify B2 storage structure and accessibility of reports\n   - Test with various project sizes (small, medium, large number of tasks)\n\n3. Error handling tests:\n   - Test behavior when ReportExecutor fails\n   - Test behavior when B2 upload fails\n   - Test with missing project or task data",
        "priority": "high",
        "dependencies": [
          "180"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:53:55.949Z"
      },
      {
        "id": "182",
        "title": "Implement Retrieval Executor Service Integrations",
        "description": "Complete the Retrieval Executor service by integrating NLQ service for natural language queries, Neo4j service for graph retrieval, and implementing external API retrieval.",
        "details": "In `app/services/task_executors/retrieval_executor.py`, implement the following TODOs:\n\n1. Integrate NLQ service for natural language queries:\n```python\ndef execute_nlq_query(self, query, context=None):\n    \"\"\"Execute a natural language query against the database.\"\"\"\n    # Initialize NLQ service\n    nlq_service = NLQService()\n    \n    # Convert natural language to SQL\n    sql_query = nlq_service.translate_to_sql(\n        query=query,\n        context=context or {},\n        schema=self.db_schema\n    )\n    \n    # Log the generated SQL for debugging\n    logger.debug(f\"Generated SQL: {sql_query}\")\n    \n    # Execute the SQL query\n    try:\n        results = self.db.execute_raw(sql_query)\n        return {\n            \"success\": True,\n            \"results\": results,\n            \"query\": query,\n            \"sql\": sql_query\n        }\n    except Exception as e:\n        logger.error(f\"Error executing NLQ query: {str(e)}\")\n        return {\n            \"success\": False,\n            \"error\": str(e),\n            \"query\": query,\n            \"sql\": sql_query\n        }\n```\n\n2. Integrate Neo4j service for graph retrieval:\n```python\ndef execute_graph_query(self, query, params=None):\n    \"\"\"Execute a query against the Neo4j knowledge graph.\"\"\"\n    # Initialize Neo4j client\n    neo4j_client = Neo4jClient(\n        uri=settings.NEO4J_URI,\n        user=settings.NEO4J_USER,\n        password=settings.NEO4J_PASSWORD\n    )\n    \n    # Execute Cypher query\n    try:\n        results = neo4j_client.execute_query(\n            query=query,\n            params=params or {}\n        )\n        \n        # Process results into a more usable format\n        processed_results = []\n        for record in results:\n            processed_record = {}\n            for key, value in record.items():\n                # Handle Neo4j node objects\n                if hasattr(value, \"labels\") and hasattr(value, \"items\"):\n                    # It's a node\n                    processed_record[key] = {\n                        \"labels\": list(value.labels),\n                        \"properties\": dict(value.items())\n                    }\n                # Handle Neo4j relationship objects\n                elif hasattr(value, \"type\") and hasattr(value, \"start_node\"):\n                    # It's a relationship\n                    processed_record[key] = {\n                        \"type\": value.type,\n                        \"properties\": dict(value.items())\n                    }\n                else:\n                    # Regular value\n                    processed_record[key] = value\n            processed_results.append(processed_record)\n        \n        return {\n            \"success\": True,\n            \"results\": processed_results,\n            \"query\": query\n        }\n    except Exception as e:\n        logger.error(f\"Error executing graph query: {str(e)}\")\n        return {\n            \"success\": False,\n            \"error\": str(e),\n            \"query\": query\n        }\n```\n\n3. Implement external API retrieval:\n```python\ndef execute_api_retrieval(self, endpoint, params=None, headers=None, method=\"GET\", body=None):\n    \"\"\"Execute a request to an external API endpoint.\"\"\"\n    # Validate the endpoint against allowed list\n    if not self._is_allowed_endpoint(endpoint):\n        return {\n            \"success\": False,\n            \"error\": f\"Endpoint {endpoint} is not in the allowed list\"\n        }\n    \n    # Prepare request\n    request_params = params or {}\n    request_headers = headers or {}\n    \n    # Add authentication if configured for this endpoint\n    auth_config = self._get_endpoint_auth(endpoint)\n    if auth_config:\n        if auth_config[\"type\"] == \"bearer\":\n            request_headers[\"Authorization\"] = f\"Bearer {auth_config['token']}\"\n        elif auth_config[\"type\"] == \"api_key\":\n            if auth_config[\"in\"] == \"header\":\n                request_headers[auth_config[\"name\"]] = auth_config[\"value\"]\n            elif auth_config[\"in\"] == \"query\":\n                request_params[auth_config[\"name\"]] = auth_config[\"value\"]\n    \n    # Execute request\n    try:\n        response = requests.request(\n            method=method.upper(),\n            url=endpoint,\n            params=request_params,\n            headers=request_headers,\n            json=body if method.upper() in [\"POST\", \"PUT\", \"PATCH\"] else None,\n            timeout=30  # 30 second timeout\n        )\n        \n        # Try to parse as JSON\n        try:\n            response_data = response.json()\n        except ValueError:\n            response_data = response.text\n        \n        return {\n            \"success\": response.status_code < 400,\n            \"status_code\": response.status_code,\n            \"data\": response_data,\n            \"headers\": dict(response.headers)\n        }\n    except Exception as e:\n        logger.error(f\"Error executing API retrieval: {str(e)}\")\n        return {\n            \"success\": False,\n            \"error\": str(e),\n            \"endpoint\": endpoint\n        }\n\ndef _is_allowed_endpoint(self, endpoint):\n    \"\"\"Check if the endpoint is in the allowed list.\"\"\"\n    allowed_endpoints = settings.ALLOWED_API_ENDPOINTS\n    \n    # Direct match\n    if endpoint in allowed_endpoints:\n        return True\n    \n    # Pattern match\n    for pattern in allowed_endpoints:\n        if pattern.endswith(\"*\"):\n            prefix = pattern[:-1]\n            if endpoint.startswith(prefix):\n                return True\n    \n    return False\n\ndef _get_endpoint_auth(self, endpoint):\n    \"\"\"Get authentication configuration for an endpoint.\"\"\"\n    endpoint_auth = settings.API_ENDPOINT_AUTH or {}\n    \n    # Find the matching configuration\n    for pattern, auth in endpoint_auth.items():\n        if pattern.endswith(\"*\"):\n            prefix = pattern[:-1]\n            if endpoint.startswith(prefix):\n                return auth\n        elif pattern == endpoint:\n            return auth\n    \n    return None\n```",
        "testStrategy": "1. Unit tests for each retrieval method:\n   - Test NLQ translation with various query types\n   - Test Neo4j query execution with mock Neo4j client\n   - Test API retrieval with mock requests library\n   - Test endpoint validation and authentication logic\n\n2. Integration tests:\n   - Test NLQ against actual database with sample queries\n   - Test Neo4j retrieval with actual Neo4j instance\n   - Test API retrieval with mock API server\n\n3. Security tests:\n   - Verify endpoint validation prevents unauthorized API access\n   - Test SQL injection protection in NLQ service\n   - Test authentication handling for API endpoints",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:39:15.886Z"
      },
      {
        "id": "183",
        "title": "Implement Upload API B2 Verification",
        "description": "Enhance the Upload API to verify file existence in B2 before returning success and check processing status from the database.",
        "details": "In `app/api/upload.py`, implement the following TODOs:\n\n1. Verify file exists in B2 before returning success:\n```python\n@router.post(\"/upload\")\nasync def upload_file(\n    file: UploadFile = File(...),\n    project_id: str = Form(...),\n    document_type: str = Form(...),\n    metadata: str = Form(\"{}\"),\n    current_user: User = Depends(get_current_user)\n):\n    \"\"\"Upload a file to B2 storage and start processing.\"\"\"\n    # Parse metadata JSON\n    try:\n        metadata_dict = json.loads(metadata)\n    except json.JSONDecodeError:\n        raise HTTPException(status_code=400, detail=\"Invalid metadata JSON\")\n    \n    # Save file to temporary location\n    temp_file_path = f\"/tmp/{file.filename}\"\n    with open(temp_file_path, \"wb\") as buffer:\n        buffer.write(await file.read())\n    \n    try:\n        # Upload to B2 using document service\n        document_service = DocumentManagementService()\n        document = document_service.upload_document(\n            file_path=temp_file_path,\n            project_id=project_id,\n            document_type=document_type,\n            metadata=metadata_dict\n        )\n        \n        # Verify file exists in B2\n        b2_service = B2StorageService()\n        file_exists = b2_service.check_file_exists(document[\"b2_path\"])\n        \n        if not file_exists:\n            raise HTTPException(\n                status_code=500,\n                detail=\"File upload to B2 failed verification\"\n            )\n        \n        # Start processing task\n        task = process_document.delay(document[\"id\"])\n        \n        # Update document with task ID\n        db = Database()\n        db.update(\"documents\", document[\"id\"], {\n            \"processing_task_id\": task.id,\n            \"status\": \"processing\"\n        })\n        \n        # Return document info with task ID\n        return {\n            \"success\": True,\n            \"document_id\": document[\"id\"],\n            \"task_id\": task.id,\n            \"status\": \"processing\"\n        }\n    except Exception as e:\n        # Log the error\n        logger.error(f\"Upload failed: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n    finally:\n        # Clean up temp file\n        if os.path.exists(temp_file_path):\n            os.remove(temp_file_path)\n```\n\n2. Check processing status from database:\n```python\n@router.get(\"/upload/{document_id}/status\")\nasync def check_upload_status(\n    document_id: str,\n    current_user: User = Depends(get_current_user)\n):\n    \"\"\"Check the status of an uploaded document.\"\"\"\n    db = Database()\n    document = db.get(\"documents\", document_id)\n    \n    if not document:\n        raise HTTPException(status_code=404, detail=\"Document not found\")\n    \n    # Check if user has access to this document\n    if not has_document_access(current_user, document):\n        raise HTTPException(status_code=403, detail=\"Access denied\")\n    \n    # Get processing task status if available\n    task_status = None\n    if document.get(\"processing_task_id\"):\n        try:\n            # Get task status from Celery\n            task = AsyncResult(document[\"processing_task_id\"])\n            task_status = task.status\n        except Exception as e:\n            logger.error(f\"Error getting task status: {str(e)}\")\n    \n    # Check B2 file existence\n    b2_service = B2StorageService()\n    file_exists = b2_service.check_file_exists(document[\"b2_path\"])\n    \n    return {\n        \"document_id\": document_id,\n        \"status\": document.get(\"status\", \"unknown\"),\n        \"task_status\": task_status,\n        \"file_exists\": file_exists,\n        \"filename\": document.get(\"filename\"),\n        \"document_type\": document.get(\"document_type\"),\n        \"created_at\": document.get(\"created_at\"),\n        \"updated_at\": document.get(\"updated_at\")\n    }\n\ndef has_document_access(user, document):\n    \"\"\"Check if user has access to the document.\"\"\"\n    # Admin users have access to all documents\n    if user.get(\"role\") == \"admin\":\n        return True\n    \n    # Check if document belongs to a project the user has access to\n    db = Database()\n    project = db.get(\"research_projects\", document.get(\"project_id\"))\n    \n    if not project:\n        return False\n    \n    # Check if user is project owner or member\n    if project.get(\"owner_id\") == user.get(\"id\"):\n        return True\n    \n    # Check project members\n    members = project.get(\"members\", [])\n    if user.get(\"id\") in members:\n        return True\n    \n    return False\n```",
        "testStrategy": "1. Unit tests:\n   - Test file upload with mock B2 service\n   - Test B2 verification logic\n   - Test status checking with various document states\n   - Test access control logic\n\n2. Integration tests:\n   - Test actual file uploads to B2\n   - Test status checking with actual Celery tasks\n   - Test error handling for various failure scenarios\n\n3. Security tests:\n   - Test access control for documents across different users\n   - Test handling of invalid document IDs\n   - Test handling of unauthorized access attempts",
        "priority": "high",
        "dependencies": [
          "180"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:55:24.281Z"
      },
      {
        "id": "184",
        "title": "Implement LangGraph Tool Calling",
        "description": "Implement proper tool calling with LLM.bind_tools() in the LangGraph workflows to enable structured tool usage.",
        "details": "In `app/workflows/langgraph_workflows.py`, implement the following TODO:\n\n```python\ndef create_research_workflow(tools=None):\n    \"\"\"Create a research workflow with tool calling capabilities.\"\"\"\n    # Initialize the LLM\n    llm = ChatAnthropic(model=\"claude-3-opus-20240229\", temperature=0.2)\n    \n    # Define the tools if provided\n    if tools:\n        # Implement proper tool calling with LLM.bind_tools()\n        llm_with_tools = llm.bind_tools(tools)\n    else:\n        llm_with_tools = llm\n    \n    # Define workflow states\n    @workflow.state\n    class State:\n        question: str\n        context: Optional[List[str]] = None\n        tools_results: Optional[List[Dict]] = None\n        reasoning: Optional[str] = None\n        answer: Optional[str] = None\n        follow_up_questions: Optional[List[str]] = None\n    \n    # Define workflow nodes\n    @workflow.node\n    async def analyze_question(state: State):\n        \"\"\"Analyze the question and determine what tools are needed.\"\"\"\n        prompt = PromptTemplate(\"\"\"\n        You are a research assistant analyzing a question.\n        \n        Question: {question}\n        \n        First, think about what information you need to answer this question.\n        Then, determine which tools would be most helpful to gather this information.\n        \n        Available tools: {tool_descriptions}\n        \n        Provide your reasoning about what information is needed and which tools to use.\n        \"\"\")\n        \n        # Get tool descriptions\n        tool_descriptions = \"\\n\".join([f\"- {tool.name}: {tool.description}\" for tool in tools]) if tools else \"No tools available\"\n        \n        # Call LLM\n        response = await llm.ainvoke(\n            prompt.format(question=state.question, tool_descriptions=tool_descriptions)\n        )\n        \n        return {\"reasoning\": response.content}\n    \n    @workflow.node\n    async def execute_tools(state: State):\n        \"\"\"Execute the appropriate tools based on the question.\"\"\"\n        if not tools:\n            return {\"tools_results\": []}\n        \n        prompt = PromptTemplate(\"\"\"\n        You are a research assistant with access to several tools.\n        \n        Question: {question}\n        Your reasoning: {reasoning}\n        \n        Based on your reasoning, use the appropriate tools to gather information needed to answer the question.\n        Be specific in your tool calls and only call tools that are necessary.\n        \"\"\")\n        \n        # Call LLM with tools\n        response = await llm_with_tools.ainvoke(\n            prompt.format(question=state.question, reasoning=state.reasoning)\n        )\n        \n        # Extract tool calls and results\n        tool_results = []\n        for tool_call in response.tool_calls:\n            tool_name = tool_call[\"name\"]\n            tool_args = tool_call[\"args\"]\n            \n            # Find the matching tool\n            matching_tool = next((t for t in tools if t.name == tool_name), None)\n            if matching_tool:\n                try:\n                    # Execute the tool\n                    result = await matching_tool.ainvoke(**tool_args)\n                    tool_results.append({\n                        \"tool\": tool_name,\n                        \"args\": tool_args,\n                        \"result\": result\n                    })\n                except Exception as e:\n                    tool_results.append({\n                        \"tool\": tool_name,\n                        \"args\": tool_args,\n                        \"error\": str(e)\n                    })\n        \n        return {\"tools_results\": tool_results}\n    \n    @workflow.node\n    async def generate_answer(state: State):\n        \"\"\"Generate a comprehensive answer based on tool results.\"\"\"\n        prompt = PromptTemplate(\"\"\"\n        You are a research assistant providing an answer to a question.\n        \n        Question: {question}\n        Your reasoning: {reasoning}\n        \n        Tool results:\n        {tool_results}\n        \n        Based on the information gathered, provide a comprehensive answer to the question.\n        Include citations to specific tool results where appropriate.\n        If you don't have enough information, acknowledge the limitations in your answer.\n        \"\"\")\n        \n        # Format tool results\n        tool_results_text = \"\"\n        for i, result in enumerate(state.tools_results):\n            tool_results_text += f\"Result {i+1} - {result['tool']}:\\n\"\n            if \"error\" in result:\n                tool_results_text += f\"Error: {result['error']}\\n\"\n            else:\n                tool_results_text += f\"Args: {json.dumps(result['args'])}\\n\"\n                tool_results_text += f\"Result: {json.dumps(result['result'])}\\n\"\n            tool_results_text += \"\\n\"\n        \n        # Call LLM\n        response = await llm.ainvoke(\n            prompt.format(\n                question=state.question,\n                reasoning=state.reasoning,\n                tool_results=tool_results_text\n            )\n        )\n        \n        return {\"answer\": response.content}\n    \n    @workflow.node\n    async def suggest_follow_ups(state: State):\n        \"\"\"Suggest follow-up questions based on the answer.\"\"\"\n        prompt = PromptTemplate(\"\"\"\n        You are a research assistant suggesting follow-up questions.\n        \n        Original question: {question}\n        Your answer: {answer}\n        \n        Based on the answer provided, suggest 3 follow-up questions that would be logical next steps in this research.\n        Format each question on a new line, starting with a number and a period.\n        \"\"\")\n        \n        # Call LLM\n        response = await llm.ainvoke(\n            prompt.format(question=state.question, answer=state.answer)\n        )\n        \n        # Parse follow-up questions\n        follow_ups = []\n        for line in response.content.split(\"\\n\"):\n            match = re.match(r'^\\d+\\.\\s+(.+)$', line)\n            if match:\n                follow_ups.append(match.group(1))\n        \n        return {\"follow_up_questions\": follow_ups}\n    \n    # Define the workflow\n    builder = workflow.builder()\n    builder.add_node(\"analyze_question\", analyze_question)\n    builder.add_node(\"execute_tools\", execute_tools)\n    builder.add_node(\"generate_answer\", generate_answer)\n    builder.add_node(\"suggest_follow_ups\", suggest_follow_ups)\n    \n    # Define the edges\n    builder.add_edge(\"analyze_question\", \"execute_tools\")\n    builder.add_edge(\"execute_tools\", \"generate_answer\")\n    builder.add_edge(\"generate_answer\", \"suggest_follow_ups\")\n    \n    # Set the entry point\n    builder.set_entry_point(\"analyze_question\")\n    \n    # Build and return the workflow\n    return builder.build()\n```",
        "testStrategy": "1. Unit tests:\n   - Test tool binding with mock tools\n   - Test each workflow node individually\n   - Test state transitions between nodes\n\n2. Integration tests:\n   - Test the complete workflow with sample questions\n   - Test with various tool combinations\n   - Test error handling when tools fail\n\n3. Functional tests:\n   - Test with actual LLM calls (in staging environment)\n   - Verify tool calling format is correct\n   - Verify follow-up question generation",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-17T21:10:42.929Z"
      },
      {
        "id": "185",
        "title": "Implement WebSocket Authentication",
        "description": "Implement proper WebSocket authentication in the research projects route to ensure secure real-time communication.",
        "details": "In `app/routes/research_projects.py`, implement the following TODO:\n\n```python\n@router.websocket(\"/ws/research_projects/{project_id}\")\nasync def research_project_updates(websocket: WebSocket, project_id: str):\n    \"\"\"WebSocket endpoint for real-time research project updates.\"\"\"\n    await websocket.accept()\n    \n    try:\n        # Implement proper WebSocket authentication\n        # Get the token from the query parameters\n        token = websocket.query_params.get(\"token\")\n        if not token:\n            await websocket.send_json({\"error\": \"Authentication required\"})\n            await websocket.close(code=1008)  # Policy violation\n            return\n        \n        # Verify the token\n        try:\n            payload = jwt.decode(\n                token,\n                settings.JWT_SECRET_KEY,\n                algorithms=[settings.JWT_ALGORITHM]\n            )\n            user_id = payload.get(\"sub\")\n            if not user_id:\n                raise ValueError(\"Invalid token payload\")\n        except jwt.PyJWTError as e:\n            await websocket.send_json({\"error\": f\"Invalid token: {str(e)}\"})\n            await websocket.close(code=1008)  # Policy violation\n            return\n        \n        # Check if user has access to the project\n        db = Database()\n        project = db.get(\"research_projects\", project_id)\n        if not project:\n            await websocket.send_json({\"error\": \"Project not found\"})\n            await websocket.close(code=1008)\n            return\n        \n        # Check project access\n        if project.get(\"owner_id\") != user_id and user_id not in project.get(\"members\", []):\n            # Check if user is admin\n            user = db.get(\"users\", user_id)\n            if not user or user.get(\"role\") != \"admin\":\n                await websocket.send_json({\"error\": \"Access denied\"})\n                await websocket.close(code=1008)\n                return\n        \n        # Register the connection with the project ID\n        connection_id = str(uuid.uuid4())\n        await websocket_manager.connect(connection_id, websocket, project_id)\n        \n        # Send initial project state\n        await websocket.send_json({\n            \"type\": \"initial_state\",\n            \"project\": project\n        })\n        \n        # Listen for messages until client disconnects\n        while True:\n            data = await websocket.receive_text()\n            message = json.loads(data)\n            \n            # Process messages (commands, etc.)\n            if message.get(\"type\") == \"command\":\n                # Handle commands\n                command = message.get(\"command\")\n                if command == \"refresh\":\n                    # Refresh project data\n                    project = db.get(\"research_projects\", project_id)\n                    await websocket.send_json({\n                        \"type\": \"project_update\",\n                        \"project\": project\n                    })\n            \n    except WebSocketDisconnect:\n        # Client disconnected\n        if connection_id:\n            await websocket_manager.disconnect(connection_id)\n    except Exception as e:\n        # Log the error\n        logger.error(f\"WebSocket error: {str(e)}\")\n        try:\n            await websocket.send_json({\"error\": str(e)})\n            await websocket.close()\n        except:\n            pass\n```",
        "testStrategy": "1. Unit tests:\n   - Test token validation logic\n   - Test project access control logic\n   - Test WebSocket message handling\n\n2. Integration tests:\n   - Test WebSocket connections with valid and invalid tokens\n   - Test access control with different user roles\n   - Test message exchange between server and client\n\n3. Security tests:\n   - Test token expiration handling\n   - Test token tampering detection\n   - Test connection limits and rate limiting\n   - Test handling of malformed messages",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:40:54.724Z"
      },
      {
        "id": "186",
        "title": "Implement Admin Authorization Check",
        "description": "Implement admin authorization check in the documents route to ensure proper access control for approval listings.",
        "details": "In `app/routes/documents.py`, implement the following TODO:\n\n```python\n@router.get(\"/documents/pending_approval\")\nasync def get_pending_approval_documents(\n    current_user: User = Depends(get_current_user),\n    skip: int = 0,\n    limit: int = 100\n):\n    \"\"\"Get documents pending approval.\"\"\"\n    db = Database()\n    \n    # Implement admin check for approval listings\n    is_admin = current_user.get(\"role\") == \"admin\"\n    \n    # Build query based on user role\n    query = {\"status\": \"pending_approval\"}\n    \n    # If not admin, only show user's own documents\n    if not is_admin:\n        query[\"created_by\"] = current_user.get(\"id\")\n    \n    # Get documents with pagination\n    documents = db.query(\n        \"documents\",\n        query,\n        limit=limit,\n        offset=skip,\n        order_by=\"created_at\",\n        order_direction=\"desc\"\n    )\n    \n    # Get total count for pagination\n    total_count = db.count(\"documents\", query)\n    \n    # Enrich documents with user info\n    for doc in documents:\n        if doc.get(\"created_by\"):\n            user = db.get(\"users\", doc[\"created_by\"])\n            if user:\n                doc[\"created_by_name\"] = f\"{user.get('first_name', '')} {user.get('last_name', '')}\".strip()\n                doc[\"created_by_email\"] = user.get(\"email\")\n    \n    return {\n        \"documents\": documents,\n        \"total\": total_count,\n        \"skip\": skip,\n        \"limit\": limit\n    }\n\n@router.post(\"/documents/{document_id}/approve\")\nasync def approve_document(\n    document_id: str,\n    current_user: User = Depends(get_current_user)\n):\n    \"\"\"Approve a document pending approval.\"\"\"\n    # Check if user is admin\n    if current_user.get(\"role\") != \"admin\":\n        raise HTTPException(\n            status_code=403,\n            detail=\"Only administrators can approve documents\"\n        )\n    \n    db = Database()\n    document = db.get(\"documents\", document_id)\n    \n    if not document:\n        raise HTTPException(status_code=404, detail=\"Document not found\")\n    \n    if document.get(\"status\") != \"pending_approval\":\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Document is not pending approval (current status: {document.get('status')})\"\n        )\n    \n    # Update document status\n    db.update(\"documents\", document_id, {\n        \"status\": \"approved\",\n        \"approved_by\": current_user.get(\"id\"),\n        \"approved_at\": datetime.now().isoformat()\n    })\n    \n    # Trigger post-approval processing\n    process_approved_document.delay(document_id)\n    \n    return {\"success\": True, \"message\": \"Document approved\"}\n\n@router.post(\"/documents/{document_id}/reject\")\nasync def reject_document(\n    document_id: str,\n    rejection: DocumentRejection,\n    current_user: User = Depends(get_current_user)\n):\n    \"\"\"Reject a document pending approval.\"\"\"\n    # Check if user is admin\n    if current_user.get(\"role\") != \"admin\":\n        raise HTTPException(\n            status_code=403,\n            detail=\"Only administrators can reject documents\"\n        )\n    \n    db = Database()\n    document = db.get(\"documents\", document_id)\n    \n    if not document:\n        raise HTTPException(status_code=404, detail=\"Document not found\")\n    \n    if document.get(\"status\") != \"pending_approval\":\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Document is not pending approval (current status: {document.get('status')})\"\n        )\n    \n    # Update document status\n    db.update(\"documents\", document_id, {\n        \"status\": \"rejected\",\n        \"rejected_by\": current_user.get(\"id\"),\n        \"rejected_at\": datetime.now().isoformat(),\n        \"rejection_reason\": rejection.reason\n    })\n    \n    # Notify document owner\n    if document.get(\"created_by\"):\n        notification_service = NotificationService()\n        notification_service.send_notification(\n            user_id=document.get(\"created_by\"),\n            title=\"Document Rejected\",\n            message=f\"Your document '{document.get('filename')}' was rejected: {rejection.reason}\",\n            type=\"document_rejected\",\n            data={\n                \"document_id\": document_id,\n                \"rejection_reason\": rejection.reason\n            }\n        )\n    \n    return {\"success\": True, \"message\": \"Document rejected\"}\n```",
        "testStrategy": "1. Unit tests:\n   - Test admin role check logic\n   - Test document query filtering based on user role\n   - Test approval and rejection logic\n\n2. Integration tests:\n   - Test document listing with admin and non-admin users\n   - Test approval workflow end-to-end\n   - Test rejection workflow with notifications\n\n3. Security tests:\n   - Test authorization bypass attempts\n   - Test role spoofing prevention\n   - Test document ID manipulation protection",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:43:21.639Z"
      },
      {
        "id": "187",
        "title": "Implement Research Project Celery Integration",
        "description": "Complete the Research Project Service by implementing Celery task integration for project initialization and task revocation on project cancellation.",
        "details": "In `app/services/research_project_service.py`, implement the following TODOs:\n\n```python\ndef create_project(self, name, description, owner_id, settings=None):\n    \"\"\"Create a new research project.\"\"\"\n    project_id = str(uuid.uuid4())\n    \n    # Create project record\n    project = {\n        \"id\": project_id,\n        \"name\": name,\n        \"description\": description,\n        \"owner_id\": owner_id,\n        \"members\": [],\n        \"settings\": settings or {},\n        \"status\": \"initializing\",\n        \"created_at\": datetime.now().isoformat(),\n        \"updated_at\": datetime.now().isoformat()\n    }\n    \n    # Insert into database\n    self.db.insert(\"research_projects\", project)\n    \n    # Trigger Celery task for project initialization\n    task = initialize_research_project.delay(\n        project_id=project_id,\n        owner_id=owner_id,\n        settings=settings or {}\n    )\n    \n    # Store task ID in project record\n    self.db.update(\"research_projects\", project_id, {\n        \"initialization_task_id\": task.id\n    })\n    \n    return project\n\ndef cancel_project(self, project_id):\n    \"\"\"Cancel a research project and clean up resources.\"\"\"\n    # Get project\n    project = self.db.get(\"research_projects\", project_id)\n    if not project:\n        raise ValueError(f\"Project {project_id} not found\")\n    \n    # Check if project can be cancelled\n    if project.get(\"status\") in [\"completed\", \"cancelled\", \"failed\"]:\n        raise ValueError(f\"Project cannot be cancelled in status: {project.get('status')}\")\n    \n    # Implement task revocation on project cancellation\n    # Revoke initialization task if still running\n    if project.get(\"initialization_task_id\"):\n        try:\n            app.control.revoke(\n                project[\"initialization_task_id\"],\n                terminate=True,\n                signal=\"SIGTERM\"\n            )\n        except Exception as e:\n            logger.error(f\"Error revoking initialization task: {str(e)}\")\n    \n    # Revoke all running tasks for this project\n    running_tasks = self.db.query(\n        \"task_queue\",\n        {\"project_id\": project_id, \"status\": \"running\"}\n    )\n    \n    for task in running_tasks:\n        if task.get(\"celery_task_id\"):\n            try:\n                app.control.revoke(\n                    task[\"celery_task_id\"],\n                    terminate=True,\n                    signal=\"SIGTERM\"\n                )\n            except Exception as e:\n                logger.error(f\"Error revoking task {task['id']}: {str(e)}\")\n            \n            # Update task status\n            self.db.update(\"task_queue\", task[\"id\"], {\n                \"status\": \"cancelled\",\n                \"updated_at\": datetime.now().isoformat()\n            })\n    \n    # Update project status\n    self.db.update(\"research_projects\", project_id, {\n        \"status\": \"cancelled\",\n        \"updated_at\": datetime.now().isoformat(),\n        \"cancelled_at\": datetime.now().isoformat()\n    })\n    \n    # Log the cancellation\n    self.db.insert(\"project_logs\", {\n        \"project_id\": project_id,\n        \"action\": \"project_cancelled\",\n        \"details\": {\n            \"cancelled_tasks\": len(running_tasks)\n        },\n        \"timestamp\": datetime.now().isoformat()\n    })\n    \n    return {\"success\": True, \"message\": f\"Project {project_id} cancelled\"}\n```",
        "testStrategy": "1. Unit tests:\n   - Test project creation with task initialization\n   - Test project cancellation with task revocation\n   - Test error handling for various scenarios\n\n2. Integration tests:\n   - Test end-to-end project lifecycle\n   - Test Celery task creation and execution\n   - Test task revocation behavior\n\n3. Functional tests:\n   - Test with actual Celery worker\n   - Verify database state after operations\n   - Test concurrent operations",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:44:52.207Z"
      },
      {
        "id": "188",
        "title": "Implement Agent Feedback System",
        "description": "Implement the agent feedback system by creating the agent_feedback table in Supabase and implementing feedback storage functionality.",
        "details": "In `app/services/classification_service.py` and `app/services/asset_management_service.py`, implement the following TODOs:\n\n1. First, ensure the agent_feedback table is created in Supabase using the schema provided in the PRD:\n\n```sql\nCREATE TABLE agent_feedback (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    agent_id VARCHAR(50) NOT NULL,\n    task_id UUID,\n    feedback_type VARCHAR(50) NOT NULL, -- 'classification', 'generation', 'retrieval'\n    input_summary TEXT,\n    output_summary TEXT,\n    rating INTEGER CHECK (rating >= 1 AND rating <= 5),\n    feedback_text TEXT,\n    metadata JSONB DEFAULT '{}',\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    created_by UUID REFERENCES auth.users(id)\n);\n\nCREATE INDEX idx_agent_feedback_agent ON agent_feedback(agent_id);\nCREATE INDEX idx_agent_feedback_type ON agent_feedback(feedback_type);\nCREATE INDEX idx_agent_feedback_created ON agent_feedback(created_at DESC);\n```\n\n2. Implement feedback storage in `app/services/classification_service.py`:\n\n```python\ndef store_classification_feedback(self, classification_id, rating, feedback_text=None, user_id=None):\n    \"\"\"Store feedback for a classification result.\"\"\"\n    # Get the classification record\n    classification = self.db.get(\"classifications\", classification_id)\n    if not classification:\n        raise ValueError(f\"Classification {classification_id} not found\")\n    \n    # Create feedback record\n    feedback = {\n        \"agent_id\": \"classification_agent\",\n        \"task_id\": classification.get(\"task_id\"),\n        \"feedback_type\": \"classification\",\n        \"input_summary\": classification.get(\"input_text\", \"\")[:500],  # First 500 chars\n        \"output_summary\": json.dumps(classification.get(\"result\", {}))[:500],\n        \"rating\": rating,\n        \"feedback_text\": feedback_text,\n        \"metadata\": {\n            \"classification_id\": classification_id,\n            \"categories\": classification.get(\"result\", {}).get(\"categories\", []),\n            \"confidence\": classification.get(\"result\", {}).get(\"confidence\")\n        },\n        \"created_by\": user_id\n    }\n    \n    # Insert feedback\n    feedback_id = self.db.insert(\"agent_feedback\", feedback).get(\"id\")\n    \n    # Update classification record with feedback reference\n    self.db.update(\"classifications\", classification_id, {\n        \"has_feedback\": True,\n        \"feedback_id\": feedback_id,\n        \"feedback_rating\": rating\n    })\n    \n    return {\"success\": True, \"feedback_id\": feedback_id}\n```\n\n3. Implement feedback storage in `app/services/asset_management_service.py`:\n\n```python\ndef store_asset_feedback(self, asset_id, rating, feedback_text=None, user_id=None):\n    \"\"\"Store feedback for a generated asset.\"\"\"\n    # Get the asset record\n    asset = self.db.get(\"assets\", asset_id)\n    if not asset:\n        raise ValueError(f\"Asset {asset_id} not found\")\n    \n    # Determine agent ID based on asset type\n    agent_map = {\n        \"image\": \"image_generation_agent\",\n        \"text\": \"text_generation_agent\",\n        \"code\": \"code_generation_agent\",\n        \"chart\": \"chart_generation_agent\"\n    }\n    \n    agent_id = agent_map.get(asset.get(\"asset_type\"), \"unknown_agent\")\n    \n    # Create feedback record\n    feedback = {\n        \"agent_id\": agent_id,\n        \"task_id\": asset.get(\"task_id\"),\n        \"feedback_type\": \"generation\",\n        \"input_summary\": asset.get(\"prompt\", \"\")[:500],  # First 500 chars\n        \"output_summary\": asset.get(\"description\", \"\")[:500],\n        \"rating\": rating,\n        \"feedback_text\": feedback_text,\n        \"metadata\": {\n            \"asset_id\": asset_id,\n            \"asset_type\": asset.get(\"asset_type\"),\n            \"generation_params\": asset.get(\"generation_params\", {})\n        },\n        \"created_by\": user_id\n    }\n    \n    # Insert feedback\n    feedback_id = self.db.insert(\"agent_feedback\", feedback).get(\"id\")\n    \n    # Update asset record with feedback reference\n    self.db.update(\"assets\", asset_id, {\n        \"has_feedback\": True,\n        \"feedback_id\": feedback_id,\n        \"feedback_rating\": rating\n    })\n    \n    return {\"success\": True, \"feedback_id\": feedback_id}\n```\n\n4. Create a new `app/services/feedback_service.py` to centralize feedback functionality:\n\n```python\nfrom datetime import datetime\nimport json\nfrom app.db.database import Database\n\nclass FeedbackService:\n    \"\"\"Service for managing agent feedback.\"\"\"\n    \n    def __init__(self):\n        self.db = Database()\n    \n    def store_feedback(self, agent_id, feedback_type, input_summary, output_summary, \n                      rating, feedback_text=None, task_id=None, metadata=None, user_id=None):\n        \"\"\"Store feedback for any agent.\"\"\"\n        # Validate rating\n        if not isinstance(rating, int) or rating < 1 or rating > 5:\n            raise ValueError(\"Rating must be an integer between 1 and 5\")\n        \n        # Create feedback record\n        feedback = {\n            \"agent_id\": agent_id,\n            \"task_id\": task_id,\n            \"feedback_type\": feedback_type,\n            \"input_summary\": input_summary[:500] if input_summary else None,\n            \"output_summary\": output_summary[:500] if output_summary else None,\n            \"rating\": rating,\n            \"feedback_text\": feedback_text,\n            \"metadata\": metadata or {},\n            \"created_by\": user_id,\n            \"created_at\": datetime.now().isoformat()\n        }\n        \n        # Insert feedback\n        result = self.db.insert(\"agent_feedback\", feedback)\n        feedback_id = result.get(\"id\")\n        \n        return {\"success\": True, \"feedback_id\": feedback_id}\n    \n    def get_feedback(self, feedback_id):\n        \"\"\"Get feedback by ID.\"\"\"\n        return self.db.get(\"agent_feedback\", feedback_id)\n    \n    def get_agent_feedback(self, agent_id, limit=100, offset=0):\n        \"\"\"Get feedback for a specific agent.\"\"\"\n        return self.db.query(\n            \"agent_feedback\",\n            {\"agent_id\": agent_id},\n            limit=limit,\n            offset=offset,\n            order_by=\"created_at\",\n            order_direction=\"desc\"\n        )\n    \n    def get_feedback_stats(self, agent_id=None, feedback_type=None):\n        \"\"\"Get feedback statistics.\"\"\"\n        query = {}\n        if agent_id:\n            query[\"agent_id\"] = agent_id\n        if feedback_type:\n            query[\"feedback_type\"] = feedback_type\n        \n        # Get all matching feedback\n        feedback = self.db.query(\"agent_feedback\", query)\n        \n        if not feedback:\n            return {\n                \"count\": 0,\n                \"average_rating\": 0,\n                \"rating_distribution\": {\"1\": 0, \"2\": 0, \"3\": 0, \"4\": 0, \"5\": 0}\n            }\n        \n        # Calculate statistics\n        total_rating = sum(f.get(\"rating\", 0) for f in feedback)\n        avg_rating = total_rating / len(feedback) if feedback else 0\n        \n        # Calculate rating distribution\n        distribution = {\"1\": 0, \"2\": 0, \"3\": 0, \"4\": 0, \"5\": 0}\n        for f in feedback:\n            rating = f.get(\"rating\")\n            if rating and 1 <= rating <= 5:\n                distribution[str(rating)] += 1\n        \n        return {\n            \"count\": len(feedback),\n            \"average_rating\": round(avg_rating, 2),\n            \"rating_distribution\": distribution\n        }\n```",
        "testStrategy": "1. Unit tests:\n   - Test feedback storage for classification service\n   - Test feedback storage for asset management service\n   - Test central feedback service methods\n   - Test validation of rating values\n\n2. Integration tests:\n   - Test feedback storage with actual Supabase database\n   - Test feedback retrieval and statistics calculation\n   - Test feedback lifecycle with various agent types\n\n3. Database tests:\n   - Verify agent_feedback table schema\n   - Test index performance for common queries\n   - Test constraints (e.g., rating range check)",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-01-17T21:16:25.296Z"
      },
      {
        "id": "189",
        "title": "Implement OpenTelemetry Integration for Distributed Tracing",
        "description": "Add distributed tracing with OpenTelemetry across all services, including trace propagation through Celery tasks and export to configured backend.",
        "details": "1. Create `app/core/tracing.py` with OpenTelemetry setup:\n\n```python\nimport os\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.instrumentation.fastapi import FastAPIInstrumentor\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\nfrom opentelemetry.instrumentation.celery import CeleryInstrumentor\nfrom opentelemetry.instrumentation.psycopg2 import Psycopg2Instrumentor\nfrom opentelemetry.instrumentation.redis import RedisInstrumentor\nfrom opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator\nfrom opentelemetry.context.context import Context\nfrom opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator\nfrom opentelemetry.baggage.propagation import W3CBaggagePropagator\nfrom opentelemetry.propagators import set_global_textmap, composite\nfrom fastapi import FastAPI\nfrom app.core.config import settings\n\ndef setup_tracing():\n    \"\"\"Configure OpenTelemetry tracing.\"\"\"\n    # Create a resource with service info\n    resource = Resource.create({\n        \"service.name\": settings.SERVICE_NAME,\n        \"service.version\": settings.SERVICE_VERSION,\n        \"deployment.environment\": settings.ENVIRONMENT\n    })\n    \n    # Create a tracer provider\n    tracer_provider = TracerProvider(resource=resource)\n    \n    # Configure the exporter\n    if settings.OTLP_ENDPOINT:\n        # Use OTLP exporter (for Jaeger, Tempo, etc.)\n        otlp_exporter = OTLPSpanExporter(\n            endpoint=settings.OTLP_ENDPOINT,\n            insecure=settings.OTLP_INSECURE\n        )\n        tracer_provider.add_span_processor(BatchSpanProcessor(otlp_exporter))\n    \n    # Set the tracer provider\n    trace.set_tracer_provider(tracer_provider)\n    \n    # Set up propagators for distributed tracing\n    set_global_textmap(composite.CompositePropagator([\n        TraceContextTextMapPropagator(),\n        W3CBaggagePropagator()\n    ]))\n    \n    # Get a tracer\n    tracer = trace.get_tracer(settings.SERVICE_NAME, settings.SERVICE_VERSION)\n    \n    # Instrument libraries\n    RequestsInstrumentor().instrument()\n    Psycopg2Instrumentor().instrument()\n    RedisInstrumentor().instrument()\n    CeleryInstrumentor().instrument()\n    \n    return tracer\n\ndef instrument_fastapi(app: FastAPI):\n    \"\"\"Instrument a FastAPI application.\"\"\"\n    FastAPIInstrumentor.instrument_app(app, tracer_provider=trace.get_tracer_provider())\n\n# Helper functions for Celery task tracing\ndef inject_trace_info(headers):\n    \"\"\"Inject trace context into Celery task headers.\"\"\"\n    propagator = TraceContextTextMapPropagator()\n    propagator.inject(headers)\n    return headers\n\ndef extract_trace_info(headers):\n    \"\"\"Extract trace context from Celery task headers.\"\"\"\n    propagator = TraceContextTextMapPropagator()\n    context = propagator.extract(headers)\n    return context\n\n# Context manager for creating spans\nclass TracingSpan:\n    def __init__(self, name, attributes=None, parent_context=None):\n        self.name = name\n        self.attributes = attributes or {}\n        self.parent_context = parent_context\n        self.tracer = trace.get_tracer(settings.SERVICE_NAME, settings.SERVICE_VERSION)\n        self.span = None\n    \n    def __enter__(self):\n        if self.parent_context:\n            self.span = self.tracer.start_span(\n                name=self.name,\n                attributes=self.attributes,\n                context=self.parent_context\n            )\n        else:\n            self.span = self.tracer.start_span(\n                name=self.name,\n                attributes=self.attributes\n            )\n        return self.span\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type:\n            self.span.set_status(trace.StatusCode.ERROR, str(exc_val))\n            self.span.record_exception(exc_val)\n        self.span.end()\n```\n\n2. Update `app/main.py` to initialize tracing:\n\n```python\nfrom fastapi import FastAPI\nfrom app.core.tracing import setup_tracing, instrument_fastapi\nfrom app.core.config import settings\n\n# Setup tracing first\ntracer = setup_tracing()\n\napp = FastAPI(\n    title=settings.PROJECT_NAME,\n    description=settings.PROJECT_DESCRIPTION,\n    version=settings.SERVICE_VERSION,\n    docs_url=\"/docs\" if settings.ENVIRONMENT != \"production\" else None\n)\n\n# Instrument FastAPI with OpenTelemetry\ninstrument_fastapi(app)\n\n# Import and include routers\nfrom app.routes import health, documents, research_projects, upload\n\napp.include_router(health.router, tags=[\"health\"])\napp.include_router(documents.router, tags=[\"documents\"])\napp.include_router(research_projects.router, tags=[\"research_projects\"])\napp.include_router(upload.router, tags=[\"upload\"])\n```\n\n3. Update `app/celery_app.py` to add tracing to Celery tasks:\n\n```python\nfrom celery import Celery, Task\nfrom app.core.config import settings\nfrom app.core.tracing import extract_trace_info, TracingSpan\nfrom opentelemetry import trace\n\napp = Celery(\n    \"empire\",\n    broker=settings.CELERY_BROKER_URL,\n    backend=settings.CELERY_RESULT_BACKEND\n)\n\napp.conf.update(\n    task_serializer=\"json\",\n    accept_content=[\"json\"],\n    result_serializer=\"json\",\n    timezone=\"UTC\",\n    enable_utc=True,\n    task_track_started=True,\n    task_publish_retry=True,\n    worker_prefetch_multiplier=1,\n    task_acks_late=True,\n    task_reject_on_worker_lost=True,\n    task_queue_max_priority=10,\n    broker_connection_retry=True,\n    broker_connection_max_retries=10,\n    task_default_queue=\"default\"\n)\n\n# Define a base task class with tracing\nclass TracedTask(Task):\n    \"\"\"Base task class that includes OpenTelemetry tracing.\"\"\"\n    \n    def apply_async(self, args=None, kwargs=None, **options):\n        \"\"\"Inject trace context when scheduling a task.\"\"\"\n        # Get current headers or initialize empty dict\n        headers = options.setdefault(\"headers\", {})\n        \n        # Inject current trace context into headers\n        current_context = trace.get_current_span().get_span_context()\n        if current_context.is_valid:\n            trace_id = format(current_context.trace_id, \"032x\")\n            span_id = format(current_context.span_id, \"016x\")\n            headers[\"traceparent\"] = f\"00-{trace_id}-{span_id}-{int(current_context.trace_flags):02x}\"\n        \n        return super().apply_async(args=args, kwargs=kwargs, **options)\n    \n    def __call__(self, *args, **kwargs):\n        \"\"\"Extract trace context and create a span for the task execution.\"\"\"\n        # Get the task request\n        task_id = self.request.id\n        task_name = self.name\n        \n        # Extract trace context from headers if available\n        headers = self.request.get(\"headers\", {})\n        parent_context = None\n        if \"traceparent\" in headers:\n            parent_context = extract_trace_info(headers)\n        \n        # Create a span for this task\n        with TracingSpan(\n            name=f\"celery.task.{task_name}\",\n            attributes={\n                \"celery.task_id\": task_id,\n                \"celery.task_name\": task_name,\n                \"celery.args\": str(args),\n                \"celery.kwargs\": str(kwargs)\n            },\n            parent_context=parent_context\n        ):\n            # Execute the task\n            return super().__call__(*args, **kwargs)\n\n# Set the base task class\napp.Task = TracedTask\n\n# Import tasks to register them\nfrom app.tasks import document_processing, research_tasks, graph_sync\n```\n\n4. Add tracing to key services, for example in `app/services/document_management.py`:\n\n```python\nfrom app.core.tracing import TracingSpan\n\ndef upload_document(self, file_path, project_id, document_type, metadata=None):\n    \"\"\"Upload a document to B2 storage.\"\"\"\n    with TracingSpan(\n        name=\"document_management.upload_document\",\n        attributes={\n            \"project_id\": project_id,\n            \"document_type\": document_type,\n            \"filename\": os.path.basename(file_path)\n        }\n    ) as span:\n        # Generate a unique B2 path using project_id and filename\n        filename = os.path.basename(file_path)\n        b2_path = f\"documents/{project_id}/{document_type}/{filename}\"\n        \n        # Use existing B2StorageService to upload the file\n        span.add_event(\"uploading_to_b2\")\n        b2_url = self.b2_service.upload_file(file_path, b2_path)\n        \n        # Store metadata in Supabase\n        span.add_event(\"storing_metadata\")\n        document_record = {\n            \"project_id\": project_id,\n            \"document_type\": document_type,\n            \"filename\": filename,\n            \"b2_path\": b2_path,\n            \"b2_url\": b2_url,\n            \"metadata\": metadata or {},\n            \"status\": \"uploaded\"\n        }\n        \n        # Insert into documents table\n        document_id = self.db.insert(\"documents\", document_record).get(\"id\")\n        span.set_attribute(\"document_id\", document_id)\n        \n        # Return document record with ID\n        return {**document_record, \"id\": document_id}\n```",
        "testStrategy": "1. Unit tests:\n   - Test trace context injection and extraction\n   - Test span creation and attributes\n   - Test error handling in spans\n\n2. Integration tests:\n   - Test trace propagation across service boundaries\n   - Test trace propagation through Celery tasks\n   - Test trace export to configured backend\n\n3. Performance tests:\n   - Measure overhead of tracing instrumentation\n   - Test with different sampling rates\n   - Test with high concurrency",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create OpenTelemetry tracing setup in app/core/tracing.py",
            "description": "Implement the core tracing functionality in app/core/tracing.py with OpenTelemetry setup, including tracer provider, span processors, and exporters.",
            "dependencies": [],
            "details": "Create the tracing.py module with the following components:\n- TracerProvider configuration with service information\n- OTLP exporter setup for sending traces to the backend\n- Global propagator configuration for distributed tracing\n- Helper functions for creating spans\n- Context manager for span creation and management\n- Error handling and status code setting in spans",
            "status": "pending",
            "testStrategy": "Unit tests for tracer initialization, span creation, and context propagation. Test different exporter configurations and verify resource attributes are correctly set.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create tracing middleware for FastAPI in app/middleware/tracing.py",
            "description": "Implement FastAPI middleware for request tracing and instrumentation of FastAPI applications.",
            "dependencies": [
              1
            ],
            "details": "Create middleware/tracing.py with:\n- FastAPI instrumentation setup\n- Request/response tracing middleware\n- Automatic span creation for HTTP requests\n- Span attribute population with request details\n- Status code tracking\n- Error handling for failed requests\n- Integration with the core tracing module",
            "status": "pending",
            "testStrategy": "Test middleware with mock FastAPI requests. Verify spans are created with correct attributes, status codes are properly recorded, and errors are captured in spans.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement trace ID generation and propagation",
            "description": "Implement trace ID generation and context propagation across service boundaries using W3C TraceContext.",
            "dependencies": [
              1
            ],
            "details": "Implement trace context propagation:\n- Configure W3C TraceContext propagator\n- Create helper functions for injecting trace context into headers\n- Create helper functions for extracting trace context from headers\n- Ensure trace IDs are properly formatted and propagated\n- Implement baggage propagation for additional metadata\n- Update main.py to initialize tracing early in application startup",
            "status": "pending",
            "testStrategy": "Test trace context injection and extraction with various header formats. Verify trace IDs are correctly propagated across mock service boundaries and maintain the same trace ID.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement trace context propagation to Celery tasks",
            "description": "Extend the tracing system to propagate trace context through Celery tasks for end-to-end tracing.",
            "dependencies": [
              1,
              3
            ],
            "details": "Update celery_app.py to support tracing:\n- Create a TracedTask base class inheriting from Celery's Task\n- Override apply_async to inject trace context into task headers\n- Override __call__ to extract trace context and create spans\n- Add span attributes for task ID, name, and arguments\n- Implement error handling for task failures\n- Configure Celery instrumentation with OpenTelemetry\n- Update task modules to use the traced task base class",
            "status": "pending",
            "testStrategy": "Test trace propagation through Celery tasks with mock task executions. Verify trace context is maintained across task boundaries and spans are properly created for task execution.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement trace ID injection in structlog",
            "description": "Integrate OpenTelemetry trace IDs with structlog for correlated logging across services.",
            "dependencies": [
              1,
              3
            ],
            "details": "Create logging integration with tracing:\n- Configure structlog processor to extract current trace context\n- Add trace_id and span_id to log records\n- Create helper functions for creating logs with trace context\n- Update logging configuration to include trace IDs in log formats\n- Ensure trace IDs are consistent between logs and traces\n- Add correlation ID support for linking logs to traces\n- Document the logging pattern for developers",
            "status": "pending",
            "testStrategy": "Test log output with active traces to verify trace IDs are correctly included in logs. Test correlation between logs and traces in different scenarios including error conditions.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-17T20:50:42.956Z"
      },
      {
        "id": "190",
        "title": "Implement Enhanced Health Checks",
        "description": "Implement deep health checks for all dependencies, including readiness vs. liveness probes and dependency timeout handling.",
        "details": "1. Create a new file `app/routes/health.py` with enhanced health check endpoints:\n\n```python\nfrom fastapi import APIRouter, Depends, Response, status\nfrom typing import Dict, List, Optional\nimport time\nimport asyncio\nfrom pydantic import BaseModel\nimport requests\nfrom app.db.database import Database\nfrom app.core.config import settings\nfrom app.services.b2_storage_service import B2StorageService\nfrom app.celery_app import app as celery_app\nfrom app.core.tracing import TracingSpan\n\nrouter = APIRouter()\n\nclass HealthStatus(BaseModel):\n    status: str\n    version: str\n    environment: str\n    checks: Dict[str, Dict]\n\n@router.get(\"/health\", response_model=HealthStatus, tags=[\"health\"])\nasync def health_check():\n    \"\"\"Basic health check endpoint.\"\"\"\n    with TracingSpan(name=\"health.basic_check\"):\n        return {\n            \"status\": \"ok\",\n            \"version\": settings.SERVICE_VERSION,\n            \"environment\": settings.ENVIRONMENT,\n            \"checks\": {}\n        }\n\n@router.get(\"/health/liveness\", response_model=HealthStatus, tags=[\"health\"])\nasync def liveness_probe():\n    \"\"\"Kubernetes liveness probe endpoint.\n    \n    Checks if the application is running and responsive.\n    Should be lightweight and not check external dependencies.\n    \"\"\"\n    with TracingSpan(name=\"health.liveness_probe\"):\n        # Just check if the application is running\n        return {\n            \"status\": \"ok\",\n            \"version\": settings.SERVICE_VERSION,\n            \"environment\": settings.ENVIRONMENT,\n            \"checks\": {\n                \"app\": {\n                    \"status\": \"ok\",\n                    \"message\": \"Application is running\"\n                }\n            }\n        }\n\n@router.get(\"/health/readiness\", response_model=HealthStatus, tags=[\"health\"])\nasync def readiness_probe(response: Response):\n    \"\"\"Kubernetes readiness probe endpoint.\n    \n    Checks if the application is ready to receive traffic.\n    Should check all critical dependencies.\n    \"\"\"\n    with TracingSpan(name=\"health.readiness_probe\") as span:\n        start_time = time.time()\n        checks = {}\n        overall_status = \"ok\"\n        \n        # Check database connection\n        try:\n            with TracingSpan(name=\"health.check_database\"):\n                db = Database()\n                db_start = time.time()\n                db_result = db.execute_raw(\"SELECT 1\")\n                db_duration = time.time() - db_start\n                \n                checks[\"database\"] = {\n                    \"status\": \"ok\",\n                    \"message\": \"Database connection successful\",\n                    \"duration_ms\": round(db_duration * 1000, 2)\n                }\n        except Exception as e:\n            span.record_exception(e)\n            checks[\"database\"] = {\n                \"status\": \"error\",\n                \"message\": str(e),\n                \"duration_ms\": round((time.time() - start_time) * 1000, 2)\n            }\n            overall_status = \"error\"\n        \n        # Check B2 storage\n        try:\n            with TracingSpan(name=\"health.check_b2\"):\n                b2_start = time.time()\n                b2_service = B2StorageService()\n                b2_service.check_connection()\n                b2_duration = time.time() - b2_start\n                \n                checks[\"b2_storage\"] = {\n                    \"status\": \"ok\",\n                    \"message\": \"B2 connection successful\",\n                    \"duration_ms\": round(b2_duration * 1000, 2)\n                }\n        except Exception as e:\n            span.record_exception(e)\n            checks[\"b2_storage\"] = {\n                \"status\": \"error\",\n                \"message\": str(e),\n                \"duration_ms\": round((time.time() - b2_start) * 1000, 2)\n            }\n            overall_status = \"error\"\n        \n        # Check Celery connection\n        try:\n            with TracingSpan(name=\"health.check_celery\"):\n                celery_start = time.time()\n                celery_ping = celery_app.control.ping(timeout=1.0)\n                celery_duration = time.time() - celery_start\n                \n                if celery_ping:\n                    checks[\"celery\"] = {\n                        \"status\": \"ok\",\n                        \"message\": f\"Celery connection successful, {len(celery_ping)} workers responded\",\n                        \"duration_ms\": round(celery_duration * 1000, 2),\n                        \"workers\": len(celery_ping)\n                    }\n                else:\n                    checks[\"celery\"] = {\n                        \"status\": \"warning\",\n                        \"message\": \"No Celery workers responded\",\n                        \"duration_ms\": round(celery_duration * 1000, 2),\n                        \"workers\": 0\n                    }\n                    # Don't fail readiness for Celery, just warn\n        except Exception as e:\n            span.record_exception(e)\n            checks[\"celery\"] = {\n                \"status\": \"error\",\n                \"message\": str(e),\n                \"duration_ms\": round((time.time() - celery_start) * 1000, 2)\n            }\n            # Don't fail readiness for Celery, just report error\n        \n        # Set response status code based on overall status\n        if overall_status != \"ok\":\n            response.status_code = status.HTTP_503_SERVICE_UNAVAILABLE\n        \n        total_duration = time.time() - start_time\n        span.set_attribute(\"health.duration_ms\", round(total_duration * 1000, 2))\n        span.set_attribute(\"health.status\", overall_status)\n        \n        return {\n            \"status\": overall_status,\n            \"version\": settings.SERVICE_VERSION,\n            \"environment\": settings.ENVIRONMENT,\n            \"checks\": checks\n        }\n\n@router.get(\"/health/deep\", response_model=HealthStatus, tags=[\"health\"])\nasync def deep_health_check(response: Response):\n    \"\"\"Deep health check that tests all dependencies.\"\"\"\n    with TracingSpan(name=\"health.deep_check\") as span:\n        start_time = time.time()\n        checks = {}\n        overall_status = \"ok\"\n        \n        # Check database with more detailed query\n        try:\n            with TracingSpan(name=\"health.check_database_deep\"):\n                db = Database()\n                db_start = time.time()\n                # Check table counts to verify database is properly populated\n                tables = [\"documents\", \"research_projects\", \"task_queue\"]\n                db_details = {}\n                \n                for table in tables:\n                    count = db.count(table)\n                    db_details[table] = count\n                \n                db_duration = time.time() - db_start\n                \n                checks[\"database\"] = {\n                    \"status\": \"ok\",\n                    \"message\": \"Database checks passed\",\n                    \"duration_ms\": round(db_duration * 1000, 2),\n                    \"details\": db_details\n                }\n        except Exception as e:\n            span.record_exception(e)\n            checks[\"database\"] = {\n                \"status\": \"error\",\n                \"message\": str(e),\n                \"duration_ms\": round((time.time() - start_time) * 1000, 2)\n            }\n            overall_status = \"error\"\n        \n        # Check B2 storage with file listing\n        try:\n            with TracingSpan(name=\"health.check_b2_deep\"):\n                b2_start = time.time()\n                b2_service = B2StorageService()\n                # List files in a test directory\n                files = b2_service.list_files(\"health_check\", max_files=5)\n                b2_duration = time.time() - b2_start\n                \n                checks[\"b2_storage\"] = {\n                    \"status\": \"ok\",\n                    \"message\": \"B2 storage checks passed\",\n                    \"duration_ms\": round(b2_duration * 1000, 2),\n                    \"details\": {\n                        \"files_listed\": len(files)\n                    }\n                }\n        except Exception as e:\n            span.record_exception(e)\n            checks[\"b2_storage\"] = {\n                \"status\": \"error\",\n                \"message\": str(e),\n                \"duration_ms\": round((time.time() - b2_start) * 1000, 2)\n            }\n            overall_status = \"error\"\n        \n        # Check Celery with more details\n        try:\n            with TracingSpan(name=\"health.check_celery_deep\"):\n                celery_start = time.time()\n                # Get stats from workers\n                stats = celery_app.control.inspect().stats()\n                # Get active tasks\n                active = celery_app.control.inspect().active()\n                celery_duration = time.time() - celery_start\n                \n                if stats:\n                    worker_details = {}\n                    for worker, worker_stats in stats.items():\n                        worker_details[worker] = {\n                            \"processed\": worker_stats.get(\"total\", {}).get(\"processed\", 0),\n                            \"active\": len(active.get(worker, [])) if active else 0\n                        }\n                    \n                    checks[\"celery\"] = {\n                        \"status\": \"ok\",\n                        \"message\": f\"Celery checks passed, {len(stats)} workers active\",\n                        \"duration_ms\": round(celery_duration * 1000, 2),\n                        \"details\": {\n                            \"workers\": len(stats),\n                            \"worker_stats\": worker_details\n                        }\n                    }\n                else:\n                    checks[\"celery\"] = {\n                        \"status\": \"warning\",\n                        \"message\": \"No Celery workers responded\",\n                        \"duration_ms\": round(celery_duration * 1000, 2),\n                        \"details\": {\n                            \"workers\": 0\n                        }\n                    }\n        except Exception as e:\n            span.record_exception(e)\n            checks[\"celery\"] = {\n                \"status\": \"error\",\n                \"message\": str(e),\n                \"duration_ms\": round((time.time() - celery_start) * 1000, 2)\n            }\n        \n        # Check external APIs if configured\n        if settings.EXTERNAL_API_HEALTH_CHECKS:\n            for api_name, api_url in settings.EXTERNAL_API_HEALTH_CHECKS.items():\n                try:\n                    with TracingSpan(name=f\"health.check_api.{api_name}\"):\n                        api_start = time.time()\n                        response = requests.get(api_url, timeout=5.0)\n                        api_duration = time.time() - api_start\n                        \n                        if response.status_code < 400:\n                            checks[f\"api_{api_name}\"] = {\n                                \"status\": \"ok\",\n                                \"message\": f\"API {api_name} responded with status {response.status_code}\",\n                                \"duration_ms\": round(api_duration * 1000, 2)\n                            }\n                        else:\n                            checks[f\"api_{api_name}\"] = {\n                                \"status\": \"error\",\n                                \"message\": f\"API {api_name} responded with error status {response.status_code}\",\n                                \"duration_ms\": round(api_duration * 1000, 2)\n                            }\n                            overall_status = \"error\"\n                except Exception as e:\n                    span.record_exception(e)\n                    checks[f\"api_{api_name}\"] = {\n                        \"status\": \"error\",\n                        \"message\": str(e),\n                        \"duration_ms\": round((time.time() - api_start) * 1000, 2)\n                    }\n                    overall_status = \"error\"\n        \n        # Set response status code based on overall status\n        if overall_status != \"ok\":\n            response.status_code = status.HTTP_503_SERVICE_UNAVAILABLE\n        \n        total_duration = time.time() - start_time\n        span.set_attribute(\"health.duration_ms\", round(total_duration * 1000, 2))\n        span.set_attribute(\"health.status\", overall_status)\n        \n        return {\n            \"status\": overall_status,\n            \"version\": settings.SERVICE_VERSION,\n            \"environment\": settings.ENVIRONMENT,\n            \"checks\": checks\n        }\n```\n\n2. Add a method to check B2 connection in `app/services/b2_storage_service.py`:\n\n```python\ndef check_connection(self):\n    \"\"\"Check if B2 connection is working.\"\"\"\n    try:\n        # Try to get account info or list buckets as a simple check\n        self.b2_api.get_bucket_by_name(self.bucket_name)\n        return True\n    except Exception as e:\n        raise ConnectionError(f\"B2 connection failed: {str(e)}\")\n\ndef list_files(self, prefix, max_files=100):\n    \"\"\"List files in B2 with the given prefix.\"\"\"\n    bucket = self.b2_api.get_bucket_by_name(self.bucket_name)\n    files = []\n    \n    for file_info, _ in bucket.ls(prefix, max_count=max_files):\n        files.append({\n            \"file_name\": file_info.file_name,\n            \"size\": file_info.size,\n            \"content_type\": file_info.content_type,\n            \"upload_timestamp\": file_info.upload_timestamp\n        })\n    \n    return files\n```\n\n3. Update `app/core/config.py` to include health check settings:\n\n```python\nclass Settings(BaseSettings):\n    # ... existing settings ...\n    \n    # Health check settings\n    EXTERNAL_API_HEALTH_CHECKS: Dict[str, str] = {\n        \"llama_index\": \"https://api.llamaindex.example.com/health\",\n        \"embedding\": \"https://api.embedding.example.com/health\"\n    }\n    HEALTH_CHECK_TIMEOUT: float = 5.0\n```",
        "testStrategy": "1. Unit tests:\n   - Test each health check endpoint\n   - Test timeout handling for dependencies\n   - Test error handling for various failure scenarios\n\n2. Integration tests:\n   - Test health checks with actual dependencies\n   - Test behavior when dependencies are down\n   - Test response formats and status codes\n\n3. Load tests:\n   - Test health check performance under load\n   - Verify health checks don't impact main application performance\n   - Test with concurrent health check requests",
        "priority": "high",
        "dependencies": [
          "189"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create health check models and base structure",
            "description": "Create the base health check models and structure in app/core/health.py",
            "dependencies": [],
            "details": "Create a new file app/core/health.py that defines the Pydantic models for health check responses. Include HealthStatus model with status, version, environment, and checks fields. Also implement base health check utility functions for timeout handling and dependency status aggregation.",
            "status": "pending",
            "testStrategy": "Unit test the health check models with various status combinations. Test timeout utility functions with mocked dependencies.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement liveness probe endpoint",
            "description": "Implement the /health/liveness endpoint that checks if the application is running",
            "dependencies": [
              1
            ],
            "details": "Create the liveness probe endpoint in app/routes/health.py that checks only if the application is running without checking external dependencies. This endpoint should be lightweight and fast, returning a 200 status code if the application is responsive. Include basic application information in the response.",
            "status": "pending",
            "testStrategy": "Unit test the liveness endpoint response format and status codes. Verify it doesn't make any external calls to dependencies.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement readiness probe endpoint",
            "description": "Implement the /health/readiness endpoint that checks critical dependencies",
            "dependencies": [
              1,
              2
            ],
            "details": "Create the readiness probe endpoint in app/routes/health.py that checks if all critical dependencies (database, B2 storage) are available and the application is ready to receive traffic. Implement timeout handling for dependency checks and return appropriate status codes (503 if any dependency is unavailable).",
            "status": "pending",
            "testStrategy": "Test the readiness endpoint with mocked dependencies in various states (available, unavailable, slow). Verify timeout handling and correct status code responses.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement deep health check endpoint",
            "description": "Implement the /health/deep endpoint for comprehensive dependency checking",
            "dependencies": [
              3
            ],
            "details": "Create the deep health check endpoint in app/routes/health.py that performs comprehensive checks on all dependencies including database table counts, B2 storage file listing, Celery worker stats, and external API checks. Implement detailed reporting of dependency status with timing information and proper error handling.",
            "status": "pending",
            "testStrategy": "Test the deep health check endpoint with various dependency configurations. Verify it correctly reports detailed status information and handles errors appropriately.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement dependency-specific health checks",
            "description": "Implement health check methods for Supabase, Neo4j, Redis, and B2 dependencies",
            "dependencies": [
              1
            ],
            "details": "Add health check methods to the respective service classes for each dependency: Database class for Supabase checks, Neo4jService for graph database checks, RedisService for cache checks, and B2StorageService for object storage checks. Each method should verify connectivity and basic functionality with appropriate timeout handling.",
            "status": "pending",
            "testStrategy": "Unit test each dependency health check method with mocked responses. Test error handling and timeout scenarios for each dependency type.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-17T21:08:45.082Z"
      },
      {
        "id": "191",
        "title": "Implement B2 Integration for Document Management Service",
        "description": "Complete the B2 storage integration in the document management service by implementing upload, deletion, reprocessing, and embedding regeneration functionality.",
        "details": "This task involves completing 4 TODOs in app/services/document_management.py:\n\n1. Implement B2 upload in `upload_document()` using existing B2StorageService:\n```python\ndef upload_document(file, metadata, project_id):\n    # Generate unique file path in B2\n    file_path = f\"documents/{project_id}/{uuid.uuid4()}/{file.filename}\"\n    \n    # Upload to B2 using existing service\n    b2_storage = B2StorageService()\n    b2_url = b2_storage.upload_file(file.file, file_path, file.content_type)\n    \n    # Store metadata in Supabase\n    document_record = {\n        \"filename\": file.filename,\n        \"file_path\": file_path,\n        \"b2_url\": b2_url,\n        \"project_id\": project_id,\n        \"metadata\": metadata,\n        \"status\": \"uploaded\"\n    }\n    db.table(\"documents\").insert(document_record).execute()\n    \n    # Trigger async processing\n    process_document.delay(document_record[\"id\"])\n    \n    return document_record\n```\n\n2. Implement B2 deletion in `delete_document()`:\n```python\ndef delete_document(document_id):\n    # Get document record\n    document = db.table(\"documents\").select(\"*\").eq(\"id\", document_id).single().execute()\n    \n    if not document:\n        raise DocumentNotFoundError(f\"Document {document_id} not found\")\n    \n    # Delete from B2\n    b2_storage = B2StorageService()\n    b2_storage.delete_file(document[\"file_path\"])\n    \n    # Delete from database\n    db.table(\"documents\").delete().eq(\"id\", document_id).execute()\n    \n    return {\"success\": True, \"message\": f\"Document {document_id} deleted\"}\n```\n\n3. Implement text re-extraction in `reprocess_document()` when `force_reparse=True`:\n```python\ndef reprocess_document(document_id, force_reparse=False, update_embeddings=False):\n    # Get document record\n    document = db.table(\"documents\").select(\"*\").eq(\"id\", document_id).single().execute()\n    \n    if not document:\n        raise DocumentNotFoundError(f\"Document {document_id} not found\")\n    \n    if force_reparse:\n        # Download from B2\n        b2_storage = B2StorageService()\n        file_content = b2_storage.download_file(document[\"file_path\"])\n        \n        # Use LlamaIndex to extract text\n        llama_service = LlamaIndexService()\n        extracted_text = llama_service.extract_text(file_content, document[\"filename\"])\n        \n        # Update document record\n        db.table(\"documents\").update({\"extracted_text\": extracted_text, \"last_processed\": datetime.now()}).eq(\"id\", document_id).execute()\n```\n\n4. Implement embedding regeneration when `update_embeddings=True`:\n```python\n    if update_embeddings or force_reparse:\n        # Get latest text\n        updated_doc = db.table(\"documents\").select(\"*\").eq(\"id\", document_id).single().execute()\n        \n        # Generate embeddings\n        embedding_service = EmbeddingService()\n        embeddings = embedding_service.generate_embeddings(updated_doc[\"extracted_text\"])\n        \n        # Store embeddings\n        db.table(\"document_embeddings\").upsert({\n            \"document_id\": document_id,\n            \"embeddings\": embeddings,\n            \"updated_at\": datetime.now()\n        }).execute()\n    \n    return {\"success\": True, \"message\": f\"Document {document_id} reprocessed\"}\n```",
        "testStrategy": "1. Unit tests for each function with mocked B2StorageService, LlamaIndexService, and EmbeddingService\n2. Integration tests with actual services in a test environment\n3. Test cases:\n   - Upload: Verify file uploads to B2 and metadata stored in Supabase\n   - Delete: Verify file removed from B2 and database\n   - Reprocess: Test with force_reparse=True and update_embeddings=True\n   - Error handling: Test with invalid document IDs, B2 service failures\n4. Verify proper folder structure in B2: documents/{project_id}/{uuid}/{filename}",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:30:00.424Z"
      },
      {
        "id": "192",
        "title": "Implement Research Tasks Report Generation",
        "description": "Complete the report generation functionality in the research tasks module by implementing the actual report generation using ReportExecutor and connecting to B2 for report storage.",
        "details": "This task involves completing 2 TODOs in app/tasks/research_tasks.py:\n\n1. Implement actual report generation using ReportExecutor:\n```python\ndef generate_research_report(project_id, task_id, report_type=\"summary\"):\n    # Get project and task data\n    project = db.table(\"research_projects\").select(\"*\").eq(\"id\", project_id).single().execute()\n    task = db.table(\"research_tasks\").select(\"*\").eq(\"id\", task_id).single().execute()\n    \n    if not project or not task:\n        raise ValueError(f\"Project or task not found: {project_id}, {task_id}\")\n    \n    # Get relevant documents and data\n    documents = db.table(\"documents\")\\\n        .select(\"*\")\\\n        .eq(\"project_id\", project_id)\\\n        .eq(\"status\", \"processed\")\\\n        .execute()\n    \n    # Initialize report executor\n    report_executor = ReportExecutor(\n        model=\"claude-3-opus-20240229\",  # Use latest Claude model\n        max_tokens=100000\n    )\n    \n    # Generate report based on type\n    if report_type == \"summary\":\n        report_content = report_executor.generate_summary_report(\n            project=project,\n            task=task,\n            documents=documents\n        )\n    elif report_type == \"detailed\":\n        report_content = report_executor.generate_detailed_report(\n            project=project,\n            task=task,\n            documents=documents\n        )\n    else:\n        raise ValueError(f\"Unknown report type: {report_type}\")\n    \n    # Format report as markdown\n    formatted_report = f\"# {task['title']} Report\\n\\n{report_content}\"\n    \n    return formatted_report\n```\n\n2. Connect to B2 for report storage:\n```python\ndef store_report(project_id, task_id, report_content, report_type=\"summary\"):\n    # Generate filename and path\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f\"{report_type}_report_{timestamp}.md\"\n    file_path = f\"reports/{project_id}/{task_id}/{filename}\"\n    \n    # Convert report content to bytes\n    report_bytes = report_content.encode('utf-8')\n    content_stream = io.BytesIO(report_bytes)\n    \n    # Upload to B2\n    b2_storage = B2StorageService()\n    b2_url = b2_storage.upload_file(\n        content_stream, \n        file_path, \n        content_type=\"text/markdown\"\n    )\n    \n    # Store metadata in Supabase\n    report_record = {\n        \"project_id\": project_id,\n        \"task_id\": task_id,\n        \"report_type\": report_type,\n        \"filename\": filename,\n        \"file_path\": file_path,\n        \"b2_url\": b2_url,\n        \"created_at\": datetime.now()\n    }\n    db.table(\"research_reports\").insert(report_record).execute()\n    \n    return report_record\n```\n\nUpdate the main task function to use these new functions:\n```python\n@celery_app.task\ndef generate_and_store_report(project_id, task_id, report_type=\"summary\"):\n    try:\n        # Generate report content\n        report_content = generate_research_report(project_id, task_id, report_type)\n        \n        # Store report in B2\n        report_record = store_report(project_id, task_id, report_content, report_type)\n        \n        # Update task status\n        db.table(\"research_tasks\")\\\n            .update({\"report_status\": \"completed\", \"report_id\": report_record[\"id\"]})\\\n            .eq(\"id\", task_id)\\\n            .execute()\n            \n        return {\"success\": True, \"report_id\": report_record[\"id\"]}\n    except Exception as e:\n        # Update task status on failure\n        db.table(\"research_tasks\")\\\n            .update({\"report_status\": \"failed\", \"report_error\": str(e)})\\\n            .eq(\"id\", task_id)\\\n            .execute()\n        raise\n```",
        "testStrategy": "1. Unit tests for each function with mocked ReportExecutor and B2StorageService\n2. Integration tests with actual services in a test environment\n3. Test cases:\n   - Report generation: Test with different report types\n   - B2 storage: Verify reports stored in correct location with proper naming\n   - Error handling: Test with invalid project/task IDs, service failures\n4. Verify proper folder structure in B2: reports/{project_id}/{task_id}/{filename}\n5. Test Celery task execution and error handling\n6. Verify report metadata stored correctly in database",
        "priority": "high",
        "dependencies": [
          "191"
        ],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:30:00.522Z"
      },
      {
        "id": "193",
        "title": "Implement Retrieval Executor Service Integrations",
        "description": "Complete the Retrieval Executor service by integrating NLQ service for natural language queries, Neo4j service for graph retrieval, and implementing external API retrieval.",
        "details": "This task involves completing 3 TODOs in app/services/task_executors/retrieval_executor.py:\n\n1. Integrate NLQ service for natural language queries:\n```python\nclass RetrievalExecutor:\n    def __init__(self, db_client=None, neo4j_client=None, nlq_service=None):\n        self.db_client = db_client or get_supabase_client()\n        self.neo4j_client = neo4j_client or Neo4jClient()\n        self.nlq_service = nlq_service or NLQService()\n    \n    def execute_natural_language_query(self, query, context=None):\n        \"\"\"Execute a natural language query against the database\"\"\"\n        try:\n            # Use NLQ service to translate natural language to SQL\n            sql_query = self.nlq_service.translate_to_sql(query, context)\n            \n            # Log the generated SQL for debugging\n            logger.debug(f\"Generated SQL: {sql_query}\")\n            \n            # Execute the SQL query\n            result = self.db_client.rpc(\"execute_raw_query\", {\"query\": sql_query}).execute()\n            \n            # Format and return results\n            return {\n                \"success\": True,\n                \"query\": query,\n                \"sql\": sql_query,\n                \"results\": result.data\n            }\n        except Exception as e:\n            logger.error(f\"NLQ query failed: {str(e)}\")\n            return {\n                \"success\": False,\n                \"query\": query,\n                \"error\": str(e)\n            }\n```\n\n2. Integrate Neo4j service for graph retrieval:\n```python\n    def execute_graph_query(self, query_type, parameters):\n        \"\"\"Execute a graph query against Neo4j\"\"\"\n        try:\n            # Map query types to Neo4j Cypher queries\n            query_templates = {\n                \"related_entities\": \"\"\"\n                MATCH (n {id: $entity_id})-[r]-(m)\n                RETURN n, r, m\n                LIMIT $limit\n                \"\"\",\n                \"shortest_path\": \"\"\"\n                MATCH p=shortestPath((a {id: $source_id})-[*]-(b {id: $target_id}))\n                RETURN p\n                \"\"\",\n                \"entity_search\": \"\"\"\n                MATCH (n)\n                WHERE n.name CONTAINS $search_term\n                RETURN n\n                LIMIT $limit\n                \"\"\"\n            }\n            \n            # Get the appropriate query template\n            if query_type not in query_templates:\n                raise ValueError(f\"Unknown graph query type: {query_type}\")\n                \n            cypher_query = query_templates[query_type]\n            \n            # Execute the query\n            result = self.neo4j_client.execute_query(cypher_query, parameters)\n            \n            # Process and format the results\n            formatted_result = self._format_graph_results(result, query_type)\n            \n            return {\n                \"success\": True,\n                \"query_type\": query_type,\n                \"results\": formatted_result\n            }\n        except Exception as e:\n            logger.error(f\"Graph query failed: {str(e)}\")\n            return {\n                \"success\": False,\n                \"query_type\": query_type,\n                \"error\": str(e)\n            }\n            \n    def _format_graph_results(self, result, query_type):\n        \"\"\"Format Neo4j results based on query type\"\"\"\n        if query_type == \"related_entities\":\n            return [{\n                \"source\": self._node_to_dict(record[\"n\"]),\n                \"relationship\": self._rel_to_dict(record[\"r\"]),\n                \"target\": self._node_to_dict(record[\"m\"])\n            } for record in result]\n        elif query_type == \"shortest_path\":\n            # Extract nodes and relationships from path\n            path = result[0][\"p\"]\n            return {\n                \"nodes\": [self._node_to_dict(node) for node in path.nodes],\n                \"relationships\": [self._rel_to_dict(rel) for rel in path.relationships]\n            }\n        elif query_type == \"entity_search\":\n            return [self._node_to_dict(record[\"n\"]) for record in result]\n        \n        return result\n        \n    def _node_to_dict(self, node):\n        \"\"\"Convert Neo4j node to dictionary\"\"\"\n        return {\n            \"id\": node.id,\n            \"labels\": list(node.labels),\n            \"properties\": dict(node)\n        }\n        \n    def _rel_to_dict(self, relationship):\n        \"\"\"Convert Neo4j relationship to dictionary\"\"\"\n        return {\n            \"id\": relationship.id,\n            \"type\": relationship.type,\n            \"properties\": dict(relationship)\n        }\n```\n\n3. Implement external API retrieval:\n```python\n    def execute_api_retrieval(self, api_config, parameters):\n        \"\"\"Execute retrieval from external API\"\"\"\n        try:\n            # Get API configuration\n            if isinstance(api_config, str):\n                # Look up predefined API config by name\n                config = self.db_client.table(\"api_configs\").select(\"*\").eq(\"name\", api_config).single().execute()\n                if not config.data:\n                    raise ValueError(f\"API configuration not found: {api_config}\")\n                api_config = config.data\n            \n            # Extract API details\n            url = api_config[\"base_url\"]\n            method = api_config.get(\"method\", \"GET\").upper()\n            headers = api_config.get(\"headers\", {})\n            auth = None\n            \n            # Handle authentication if specified\n            if \"auth\" in api_config:\n                auth_type = api_config[\"auth\"].get(\"type\")\n                if auth_type == \"basic\":\n                    auth = (api_config[\"auth\"][\"username\"], api_config[\"auth\"][\"password\"])\n                elif auth_type == \"bearer\":\n                    headers[\"Authorization\"] = f\"Bearer {api_config['auth']['token']}\"\n            \n            # Replace URL parameters\n            for key, value in parameters.items():\n                placeholder = f\"{{{key}}}\"\n                if placeholder in url:\n                    url = url.replace(placeholder, str(value))\n            \n            # Make the request\n            response = requests.request(\n                method=method,\n                url=url,\n                headers=headers,\n                auth=auth,\n                params=parameters if method == \"GET\" else None,\n                json=parameters if method != \"GET\" else None,\n                timeout=30\n            )\n            \n            # Check for successful response\n            response.raise_for_status()\n            \n            # Parse response based on content type\n            content_type = response.headers.get(\"Content-Type\", \"\")\n            if \"application/json\" in content_type:\n                result = response.json()\n            else:\n                result = response.text\n            \n            return {\n                \"success\": True,\n                \"api\": api_config.get(\"name\", \"custom\"),\n                \"status_code\": response.status_code,\n                \"results\": result\n            }\n        except requests.RequestException as e:\n            logger.error(f\"API retrieval failed: {str(e)}\")\n            return {\n                \"success\": False,\n                \"api\": api_config.get(\"name\", \"custom\"),\n                \"error\": str(e),\n                \"status_code\": e.response.status_code if hasattr(e, \"response\") else None\n            }\n        except Exception as e:\n            logger.error(f\"API retrieval failed: {str(e)}\")\n            return {\n                \"success\": False,\n                \"api\": api_config.get(\"name\", \"custom\"),\n                \"error\": str(e)\n            }\n```",
        "testStrategy": "1. Unit tests for each retrieval method with mocked dependencies\n   - Test NLQ service with various natural language queries\n   - Test Neo4j integration with different query types\n   - Test API retrieval with various configurations\n2. Integration tests with actual services in test environment\n3. Test cases:\n   - NLQ: Test translation to SQL and execution\n   - Graph: Test related entities, shortest path, and entity search\n   - API: Test with different HTTP methods, auth types, and response formats\n4. Error handling tests:\n   - Invalid queries\n   - Neo4j connection failures\n   - API timeouts and error responses\n5. Performance tests for complex queries",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:30:00.617Z"
      },
      {
        "id": "194",
        "title": "Implement Upload API B2 Verification",
        "description": "Enhance the upload API endpoint to verify file existence in B2 before returning success and check processing status from the database.",
        "details": "This task involves completing 2 TODOs in app/api/upload.py:\n\n1. Verify file exists in B2 before returning success:\n```python\n@router.post(\"/upload\")\nasync def upload_file(\n    file: UploadFile = File(...),\n    project_id: str = Form(...),\n    metadata: str = Form(\"{}\"),\n    current_user: User = Depends(get_current_user)\n):\n    try:\n        # Parse metadata JSON\n        metadata_dict = json.loads(metadata)\n        \n        # Upload document using service\n        document_service = DocumentManagementService()\n        document = document_service.upload_document(file, metadata_dict, project_id)\n        \n        # Verify file exists in B2\n        b2_storage = B2StorageService()\n        file_exists = b2_storage.verify_file_exists(document[\"file_path\"])\n        \n        if not file_exists:\n            # If verification fails, delete the document record and raise error\n            db.table(\"documents\").delete().eq(\"id\", document[\"id\"]).execute()\n            raise HTTPException(\n                status_code=500,\n                detail=\"File upload to B2 failed verification\"\n            )\n        \n        return {\n            \"success\": True,\n            \"document_id\": document[\"id\"],\n            \"status\": document[\"status\"],\n            \"message\": \"File uploaded successfully and processing started\"\n        }\n    except json.JSONDecodeError:\n        raise HTTPException(\n            status_code=400,\n            detail=\"Invalid metadata JSON format\"\n        )\n    except Exception as e:\n        logger.error(f\"Upload failed: {str(e)}\")\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Upload failed: {str(e)}\"\n        )\n```\n\n2. Check processing status from database:\n```python\n@router.get(\"/upload/{document_id}/status\")\nasync def get_upload_status(\n    document_id: str,\n    current_user: User = Depends(get_current_user)\n):\n    try:\n        # Get document record\n        document = db.table(\"documents\").select(\"*\").eq(\"id\", document_id).single().execute()\n        \n        if not document.data:\n            raise HTTPException(\n                status_code=404,\n                detail=f\"Document {document_id} not found\"\n            )\n        \n        # Check if user has access to this document\n        project_id = document.data[\"project_id\"]\n        project_access = db.table(\"project_members\")\\\n            .select(\"*\")\\\n            .eq(\"project_id\", project_id)\\\n            .eq(\"user_id\", current_user.id)\\\n            .execute()\n            \n        if not project_access.data and not current_user.is_admin:\n            raise HTTPException(\n                status_code=403,\n                detail=\"You don't have access to this document\"\n            )\n        \n        # Get processing status\n        status = document.data[\"status\"]\n        processing_details = {}\n        \n        # Get additional processing details if available\n        if status == \"processing\":\n            task_status = db.table(\"document_processing_tasks\")\\\n                .select(\"*\")\\\n                .eq(\"document_id\", document_id)\\\n                .order(\"created_at\", desc=True)\\\n                .limit(1)\\\n                .execute()\n                \n            if task_status.data:\n                processing_details = {\n                    \"current_step\": task_status.data[0][\"current_step\"],\n                    \"progress\": task_status.data[0][\"progress\"],\n                    \"started_at\": task_status.data[0][\"created_at\"],\n                }\n        \n        return {\n            \"document_id\": document_id,\n            \"status\": status,\n            \"filename\": document.data[\"filename\"],\n            \"processing_details\": processing_details,\n            \"last_updated\": document.data[\"updated_at\"]\n        }\n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Failed to get upload status: {str(e)}\")\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to get upload status: {str(e)}\"\n        )\n```\n\nAdd the verify_file_exists method to B2StorageService if it doesn't exist:\n```python\ndef verify_file_exists(self, file_path):\n    \"\"\"Verify a file exists in B2\"\"\"\n    try:\n        # Get file info from B2\n        file_info = self.b2_api.get_file_info_by_name(\n            self.bucket_name,\n            file_path\n        )\n        return True\n    except Exception as e:\n        logger.error(f\"B2 file verification failed: {str(e)}\")\n        return False\n```",
        "testStrategy": "1. Unit tests for upload endpoint with mocked B2StorageService\n2. Unit tests for status endpoint with mocked database responses\n3. Integration tests with actual B2 storage in test environment\n4. Test cases:\n   - Successful upload with B2 verification\n   - Failed B2 verification (should clean up database record)\n   - Status check for various document states (uploaded, processing, completed, failed)\n   - Authorization checks for status endpoint\n5. Error handling tests:\n   - Invalid document IDs\n   - B2 service failures\n   - Database errors",
        "priority": "high",
        "dependencies": [
          "191"
        ],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:30:00.722Z"
      },
      {
        "id": "195",
        "title": "Implement LangGraph Tool Calling",
        "description": "Implement proper tool calling with LLM.bind_tools() in the LangGraph workflows to enable effective tool usage within the workflow.",
        "details": "This task involves completing 1 TODO in app/workflows/langgraph_workflows.py:\n\n```python\nfrom langchain_core.tools import tool\nfrom langchain_core.language_models import BaseChatModel\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, END\nfrom typing import Dict, List, Any, Optional, TypedDict, Annotated\n\n# Define tool schemas\nclass SearchTool(TypedDict):\n    \"\"\"Tool for searching documents\"\"\"\n    query: str\n    filters: Optional[Dict[str, Any]]\n\nclass CalculateTool(TypedDict):\n    \"\"\"Tool for performing calculations\"\"\"\n    expression: str\n\nclass RetrieveTool(TypedDict):\n    \"\"\"Tool for retrieving specific information\"\"\"\n    entity_id: str\n    fields: Optional[List[str]]\n\n# Implement tools\n@tool\ndef search_documents(args: SearchTool) -> Dict[str, Any]:\n    \"\"\"Search for documents matching the query and filters\"\"\"\n    query = args[\"query\"]\n    filters = args.get(\"filters\", {})\n    \n    # Implement actual search logic\n    # This is a placeholder\n    results = [{\"id\": \"doc1\", \"title\": \"Example Document\", \"score\": 0.95}]\n    \n    return {\"results\": results, \"count\": len(results)}\n\n@tool\ndef calculate(args: CalculateTool) -> Dict[str, Any]:\n    \"\"\"Evaluate a mathematical expression\"\"\"\n    expression = args[\"expression\"]\n    \n    try:\n        # Use safer eval with restricted globals\n        result = eval(expression, {\"__builtins__\": {}}, {\"math\": __import__(\"math\")})\n        return {\"result\": result, \"expression\": expression}\n    except Exception as e:\n        return {\"error\": str(e), \"expression\": expression}\n\n@tool\ndef retrieve_entity(args: RetrieveTool) -> Dict[str, Any]:\n    \"\"\"Retrieve information about a specific entity\"\"\"\n    entity_id = args[\"entity_id\"]\n    fields = args.get(\"fields\", [\"name\", \"description\"])\n    \n    # Implement actual retrieval logic\n    # This is a placeholder\n    entity = {\"id\": entity_id, \"name\": \"Example Entity\", \"description\": \"This is an example entity\"}\n    \n    # Filter to requested fields\n    result = {field: entity.get(field) for field in fields if field in entity}\n    \n    return {\"entity\": result}\n\n# Define available tools\nAVAILABLE_TOOLS = [search_documents, calculate, retrieve_entity]\n\n# Define state schema\nclass WorkflowState(TypedDict):\n    \"\"\"State for the workflow\"\"\"\n    question: str\n    context: Optional[Dict[str, Any]]\n    tools_output: Optional[List[Dict[str, Any]]]\n    answer: Optional[str]\n\n# Define workflow nodes\ndef analyze_question(state: WorkflowState, llm: BaseChatModel) -> Dict[str, Any]:\n    \"\"\"Analyze the question and determine next steps\"\"\"\n    question = state[\"question\"]\n    context = state.get(\"context\", {})\n    \n    # Bind tools to the LLM\n    llm_with_tools = llm.bind_tools(AVAILABLE_TOOLS)\n    \n    # Create prompt for analysis\n    prompt = f\"\"\"Analyze this question and determine if you need to use tools to answer it.\n    Question: {question}\n    \n    If you need to use tools, use them directly. If you can answer without tools, provide the answer.\n    \"\"\"\n    \n    # Get response from LLM\n    response = llm_with_tools.invoke(prompt)\n    \n    # Check if tools were used\n    tool_calls = getattr(response, \"tool_calls\", [])\n    \n    if tool_calls:\n        return {\"next\": \"execute_tools\", \"tool_calls\": tool_calls}\n    else:\n        return {\"next\": \"generate_answer\", \"direct_answer\": response.content}\n\ndef execute_tools(state: WorkflowState) -> Dict[str, Any]:\n    \"\"\"Execute the tools requested by the LLM\"\"\"\n    tool_calls = state.get(\"tool_calls\", [])\n    tools_output = []\n    \n    for tool_call in tool_calls:\n        tool_name = tool_call[\"name\"]\n        tool_args = tool_call[\"args\"]\n        \n        # Find the matching tool\n        matching_tool = next((t for t in AVAILABLE_TOOLS if t.name == tool_name), None)\n        \n        if matching_tool:\n            try:\n                # Execute the tool\n                result = matching_tool(tool_args)\n                tools_output.append({\n                    \"tool\": tool_name,\n                    \"args\": tool_args,\n                    \"result\": result,\n                    \"success\": True\n                })\n            except Exception as e:\n                tools_output.append({\n                    \"tool\": tool_name,\n                    \"args\": tool_args,\n                    \"error\": str(e),\n                    \"success\": False\n                })\n    \n    return {\"next\": \"generate_answer\", \"tools_output\": tools_output}\n\ndef generate_answer(state: WorkflowState, llm: BaseChatModel) -> Dict[str, Any]:\n    \"\"\"Generate the final answer based on tool outputs and context\"\"\"\n    question = state[\"question\"]\n    tools_output = state.get(\"tools_output\", [])\n    direct_answer = state.get(\"direct_answer\")\n    \n    if direct_answer:\n        # If we already have a direct answer, use it\n        return {\"next\": END, \"answer\": direct_answer}\n    \n    # Create prompt with tool outputs\n    tools_output_str = \"\\n\".join([f\"Tool: {o['tool']}\\nResult: {o['result']}\" for o in tools_output if o.get(\"success\")])\n    \n    prompt = f\"\"\"Based on the following tool outputs, answer the original question.\n    \n    Question: {question}\n    \n    Tool Outputs:\n    {tools_output_str}\n    \n    Provide a comprehensive answer based on this information.\n    \"\"\"\n    \n    # Get response from LLM\n    response = llm.invoke(prompt)\n    \n    return {\"next\": END, \"answer\": response.content}\n\n# Create the workflow\ndef create_research_workflow(llm: Optional[BaseChatModel] = None) -> StateGraph:\n    \"\"\"Create a research workflow with tool calling\"\"\"\n    # Use provided LLM or default to OpenAI\n    workflow_llm = llm or ChatOpenAI(model=\"gpt-4-turbo\")\n    \n    # Create workflow graph\n    workflow = StateGraph(WorkflowState)\n    \n    # Add nodes\n    workflow.add_node(\"analyze_question\", lambda state: analyze_question(state, workflow_llm))\n    workflow.add_node(\"execute_tools\", execute_tools)\n    workflow.add_node(\"generate_answer\", lambda state: generate_answer(state, workflow_llm))\n    \n    # Add edges\n    workflow.add_edge(\"analyze_question\", \"execute_tools\")\n    workflow.add_edge(\"analyze_question\", \"generate_answer\")\n    workflow.add_edge(\"execute_tools\", \"generate_answer\")\n    \n    # Set entry point\n    workflow.set_entry_point(\"analyze_question\")\n    \n    return workflow.compile()\n\n# Example usage\ndef run_research_workflow(question: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    \"\"\"Run the research workflow with a question\"\"\"\n    workflow = create_research_workflow()\n    \n    # Initialize state\n    initial_state = {\"question\": question, \"context\": context or {}}\n    \n    # Run workflow\n    result = workflow.invoke(initial_state)\n    \n    return result\n```",
        "testStrategy": "1. Unit tests for each tool function with various inputs\n2. Unit tests for workflow nodes (analyze_question, execute_tools, generate_answer)\n3. Integration tests for the complete workflow\n4. Test cases:\n   - Questions that can be answered directly\n   - Questions requiring tool usage\n   - Questions requiring multiple tools\n   - Error handling in tool execution\n5. Test with different LLM models to ensure compatibility\n6. Verify tool binding works correctly with different LangChain versions\n7. Performance testing with complex queries requiring multiple tool calls",
        "priority": "medium",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:30:00.818Z"
      },
      {
        "id": "196",
        "title": "Implement WebSocket Authentication",
        "description": "Implement proper WebSocket authentication in the research projects route to ensure secure WebSocket connections with valid JWT tokens.",
        "details": "This task involves completing 1 TODO in app/routes/research_projects.py to implement proper WebSocket authentication:\n\n```python\nfrom fastapi import APIRouter, Depends, HTTPException, WebSocket, WebSocketDisconnect, Query, status\nfrom typing import Optional, List, Dict, Any\nfrom app.core.auth import get_current_user, decode_jwt_token\nfrom app.models.user import User\nfrom app.services.research_project_service import ResearchProjectService\nimport json\n\nrouter = APIRouter()\n\n# WebSocket connection manager\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: Dict[str, List[WebSocket]] = {}\n\n    async def connect(self, websocket: WebSocket, project_id: str):\n        await websocket.accept()\n        if project_id not in self.active_connections:\n            self.active_connections[project_id] = []\n        self.active_connections[project_id].append(websocket)\n\n    def disconnect(self, websocket: WebSocket, project_id: str):\n        if project_id in self.active_connections:\n            if websocket in self.active_connections[project_id]:\n                self.active_connections[project_id].remove(websocket)\n            if not self.active_connections[project_id]:\n                del self.active_connections[project_id]\n\n    async def broadcast(self, message: str, project_id: str):\n        if project_id in self.active_connections:\n            for connection in self.active_connections[project_id]:\n                await connection.send_text(message)\n\nmanager = ConnectionManager()\n\n@router.websocket(\"/ws/projects/{project_id}\")\nasync def websocket_endpoint(websocket: WebSocket, project_id: str, token: str = Query(...)):\n    # Implement WebSocket authentication\n    try:\n        # Decode and validate JWT token\n        payload = decode_jwt_token(token)\n        if not payload:\n            await websocket.close(code=status.WS_1008_POLICY_VIOLATION, reason=\"Invalid authentication token\")\n            return\n            \n        # Extract user information from token\n        user_id = payload.get(\"sub\")\n        if not user_id:\n            await websocket.close(code=status.WS_1008_POLICY_VIOLATION, reason=\"Invalid user token\")\n            return\n            \n        # Verify user has access to this project\n        service = ResearchProjectService()\n        has_access = service.check_user_project_access(user_id, project_id)\n        \n        if not has_access:\n            await websocket.close(code=status.WS_1008_POLICY_VIOLATION, reason=\"Unauthorized access to project\")\n            return\n            \n        # Accept the connection if authentication is successful\n        await manager.connect(websocket, project_id)\n        \n        try:\n            # Send initial connection confirmation\n            await websocket.send_text(json.dumps({\n                \"type\": \"connection_established\",\n                \"project_id\": project_id\n            }))\n            \n            # Handle incoming messages\n            while True:\n                data = await websocket.receive_text()\n                message = json.loads(data)\n                \n                # Process message based on type\n                if message.get(\"type\") == \"ping\":\n                    await websocket.send_text(json.dumps({\"type\": \"pong\"}))\n                elif message.get(\"type\") == \"request_update\":\n                    # Get latest project data\n                    project_data = service.get_project_updates(project_id)\n                    await websocket.send_text(json.dumps({\n                        \"type\": \"project_update\",\n                        \"data\": project_data\n                    }))\n                else:\n                    # For other message types, broadcast to all project connections\n                    await manager.broadcast(data, project_id)\n                    \n        except WebSocketDisconnect:\n            manager.disconnect(websocket, project_id)\n            \n    except Exception as e:\n        # Log the error\n        print(f\"WebSocket error: {str(e)}\")\n        # Close with error\n        try:\n            await websocket.close(code=status.WS_1011_INTERNAL_ERROR, reason=\"Internal server error\")\n        except:\n            pass\n\n# Add the decode_jwt_token function to app/core/auth.py if not already present\n\"\"\"\nfrom jose import jwt, JWTError\nfrom app.core.config import settings\n\ndef decode_jwt_token(token: str):\n    \"\"\"Decode and validate JWT token\"\"\"\n    try:\n        payload = jwt.decode(\n            token,\n            settings.JWT_SECRET_KEY,\n            algorithms=[settings.JWT_ALGORITHM]\n        )\n        return payload\n    except JWTError:\n        return None\n\"\"\"\n\n# Add the check_user_project_access method to ResearchProjectService if not already present\n\"\"\"\ndef check_user_project_access(self, user_id: str, project_id: str) -> bool:\n    \"\"\"Check if a user has access to a specific project\"\"\"\n    try:\n        # Check if user is project owner\n        project = self.db.table(\"research_projects\")\\\n            .select(\"owner_id\")\\\n            .eq(\"id\", project_id)\\\n            .single()\\\n            .execute()\n            \n        if project.data and project.data[\"owner_id\"] == user_id:\n            return True\n            \n        # Check if user is project member\n        member = self.db.table(\"project_members\")\\\n            .select(\"*\")\\\n            .eq(\"project_id\", project_id)\\\n            .eq(\"user_id\", user_id)\\\n            .execute()\n            \n        return len(member.data) > 0\n    except Exception as e:\n        print(f\"Error checking project access: {str(e)}\")\n        return False\n\"\"\"\n```",
        "testStrategy": "1. Unit tests for WebSocket authentication with various token scenarios\n   - Valid token with project access\n   - Valid token without project access\n   - Invalid token\n   - Expired token\n   - Malformed token\n2. Integration tests for WebSocket connections\n3. Test cases:\n   - Connection establishment with proper authentication\n   - Message handling for different message types\n   - Disconnection handling\n   - Broadcast functionality\n4. Security testing:\n   - Attempt to connect without token\n   - Attempt to connect with token for different project\n   - Test token tampering\n5. Performance testing with multiple simultaneous connections",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:30:07.481Z"
      },
      {
        "id": "197",
        "title": "Implement Admin Authorization Check",
        "description": "Implement the admin authorization check in the documents route to ensure that non-admin users can only see their own pending approvals while admin users can see all pending approvals.",
        "details": "This task involves completing 1 TODO in app/routes/documents.py to implement the admin authorization check:\n\n```python\nfrom fastapi import APIRouter, Depends, HTTPException, Query\nfrom typing import List, Optional\nfrom app.core.auth import get_current_user\nfrom app.models.user import User\nfrom app.services.document_management import DocumentManagementService\n\nrouter = APIRouter()\n\n@router.get(\"/documents/pending-approval\")\nasync def get_pending_approvals(\n    limit: int = Query(10, ge=1, le=100),\n    offset: int = Query(0, ge=0),\n    current_user: User = Depends(get_current_user)\n):\n    \"\"\"Get documents pending approval\"\"\"\n    try:\n        # Initialize service\n        doc_service = DocumentManagementService()\n        \n        # Implement admin check for approval listings\n        if current_user.is_admin:\n            # Admin users can see all pending approvals\n            pending_docs = doc_service.get_all_pending_approvals(limit, offset)\n        else:\n            # Non-admin users can only see their own pending approvals\n            pending_docs = doc_service.get_user_pending_approvals(current_user.id, limit, offset)\n        \n        return {\n            \"success\": True,\n            \"documents\": pending_docs,\n            \"count\": len(pending_docs),\n            \"limit\": limit,\n            \"offset\": offset\n        }\n    except Exception as e:\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to retrieve pending approvals: {str(e)}\"\n        )\n\n# Add the following methods to DocumentManagementService if they don't exist\n\"\"\"\ndef get_all_pending_approvals(self, limit: int = 10, offset: int = 0):\n    \"\"\"Get all documents pending approval (admin only)\"\"\"\n    result = self.db.table(\"documents\")\\\n        .select(\"*\")\\\n        .eq(\"status\", \"pending_approval\")\\\n        .order(\"created_at\", desc=True)\\\n        .range(offset, offset + limit - 1)\\\n        .execute()\n        \n    return result.data\n\ndef get_user_pending_approvals(self, user_id: str, limit: int = 10, offset: int = 0):\n    \"\"\"Get documents pending approval for a specific user\"\"\"\n    # Get projects where user is a member\n    user_projects = self.db.table(\"project_members\")\\\n        .select(\"project_id\")\\\n        .eq(\"user_id\", user_id)\\\n        .execute()\n        \n    project_ids = [item[\"project_id\"] for item in user_projects.data]\n    \n    # Also include projects owned by the user\n    owned_projects = self.db.table(\"research_projects\")\\\n        .select(\"id\")\\\n        .eq(\"owner_id\", user_id)\\\n        .execute()\n        \n    project_ids.extend([item[\"id\"] for item in owned_projects.data])\n    \n    # Remove duplicates\n    project_ids = list(set(project_ids))\n    \n    if not project_ids:\n        return []\n    \n    # Get pending documents for these projects\n    result = self.db.table(\"documents\")\\\n        .select(\"*\")\\\n        .eq(\"status\", \"pending_approval\")\\\n        .in_(\"project_id\", project_ids)\\\n        .order(\"created_at\", desc=True)\\\n        .range(offset, offset + limit - 1)\\\n        .execute()\n        \n    return result.data\n\"\"\"\n\n@router.post(\"/documents/{document_id}/approve\")\nasync def approve_document(\n    document_id: str,\n    current_user: User = Depends(get_current_user)\n):\n    \"\"\"Approve a document pending approval\"\"\"\n    try:\n        # Check if user is admin or has rights to approve\n        doc_service = DocumentManagementService()\n        document = doc_service.get_document(document_id)\n        \n        if not document:\n            raise HTTPException(\n                status_code=404,\n                detail=f\"Document {document_id} not found\"\n            )\n        \n        # Only admins or project owners can approve documents\n        if not current_user.is_admin:\n            # Check if user is project owner\n            project = doc_service.db.table(\"research_projects\")\\\n                .select(\"owner_id\")\\\n                .eq(\"id\", document[\"project_id\"])\\\n                .single()\\\n                .execute()\n                \n            if not project.data or project.data[\"owner_id\"] != current_user.id:\n                raise HTTPException(\n                    status_code=403,\n                    detail=\"You don't have permission to approve this document\"\n                )\n        \n        # Approve the document\n        result = doc_service.approve_document(document_id, current_user.id)\n        \n        return {\n            \"success\": True,\n            \"document_id\": document_id,\n            \"message\": \"Document approved successfully\"\n        }\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to approve document: {str(e)}\"\n        )\n```",
        "testStrategy": "1. Unit tests for authorization logic\n   - Test admin user access to all pending approvals\n   - Test non-admin user access to only their approvals\n   - Test document approval permissions\n2. Integration tests with database\n3. Test cases:\n   - Admin user retrieves all pending approvals\n   - Regular user retrieves only their pending approvals\n   - Admin approves any document\n   - Project owner approves their document\n   - Non-admin tries to approve document they don't own\n4. Security testing:\n   - Attempt to bypass authorization checks\n   - Test with manipulated user roles\n5. Database query testing to ensure correct filtering",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:30:07.555Z"
      },
      {
        "id": "198",
        "title": "Implement Research Project Celery Integration",
        "description": "Complete the Celery task integration for research projects by implementing task triggering for project initialization and task revocation on project cancellation.",
        "details": "This task involves completing 2 TODOs in app/services/research_project_service.py:\n\n```python\nfrom app.db.supabase import get_supabase_client\nfrom app.celery_app import celery_app\nfrom celery.result import AsyncResult\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass ResearchProjectService:\n    def __init__(self, db_client=None):\n        self.db = db_client or get_supabase_client()\n    \n    def create_project(self, project_data, user_id):\n        \"\"\"Create a new research project and trigger initialization\"\"\"\n        try:\n            # Set default values\n            project_data[\"owner_id\"] = user_id\n            project_data[\"status\"] = \"initializing\"\n            project_data[\"created_at\"] = datetime.now()\n            \n            # Insert project into database\n            result = self.db.table(\"research_projects\").insert(project_data).execute()\n            \n            if not result.data:\n                raise Exception(\"Failed to create project\")\n                \n            project_id = result.data[0][\"id\"]\n            \n            # Trigger Celery task for project initialization\n            task = celery_app.send_task(\n                \"app.tasks.research_tasks.initialize_project\",\n                args=[project_id, user_id],\n                kwargs={\"project_data\": project_data}\n            )\n            \n            # Store task ID in the project record\n            self.db.table(\"research_projects\")\\\n                .update({\"initialization_task_id\": task.id})\\\n                .eq(\"id\", project_id)\\\n                .execute()\n            \n            logger.info(f\"Created project {project_id} with initialization task {task.id}\")\n            \n            return {\"id\": project_id, \"task_id\": task.id}\n        except Exception as e:\n            logger.error(f\"Failed to create project: {str(e)}\")\n            raise\n    \n    def cancel_project(self, project_id):\n        \"\"\"Cancel a research project and revoke pending tasks\"\"\"\n        try:\n            # Get project details\n            project = self.db.table(\"research_projects\")\\\n                .select(\"*\")\\\n                .eq(\"id\", project_id)\\\n                .single()\\\n                .execute()\n                \n            if not project.data:\n                raise Exception(f\"Project {project_id} not found\")\n                \n            # Check if project can be cancelled\n            if project.data[\"status\"] in [\"completed\", \"cancelled\", \"failed\"]:\n                raise Exception(f\"Project {project_id} is already in final state: {project.data['status']}\")\n            \n            # Implement task revocation on project cancellation\n            # Revoke initialization task if it exists and is still running\n            init_task_id = project.data.get(\"initialization_task_id\")\n            if init_task_id:\n                # Check task status\n                task_result = AsyncResult(init_task_id)\n                if not task_result.ready():\n                    # Revoke the task\n                    celery_app.control.revoke(init_task_id, terminate=True)\n                    logger.info(f\"Revoked initialization task {init_task_id} for project {project_id}\")\n            \n            # Revoke any other pending tasks for this project\n            pending_tasks = self.db.table(\"research_tasks\")\\\n                .select(\"id,celery_task_id\")\\\n                .eq(\"project_id\", project_id)\\\n                .in_(\"status\", [\"pending\", \"running\"])\\\n                .execute()\n                \n            for task in pending_tasks.data:\n                if task.get(\"celery_task_id\"):\n                    celery_app.control.revoke(task[\"celery_task_id\"], terminate=True)\n                    logger.info(f\"Revoked task {task['celery_task_id']} for project {project_id}\")\n                    \n                    # Update task status\n                    self.db.table(\"research_tasks\")\\\n                        .update({\"status\": \"cancelled\", \"updated_at\": datetime.now()})\\\n                        .eq(\"id\", task[\"id\"])\\\n                        .execute()\n            \n            # Update project status\n            self.db.table(\"research_projects\")\\\n                .update({\"status\": \"cancelled\", \"updated_at\": datetime.now()})\\\n                .eq(\"id\", project_id)\\\n                .execute()\n                \n            return {\"success\": True, \"message\": f\"Project {project_id} cancelled successfully\"}\n        except Exception as e:\n            logger.error(f\"Failed to cancel project {project_id}: {str(e)}\")\n            raise\n\n# Add the initialize_project task to app/tasks/research_tasks.py if it doesn't exist\n\"\"\"\n@celery_app.task(bind=True, max_retries=3)\ndef initialize_project(self, project_id, user_id, project_data=None):\n    \"\"\"Initialize a research project\"\"\"\n    try:\n        logger.info(f\"Initializing project {project_id}\")\n        db = get_supabase_client()\n        \n        # Update project status\n        db.table(\"research_projects\")\\\n            .update({\"status\": \"initializing\", \"updated_at\": datetime.now()})\\\n            .eq(\"id\", project_id)\\\n            .execute()\n        \n        # Create default folders\n        folders = [\n            {\"name\": \"Documents\", \"project_id\": project_id, \"parent_id\": None},\n            {\"name\": \"Research\", \"project_id\": project_id, \"parent_id\": None},\n            {\"name\": \"Reports\", \"project_id\": project_id, \"parent_id\": None}\n        ]\n        db.table(\"project_folders\").insert(folders).execute()\n        \n        # Create default tasks based on project type\n        if project_data and project_data.get(\"project_type\"):\n            project_type = project_data[\"project_type\"]\n            \n            # Get task templates for this project type\n            templates = db.table(\"task_templates\")\\\n                .select(\"*\")\\\n                .eq(\"project_type\", project_type)\\\n                .execute()\n                \n            # Create tasks from templates\n            for template in templates.data:\n                task = {\n                    \"project_id\": project_id,\n                    \"title\": template[\"title\"],\n                    \"description\": template[\"description\"],\n                    \"status\": \"pending\",\n                    \"created_by\": user_id,\n                    \"created_at\": datetime.now()\n                }\n                db.table(\"research_tasks\").insert(task).execute()\n        \n        # Update project status to active\n        db.table(\"research_projects\")\\\n            .update({\"status\": \"active\", \"updated_at\": datetime.now()})\\\n            .eq(\"id\", project_id)\\\n            .execute()\n            \n        logger.info(f\"Project {project_id} initialized successfully\")\n        return {\"success\": True, \"project_id\": project_id}\n    except Exception as e:\n        logger.error(f\"Failed to initialize project {project_id}: {str(e)}\")\n        # Retry up to max_retries\n        self.retry(exc=e, countdown=60)  # Retry after 1 minute\n\"\"\"\n```",
        "testStrategy": "1. Unit tests for create_project and cancel_project methods\n   - Test project creation with task triggering\n   - Test project cancellation with task revocation\n2. Integration tests with Celery\n3. Test cases:\n   - Create project and verify initialization task is triggered\n   - Cancel project in different states (initializing, active)\n   - Verify task revocation works for different task states\n   - Test error handling and retries\n4. Mock Celery for unit tests\n5. Test with actual Celery worker in integration environment\n6. Verify database state after operations",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:30:07.631Z"
      },
      {
        "id": "199",
        "title": "Implement OpenTelemetry Integration for Distributed Tracing",
        "description": "Add distributed tracing with OpenTelemetry across all services, including trace propagation through Celery tasks and export to a configured backend.",
        "details": "This task involves creating app/core/tracing.py with OpenTelemetry setup and integrating it throughout the application:\n\n```python\n# app/core/tracing.py\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.instrumentation.fastapi import FastAPIInstrumentor\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\nfrom opentelemetry.instrumentation.celery import CeleryInstrumentor\nfrom opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor\nfrom opentelemetry.instrumentation.redis import RedisInstrumentor\nfrom opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator\nfrom opentelemetry.context.context import Context\nfrom opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator\nfrom app.core.config import settings\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Global propagator for trace context\npropagator = TraceContextTextMapPropagator()\n\ndef setup_tracing(service_name=\"empire-api\"):\n    \"\"\"Setup OpenTelemetry tracing\"\"\"\n    try:\n        # Create a resource with service info\n        resource = Resource.create({\n            \"service.name\": service_name,\n            \"service.version\": settings.VERSION,\n            \"deployment.environment\": settings.ENVIRONMENT\n        })\n        \n        # Create a tracer provider\n        tracer_provider = TracerProvider(resource=resource)\n        \n        # Configure the exporter\n        otlp_exporter = OTLPSpanExporter(\n            endpoint=settings.OTLP_ENDPOINT,\n            insecure=settings.ENVIRONMENT != \"production\"\n        )\n        \n        # Add span processor to the tracer provider\n        tracer_provider.add_span_processor(BatchSpanProcessor(otlp_exporter))\n        \n        # Set the tracer provider\n        trace.set_tracer_provider(tracer_provider)\n        \n        # Get a tracer\n        tracer = trace.get_tracer(service_name)\n        \n        # Instrument libraries\n        RequestsInstrumentor().instrument()\n        CeleryInstrumentor().instrument()\n        RedisInstrumentor().instrument()\n        \n        logger.info(f\"OpenTelemetry tracing initialized for {service_name}\")\n        \n        return tracer\n    except Exception as e:\n        logger.error(f\"Failed to initialize OpenTelemetry: {str(e)}\")\n        # Return a no-op tracer that won't break the application if tracing fails\n        return trace.get_tracer(service_name)\n\ndef instrument_fastapi(app):\n    \"\"\"Instrument FastAPI application\"\"\"\n    try:\n        FastAPIInstrumentor.instrument_app(app, tracer_provider=trace.get_tracer_provider())\n        logger.info(\"FastAPI instrumented with OpenTelemetry\")\n    except Exception as e:\n        logger.error(f\"Failed to instrument FastAPI: {str(e)}\")\n\ndef instrument_sqlalchemy(engine):\n    \"\"\"Instrument SQLAlchemy engine\"\"\"\n    try:\n        SQLAlchemyInstrumentor().instrument(engine=engine)\n        logger.info(\"SQLAlchemy instrumented with OpenTelemetry\")\n    except Exception as e:\n        logger.error(f\"Failed to instrument SQLAlchemy: {str(e)}\")\n\n# Celery task tracing\ndef get_celery_carrier(task_id, parent_span=None):\n    \"\"\"Create carrier for Celery task with trace context\"\"\"\n    carrier = {}\n    if parent_span:\n        ctx = trace.set_span_in_context(parent_span)\n    else:\n        ctx = Context()\n    propagator.inject(carrier=carrier, context=ctx)\n    return carrier\n\ndef extract_celery_context(headers):\n    \"\"\"Extract trace context from Celery task headers\"\"\"\n    return propagator.extract(carrier=headers)\n\n# Trace decorator for functions\ndef traced(name=None):\n    \"\"\"Decorator to add tracing to a function\"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Get the tracer\n            tracer = trace.get_tracer(\"empire-api\")\n            # Start a span\n            span_name = name or func.__name__\n            with tracer.start_as_current_span(span_name) as span:\n                # Add attributes to the span\n                span.set_attribute(\"function.name\", func.__name__)\n                span.set_attribute(\"function.args_length\", len(args))\n                span.set_attribute(\"function.kwargs_length\", len(kwargs))\n                \n                # Execute the function\n                try:\n                    result = func(*args, **kwargs)\n                    return result\n                except Exception as e:\n                    # Record exception in span\n                    span.record_exception(e)\n                    span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n                    raise\n        return wrapper\n    return decorator\n```\n\nIntegrate OpenTelemetry in the main FastAPI application (app/main.py):\n\n```python\nfrom fastapi import FastAPI, Request, Response\nfrom app.core.tracing import setup_tracing, instrument_fastapi\nfrom app.core.config import settings\nimport time\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Initialize tracing\ntracer = setup_tracing(\"empire-api\")\n\napp = FastAPI(\n    title=\"Empire API\",\n    description=\"Empire v7.3 API\",\n    version=settings.VERSION\n)\n\n# Instrument FastAPI with OpenTelemetry\ninstrument_fastapi(app)\n\n# Add middleware for request logging with trace IDs\n@app.middleware(\"http\")\nasync def add_trace_id_to_response(request: Request, call_next):\n    start_time = time.time()\n    \n    # Get current span and trace ID\n    current_span = trace.get_current_span()\n    trace_id = current_span.get_span_context().trace_id if current_span else None\n    \n    # Convert trace ID to hex string if it exists\n    trace_id_hex = format(trace_id, '032x') if trace_id else None\n    \n    # Add trace context to logs\n    if trace_id_hex:\n        logger.info(f\"Request started: {request.method} {request.url.path} [trace_id={trace_id_hex}]\")\n    else:\n        logger.info(f\"Request started: {request.method} {request.url.path}\")\n    \n    # Process the request\n    response = await call_next(request)\n    \n    # Calculate duration\n    duration = time.time() - start_time\n    \n    # Add trace ID to response headers\n    if trace_id_hex:\n        response.headers[\"X-Trace-ID\"] = trace_id_hex\n        logger.info(f\"Request completed: {request.method} {request.url.path} {response.status_code} in {duration:.3f}s [trace_id={trace_id_hex}]\")\n    else:\n        logger.info(f\"Request completed: {request.method} {request.url.path} {response.status_code} in {duration:.3f}s\")\n    \n    return response\n```\n\nIntegrate OpenTelemetry in Celery tasks (app/celery_app.py):\n\n```python\nfrom celery import Celery, Task\nfrom app.core.config import settings\nfrom app.core.tracing import setup_tracing, extract_celery_context\nfrom opentelemetry import trace\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Initialize tracing\ntracer = setup_tracing(\"empire-celery\")\n\nclass TracedTask(Task):\n    \"\"\"Custom Celery Task class with tracing\"\"\"\n    def __call__(self, *args, **kwargs):\n        # Extract trace context from headers if available\n        headers = self.request.headers or {}\n        context = extract_celery_context(headers)\n        \n        # Start a new span with the extracted context\n        with tracer.start_as_current_span(\n            f\"celery.task.{self.name}\",\n            context=context\n        ) as span:\n            # Add task info to span\n            span.set_attribute(\"celery.task.id\", self.request.id)\n            span.set_attribute(\"celery.task.name\", self.name)\n            span.set_attribute(\"celery.task.args_length\", len(args))\n            span.set_attribute(\"celery.task.kwargs_length\", len(kwargs))\n            \n            # Get trace ID for logging\n            trace_id = span.get_span_context().trace_id\n            trace_id_hex = format(trace_id, '032x') if trace_id else None\n            \n            # Log task execution with trace ID\n            if trace_id_hex:\n                logger.info(f\"Executing task {self.name}[{self.request.id}] [trace_id={trace_id_hex}]\")\n            else:\n                logger.info(f\"Executing task {self.name}[{self.request.id}]\")\n            \n            try:\n                # Execute the task\n                return super().__call__(*args, **kwargs)\n            except Exception as e:\n                # Record exception in span\n                span.record_exception(e)\n                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n                raise\n\n# Initialize Celery app with tracing\ncelery_app = Celery(\n    \"empire\",\n    broker=settings.CELERY_BROKER_URL,\n    backend=settings.CELERY_RESULT_BACKEND\n)\n\n# Configure Celery\ncelery_app.conf.update(\n    task_serializer=\"json\",\n    accept_content=[\"json\"],\n    result_serializer=\"json\",\n    timezone=\"UTC\",\n    enable_utc=True,\n    task_default_queue=\"default\",\n    worker_prefetch_multiplier=1,\n    task_acks_late=True,\n    task_reject_on_worker_lost=True,\n    task_track_started=True,\n    task_send_sent_event=True,\n    task_default_rate_limit=\"100/m\",\n    worker_concurrency=settings.CELERY_WORKER_CONCURRENCY,\n    task_routes={\n        \"app.tasks.document_processing.*\": {\"queue\": \"documents\"},\n        \"app.tasks.research_tasks.*\": {\"queue\": \"research\"},\n        \"app.tasks.graph_sync.*\": {\"queue\": \"graph\"}\n    }\n)\n\n# Set default task base class to TracedTask\ncelery_app.Task = TracedTask\n```\n\nAdd OpenTelemetry configuration to app/core/config.py:\n\n```python\n# Add to existing settings\nclass Settings:\n    # ... existing settings ...\n    \n    # OpenTelemetry settings\n    OTLP_ENDPOINT: str = os.getenv(\"OTLP_ENDPOINT\", \"http://jaeger:4317\")\n    \n    # Celery settings\n    CELERY_BROKER_URL: str = os.getenv(\"CELERY_BROKER_URL\", \"redis://redis:6379/0\")\n    CELERY_RESULT_BACKEND: str = os.getenv(\"CELERY_RESULT_BACKEND\", \"redis://redis:6379/0\")\n    CELERY_WORKER_CONCURRENCY: int = int(os.getenv(\"CELERY_WORKER_CONCURRENCY\", \"4\"))\n```",
        "testStrategy": "1. Unit tests for tracing functionality\n   - Test span creation and propagation\n   - Test context extraction and injection\n   - Test traced decorator\n2. Integration tests for tracing across services\n   - Test trace propagation from API to Celery tasks\n   - Test trace propagation through multiple services\n3. Test cases:\n   - API request with trace context\n   - Celery task with trace context\n   - Error handling and exception recording\n   - Trace ID in response headers\n4. Verify traces in configured backend (Jaeger, etc.)\n5. Performance impact testing\n   - Measure overhead of tracing\n   - Test with different sampling rates",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:30:07.711Z"
      },
      {
        "id": "200",
        "title": "Implement Agent Feedback System",
        "description": "Create the agent feedback system by implementing the agent_feedback table in Supabase and adding feedback storage functionality to the classification and asset management services.",
        "details": "This task involves implementing the agent feedback system across multiple files:\n\n1. First, create the agent_feedback table in Supabase using the provided schema:\n\n```sql\nCREATE TABLE agent_feedback (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    agent_id VARCHAR(50) NOT NULL,\n    task_id UUID,\n    feedback_type VARCHAR(50) NOT NULL, -- 'classification', 'generation', 'retrieval'\n    input_summary TEXT,\n    output_summary TEXT,\n    rating INTEGER CHECK (rating >= 1 AND rating <= 5),\n    feedback_text TEXT,\n    metadata JSONB DEFAULT '{}',\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    created_by UUID REFERENCES auth.users(id)\n);\n\nCREATE INDEX idx_agent_feedback_agent ON agent_feedback(agent_id);\nCREATE INDEX idx_agent_feedback_type ON agent_feedback(feedback_type);\nCREATE INDEX idx_agent_feedback_created ON agent_feedback(created_at DESC);\n```\n\n2. Implement feedback storage in app/services/classification_service.py:\n\n```python\nfrom app.db.supabase import get_supabase_client\nfrom app.core.tracing import traced\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass ClassificationService:\n    def __init__(self, db_client=None):\n        self.db = db_client or get_supabase_client()\n        self.agent_id = \"classification_agent_v1\"\n    \n    @traced(\"classify_document\")\n    def classify_document(self, document_text, metadata=None):\n        \"\"\"Classify a document based on its content\"\"\"\n        # Existing classification logic...\n        \n        # Return classification result\n        return classification_result\n    \n    @traced(\"store_feedback\")\n    def store_feedback(self, task_id, input_summary, output_summary, rating, feedback_text, user_id, metadata=None):\n        \"\"\"Store feedback for classification\"\"\"\n        try:\n            # Create feedback record\n            feedback = {\n                \"agent_id\": self.agent_id,\n                \"task_id\": task_id,\n                \"feedback_type\": \"classification\",\n                \"input_summary\": input_summary,\n                \"output_summary\": output_summary,\n                \"rating\": rating,\n                \"feedback_text\": feedback_text,\n                \"metadata\": metadata or {},\n                \"created_by\": user_id\n            }\n            \n            # Insert into agent_feedback table\n            result = self.db.table(\"agent_feedback\").insert(feedback).execute()\n            \n            if not result.data:\n                raise Exception(\"Failed to store feedback\")\n                \n            feedback_id = result.data[0][\"id\"]\n            logger.info(f\"Stored classification feedback {feedback_id} for task {task_id}\")\n            \n            return {\"id\": feedback_id}\n        except Exception as e:\n            logger.error(f\"Failed to store classification feedback: {str(e)}\")\n            raise\n```\n\n3. Implement feedback storage in app/services/asset_management_service.py:\n\n```python\nfrom app.db.supabase import get_supabase_client\nfrom app.core.tracing import traced\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass AssetManagementService:\n    def __init__(self, db_client=None):\n        self.db = db_client or get_supabase_client()\n        self.agent_id = \"asset_management_agent_v1\"\n    \n    @traced(\"generate_asset\")\n    def generate_asset(self, asset_type, parameters, user_id):\n        \"\"\"Generate an asset based on parameters\"\"\"\n        # Existing asset generation logic...\n        \n        # Return generated asset\n        return asset_result\n    \n    @traced(\"store_feedback\")\n    def store_feedback(self, task_id, input_summary, output_summary, rating, feedback_text, user_id, metadata=None):\n        \"\"\"Store feedback for asset generation\"\"\"\n        try:\n            # Create feedback record\n            feedback = {\n                \"agent_id\": self.agent_id,\n                \"task_id\": task_id,\n                \"feedback_type\": \"generation\",\n                \"input_summary\": input_summary,\n                \"output_summary\": output_summary,\n                \"rating\": rating,\n                \"feedback_text\": feedback_text,\n                \"metadata\": metadata or {},\n                \"created_by\": user_id\n            }\n            \n            # Insert into agent_feedback table\n            result = self.db.table(\"agent_feedback\").insert(feedback).execute()\n            \n            if not result.data:\n                raise Exception(\"Failed to store feedback\")\n                \n            feedback_id = result.data[0][\"id\"]\n            logger.info(f\"Stored asset generation feedback {feedback_id} for task {task_id}\")\n            \n            return {\"id\": feedback_id}\n        except Exception as e:\n            logger.error(f\"Failed to store asset generation feedback: {str(e)}\")\n            raise\n```\n\n4. Create a generic FeedbackService for reuse across different agents:\n\n```python\n# app/services/feedback_service.py\nfrom app.db.supabase import get_supabase_client\nfrom app.core.tracing import traced\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass FeedbackService:\n    def __init__(self, db_client=None):\n        self.db = db_client or get_supabase_client()\n    \n    @traced(\"store_feedback\")\n    def store_feedback(self, agent_id, feedback_type, task_id, input_summary, output_summary, \n                      rating, feedback_text, user_id, metadata=None):\n        \"\"\"Store feedback for any agent\"\"\"\n        try:\n            # Validate inputs\n            if rating < 1 or rating > 5:\n                raise ValueError(\"Rating must be between 1 and 5\")\n                \n            if not agent_id or not feedback_type:\n                raise ValueError(\"Agent ID and feedback type are required\")\n            \n            # Create feedback record\n            feedback = {\n                \"agent_id\": agent_id,\n                \"task_id\": task_id,\n                \"feedback_type\": feedback_type,\n                \"input_summary\": input_summary,\n                \"output_summary\": output_summary,\n                \"rating\": rating,\n                \"feedback_text\": feedback_text,\n                \"metadata\": metadata or {},\n                \"created_by\": user_id\n            }\n            \n            # Insert into agent_feedback table\n            result = self.db.table(\"agent_feedback\").insert(feedback).execute()\n            \n            if not result.data:\n                raise Exception(\"Failed to store feedback\")\n                \n            feedback_id = result.data[0][\"id\"]\n            logger.info(f\"Stored {feedback_type} feedback {feedback_id} for agent {agent_id}\")\n            \n            return {\"id\": feedback_id}\n        except Exception as e:\n            logger.error(f\"Failed to store feedback: {str(e)}\")\n            raise\n    \n    @traced(\"get_agent_feedback\")\n    def get_agent_feedback(self, agent_id=None, feedback_type=None, limit=100, offset=0):\n        \"\"\"Get feedback for agents with optional filtering\"\"\"\n        try:\n            query = self.db.table(\"agent_feedback\").select(\"*\").order(\"created_at\", desc=True)\n            \n            if agent_id:\n                query = query.eq(\"agent_id\", agent_id)\n                \n            if feedback_type:\n                query = query.eq(\"feedback_type\", feedback_type)\n                \n            result = query.range(offset, offset + limit - 1).execute()\n            \n            return result.data\n        except Exception as e:\n            logger.error(f\"Failed to get agent feedback: {str(e)}\")\n            raise\n    \n    @traced(\"get_feedback_stats\")\n    def get_feedback_stats(self, agent_id=None, feedback_type=None, days=30):\n        \"\"\"Get feedback statistics\"\"\"\n        try:\n            # Use SQL function to get statistics\n            result = self.db.rpc(\n                \"get_feedback_stats\",\n                {\n                    \"p_agent_id\": agent_id,\n                    \"p_feedback_type\": feedback_type,\n                    \"p_days\": days\n                }\n            ).execute()\n            \n            return result.data\n        except Exception as e:\n            logger.error(f\"Failed to get feedback stats: {str(e)}\")\n            raise\n```\n\n5. Create a SQL function for feedback statistics:\n\n```sql\nCREATE OR REPLACE FUNCTION get_feedback_stats(\n    p_agent_id VARCHAR DEFAULT NULL,\n    p_feedback_type VARCHAR DEFAULT NULL,\n    p_days INTEGER DEFAULT 30\n) RETURNS TABLE (\n    agent_id VARCHAR,\n    feedback_type VARCHAR,\n    total_count INTEGER,\n    avg_rating NUMERIC(3,2),\n    rating_distribution JSONB\n) AS $$\nBEGIN\n    RETURN QUERY\n    WITH feedback_filtered AS (\n        SELECT *\n        FROM agent_feedback\n        WHERE (p_agent_id IS NULL OR agent_id = p_agent_id)\n        AND (p_feedback_type IS NULL OR feedback_type = p_feedback_type)\n        AND created_at >= NOW() - (p_days || ' days')::INTERVAL\n    ),\n    rating_counts AS (\n        SELECT \n            agent_id,\n            feedback_type,\n            rating,\n            COUNT(*) as count\n        FROM feedback_filtered\n        GROUP BY agent_id, feedback_type, rating\n    )\n    SELECT \n        f.agent_id,\n        f.feedback_type,\n        COUNT(*) as total_count,\n        AVG(f.rating)::NUMERIC(3,2) as avg_rating,\n        jsonb_object_agg(\n            r.rating::TEXT, \n            r.count\n        ) as rating_distribution\n    FROM feedback_filtered f\n    LEFT JOIN rating_counts r ON \n        f.agent_id = r.agent_id AND \n        f.feedback_type = r.feedback_type AND\n        f.rating = r.rating\n    GROUP BY f.agent_id, f.feedback_type;\n    \nEND;\n$$ LANGUAGE plpgsql;\n```\n\n6. Create API endpoints for feedback:\n\n```python\n# app/routes/feedback.py\nfrom fastapi import APIRouter, Depends, HTTPException, Query\nfrom typing import Optional, Dict, Any\nfrom app.core.auth import get_current_user\nfrom app.models.user import User\nfrom app.services.feedback_service import FeedbackService\nfrom pydantic import BaseModel, Field\n\nrouter = APIRouter()\n\nclass FeedbackRequest(BaseModel):\n    agent_id: str = Field(..., description=\"ID of the agent receiving feedback\")\n    feedback_type: str = Field(..., description=\"Type of feedback (classification, generation, retrieval)\")\n    task_id: Optional[str] = Field(None, description=\"ID of the task if applicable\")\n    input_summary: str = Field(..., description=\"Summary of the input to the agent\")\n    output_summary: str = Field(..., description=\"Summary of the output from the agent\")\n    rating: int = Field(..., ge=1, le=5, description=\"Rating from 1-5\")\n    feedback_text: str = Field(..., description=\"Detailed feedback text\")\n    metadata: Optional[Dict[str, Any]] = Field(None, description=\"Additional metadata\")\n\n@router.post(\"/feedback\")\nasync def submit_feedback(\n    feedback: FeedbackRequest,\n    current_user: User = Depends(get_current_user)\n):\n    \"\"\"Submit feedback for an agent\"\"\"\n    try:\n        service = FeedbackService()\n        result = service.store_feedback(\n            agent_id=feedback.agent_id,\n            feedback_type=feedback.feedback_type,\n            task_id=feedback.task_id,\n            input_summary=feedback.input_summary,\n            output_summary=feedback.output_summary,\n            rating=feedback.rating,\n            feedback_text=feedback.feedback_text,\n            user_id=current_user.id,\n            metadata=feedback.metadata\n        )\n        \n        return {\n            \"success\": True,\n            \"feedback_id\": result[\"id\"],\n            \"message\": \"Feedback submitted successfully\"\n        }\n    except Exception as e:\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to submit feedback: {str(e)}\"\n        )\n\n@router.get(\"/feedback\")\nasync def get_feedback(\n    agent_id: Optional[str] = Query(None),\n    feedback_type: Optional[str] = Query(None),\n    limit: int = Query(100, ge=1, le=1000),\n    offset: int = Query(0, ge=0),\n    current_user: User = Depends(get_current_user)\n):\n    \"\"\"Get feedback with optional filtering\"\"\"\n    try:\n        # Only admins can view all feedback\n        if not current_user.is_admin and (not agent_id or not feedback_type):\n            raise HTTPException(\n                status_code=403,\n                detail=\"Non-admin users must specify both agent_id and feedback_type\"\n            )\n        \n        service = FeedbackService()\n        feedback_items = service.get_agent_feedback(\n            agent_id=agent_id,\n            feedback_type=feedback_type,\n            limit=limit,\n            offset=offset\n        )\n        \n        return {\n            \"success\": True,\n            \"feedback\": feedback_items,\n            \"count\": len(feedback_items),\n            \"limit\": limit,\n            \"offset\": offset\n        }\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to get feedback: {str(e)}\"\n        )\n\n@router.get(\"/feedback/stats\")\nasync def get_feedback_stats(\n    agent_id: Optional[str] = Query(None),\n    feedback_type: Optional[str] = Query(None),\n    days: int = Query(30, ge=1, le=365),\n    current_user: User = Depends(get_current_user)\n):\n    \"\"\"Get feedback statistics\"\"\"\n    try:\n        service = FeedbackService()\n        stats = service.get_feedback_stats(\n            agent_id=agent_id,\n            feedback_type=feedback_type,\n            days=days\n        )\n        \n        return {\n            \"success\": True,\n            \"stats\": stats\n        }\n    except Exception as e:\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Failed to get feedback stats: {str(e)}\"\n        )\n```",
        "testStrategy": "1. Unit tests for feedback storage in each service\n   - Test FeedbackService methods\n   - Test service-specific feedback methods\n2. Integration tests with database\n   - Test table creation and constraints\n   - Test SQL function for statistics\n3. API endpoint tests\n   - Test feedback submission\n   - Test feedback retrieval with filtering\n   - Test statistics endpoint\n4. Test cases:\n   - Submit feedback with valid data\n   - Submit feedback with invalid ratings\n   - Get feedback with different filters\n   - Get statistics for different time periods\n5. Authorization tests:\n   - Test admin vs non-admin access\n   - Test user-specific feedback access",
        "priority": "medium",
        "dependencies": [
          "199"
        ],
        "status": "cancelled",
        "subtasks": [],
        "updatedAt": "2026-01-17T20:30:07.925Z"
      },
      {
        "id": "201",
        "title": "Implement Token Counting Service",
        "description": "Create a service that accurately counts tokens in real-time for all message types in the conversation",
        "details": "Develop a token counting service that integrates with tiktoken and Anthropic API to provide accurate token counts for different content types. The service should:\n\n1. Count tokens for text, code blocks, and other content types\n2. Support different models with varying token limits\n3. Provide real-time updates as messages are added\n4. Calculate total usage, reserved space, and available space\n\n```python\n# Token counting service implementation\nclass TokenCountingService:\n    def __init__(self, model: str):\n        self.model = model\n        self.max_tokens = self._get_model_context_limit(model)\n        self.reserved_tokens = int(self.max_tokens * 0.2)  # Default 20% reservation\n    \n    def _get_model_context_limit(self, model: str) -> int:\n        # Model-specific context limits\n        limits = {\n            \"claude-3-opus\": 200000,\n            \"claude-3-sonnet\": 180000,\n            \"claude-3-haiku\": 128000,\n            # Add other models as needed\n        }\n        return limits.get(model, 100000)  # Default fallback\n    \n    def count_message_tokens(self, message: ContextMessage) -> int:\n        # Use tiktoken for local estimation\n        try:\n            return self._count_with_tiktoken(message.content, message.role)\n        except Exception:\n            # Fallback to simpler estimation if tiktoken fails\n            return self._estimate_tokens(message.content)\n    \n    def _count_with_tiktoken(self, content: str, role: str) -> int:\n        # Implementation using tiktoken library\n        # This is a simplified version - actual implementation would be model-specific\n        import tiktoken\n        encoding = tiktoken.encoding_for_model(\"cl100k_base\")  # Claude compatible encoding\n        tokens = encoding.encode(content)\n        return len(tokens) + 10  # Add overhead for role formatting\n    \n    def _estimate_tokens(self, content: str) -> int:\n        # Simple fallback estimation (4 chars ~= 1 token)\n        return len(content) // 4 + 1\n    \n    def get_context_status(self, messages: List[ContextMessage]) -> ContextWindowStatus:\n        current_tokens = sum(msg.token_count for msg in messages)\n        available = self.max_tokens - current_tokens - self.reserved_tokens\n        usage_percent = (current_tokens / self.max_tokens) * 100\n        \n        # Determine status based on usage percentage\n        if usage_percent < 70:\n            status = \"normal\"\n        elif usage_percent < 85:\n            status = \"warning\"\n        else:\n            status = \"critical\"\n            \n        # Estimate remaining messages (assuming avg 200 tokens per message)\n        est_remaining = available // 200\n        \n        return ContextWindowStatus(\n            session_id=messages[0].session_id if messages else \"\",\n            current_tokens=current_tokens,\n            max_tokens=self.max_tokens,\n            reserved_tokens=self.reserved_tokens,\n            available_tokens=available,\n            usage_percent=usage_percent,\n            status=status,\n            estimated_messages_remaining=est_remaining,\n            last_updated=datetime.now()\n        )\n```\n\nThe service should be implemented as a singleton that can be accessed throughout the application. It should update token counts whenever messages are added or removed from the conversation.",
        "testStrategy": "1. Unit tests for token counting accuracy:\n   - Test with various content types (plain text, code blocks, mixed content)\n   - Compare results with Anthropic API token counts for validation\n   - Test edge cases (empty messages, very long messages, non-ASCII characters)\n\n2. Integration tests:\n   - Verify real-time updates when messages are added\n   - Test with different model configurations\n   - Verify status transitions (normal → warning → critical)\n\n3. Performance tests:\n   - Measure token counting speed for large conversations\n   - Ensure counting completes in <100ms for any conversation length\n\n4. Accuracy validation:\n   - Compare token counts with actual API usage\n   - Ensure accuracy is within 5% of actual API usage",
        "priority": "high",
        "dependencies": [
          "211"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create token counter module with tiktoken integration",
            "description": "Implement the core token counting module that integrates with tiktoken library for accurate token estimation",
            "dependencies": [],
            "details": "Create app/core/token_counter.py module that implements the base TokenCountingService class with tiktoken integration. Include model context limits for Claude models and implement the _get_model_context_limit method. Set up proper initialization with model selection and reserved token calculation.",
            "status": "pending",
            "testStrategy": "Unit test token counting accuracy with different models. Compare results with actual Anthropic API token counts. Test initialization with various models and verify correct context limits are applied.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement count_tokens() function using tiktoken cl100k_base encoding",
            "description": "Create the primary token counting function that uses tiktoken's cl100k_base encoding for Claude-compatible token counting",
            "dependencies": [
              1
            ],
            "details": "Implement the count_tokens() function in the TokenCountingService class that uses tiktoken's cl100k_base encoding. Handle different content types including text, code blocks, and mixed content. Add proper error handling and fallback mechanisms if tiktoken fails.",
            "status": "pending",
            "testStrategy": "Test with various content types (plain text, code blocks, JSON, markdown). Compare results with Anthropic's official token counting. Test edge cases like empty strings, very long content, and non-ASCII characters.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement count_message_tokens() helper for ContextMessage objects",
            "description": "Create a specialized method to count tokens in ContextMessage objects, accounting for message role and content",
            "dependencies": [
              2
            ],
            "details": "Implement the count_message_tokens() method that takes a ContextMessage object and returns the token count. Account for both the message content and the role (user, assistant, system) in the token calculation. Include proper handling for different message formats and content types within messages.",
            "status": "pending",
            "testStrategy": "Test with various ContextMessage objects with different roles and content types. Verify correct token counting for system, user, and assistant messages. Test with multi-part messages containing different content formats.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create ContextWindowStatus Pydantic model",
            "description": "Implement a Pydantic model to represent the current state of the context window including token usage statistics",
            "dependencies": [
              1
            ],
            "details": "Create the ContextWindowStatus Pydantic model in app/models/context_models.py that includes fields for session_id, current_tokens, max_tokens, reserved_tokens, available_tokens, usage_percent, status (normal/warning/critical), estimated_messages_remaining, and last_updated timestamp. Implement the get_context_status() method in TokenCountingService that returns this model.",
            "status": "pending",
            "testStrategy": "Test model validation with various input combinations. Verify correct calculation of derived fields like usage_percent and estimated_messages_remaining. Test status determination logic based on different usage percentages.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add Redis caching for real-time context state updates",
            "description": "Implement Redis-based caching to store and retrieve context window status for real-time updates",
            "dependencies": [
              3,
              4
            ],
            "details": "Add Redis integration to the TokenCountingService to cache context window status. Implement methods to update the cache whenever messages are added or removed. Create functions to retrieve the latest context status from Redis. Use session_id as the key for storing context information. Implement proper serialization/deserialization of ContextWindowStatus objects.",
            "status": "pending",
            "testStrategy": "Test Redis cache updates with message additions and removals. Verify cache retrieval returns the correct context status. Test cache expiration and refresh mechanisms. Test concurrent access patterns.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Create unit test file for token counter",
            "description": "Implement comprehensive unit tests for the token counting service to ensure accuracy and reliability",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Create tests/unit/test_token_counter.py with test cases for all TokenCountingService methods. Include tests for token counting accuracy with different content types, model context limits, and edge cases. Mock Redis dependencies for isolated testing. Test the ContextWindowStatus calculation logic with various message combinations.",
            "status": "pending",
            "testStrategy": "Use pytest fixtures to set up test environments. Compare token counting results with expected values. Test with real examples from Anthropic documentation. Include performance tests for large message histories.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Add Prometheus metrics for token usage monitoring",
            "description": "Implement Prometheus metrics to track token usage and context window utilization",
            "dependencies": [
              5
            ],
            "details": "Add Prometheus metrics integration to track context_tokens_total and context_usage_percent. Create a metrics registry in the TokenCountingService. Update metrics whenever the context window status changes. Add labels for model type and session_id to enable detailed monitoring. Implement a method to expose metrics for Prometheus scraping.",
            "status": "pending",
            "testStrategy": "Test metric registration and updates. Verify correct values are recorded for different scenarios. Test metric exposure endpoint format. Test with different models and session configurations.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-20T21:00:32.477Z"
      },
      {
        "id": "202",
        "title": "Develop Context Window Progress Bar UI Component",
        "description": "Create a visual progress bar component that displays real-time context window usage with color-coded status indicators",
        "details": "Implement a progress bar UI component that shows the current context window usage. The component should:\n\n1. Display a visual progress bar with color-coded sections\n2. Show numeric token counts on hover\n3. Indicate reserved space for AI responses\n4. Update in real-time as messages are added\n5. Change colors based on usage thresholds\n\n```jsx\n// React component for context window progress bar\nimport React, { useState, useEffect } from 'react';\nimport { Tooltip } from './Tooltip';\n\nconst ContextWindowProgressBar = ({ \n  currentTokens, \n  maxTokens, \n  reservedTokens,\n  status,\n  onThresholdReached = () => {}\n}) => {\n  // Calculate percentages for display\n  const usedPercent = (currentTokens / maxTokens) * 100;\n  const reservedPercent = (reservedTokens / maxTokens) * 100;\n  const availablePercent = 100 - usedPercent - reservedPercent;\n  \n  // Determine color based on status\n  const getStatusColor = (status) => {\n    switch(status) {\n      case 'critical': return 'bg-red-500';\n      case 'warning': return 'bg-yellow-500';\n      case 'normal': return 'bg-green-500';\n      default: return 'bg-green-500';\n    }\n  };\n  \n  // Format token counts for display\n  const formatTokens = (tokens) => {\n    return tokens >= 1000 ? `${(tokens/1000).toFixed(1)}K` : tokens;\n  };\n  \n  // Tooltip content\n  const tooltipContent = (\n    <div className=\"p-2\">\n      <div>Used: {formatTokens(currentTokens)} tokens ({usedPercent.toFixed(1)}%)</div>\n      <div>Reserved: {formatTokens(reservedTokens)} tokens ({reservedPercent.toFixed(1)}%)</div>\n      <div>Available: {formatTokens(maxTokens - currentTokens - reservedTokens)} tokens ({availablePercent.toFixed(1)}%)</div>\n      <div>Total capacity: {formatTokens(maxTokens)} tokens</div>\n    </div>\n  );\n  \n  // Pulse animation when approaching threshold\n  const shouldPulse = status === 'warning' || status === 'critical';\n  \n  return (\n    <Tooltip content={tooltipContent}>\n      <div className=\"w-full h-6 rounded-md overflow-hidden border border-gray-300 flex\">\n        {/* Used tokens section */}\n        <div \n          className={`h-full ${getStatusColor(status)} ${shouldPulse ? 'animate-pulse' : ''}`}\n          style={{ width: `${usedPercent}%` }}\n        />\n        \n        {/* Reserved tokens section */}\n        <div \n          className=\"h-full bg-gray-400 opacity-50\"\n          style={{ width: `${reservedPercent}%` }}\n        />\n        \n        {/* Available tokens section */}\n        <div \n          className=\"h-full bg-gray-200\"\n          style={{ width: `${availablePercent}%` }}\n        />\n      </div>\n      <div className=\"text-xs text-center mt-1\">\n        Context: {formatTokens(currentTokens)} / {formatTokens(maxTokens)} tokens ({usedPercent.toFixed(0)}%)\n      </div>\n    </Tooltip>\n  );\n};\n\nexport default ContextWindowProgressBar;\n```\n\nThe component should be integrated into the chat interface header or footer area and remain visible at all times during the conversation. It should update smoothly as new messages are added without causing UI lag.",
        "testStrategy": "1. Unit tests:\n   - Test rendering with different token counts and statuses\n   - Verify color changes at different thresholds\n   - Test tooltip content accuracy\n\n2. Integration tests:\n   - Verify real-time updates when messages are added\n   - Test animation triggers at warning/critical thresholds\n   - Verify tooltip displays correct information\n\n3. Visual regression tests:\n   - Capture screenshots at different states (normal, warning, critical)\n   - Ensure consistent rendering across browsers\n\n4. User acceptance testing:\n   - Verify progress bar is easily visible but not distracting\n   - Confirm color coding is intuitive\n   - Test responsiveness on different screen sizes",
        "priority": "high",
        "dependencies": [
          "201"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create ContextProgressBar.tsx component",
            "description": "Implement the React component for displaying the context window usage progress bar with color-coded sections.",
            "dependencies": [],
            "details": "Create the ContextProgressBar.tsx component in empire-desktop/src/components/ directory. Implement the visual progress bar with color-coded sections based on usage thresholds. Include the basic structure with used, reserved, and available token sections as shown in the provided code sample.",
            "status": "pending",
            "testStrategy": "Write unit tests to verify the component renders correctly with different token counts and properly displays the progress bar sections with appropriate widths.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement tooltip functionality for token details",
            "description": "Add tooltip functionality to display detailed token information when hovering over the progress bar.",
            "dependencies": [
              1
            ],
            "details": "Enhance the ContextProgressBar component to show numeric token counts on hover using the Tooltip component. Display used tokens, reserved tokens, available tokens, and total capacity in a formatted way. Implement the formatTokens helper function to properly display token counts (e.g., '4.2K' for 4200).",
            "status": "pending",
            "testStrategy": "Test tooltip content accuracy with various token values and verify proper formatting of token numbers.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create useContextWindow.ts custom hook",
            "description": "Develop a custom React hook to manage context window state and calculations.",
            "dependencies": [],
            "details": "Create useContextWindow.ts hook in empire-desktop/src/hooks/ directory. The hook should manage the current token count, max tokens, reserved tokens, and calculate percentages. It should also determine the status based on thresholds (green <70%, yellow 70-85%, red >85%) and provide functions to update the token count.",
            "status": "pending",
            "testStrategy": "Write unit tests to verify the hook correctly calculates percentages, determines status based on thresholds, and properly updates state.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement color-coded status display with animations",
            "description": "Add color-coded status indicators and pulse animations for warning states.",
            "dependencies": [
              1,
              3
            ],
            "details": "Enhance the ContextProgressBar component to change colors based on usage thresholds (green <70%, yellow 70-85%, red >85%). Implement the getStatusColor function to return appropriate color classes. Add pulse animation effect when the status is 'warning' or 'critical' to draw user attention to approaching limits.",
            "status": "pending",
            "testStrategy": "Test color changes at different thresholds and verify animation triggers at warning/critical thresholds.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement GET /context/{conversation_id} API endpoint",
            "description": "Create a backend API endpoint to retrieve context window information for a specific conversation.",
            "dependencies": [],
            "details": "Implement the GET /context/{conversation_id} endpoint in app/routes/context_management.py. The endpoint should return the current token count, max tokens allowed, reserved tokens, and status for the specified conversation. Include proper error handling for invalid conversation IDs.",
            "status": "pending",
            "testStrategy": "Create API tests to verify the endpoint returns correct context information and properly handles error cases.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Add WebSocket subscription for real-time updates",
            "description": "Implement WebSocket functionality to provide real-time context window usage updates.",
            "dependencies": [
              3,
              5
            ],
            "details": "Set up WebSocket subscription in the useContextWindow hook to receive real-time updates on context window usage. Implement the necessary backend WebSocket handler to emit events when the context window usage changes. Ensure the progress bar updates smoothly without causing UI lag.",
            "status": "pending",
            "testStrategy": "Test WebSocket connection establishment, event handling, and verify real-time updates when messages are added to the conversation.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Integrate progress bar into chat interface and create tests",
            "description": "Add the context window progress bar to the chat interface and create comprehensive tests.",
            "dependencies": [
              1,
              2,
              3,
              4,
              6
            ],
            "details": "Integrate the ContextProgressBar component into the chat interface header or footer area to remain visible during conversations. Create integration test for context API in tests/integration/test_context_api.py to verify the entire feature works correctly. Ensure the component updates smoothly as new messages are added without causing UI lag.",
            "status": "pending",
            "testStrategy": "Create end-to-end tests to verify the progress bar displays correctly in the chat interface and updates appropriately as messages are added. Test with various screen sizes to ensure responsive behavior.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-20T21:02:36.561Z"
      },
      {
        "id": "203",
        "title": "Implement Intelligent Context Condensing Engine",
        "description": "Create a service that automatically summarizes older parts of conversation while preserving essential information",
        "details": "Develop a context condensing engine that uses AI to intelligently summarize conversation history. The service should:\n\n1. Identify when to trigger summarization based on token thresholds\n2. Preserve key information like code snippets, decisions, and file paths\n3. Use Anthropic API (Claude Haiku) for summarization\n4. Track token reduction metrics\n\n```python\nclass ContextCondensingEngine:\n    def __init__(self, token_service: TokenCountingService):\n        self.token_service = token_service\n        self.default_prompt = self._load_default_prompt()\n        self.anthropic_client = AnthropicClient(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n    \n    def _load_default_prompt(self) -> str:\n        # Load the default summarization prompt from a template file\n        with open(\"templates/summarization_prompt.txt\", \"r\") as f:\n            return f.read()\n    \n    async def should_compact(self, context: ConversationContext) -> bool:\n        # Check if compaction should be triggered based on token usage\n        threshold = context.settings.get(\"auto_compact_threshold\", 0.8)  # Default 80%\n        current_usage = context.total_tokens / context.max_tokens\n        \n        # Don't compact if we recently compacted (cooldown period)\n        if context.last_compaction:\n            cooldown = timedelta(seconds=30)  # 30 second cooldown\n            if datetime.now() - context.last_compaction < cooldown:\n                return False\n        \n        return current_usage >= threshold\n    \n    async def compact_conversation(self, \n                                  context: ConversationContext, \n                                  trigger: str = \"auto\",\n                                  custom_prompt: Optional[str] = None) -> CompactionResult:\n        # Start timing for performance metrics\n        start_time = time.time()\n        \n        # Get pre-compaction token count\n        pre_tokens = context.total_tokens\n        \n        # Identify messages to condense (exclude protected messages)\n        condensable_messages = [\n            msg for msg in context.messages \n            if not msg.is_protected and msg.id not in context.protected_message_ids\n        ]\n        \n        # If we have very few condensable messages, don't bother\n        if len(condensable_messages) < 3:\n            return CompactionResult(\n                session_id=context.session_id,\n                trigger=trigger,\n                pre_tokens=pre_tokens,\n                post_tokens=pre_tokens,\n                reduction_percent=0,\n                messages_condensed=0,\n                summary_preview=\"No condensation performed - too few messages\",\n                duration_ms=0,\n                cost_usd=0,\n                created_at=datetime.now()\n            )\n        \n        # Prepare messages for summarization\n        messages_to_summarize = self._prepare_messages_for_summarization(condensable_messages)\n        \n        # Use custom prompt if provided, otherwise use default\n        prompt = custom_prompt if custom_prompt else self.default_prompt\n        \n        # Call Anthropic API for summarization\n        try:\n            summary = await self._summarize_with_anthropic(\n                messages_to_summarize, \n                prompt\n            )\n            \n            # Create a new summary message\n            summary_message = ContextMessage(\n                id=str(uuid.uuid4()),\n                role=\"assistant\",\n                content=summary,\n                token_count=self.token_service.count_message_tokens(summary),\n                is_protected=False,\n                is_summarized=True,\n                original_message_ids=[msg.id for msg in condensable_messages],\n                created_at=datetime.now(),\n                metadata={\"summary_type\": trigger}\n            )\n            \n            # Replace condensed messages with summary\n            new_messages = [\n                msg for msg in context.messages \n                if msg.id not in [cm.id for cm in condensable_messages]\n            ]\n            new_messages.append(summary_message)\n            \n            # Sort messages by creation time\n            new_messages.sort(key=lambda x: x.created_at)\n            \n            # Update context with new messages\n            context.messages = new_messages\n            context.total_tokens = sum(msg.token_count for msg in new_messages)\n            context.last_compaction = datetime.now()\n            context.compaction_count += 1\n            \n            # Calculate metrics\n            post_tokens = context.total_tokens\n            duration_ms = int((time.time() - start_time) * 1000)\n            reduction_percent = ((pre_tokens - post_tokens) / pre_tokens) * 100 if pre_tokens > 0 else 0\n            \n            # Estimate cost (simplified)\n            cost_usd = (pre_tokens / 1000) * 0.0001  # Simplified cost calculation\n            \n            # Create result\n            result = CompactionResult(\n                session_id=context.session_id,\n                trigger=trigger,\n                pre_tokens=pre_tokens,\n                post_tokens=post_tokens,\n                reduction_percent=reduction_percent,\n                messages_condensed=len(condensable_messages),\n                summary_preview=summary[:100] + \"...\",\n                duration_ms=duration_ms,\n                cost_usd=cost_usd,\n                created_at=datetime.now()\n            )\n            \n            # Log compaction result to database\n            await self._log_compaction(result)\n            \n            return result\n            \n        except Exception as e:\n            # Log error and return failed result\n            logger.error(f\"Compaction failed: {str(e)}\")\n            return CompactionResult(\n                session_id=context.session_id,\n                trigger=trigger,\n                pre_tokens=pre_tokens,\n                post_tokens=pre_tokens,\n                reduction_percent=0,\n                messages_condensed=0,\n                summary_preview=f\"Compaction failed: {str(e)}\",\n                duration_ms=int((time.time() - start_time) * 1000),\n                cost_usd=0,\n                created_at=datetime.now()\n            )\n    \n    async def _summarize_with_anthropic(self, messages: List[dict], prompt: str) -> str:\n        # Format messages for Anthropic API\n        formatted_content = \"\\n\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in messages])\n        \n        # Call Anthropic API with Claude Haiku for summarization\n        response = await self.anthropic_client.messages.create(\n            model=\"claude-3-haiku-20240307\",\n            max_tokens=4000,\n            messages=[\n                {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nHERE IS THE CONVERSATION TO SUMMARIZE:\\n\\n{formatted_content}\"}\n            ]\n        )\n        \n        return response.content[0].text\n    \n    def _prepare_messages_for_summarization(self, messages: List[ContextMessage]) -> List[dict]:\n        # Convert ContextMessage objects to dict format for API\n        return [\n            {\"role\": msg.role, \"content\": msg.content}\n            for msg in messages\n        ]\n    \n    async def _log_compaction(self, result: CompactionResult) -> None:\n        # Log compaction result to database\n        async with get_db_connection() as conn:\n            await conn.execute(\n                \"\"\"INSERT INTO compaction_logs \n                   (session_id, trigger, pre_tokens, post_tokens, reduction_percent, \n                    messages_condensed, summary_preview, duration_ms, cost_usd, created_at) \n                   VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)\"\"\",\n                result.session_id, result.trigger, result.pre_tokens, result.post_tokens,\n                result.reduction_percent, result.messages_condensed, result.summary_preview,\n                result.duration_ms, result.cost_usd, result.created_at\n            )\n```\n\nThe service should be integrated with the token counting service to monitor token usage and trigger compaction when necessary. It should also provide hooks for manual compaction via the `/compact` command.",
        "testStrategy": "1. Unit tests:\n   - Test summarization prompt effectiveness with different conversation types\n   - Verify protected messages are never condensed\n   - Test token reduction calculations\n\n2. Integration tests:\n   - Test automatic triggering at configured thresholds\n   - Verify cooldown period prevents rapid successive compactions\n   - Test error handling and recovery\n\n3. Performance tests:\n   - Measure summarization latency for different conversation sizes\n   - Verify summarization completes in <3 seconds\n\n4. Quality tests:\n   - Evaluate summarization quality with different conversation types\n   - Verify key information (code, decisions, file paths) is preserved\n   - Compare token reduction rates across different conversation types",
        "priority": "high",
        "dependencies": [
          "201"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement summarization prompt template",
            "description": "Create the template file for summarization prompts that will guide the AI in condensing conversation context.",
            "dependencies": [],
            "details": "Create the file templates/summarization_prompt.txt with detailed instructions for the AI to preserve important information like code snippets, decisions, and file paths while summarizing conversation history. Include examples of good summaries and specific preservation guidelines.",
            "status": "pending",
            "testStrategy": "Test the prompt with various conversation samples to ensure it preserves critical information. Verify it works well with different conversation types (coding discussions, debugging sessions, etc).",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement TokenCountingService integration",
            "description": "Create the token counting service integration to accurately measure token usage in conversations.",
            "dependencies": [],
            "details": "Implement the TokenCountingService class that will be used by the ContextCondensingEngine to count tokens in messages and track overall token usage. Include methods for counting tokens in individual messages and entire conversations using the appropriate tokenizer for Claude models.",
            "status": "pending",
            "testStrategy": "Unit test with various message types to verify accurate token counting. Compare results with Anthropic's tokenizer to ensure consistency.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement AnthropicClient wrapper",
            "description": "Create a wrapper for the Anthropic API client to handle summarization requests.",
            "dependencies": [],
            "details": "Implement the AnthropicClient class that will handle communication with the Anthropic API. Include proper error handling, retry logic, and API key management. Ensure it supports both Claude Haiku for fast summarization and Claude Sonnet for higher quality summarization when needed.",
            "status": "pending",
            "testStrategy": "Test API integration with mock responses. Verify error handling for API failures, rate limits, and timeout scenarios.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement core ContextCondensingEngine class",
            "description": "Implement the main engine class that will manage the context condensation process.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Complete the implementation of the ContextCondensingEngine class with methods for checking when compaction should occur (should_compact), performing compaction (compact_conversation), and handling the summarization process. Include logic for preserving protected messages and tracking compaction metrics.\n<info added on 2026-01-20T15:11:47.015Z>\nImplemented the ContextCondensingEngine class at app/services/context_condensing_engine.py with comprehensive functionality including:\n\n- Integration with Anthropic API using Claude Haiku/Sonnet models for conversation summarization\n- Token counting via the existing TokenCounter service to determine when compaction is needed\n- Prometheus metrics tracking (COMPACTION_COUNT, COMPACTION_LATENCY, COMPACTION_REDUCTION, COMPACTION_IN_PROGRESS, COMPACTION_COST)\n- Redis-based locking mechanism to prevent concurrent compaction operations\n- 30-second cooldown period between compaction operations to prevent excessive API usage\n- Progress tracking through Redis for monitoring compaction status\n- Cost calculation for API usage to track expenditure\n- Database operations to apply and store compaction results\n- Methods for should_compact and compact_conversation as specified in the requirements\n</info added on 2026-01-20T15:11:47.015Z>",
            "status": "pending",
            "testStrategy": "Unit test the should_compact logic with various token thresholds. Test compact_conversation with mock conversations to verify proper summarization and message handling.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement database logging for compaction results",
            "description": "Create the database schema and logging functionality for tracking compaction results.",
            "dependencies": [
              4
            ],
            "details": "Create the compaction_logs table in the database schema and implement the _log_compaction method to store detailed metrics about each compaction operation. Include fields for session_id, trigger type, pre/post token counts, reduction percentage, duration, and cost estimates.",
            "status": "pending",
            "testStrategy": "Test database logging with mock compaction results. Verify all fields are correctly stored and can be retrieved for reporting.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement Celery task for background compaction",
            "description": "Create a Celery task to handle compaction operations asynchronously.",
            "dependencies": [
              4,
              5
            ],
            "details": "Implement the compact_context Celery task in app/tasks/compaction_tasks.py that will run the compaction process in the background. Include progress tracking using Redis and implement a compaction lock mechanism to prevent duplicate compaction operations on the same conversation.",
            "status": "pending",
            "testStrategy": "Test the Celery task with mock conversations. Verify lock mechanism prevents duplicate compactions. Test progress tracking updates correctly.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Implement rate limiting and cooldown mechanism",
            "description": "Add rate limiting logic to prevent excessive compaction operations.",
            "dependencies": [
              6
            ],
            "details": "Enhance the should_compact method to include a 30-second cooldown period between compaction operations. Implement a Redis-based tracking system to store the timestamp of the last compaction for each conversation and enforce the cooldown period.",
            "status": "pending",
            "testStrategy": "Test rate limiting with rapid compaction requests. Verify cooldown period is enforced correctly. Test edge cases like system clock changes.",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Implement REST API endpoint for manual compaction",
            "description": "Create an API endpoint to allow manual triggering of context compaction.",
            "dependencies": [
              6,
              7
            ],
            "details": "Implement the POST /context/{conversation_id}/compact endpoint in app/routes/context.py that allows users to manually trigger compaction. Include options for fast compaction (using Claude Haiku) vs. standard compaction (using Claude Sonnet). Return a task ID that can be used to check compaction progress.",
            "status": "pending",
            "testStrategy": "Test API endpoint with various request parameters. Verify task creation and progress tracking. Test error handling for invalid conversation IDs.",
            "parentId": "undefined"
          },
          {
            "id": 9,
            "title": "Implement progress tracking API",
            "description": "Create an API endpoint to check the progress of ongoing compaction tasks.",
            "dependencies": [
              8
            ],
            "details": "Implement the GET /tasks/{task_id}/progress endpoint that returns the current progress of a compaction task. Use the Redis-based progress tracking system to provide updates on long-running compaction operations. Include status, percent complete, and estimated time remaining.",
            "status": "pending",
            "testStrategy": "Test progress reporting with mock tasks at various stages of completion. Verify accurate progress reporting and proper handling of completed and failed tasks.",
            "parentId": "undefined"
          },
          {
            "id": 10,
            "title": "Implement Prometheus metrics for compaction monitoring",
            "description": "Add Prometheus metrics to track compaction performance and effectiveness.",
            "dependencies": [
              5,
              8
            ],
            "details": "Implement Prometheus metrics for monitoring compaction operations, including compaction_count (counter), compaction_latency_seconds (histogram), and compaction_reduction_percent (gauge). Add these metrics to the existing Prometheus monitoring system and create a Grafana dashboard for visualizing compaction performance.",
            "status": "pending",
            "testStrategy": "Test metric recording with mock compaction operations. Verify metrics are correctly exposed via the Prometheus endpoint. Test dashboard visualization with sample data.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-20T15:11:47.398Z"
      },
      {
        "id": "204",
        "title": "Create Inline Compaction UI with Visual Divider",
        "description": "Implement the UI components for displaying compaction results inline with a visual divider and collapsible summary section",
        "details": "Develop UI components that show when compaction has occurred in the conversation. The implementation should:\n\n1. Display a visual divider showing token reduction metrics\n2. Provide a collapsible section with the condensed summary\n3. Allow users to expand/collapse the summary\n4. Show animation during compaction process\n\n```jsx\n// React component for compaction divider\nimport React, { useState } from 'react';\nimport { ChevronDownIcon, ChevronRightIcon } from '@heroicons/react/solid';\n\nconst CompactionDivider = ({ \n  preTokens, \n  postTokens, \n  summary,\n  timestamp,\n  isExpanded = false\n}) => {\n  const [expanded, setExpanded] = useState(isExpanded);\n  const reductionPercent = ((preTokens - postTokens) / preTokens * 100).toFixed(0);\n  \n  // Format token counts for display\n  const formatTokens = (tokens) => {\n    return tokens >= 1000 ? `${(tokens/1000).toFixed(1)}K` : tokens;\n  };\n  \n  return (\n    <div className=\"my-4 border-t border-b border-gray-200 py-2\">\n      {/* Divider header with metrics */}\n      <div \n        className=\"flex items-center cursor-pointer hover:bg-gray-50 p-2 rounded\"\n        onClick={() => setExpanded(!expanded)}\n      >\n        {expanded ? \n          <ChevronDownIcon className=\"h-5 w-5 text-gray-500\" /> : \n          <ChevronRightIcon className=\"h-5 w-5 text-gray-500\" />\n        }\n        <div className=\"ml-2 text-sm text-gray-600\">\n          <span className=\"font-medium\">Context condensed</span> ({formatTokens(preTokens)} → {formatTokens(postTokens)} tokens, {reductionPercent}% reduction)\n        </div>\n        <div className=\"ml-auto text-xs text-gray-400\">\n          {new Date(timestamp).toLocaleTimeString()}\n        </div>\n      </div>\n      \n      {/* Collapsible summary section */}\n      {expanded && (\n        <div className=\"mt-2 p-3 bg-gray-50 rounded text-sm border-l-4 border-blue-400\">\n          <div className=\"font-medium mb-2 text-gray-700\">Condensed Summary:</div>\n          <div className=\"whitespace-pre-wrap\">{summary}</div>\n        </div>\n      )}\n    </div>\n  );\n};\n\n// Animation component for compaction in progress\nconst CompactionInProgress = () => {\n  return (\n    <div className=\"my-4 p-4 border border-yellow-200 bg-yellow-50 rounded-md flex items-center\">\n      <div className=\"animate-spin rounded-full h-5 w-5 border-b-2 border-yellow-700 mr-3\"></div>\n      <div className=\"text-sm text-yellow-800\">\n        Condensing conversation context...\n      </div>\n    </div>\n  );\n};\n\nexport { CompactionDivider, CompactionInProgress };\n```\n\nThis component should be inserted into the conversation flow whenever compaction occurs. It should provide a clear visual indication that compaction has happened while allowing users to see what was condensed if needed.",
        "testStrategy": "1. Unit tests:\n   - Test rendering with different token counts and summaries\n   - Verify expand/collapse functionality works correctly\n   - Test animation component rendering\n\n2. Integration tests:\n   - Verify divider appears after compaction completes\n   - Test interaction with the chat interface\n   - Verify metrics display correctly\n\n3. Visual regression tests:\n   - Capture screenshots of divider in different states\n   - Ensure consistent rendering across browsers\n\n4. User acceptance testing:\n   - Verify divider is noticeable but not disruptive\n   - Test expand/collapse interaction\n   - Confirm summary content is readable and well-formatted",
        "priority": "medium",
        "dependencies": [
          "202",
          "203"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create CompactionDivider.tsx component",
            "description": "Implement the base CompactionDivider component with expand/collapse functionality",
            "dependencies": [],
            "details": "Create the CompactionDivider.tsx file in empire-desktop/src/components/ directory. Implement the component with props for preTokens, postTokens, summary, timestamp, and isExpanded. Include the expand/collapse toggle functionality with useState hook and appropriate styling for the divider header.",
            "status": "pending",
            "testStrategy": "Unit test the component rendering with various prop combinations. Test the expand/collapse functionality works correctly. Verify token formatting logic works for different token counts.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement CompactionInProgress animation component",
            "description": "Create the animation component that displays during the compaction process",
            "dependencies": [],
            "details": "Implement the CompactionInProgress component in the same file as CompactionDivider. Add the loading spinner animation and appropriate styling to indicate that compaction is in progress. Ensure the component is visually distinct from the CompactionDivider component.",
            "status": "pending",
            "testStrategy": "Test that the animation renders correctly. Verify the spinner animation works in different browsers. Test accessibility features for users with motion sensitivity.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement token reduction metrics display",
            "description": "Add functionality to calculate and display token reduction metrics in the divider header",
            "dependencies": [
              1
            ],
            "details": "Enhance the CompactionDivider component to calculate and display token reduction metrics. Implement the formatTokens function to display large token counts in a readable format (e.g., 1.2K). Calculate and display the reduction percentage. Ensure the metrics are properly aligned in the UI.",
            "status": "pending",
            "testStrategy": "Test the token formatting function with various input values. Verify reduction percentage calculation is accurate. Test edge cases like 0 tokens or when reduction is negative.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create collapsible summary section with styling",
            "description": "Implement the collapsible summary section that displays the condensed content",
            "dependencies": [
              1
            ],
            "details": "Enhance the CompactionDivider component to display the condensed summary in a collapsible section. Style the summary section with appropriate background, borders, and typography. Ensure the summary text preserves whitespace and formatting. Add proper padding and margins for readability.",
            "status": "pending",
            "testStrategy": "Test rendering of different summary content types (plain text, code blocks, etc.). Verify whitespace preservation. Test with very long summaries to ensure proper scrolling behavior.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add timestamp display and responsive design",
            "description": "Implement timestamp display and ensure responsive design across different screen sizes",
            "dependencies": [
              1,
              3
            ],
            "details": "Add functionality to format and display the timestamp when compaction occurred. Ensure the component layout is responsive and works well on different screen sizes. Implement proper alignment of elements in the header (chevron icon, text, timestamp) using flexbox. Test on mobile and desktop viewports.",
            "status": "pending",
            "testStrategy": "Test timestamp formatting with different locales and timezones. Verify responsive behavior on various screen sizes. Test accessibility of the component with screen readers.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Integrate compaction UI into chat message list",
            "description": "Integrate the CompactionDivider and CompactionInProgress components into the chat message list",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Update the chat message list component to include CompactionDivider and CompactionInProgress components at appropriate positions in the conversation flow. Add logic to determine when to show the compaction in progress animation versus the completed divider. Ensure proper spacing between messages and compaction dividers.",
            "status": "pending",
            "testStrategy": "Integration tests to verify divider appears after compaction completes. Test interaction with the chat interface. Verify proper positioning of dividers between message groups. Test with multiple compaction events in a single conversation.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-20T21:15:40.585Z"
      },
      {
        "id": "205",
        "title": "Implement Protected Message Handling",
        "description": "Create functionality to mark and preserve specific messages from being condensed during compaction",
        "details": "Implement a system to mark certain messages as protected so they are never condensed during compaction. The implementation should:\n\n1. Automatically protect system prompts and initial messages\n2. Allow users to mark additional messages as protected\n3. Provide visual indicators for protected messages\n4. Store protection status in the message metadata\n\n```python\nclass ProtectedMessageService:\n    def __init__(self, db_service):\n        self.db = db_service\n    \n    async def mark_as_protected(self, session_id: str, message_id: str) -> bool:\n        \"\"\"Mark a specific message as protected from compaction\"\"\"\n        try:\n            # Update message protection status\n            async with self.db.transaction() as conn:\n                # Update the message itself\n                await conn.execute(\n                    \"\"\"UPDATE messages \n                       SET is_protected = true \n                       WHERE id = $1 AND session_id = $2\"\"\",\n                    message_id, session_id\n                )\n                \n                # Add to protected_message_ids array in conversation_contexts\n                await conn.execute(\n                    \"\"\"UPDATE conversation_contexts \n                       SET protected_message_ids = array_append(protected_message_ids, $1) \n                       WHERE session_id = $2\"\"\",\n                    message_id, session_id\n                )\n            \n            return True\n        except Exception as e:\n            logger.error(f\"Failed to mark message as protected: {str(e)}\")\n            return False\n    \n    async def unmark_as_protected(self, session_id: str, message_id: str) -> bool:\n        \"\"\"Remove protection from a message\"\"\"\n        try:\n            # Update message protection status\n            async with self.db.transaction() as conn:\n                # Update the message itself\n                await conn.execute(\n                    \"\"\"UPDATE messages \n                       SET is_protected = false \n                       WHERE id = $1 AND session_id = $2\"\"\",\n                    message_id, session_id\n                )\n                \n                # Remove from protected_message_ids array in conversation_contexts\n                await conn.execute(\n                    \"\"\"UPDATE conversation_contexts \n                       SET protected_message_ids = array_remove(protected_message_ids, $1) \n                       WHERE session_id = $2\"\"\",\n                    message_id, session_id\n                )\n            \n            return True\n        except Exception as e:\n            logger.error(f\"Failed to unmark message as protected: {str(e)}\")\n            return False\n    \n    async def get_protected_messages(self, session_id: str) -> List[str]:\n        \"\"\"Get all protected message IDs for a session\"\"\"\n        try:\n            result = await self.db.fetch_one(\n                \"\"\"SELECT protected_message_ids \n                   FROM conversation_contexts \n                   WHERE session_id = $1\"\"\",\n                session_id\n            )\n            \n            return result[\"protected_message_ids\"] if result else []\n        except Exception as e:\n            logger.error(f\"Failed to get protected messages: {str(e)}\")\n            return []\n    \n    def is_system_message(self, message: ContextMessage) -> bool:\n        \"\"\"Check if a message is a system message that should be auto-protected\"\"\"\n        # System messages are always protected\n        if message.role == \"system\":\n            return True\n            \n        # Check if it's the first user message (often contains setup instructions)\n        if message.metadata.get(\"is_first_message\", False):\n            return True\n            \n        # Check for slash commands that set up the conversation\n        if message.role == \"user\" and message.content.startswith(\"/\"):\n            # List of setup commands that should be protected\n            setup_commands = [\"/system\", \"/config\", \"/mode\", \"/project\", \"/setup\"]\n            for cmd in setup_commands:\n                if message.content.startswith(cmd):\n                    return True\n        \n        return False\n```\n\n```jsx\n// React component for protected message indicator\nconst ProtectedMessageIndicator = ({ message, onToggleProtection }) => {\n  const isProtected = message.is_protected;\n  \n  return (\n    <div className=\"message-protection-controls\">\n      <button \n        className={`protection-toggle ${isProtected ? 'active' : ''}`}\n        onClick={() => onToggleProtection(message.id, !isProtected)}\n        title={isProtected ? \"Protected from summarization\" : \"Click to protect\"}\n      >\n        {isProtected ? (\n          <LockClosedIcon className=\"h-4 w-4 text-blue-500\" />\n        ) : (\n          <LockOpenIcon className=\"h-4 w-4 text-gray-400 hover:text-blue-500\" />\n        )}\n      </button>\n    </div>\n  );\n};\n```\n\nThe system should automatically identify and protect system messages and initial setup instructions. It should also provide a way for users to manually mark/unmark messages as protected via the UI or API.",
        "testStrategy": "1. Unit tests:\n   - Test automatic protection of system messages\n   - Verify protection status is correctly stored and retrieved\n   - Test protection toggle functionality\n\n2. Integration tests:\n   - Verify protected messages are excluded from compaction\n   - Test UI indicator for protected messages\n   - Verify protection status persists across sessions\n\n3. API tests:\n   - Test protection API endpoints\n   - Verify error handling for invalid message IDs\n\n4. User acceptance testing:\n   - Verify protection controls are intuitive\n   - Test protection status is clearly indicated\n   - Confirm protected messages are never condensed",
        "priority": "medium",
        "dependencies": [
          "203"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement toggle_message_protection() in context_manager_service.py",
            "description": "Create a method to toggle protection status of messages in the context manager service",
            "dependencies": [],
            "details": "Add a toggle_message_protection() method to app/services/context_manager_service.py that calls the appropriate methods from ProtectedMessageService. This method should accept session_id and message_id parameters and toggle the current protection status. It should handle both protecting and unprotecting messages based on current state.",
            "status": "pending",
            "testStrategy": "Unit test the toggle_message_protection() method with various scenarios including protecting an unprotected message, unprotecting a protected message, and handling edge cases like non-existent messages.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement PATCH endpoint for message protection",
            "description": "Create API endpoint to toggle protection status of specific messages",
            "dependencies": [
              1
            ],
            "details": "Implement the PATCH /context/{conversation_id}/messages/{message_id}/protect endpoint in the appropriate router file. This endpoint should validate the request, call the toggle_message_protection() method from the context manager service, and return appropriate success/error responses with proper status codes.",
            "status": "pending",
            "testStrategy": "Test the API endpoint with valid and invalid requests, verify authentication requirements, and confirm proper error handling for non-existent conversations or messages.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Update summarization service to exclude protected messages",
            "description": "Modify the summarization logic to filter out protected messages before compaction",
            "dependencies": [
              1
            ],
            "details": "Update the summarization/compaction service to check the protection status of messages before including them in the summarization process. Implement a filter that excludes any messages marked as protected or that are in the protected_message_ids array of the conversation context. Ensure the summarization logic preserves these messages in their original form.",
            "status": "pending",
            "testStrategy": "Test the summarization process with a mix of protected and unprotected messages to verify that protected messages are correctly excluded from summarization while still maintaining conversation coherence.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement automatic protection for system and initial messages",
            "description": "Add logic to automatically protect system prompts and initial conversation messages",
            "dependencies": [
              1
            ],
            "details": "Enhance the message creation or processing flow to automatically mark system messages and initial conversation messages as protected. Use the is_system_message() method from ProtectedMessageService to identify messages that should be auto-protected. Update the database schema if needed to support tracking the 'is_first_message' flag in message metadata.",
            "status": "pending",
            "testStrategy": "Test automatic protection by creating new conversations with system prompts and initial messages, then verify they are correctly marked as protected. Test edge cases like conversations without explicit system messages.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create protected message UI indicator component",
            "description": "Implement the React component for displaying protection status of messages",
            "dependencies": [],
            "details": "Implement the ProtectedMessageIndicator React component that displays a lock icon to indicate message protection status. The component should render differently based on the protection status of the message and provide visual feedback when hovering or clicking. Include proper styling for both protected and unprotected states.",
            "status": "pending",
            "testStrategy": "Test the component rendering in different states (protected/unprotected), verify proper icon display, and test the click handler functionality. Use React Testing Library to simulate user interactions.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Add protection toggle to message context menu",
            "description": "Integrate protection toggle functionality into the message context menu in the UI",
            "dependencies": [
              2,
              5
            ],
            "details": "Extend the message context menu in the desktop UI to include an option for toggling message protection. Connect the UI action to the API endpoint for toggling protection status. Update the message display to show the protection indicator when a message is protected. Ensure the UI state updates immediately after toggling while also handling potential API failures gracefully.",
            "status": "pending",
            "testStrategy": "Test the integration of the protection toggle in the message context menu, verify the API calls are made correctly, and test error handling when the API call fails. Test the UI state updates correctly after toggling protection status.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-20T15:16:49.765Z"
      },
      {
        "id": "206",
        "title": "Implement Automatic Checkpoint System",
        "description": "Create a system that automatically saves conversation checkpoints at key moments to protect against crashes and data loss",
        "details": "Develop an automatic checkpoint system that saves the conversation state at important moments. The implementation should:\n\n1. Detect important context like code generation and decisions\n2. Save checkpoints before compaction and at regular intervals\n3. Store checkpoints in Supabase with metadata\n4. Implement cleanup to maintain only the most recent checkpoints\n\n```python\nclass CheckpointService:\n    def __init__(self, db_service):\n        self.db = db_service\n        self.checkpoint_limit = 50  # Maximum checkpoints per session\n    \n    async def create_checkpoint(self, \n                               context: ConversationContext, \n                               trigger: str = \"auto\",\n                               label: Optional[str] = None) -> str:\n        \"\"\"Create a new checkpoint of the current conversation state\"\"\"\n        try:\n            # Generate checkpoint ID\n            checkpoint_id = str(uuid.uuid4())\n            \n            # Auto-detect tags based on content\n            auto_tags = await self._detect_content_tags(context.messages)\n            \n            # Generate automatic label if none provided\n            if not label:\n                label = await self._generate_auto_label(context.messages, auto_tags)\n            \n            # Prepare checkpoint data\n            checkpoint = {\n                \"id\": checkpoint_id,\n                \"session_id\": context.session_id,\n                \"user_id\": context.user_id,\n                \"project_id\": context.project_id,\n                \"label\": label,\n                \"trigger\": trigger,\n                \"messages_snapshot\": [msg.dict() for msg in context.messages],\n                \"token_count\": context.total_tokens,\n                \"auto_tags\": auto_tags,\n                \"metadata\": {\n                    \"model\": context.model,\n                    \"compaction_count\": context.compaction_count,\n                },\n                \"created_at\": datetime.now()\n            }\n            \n            # Save checkpoint to database\n            await self.db.execute(\n                \"\"\"INSERT INTO session_checkpoints \n                   (id, session_id, user_id, project_id, label, trigger, \n                    messages_snapshot, token_count, auto_tags, metadata, created_at) \n                   VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)\"\"\",\n                checkpoint_id, context.session_id, context.user_id, context.project_id,\n                label, trigger, json.dumps(checkpoint[\"messages_snapshot\"]),\n                context.total_tokens, auto_tags, json.dumps(checkpoint[\"metadata\"]),\n                checkpoint[\"created_at\"]\n            )\n            \n            # Cleanup old checkpoints if we exceed the limit\n            await self._cleanup_old_checkpoints(context.session_id)\n            \n            return checkpoint_id\n        except Exception as e:\n            logger.error(f\"Failed to create checkpoint: {str(e)}\")\n            return None\n    \n    async def _detect_content_tags(self, messages: List[ContextMessage]) -> List[str]:\n        \"\"\"Detect content types in recent messages to auto-tag the checkpoint\"\"\"\n        tags = set()\n        \n        # Look at the last 5 messages\n        recent_messages = messages[-5:] if len(messages) > 5 else messages\n        \n        for msg in recent_messages:\n            content = msg.content.lower()\n            \n            # Check for code blocks\n            if \"```\" in content:\n                tags.add(\"code\")\n            \n            # Check for file paths\n            if re.search(r'[\\w\\-\\.]+\\.[a-zA-Z]{2,4}', content):\n                tags.add(\"file_reference\")\n            \n            # Check for decision language\n            decision_phrases = [\"decided to\", \"will use\", \"chosen\", \"selected\", \"going with\"]\n            if any(phrase in content for phrase in decision_phrases):\n                tags.add(\"decision\")\n            \n            # Check for error messages\n            error_phrases = [\"error:\", \"exception:\", \"failed:\", \"traceback\"]\n            if any(phrase in content for phrase in error_phrases):\n                tags.add(\"error\")\n                \n            # Check for task completion\n            completion_phrases = [\"completed\", \"finished\", \"done\", \"implemented\", \"fixed\"]\n            if any(phrase in content for phrase in completion_phrases):\n                tags.add(\"task_complete\")\n        \n        return list(tags)\n    \n    async def _generate_auto_label(self, messages: List[ContextMessage], tags: List[str]) -> str:\n        \"\"\"Generate an automatic label based on content and tags\"\"\"\n        # Start with timestamp\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        \n        # If we have tags, use them to create a descriptive label\n        if \"code\" in tags:\n            return f\"Code generated ({timestamp})\"\n        elif \"decision\" in tags:\n            return f\"Decision made ({timestamp})\"\n        elif \"error\" in tags:\n            return f\"Error encountered ({timestamp})\"\n        elif \"task_complete\" in tags:\n            return f\"Task completed ({timestamp})\"\n        else:\n            return f\"Checkpoint ({timestamp})\"\n    \n    async def _cleanup_old_checkpoints(self, session_id: str) -> None:\n        \"\"\"Keep only the most recent checkpoints for a session\"\"\"\n        try:\n            # Count checkpoints for this session\n            count = await self.db.fetch_val(\n                \"\"\"SELECT COUNT(*) FROM session_checkpoints WHERE session_id = $1\"\"\",\n                session_id\n            )\n            \n            # If we're over the limit, delete the oldest ones\n            if count > self.checkpoint_limit:\n                to_delete = count - self.checkpoint_limit\n                await self.db.execute(\n                    \"\"\"DELETE FROM session_checkpoints \n                       WHERE id IN (\n                           SELECT id FROM session_checkpoints \n                           WHERE session_id = $1 \n                           ORDER BY created_at ASC \n                           LIMIT $2\n                       )\"\"\",\n                    session_id, to_delete\n                )\n        except Exception as e:\n            logger.error(f\"Failed to cleanup old checkpoints: {str(e)}\")\n    \n    async def get_checkpoints(self, session_id: str) -> List[dict]:\n        \"\"\"Get all checkpoints for a session\"\"\"\n        try:\n            rows = await self.db.fetch_all(\n                \"\"\"SELECT id, label, trigger, token_count, auto_tags, created_at \n                   FROM session_checkpoints \n                   WHERE session_id = $1 \n                   ORDER BY created_at DESC\"\"\",\n                session_id\n            )\n            \n            return [dict(row) for row in rows]\n        except Exception as e:\n            logger.error(f\"Failed to get checkpoints: {str(e)}\")\n            return []\n    \n    async def restore_checkpoint(self, checkpoint_id: str) -> Optional[ConversationContext]:\n        \"\"\"Restore conversation from a checkpoint\"\"\"\n        try:\n            # Get checkpoint data\n            row = await self.db.fetch_one(\n                \"\"\"SELECT * FROM session_checkpoints WHERE id = $1\"\"\",\n                checkpoint_id\n            )\n            \n            if not row:\n                return None\n            \n            # Parse messages from JSON\n            messages_data = json.loads(row[\"messages_snapshot\"])\n            messages = [ContextMessage(**msg) for msg in messages_data]\n            \n            # Recreate conversation context\n            context = ConversationContext(\n                session_id=row[\"session_id\"],\n                user_id=row[\"user_id\"],\n                project_id=row[\"project_id\"],\n                messages=messages,\n                total_tokens=row[\"token_count\"],\n                max_tokens=messages[0].metadata.get(\"max_tokens\", 100000),  # Default fallback\n                model=json.loads(row[\"metadata\"]).get(\"model\", \"claude-3-haiku\"),\n                protected_message_ids=[msg.id for msg in messages if msg.is_protected],\n                last_compaction=None,  # Reset compaction timestamp\n                compaction_count=json.loads(row[\"metadata\"]).get(\"compaction_count\", 0),\n                created_at=messages[0].created_at if messages else datetime.now(),\n                updated_at=datetime.now()\n            )\n            \n            return context\n        except Exception as e:\n            logger.error(f\"Failed to restore checkpoint: {str(e)}\")\n            return None\n```\n\nThe checkpoint system should run in the background without disrupting the user experience. It should automatically detect important moments to save checkpoints and provide a way to manually create checkpoints via the `/save-progress` or `/checkpoint` commands.",
        "testStrategy": "1. Unit tests:\n   - Test content tag detection for different message types\n   - Verify automatic label generation\n   - Test checkpoint cleanup functionality\n\n2. Integration tests:\n   - Verify checkpoints are created at important moments\n   - Test checkpoint restoration accuracy\n   - Verify checkpoint limit enforcement\n\n3. Performance tests:\n   - Measure checkpoint creation time for different conversation sizes\n   - Verify no UI lag during checkpoint operations\n\n4. Recovery tests:\n   - Simulate crashes and verify recovery from checkpoints\n   - Test restoration of different conversation states\n   - Verify token counts are preserved during restoration",
        "priority": "high",
        "dependencies": [
          "203",
          "205",
          "211"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement create_checkpoint method",
            "description": "Implement the create_checkpoint method in the CheckpointService class to save conversation state at important moments.",
            "dependencies": [],
            "details": "Complete the implementation of the create_checkpoint method in app/services/checkpoint_service.py. This method should generate a unique ID for each checkpoint, detect content tags, generate automatic labels, and save the checkpoint data to the database. Ensure proper error handling and logging.",
            "status": "pending",
            "testStrategy": "Unit test the create_checkpoint method with various conversation contexts. Verify checkpoint creation with different triggers and validate the structure of saved checkpoints.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement checkpoint retrieval methods",
            "description": "Implement get_checkpoints and restore_checkpoint methods in the CheckpointService class.",
            "dependencies": [
              1
            ],
            "details": "Complete the implementation of get_checkpoints to retrieve all checkpoints for a session and restore_checkpoint to recreate a conversation context from a checkpoint. Ensure proper error handling, data validation, and type conversion when restoring context objects.",
            "status": "pending",
            "testStrategy": "Test checkpoint retrieval with various session IDs. Test restoration with valid and invalid checkpoint IDs. Verify the restored context matches the original context structure.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement content tag detection",
            "description": "Implement the _detect_content_tags method to identify important conversation moments for automatic checkpointing.",
            "dependencies": [],
            "details": "Complete the implementation of _detect_content_tags to analyze message content and identify patterns indicating code generation, decisions, file references, errors, and task completions. Use regex and keyword matching to identify these patterns in recent messages.",
            "status": "pending",
            "testStrategy": "Test tag detection with various message contents including code blocks, decision statements, error messages, and task completion statements. Verify correct tag assignment for each content type.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement automatic label generation",
            "description": "Implement the _generate_auto_label method to create descriptive labels for checkpoints based on content tags.",
            "dependencies": [
              3
            ],
            "details": "Complete the implementation of _generate_auto_label to create meaningful checkpoint labels based on detected content tags and timestamps. Create specific label formats for code generation, decisions, errors, and task completions.",
            "status": "pending",
            "testStrategy": "Test label generation with various tag combinations. Verify timestamps are included correctly and labels are descriptive of the checkpoint content.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement checkpoint cleanup",
            "description": "Implement the _cleanup_old_checkpoints method to maintain only the most recent checkpoints.",
            "dependencies": [
              1
            ],
            "details": "Complete the implementation of _cleanup_old_checkpoints to enforce the checkpoint limit (50 per session). Query the database to count existing checkpoints and delete the oldest ones when the limit is exceeded. Ensure proper error handling and transaction management.",
            "status": "pending",
            "testStrategy": "Test cleanup with various checkpoint counts. Verify oldest checkpoints are removed when limit is exceeded. Test error handling during cleanup operations.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement checkpoint API endpoints",
            "description": "Create REST API endpoints for manual checkpoint creation and retrieval.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement POST /checkpoints/{conversation_id} endpoint for manual checkpoint creation via /save-progress command and GET /checkpoints/{conversation_id} endpoint to list available checkpoints. Include proper request validation, authentication, and response formatting.",
            "status": "pending",
            "testStrategy": "Test API endpoints with valid and invalid requests. Verify authentication requirements. Test response formats and error handling for various scenarios.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Implement automatic checkpoint triggers",
            "description": "Create a system to automatically trigger checkpoints at key moments in conversations.",
            "dependencies": [
              1,
              3
            ],
            "details": "Implement automatic checkpoint triggers based on content detection (code generation, decisions) and system events (before compaction, at regular intervals). Create a background task or event listener to monitor conversations and trigger checkpoints when important moments are detected.",
            "status": "pending",
            "testStrategy": "Test automatic triggers with simulated conversation events. Verify checkpoints are created at appropriate moments. Test timing of automatic checkpoints relative to compaction events.",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Implement checkpoint expiration",
            "description": "Add a 30-day TTL (Time To Live) for checkpoints to manage storage efficiently.",
            "dependencies": [
              1
            ],
            "details": "Implement checkpoint expiration by adding created_at timestamps and a scheduled task to remove checkpoints older than 30 days. Update the database schema to include TTL constraints if supported, or implement a periodic cleanup job.",
            "status": "pending",
            "testStrategy": "Test expiration with checkpoints of various ages. Verify checkpoints older than 30 days are removed. Test the cleanup job scheduling and execution.",
            "parentId": "undefined"
          },
          {
            "id": 9,
            "title": "Implement crash recovery detection",
            "description": "Create an endpoint to detect abnormal session terminations and enable recovery.",
            "dependencies": [
              2
            ],
            "details": "Implement GET /recovery/check endpoint to detect if a session was abnormally terminated and needs recovery. This should check for sessions without proper closure events and with recent checkpoints available. Return recovery options if applicable.",
            "status": "pending",
            "testStrategy": "Test recovery detection with various session states. Simulate crashes and verify detection. Test recovery options presentation for different checkpoint availability scenarios.",
            "parentId": "undefined"
          },
          {
            "id": 10,
            "title": "Implement checkpoint restoration",
            "description": "Create an endpoint to restore conversation state from a checkpoint.",
            "dependencies": [
              2,
              9
            ],
            "details": "Implement POST /recovery/{conversation_id}/restore endpoint to restore a conversation from a specified checkpoint. Handle the restoration process, including rebuilding the conversation context, updating the session state, and notifying the user of the restoration.",
            "status": "pending",
            "testStrategy": "Test restoration with various checkpoint types. Verify conversation state is accurately restored. Test user notification and session state updates after restoration.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-20T21:09:24.428Z"
      },
      {
        "id": "207",
        "title": "Implement Session Memory & Persistence",
        "description": "Create a system to store and retrieve conversation summaries across sessions for continuity",
        "details": "Develop a session memory system that stores conversation summaries in Supabase and retrieves relevant memories when starting new sessions. The implementation should:\n\n1. Save conversation summaries with metadata\n2. Store per-project context memory\n3. Implement relevance-based memory retrieval\n4. Support full session resumption\n\n```python\nclass SessionMemoryService:\n    def __init__(self, db_service, embedding_service):\n        self.db = db_service\n        self.embedding_service = embedding_service\n        self.memory_expiration_days = 30  # Default TTL for memories\n    \n    async def save_session_memory(self, context: ConversationContext) -> str:\n        \"\"\"Save a summary of the current session as a memory\"\"\"\n        try:\n            # Generate a summary of the conversation\n            summary = await self._generate_conversation_summary(context.messages)\n            \n            # Extract key decisions and code references\n            key_decisions = await self._extract_key_decisions(context.messages)\n            code_references = await self._extract_code_references(context.messages)\n            \n            # Generate tags for the memory\n            tags = await self._generate_memory_tags(context.messages, summary)\n            \n            # Generate embedding for semantic search\n            embedding = await self.embedding_service.generate_embedding(summary)\n            \n            # Calculate expiration date\n            expires_at = datetime.now() + timedelta(days=self.memory_expiration_days)\n            \n            # Create memory ID\n            memory_id = str(uuid.uuid4())\n            \n            # Save memory to database\n            await self.db.execute(\n                \"\"\"INSERT INTO session_memories \n                   (id, user_id, project_id, session_id, summary, key_decisions, \n                    code_references, tags, relevance_score, embedding, created_at, expires_at) \n                   VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)\"\"\",\n                memory_id, context.user_id, context.project_id, context.session_id,\n                summary, json.dumps(key_decisions), json.dumps(code_references),\n                tags, 1.0, embedding, datetime.now(), expires_at\n            )\n            \n            return memory_id\n        except Exception as e:\n            logger.error(f\"Failed to save session memory: {str(e)}\")\n            return None\n    \n    async def _generate_conversation_summary(self, messages: List[ContextMessage]) -> str:\n        \"\"\"Generate a concise summary of the conversation\"\"\"\n        # Use the same summarization engine as the context condensing\n        # This could call the ContextCondensingEngine or implement similar logic\n        # For brevity, we'll assume a simplified implementation here\n        \n        # Format messages for summarization\n        formatted_messages = []\n        for msg in messages:\n            if not msg.is_summarized:  # Skip already summarized messages\n                formatted_messages.append({\n                    \"role\": msg.role,\n                    \"content\": msg.content\n                })\n        \n        # Call Anthropic API for summarization\n        prompt = \"\"\"Summarize this conversation into a concise memory that captures the key points, \n                   decisions, code snippets, and important context. Focus on technical details \n                   that would be useful to remember in future related conversations.\"\"\"\n        \n        # This would call the actual API in production\n        # For brevity, we're returning a placeholder\n        return \"Session summary placeholder\"  # In real implementation, call API\n    \n    async def _extract_key_decisions(self, messages: List[ContextMessage]) -> List[str]:\n        \"\"\"Extract key decisions from the conversation\"\"\"\n        decisions = []\n        \n        # Simple heuristic: Look for decision language in assistant messages\n        decision_phrases = [\"decided to\", \"will use\", \"chosen\", \"selected\", \"going with\"]\n        \n        for msg in messages:\n            if msg.role == \"assistant\":\n                content = msg.content.lower()\n                for phrase in decision_phrases:\n                    if phrase in content:\n                        # Extract the sentence containing the decision\n                        sentences = re.split(r'(?<=[.!?]) +', msg.content)\n                        for sentence in sentences:\n                            if phrase in sentence.lower():\n                                decisions.append(sentence.strip())\n        \n        return decisions\n    \n    async def _extract_code_references(self, messages: List[ContextMessage]) -> List[dict]:\n        \"\"\"Extract code snippets and file references\"\"\"\n        code_refs = []\n        \n        for msg in messages:\n            # Extract code blocks\n            code_blocks = re.findall(r'```([\\w]*)[\\n\\r]([\\s\\S]*?)```', msg.content)\n            for lang, code in code_blocks:\n                if len(code.strip()) > 0:\n                    code_refs.append({\n                        \"type\": \"code_block\",\n                        \"language\": lang.strip() or \"text\",\n                        \"content\": code.strip(),\n                        \"message_id\": msg.id\n                    })\n            \n            # Extract file paths\n            file_paths = re.findall(r'[\\w\\-\\.\\/_]+\\.[a-zA-Z]{2,4}', msg.content)\n            for path in file_paths:\n                code_refs.append({\n                    \"type\": \"file_reference\",\n                    \"path\": path,\n                    \"message_id\": msg.id\n                })\n        \n        return code_refs\n    \n    async def _generate_memory_tags(self, messages: List[ContextMessage], summary: str) -> List[str]:\n        \"\"\"Generate tags for the memory based on content\"\"\"\n        tags = set()\n        \n        # Add project name if available\n        if messages and messages[0].metadata.get(\"project_name\"):\n            tags.add(messages[0].metadata[\"project_name\"])\n        \n        # Check for common programming languages\n        languages = [\"python\", \"javascript\", \"typescript\", \"java\", \"c++\", \"rust\", \"go\"]\n        for lang in languages:\n            if lang in summary.lower():\n                tags.add(lang)\n        \n        # Check for frameworks and technologies\n        frameworks = [\"react\", \"vue\", \"angular\", \"django\", \"flask\", \"express\", \"tensorflow\"]\n        for framework in frameworks:\n            if framework in summary.lower():\n                tags.add(framework)\n        \n        # Add generic tags based on content\n        if \"```\" in summary:\n            tags.add(\"code\")\n        if \"error\" in summary.lower() or \"exception\" in summary.lower():\n            tags.add(\"debugging\")\n        if \"test\" in summary.lower() or \"spec\" in summary.lower():\n            tags.add(\"testing\")\n        \n        return list(tags)\n    \n    async def get_relevant_memories(self, query: str, project_id: Optional[str] = None, limit: int = 3) -> List[dict]:\n        \"\"\"Get relevant memories based on semantic similarity\"\"\"\n        try:\n            # Generate embedding for query\n            query_embedding = await self.embedding_service.generate_embedding(query)\n            \n            # Build SQL query\n            sql = \"\"\"\n                SELECT id, summary, key_decisions, tags, relevance_score, created_at,\n                       1 - (embedding <=> $1) as similarity\n                FROM session_memories\n                WHERE expires_at > NOW()\n            \"\"\"\n            \n            params = [query_embedding]\n            \n            # Add project filter if specified\n            if project_id:\n                sql += \" AND project_id = $2\"\n                params.append(project_id)\n            \n            # Add order and limit\n            sql += \" ORDER BY similarity DESC LIMIT $\" + str(len(params) + 1)\n            params.append(limit)\n            \n            # Execute query\n            rows = await self.db.fetch_all(sql, *params)\n            \n            # Format results\n            memories = []\n            for row in rows:\n                memories.append({\n                    \"id\": row[\"id\"],\n                    \"summary\": row[\"summary\"],\n                    \"key_decisions\": json.loads(row[\"key_decisions\"]),\n                    \"tags\": row[\"tags\"],\n                    \"relevance_score\": row[\"similarity\"],  # Use similarity as relevance\n                    \"created_at\": row[\"created_at\"]\n                })\n            \n            return memories\n        except Exception as e:\n            logger.error(f\"Failed to get relevant memories: {str(e)}\")\n            return []\n    \n    async def resume_session(self, session_id: str) -> Optional[ConversationContext]:\n        \"\"\"Resume a session from its ID\"\"\"\n        try:\n            # Get the conversation context\n            row = await self.db.fetch_one(\n                \"\"\"SELECT * FROM conversation_contexts WHERE session_id = $1\"\"\",\n                session_id\n            )\n            \n            if not row:\n                return None\n            \n            # Parse messages from JSON\n            messages_data = json.loads(row[\"messages\"])\n            messages = [ContextMessage(**msg) for msg in messages_data]\n            \n            # Recreate conversation context\n            context = ConversationContext(\n                session_id=row[\"session_id\"],\n                user_id=row[\"user_id\"],\n                project_id=row[\"project_id\"],\n                messages=messages,\n                total_tokens=row[\"total_tokens\"],\n                max_tokens=row[\"max_tokens\"],\n                model=row[\"model\"],\n                protected_message_ids=row[\"protected_message_ids\"],\n                last_compaction=row[\"last_compaction\"],\n                compaction_count=row[\"compaction_count\"],\n                created_at=row[\"created_at\"],\n                updated_at=datetime.now()  # Update the timestamp\n            )\n            \n            return context\n        except Exception as e:\n            logger.error(f\"Failed to resume session: {str(e)}\")\n            return None\n```\n\nThe session memory service should work in conjunction with the checkpoint system to provide both short-term recovery (checkpoints) and long-term memory (session memories). It should use vector embeddings to enable semantic search for relevant memories.",
        "testStrategy": "1. Unit tests:\n   - Test memory extraction functions (decisions, code references)\n   - Verify tag generation for different content types\n   - Test embedding generation and similarity search\n\n2. Integration tests:\n   - Verify memory storage and retrieval\n   - Test session resumption functionality\n   - Verify project-specific memory filtering\n\n3. Performance tests:\n   - Measure memory retrieval latency\n   - Test with large numbers of stored memories\n   - Verify embedding search performance\n\n4. User acceptance testing:\n   - Verify memories contain relevant information\n   - Test session resumption user experience\n   - Confirm memory relevance scoring effectiveness",
        "priority": "medium",
        "dependencies": [
          "206",
          "211"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create SessionMemory Pydantic model",
            "description": "Implement the SessionMemory Pydantic model in app/models/session_models.py to define the structure for storing session memories.",
            "dependencies": [],
            "details": "Create a Pydantic model that includes fields for id, user_id, project_id, session_id, summary, key_decisions, code_references, tags, relevance_score, embedding, created_at, and expires_at. Include validation for required fields and appropriate data types. Add documentation for each field and implement any necessary helper methods.",
            "status": "pending",
            "testStrategy": "Unit tests to verify model validation, field types, and serialization/deserialization. Test edge cases like empty fields and maximum field lengths.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement conversation summary generation",
            "description": "Complete the _generate_conversation_summary method to create concise summaries of conversations.",
            "dependencies": [
              1
            ],
            "details": "Implement the _generate_conversation_summary method in SessionMemoryService that processes a list of ContextMessages and generates a meaningful summary. Use the Anthropic API to create summaries that capture key points, decisions, and technical details. Include logic to handle message formatting and API integration. Implement error handling and logging for failed summary generation.",
            "status": "pending",
            "testStrategy": "Unit tests with mock messages to verify summary generation. Test with various conversation types (debugging, planning, code review). Integration tests with the actual API to verify quality of summaries.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement memory extraction functions",
            "description": "Complete the key decisions and code references extraction methods in SessionMemoryService.",
            "dependencies": [
              1
            ],
            "details": "Finalize the _extract_key_decisions and _extract_code_references methods to accurately identify and extract important information from conversations. Enhance the regex patterns for better code block detection. Improve the decision extraction logic to capture nuanced decision statements. Add support for additional programming languages and file types in the extraction process.",
            "status": "pending",
            "testStrategy": "Unit tests with various conversation samples containing different types of decisions and code blocks. Test edge cases like malformed code blocks and ambiguous decision statements.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement memory storage and retrieval",
            "description": "Complete the save_session_memory and get_relevant_memories methods for storing and retrieving session memories.",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement the database operations for saving session memories with proper error handling and transaction management. Complete the vector similarity search functionality in get_relevant_memories to retrieve contextually relevant memories. Add caching for frequently accessed memories to improve performance. Implement pagination for memory retrieval results and optimize the SQL queries for performance.",
            "status": "pending",
            "testStrategy": "Integration tests with the database to verify storage and retrieval. Performance tests for vector similarity search with large datasets. Unit tests for error handling and edge cases.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement session resumption functionality",
            "description": "Complete the resume_session method to allow users to continue previous conversations.",
            "dependencies": [
              4
            ],
            "details": "Implement the resume_session method to retrieve and reconstruct a ConversationContext from a session ID. Add validation to ensure the session belongs to the requesting user. Implement error handling for missing or expired sessions. Add logging for session resumption events. Update the session's updated_at timestamp when resumed and track resumption count for analytics.",
            "status": "pending",
            "testStrategy": "Integration tests for session resumption with various session states. Unit tests for error handling and context reconstruction. Security tests to verify proper access controls.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement data retention policies",
            "description": "Add support for different data retention policies based on project type and user preferences.",
            "dependencies": [
              4
            ],
            "details": "Implement configurable retention policies for session memories: project-based (lifetime of project), CKO (lifetime of project), and indefinite (until user deletes). Add database schema updates to support retention policy tracking. Create methods to apply retention policies during memory creation and update. Implement a background job to clean up expired memories based on their retention policy. Add user controls to manage retention preferences.",
            "status": "pending",
            "testStrategy": "Unit tests for each retention policy type. Integration tests to verify policy application and memory expiration. System tests to verify the cleanup job correctly removes expired memories while preserving others.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-20T21:22:12.337Z"
      },
      {
        "id": "208",
        "title": "Implement Session Resume & Recovery UI",
        "description": "Create a user interface for browsing, selecting, and resuming from saved sessions and checkpoints",
        "details": "Develop a UI for session management that allows users to browse and resume previous sessions. The implementation should:\n\n1. Show a session picker on app start\n2. Display recent sessions with previews\n3. Provide a checkpoint browser within sessions\n4. Support session resumption from any checkpoint\n\n```jsx\n// React components for session management UI\nimport React, { useState, useEffect } from 'react';\nimport { format } from 'date-fns';\n\n// Session picker component shown on app start\nconst SessionPicker = ({ onSelectSession, onNewSession }) => {\n  const [sessions, setSessions] = useState([]);\n  const [loading, setLoading] = useState(true);\n  const [error, setError] = useState(null);\n  \n  useEffect(() => {\n    // Fetch recent sessions on component mount\n    const fetchSessions = async () => {\n      try {\n        setLoading(true);\n        const response = await fetch('/api/session/recent');\n        if (!response.ok) throw new Error('Failed to fetch sessions');\n        const data = await response.json();\n        setSessions(data);\n      } catch (err) {\n        setError(err.message);\n      } finally {\n        setLoading(false);\n      }\n    };\n    \n    fetchSessions();\n  }, []);\n  \n  if (loading) return <div className=\"p-4 text-center\">Loading recent sessions...</div>;\n  if (error) return <div className=\"p-4 text-center text-red-500\">Error: {error}</div>;\n  \n  return (\n    <div className=\"session-picker p-4 max-w-3xl mx-auto\">\n      <h2 className=\"text-xl font-semibold mb-4\">Resume Session</h2>\n      \n      {sessions.length === 0 ? (\n        <div className=\"text-center py-8\">\n          <p className=\"text-gray-500\">No recent sessions found</p>\n          <button \n            className=\"mt-4 px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600\"\n            onClick={onNewSession}\n          >\n            Start New Session\n          </button>\n        </div>\n      ) : (\n        <>\n          <div className=\"space-y-4\">\n            {sessions.map(session => (\n              <SessionCard \n                key={session.session_id}\n                session={session}\n                onResume={() => onSelectSession(session.session_id)}\n                onBrowseCheckpoints={() => onBrowseCheckpoints(session.session_id)}\n              />\n            ))}\n          </div>\n          \n          <div className=\"mt-6 text-center\">\n            <button \n              className=\"px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600\"\n              onClick={onNewSession}\n            >\n              Start New Session\n            </button>\n          </div>\n        </>\n      )}\n    </div>\n  );\n};\n\n// Individual session card component\nconst SessionCard = ({ session, onResume, onBrowseCheckpoints }) => {\n  const { \n    session_id, \n    project_name, \n    last_message_preview, \n    updated_at, \n    checkpoint_count,\n    token_count \n  } = session;\n  \n  return (\n    <div className=\"border rounded-lg p-4 hover:shadow-md transition-shadow\">\n      <div className=\"flex items-start\">\n        <div className=\"text-2xl text-blue-500 mr-3\">📁</div>\n        <div className=\"flex-1\">\n          <h3 className=\"font-medium\">{project_name || 'Untitled Project'}</h3>\n          <p className=\"text-sm text-gray-600 mt-1 line-clamp-2\">\n            Last: \"{last_message_preview || 'No messages'}\"\n          </p>\n          <div className=\"flex items-center text-xs text-gray-500 mt-2\">\n            <span>🕐 {formatTimeAgo(updated_at)}</span>\n            <span className=\"mx-2\">•</span>\n            <span>{checkpoint_count} checkpoints</span>\n            <span className=\"mx-2\">•</span>\n            <span>{formatTokens(token_count)} tokens</span>\n          </div>\n        </div>\n      </div>\n      <div className=\"mt-3 flex justify-end space-x-2\">\n        <button \n          className=\"px-3 py-1 text-sm border border-gray-300 rounded hover:bg-gray-100\"\n          onClick={onBrowseCheckpoints}\n        >\n          Browse Checkpoints\n        </button>\n        <button \n          className=\"px-3 py-1 text-sm bg-blue-500 text-white rounded hover:bg-blue-600\"\n          onClick={onResume}\n        >\n          Resume Latest\n        </button>\n      </div>\n    </div>\n  );\n};\n\n// Checkpoint browser component\nconst CheckpointBrowser = ({ sessionId, onSelectCheckpoint, onClose }) => {\n  const [checkpoints, setCheckpoints] = useState([]);\n  const [loading, setLoading] = useState(true);\n  const [error, setError] = useState(null);\n  \n  useEffect(() => {\n    // Fetch checkpoints for the session\n    const fetchCheckpoints = async () => {\n      try {\n        setLoading(true);\n        const response = await fetch(`/api/checkpoint/session/${sessionId}`);\n        if (!response.ok) throw new Error('Failed to fetch checkpoints');\n        const data = await response.json();\n        setCheckpoints(data);\n      } catch (err) {\n        setError(err.message);\n      } finally {\n        setLoading(false);\n      }\n    };\n    \n    fetchCheckpoints();\n  }, [sessionId]);\n  \n  if (loading) return <div className=\"p-4 text-center\">Loading checkpoints...</div>;\n  if (error) return <div className=\"p-4 text-center text-red-500\">Error: {error}</div>;\n  \n  return (\n    <div className=\"checkpoint-browser p-4\">\n      <div className=\"flex justify-between items-center mb-4\">\n        <h2 className=\"text-xl font-semibold\">Session Checkpoints</h2>\n        <button \n          className=\"text-gray-500 hover:text-gray-700\"\n          onClick={onClose}\n        >\n          <XIcon className=\"h-5 w-5\" />\n        </button>\n      </div>\n      \n      {checkpoints.length === 0 ? (\n        <div className=\"text-center py-8\">\n          <p className=\"text-gray-500\">No checkpoints found for this session</p>\n        </div>\n      ) : (\n        <div className=\"space-y-3 max-h-96 overflow-y-auto\">\n          {checkpoints.map(checkpoint => (\n            <CheckpointCard \n              key={checkpoint.id}\n              checkpoint={checkpoint}\n              onSelect={() => onSelectCheckpoint(checkpoint.id)}\n            />\n          ))}\n        </div>\n      )}\n    </div>\n  );\n};\n\n// Individual checkpoint card component\nconst CheckpointCard = ({ checkpoint, onSelect }) => {\n  const { \n    id, \n    label, \n    trigger, \n    token_count, \n    auto_tags,\n    created_at \n  } = checkpoint;\n  \n  // Format trigger type for display\n  const getTriggerLabel = (trigger) => {\n    switch(trigger) {\n      case 'auto': return 'Automatic';\n      case 'manual': return 'Manual';\n      case 'pre_compaction': return 'Pre-compaction';\n      case 'important_context': return 'Important context';\n      default: return trigger;\n    }\n  };\n  \n  return (\n    <div className=\"border rounded p-3 hover:bg-gray-50\">\n      <div className=\"flex justify-between\">\n        <div>\n          <h4 className=\"font-medium\">{label}</h4>\n          <div className=\"text-xs text-gray-500 mt-1\">\n            {format(new Date(created_at), 'MMM d, yyyy h:mm a')}\n          </div>\n        </div>\n        <div className=\"text-right text-sm\">\n          <div>{formatTokens(token_count)} tokens</div>\n          <div className=\"text-xs text-gray-500\">{getTriggerLabel(trigger)}</div>\n        </div>\n      </div>\n      \n      {auto_tags.length > 0 && (\n        <div className=\"mt-2 flex flex-wrap gap-1\">\n          {auto_tags.map(tag => (\n            <span \n              key={tag} \n              className=\"px-2 py-0.5 text-xs bg-blue-100 text-blue-800 rounded-full\"\n            >\n              {tag}\n            </span>\n          ))}\n        </div>\n      )}\n      \n      <div className=\"mt-2 text-right\">\n        <button \n          className=\"px-3 py-1 text-sm bg-blue-500 text-white rounded hover:bg-blue-600\"\n          onClick={onSelect}\n        >\n          Resume from here\n        </button>\n      </div>\n    </div>\n  );\n};\n\n// Helper functions\nconst formatTimeAgo = (timestamp) => {\n  const date = new Date(timestamp);\n  const now = new Date();\n  const diffMinutes = Math.floor((now - date) / (1000 * 60));\n  \n  if (diffMinutes < 1) return 'Just now';\n  if (diffMinutes < 60) return `${diffMinutes} minutes ago`;\n  \n  const diffHours = Math.floor(diffMinutes / 60);\n  if (diffHours < 24) return `${diffHours} hours ago`;\n  \n  return format(date, 'MMM d, yyyy');\n};\n\nconst formatTokens = (tokens) => {\n  return tokens >= 1000 ? `${(tokens/1000).toFixed(1)}K` : tokens;\n};\n\nexport { SessionPicker, CheckpointBrowser };\n```\n\nThe UI should provide a seamless experience for resuming previous sessions and recovering from crashes. It should show relevant information about each session and checkpoint to help users make informed decisions.",
        "testStrategy": "1. Unit tests:\n   - Test component rendering with different data\n   - Verify formatting functions for time and tokens\n   - Test empty state handling\n\n2. Integration tests:\n   - Verify session data fetching and display\n   - Test checkpoint browser functionality\n   - Verify session resumption flow\n\n3. Visual regression tests:\n   - Capture screenshots of session picker in different states\n   - Test responsive design on different screen sizes\n\n4. User acceptance testing:\n   - Verify session picker appears on app launch\n   - Test session resumption from different checkpoints\n   - Confirm UI is intuitive and easy to navigate",
        "priority": "medium",
        "dependencies": [
          "206",
          "207"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create SessionPicker.tsx component",
            "description": "Implement the SessionPicker component that displays recent sessions on app start",
            "dependencies": [],
            "details": "Create the SessionPicker.tsx component in empire-desktop/src/components/ that displays a list of recent sessions with previews. The component should handle loading states, empty states, and provide options to resume a session or start a new one. Include the SessionCard subcomponent for displaying individual session information.",
            "status": "pending",
            "testStrategy": "Unit tests for component rendering with different data states (loading, empty, populated). Test proper display of session information and formatting functions. Verify callback handling for session selection.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create CheckpointBrowser.tsx component",
            "description": "Implement the CheckpointBrowser component with timeline view for browsing checkpoints within a session",
            "dependencies": [
              1
            ],
            "details": "Create the CheckpointBrowser.tsx component in empire-desktop/src/components/ that displays checkpoints for a selected session in a timeline view. Include the CheckpointCard subcomponent for displaying individual checkpoint details. Implement sorting, filtering, and selection of checkpoints for resumption.",
            "status": "pending",
            "testStrategy": "Unit tests for checkpoint rendering, timeline visualization, and checkpoint selection. Test empty state handling and proper display of checkpoint metadata like trigger type and tags.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create sessionApi.ts client",
            "description": "Implement the API client for session management operations",
            "dependencies": [],
            "details": "Create sessionApi.ts in empire-desktop/src/services/ that provides methods for fetching sessions, checkpoints, and handling session resumption. Implement proper error handling, request cancellation, and response parsing. Include TypeScript interfaces for session and checkpoint data structures.",
            "status": "pending",
            "testStrategy": "Unit tests for API client methods with mocked responses. Test error handling for network failures and invalid responses. Verify proper request formation and parameter handling.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement GET /sessions endpoint",
            "description": "Create backend endpoint for retrieving recent sessions with pagination and filtering",
            "dependencies": [
              3
            ],
            "details": "Implement the GET /sessions endpoint that returns a list of recent sessions with pagination support. Include metadata like project name, last message preview, updated timestamp, checkpoint count, and token count. Support filtering by date range and sorting options.",
            "status": "pending",
            "testStrategy": "Unit tests for endpoint with various query parameters. Integration tests to verify database queries and response formatting. Test pagination behavior and sorting options.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement GET /sessions/picker endpoint",
            "description": "Create optimized endpoint for the session picker shown at app launch",
            "dependencies": [
              3,
              4
            ],
            "details": "Implement the GET /sessions/picker endpoint optimized for quick loading at app launch. This should return a limited set of the most recent sessions with minimal data needed for the picker UI. Include caching headers to improve performance on subsequent app launches.",
            "status": "pending",
            "testStrategy": "Performance tests to verify quick loading times. Unit tests for response format and caching behavior. Integration tests with the SessionPicker component.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement POST /sessions/{session_id}/resume endpoint",
            "description": "Create endpoint for resuming a session from a specific checkpoint",
            "dependencies": [
              3
            ],
            "details": "Implement the POST /sessions/{session_id}/resume endpoint that handles session resumption from a specific checkpoint. Support optional checkpoint_id parameter to resume from a specific point. Handle session state restoration including conversation history, context, and any associated resources.",
            "status": "pending",
            "testStrategy": "Unit tests for resumption logic with various checkpoint scenarios. Integration tests for full resumption flow. Test error handling for invalid session or checkpoint IDs.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Implement conflict detection for sessions",
            "description": "Add last-write-wins conflict resolution for sessions updated in multiple locations",
            "dependencies": [
              6
            ],
            "details": "Implement conflict detection and resolution for sessions that may have been updated in multiple locations. Use a timestamp-based last-write-wins strategy. Add version tracking to session objects and include conflict detection in the resume endpoint. Provide clear error messages when conflicts are detected.",
            "status": "pending",
            "testStrategy": "Unit tests for conflict detection logic with various timestamp scenarios. Integration tests simulating concurrent updates from different clients. Test conflict resolution behavior and error reporting.",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Add session conflict notification UI",
            "description": "Implement UI notification for when a session has been updated elsewhere",
            "dependencies": [
              1,
              7
            ],
            "details": "Create a notification component that appears when a session conflict is detected. The notification should inform the user that the session has been updated elsewhere and provide options to either use the latest version or continue with the current version. Include visual indicators for conflicted sessions in the session picker.",
            "status": "pending",
            "testStrategy": "Unit tests for notification component rendering and interaction. Integration tests with the conflict detection system. Test user flow for both conflict resolution options.",
            "parentId": "undefined"
          },
          {
            "id": 9,
            "title": "Integrate session picker into app launch flow",
            "description": "Add the session picker to the application startup sequence",
            "dependencies": [
              1,
              5
            ],
            "details": "Integrate the SessionPicker component into the application startup flow. Show the session picker when the app launches, unless the user has disabled this feature. Handle the transition from session selection to either resuming a session or starting a new one. Implement persistence of the user's preference for showing the picker on startup.",
            "status": "pending",
            "testStrategy": "Integration tests for the complete app launch flow. Test session selection and resumption from the picker. Verify proper handling of user preferences for showing the picker on startup.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-20T21:31:44.631Z"
      },
      {
        "id": "209",
        "title": "Implement Compact Command & API",
        "description": "Create a command and API endpoint for manual control over context condensing",
        "details": "Implement a `/compact` command and corresponding API endpoint that allows users to manually trigger context condensing. The implementation should:\n\n1. Support basic and force modes\n2. Return metrics about the compaction\n3. Implement rate limiting\n4. Provide programmatic access via API\n\n```python\n# FastAPI route handlers for compact command\nfrom fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, Request\nfrom pydantic import BaseModel\nfrom typing import Optional\nimport time\n\nrouter = APIRouter(prefix=\"/api/context\", tags=[\"context\"])\n\n# Rate limiting with a simple in-memory store\n# In production, use Redis or similar for distributed rate limiting\ncompaction_timestamps = {}\n\nclass CompactRequest(BaseModel):\n    session_id: str\n    force: bool = False\n    custom_prompt: Optional[str] = None\n\nclass CompactResponse(BaseModel):\n    success: bool\n    message: str\n    pre_tokens: Optional[int] = None\n    post_tokens: Optional[int] = None\n    reduction_percent: Optional[float] = None\n    duration_ms: Optional[int] = None\n\n@router.post(\"/compact\", response_model=CompactResponse)\nasync def compact_context(request: CompactRequest, \n                         background_tasks: BackgroundTasks,\n                         context_service = Depends(get_context_service),\n                         condensing_engine = Depends(get_condensing_engine)):\n    \"\"\"Manually trigger context compaction\"\"\"\n    # Check rate limiting (30 second cooldown)\n    current_time = time.time()\n    last_compaction = compaction_timestamps.get(request.session_id, 0)\n    \n    if current_time - last_compaction < 30 and not request.force:\n        cooldown_remaining = int(30 - (current_time - last_compaction))\n        return CompactResponse(\n            success=False,\n            message=f\"Rate limited. Please wait {cooldown_remaining} seconds before compacting again.\"\n        )\n    \n    # Get the conversation context\n    context = await context_service.get_context(request.session_id)\n    if not context:\n        raise HTTPException(status_code=404, detail=\"Session not found\")\n    \n    # Check if compaction is needed\n    threshold = context.settings.get(\"auto_compact_threshold\", 0.8)  # Default 80%\n    current_usage = context.total_tokens / context.max_tokens\n    \n    if current_usage < threshold and not request.force:\n        return CompactResponse(\n            success=False,\n            message=f\"Context usage ({current_usage:.1%}) is below the threshold ({threshold:.1%}). Use 'force' option to compact anyway.\"\n        )\n    \n    # Update rate limit timestamp\n    compaction_timestamps[request.session_id] = current_time\n    \n    # Perform compaction\n    try:\n        result = await condensing_engine.compact_conversation(\n            context=context,\n            trigger=\"manual\",\n            custom_prompt=request.custom_prompt\n        )\n        \n        # Update context in database (in background)\n        background_tasks.add_task(\n            context_service.update_context,\n            context\n        )\n        \n        return CompactResponse(\n            success=True,\n            message=\"Context compacted successfully\",\n            pre_tokens=result.pre_tokens,\n            post_tokens=result.post_tokens,\n            reduction_percent=result.reduction_percent,\n            duration_ms=result.duration_ms\n        )\n    except Exception as e:\n        return CompactResponse(\n            success=False,\n            message=f\"Compaction failed: {str(e)}\"\n        )\n\n# Command handler for /compact slash command\nasync def handle_compact_command(message: str, session_id: str, context_service, condensing_engine):\n    \"\"\"Handle /compact command from chat interface\"\"\"\n    # Parse command options\n    force = \"--force\" in message or \"-f\" in message\n    \n    # Create request object\n    request = CompactRequest(\n        session_id=session_id,\n        force=force\n    )\n    \n    # Call the API handler\n    response = await compact_context(\n        request=request,\n        background_tasks=BackgroundTasks(),\n        context_service=context_service,\n        condensing_engine=condensing_engine\n    )\n    \n    # Format response for chat\n    if response.success:\n        return f\"✅ Context compacted: {response.pre_tokens:,} → {response.post_tokens:,} tokens ({response.reduction_percent:.1f}% reduction) in {response.duration_ms/1000:.2f}s\"\n    else:\n        return f\"❌ {response.message}\"\n```\n\n```javascript\n// Client-side function to call compact API\nasync function compactContext(sessionId, force = false) {\n  try {\n    const response = await fetch('/api/context/compact', {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        session_id: sessionId,\n        force: force\n      })\n    });\n    \n    const result = await response.json();\n    \n    if (!response.ok) {\n      throw new Error(result.message || 'Failed to compact context');\n    }\n    \n    return result;\n  } catch (error) {\n    console.error('Error compacting context:', error);\n    throw error;\n  }\n}\n\n// Command handler for chat interface\nfunction handleSlashCommand(command, args) {\n  if (command === '/compact') {\n    const force = args.includes('--force') || args.includes('-f');\n    \n    // Show compaction in progress indicator\n    showCompactionInProgress();\n    \n    // Call API\n    compactContext(currentSessionId, force)\n      .then(result => {\n        if (result.success) {\n          // Show compaction divider with results\n          showCompactionDivider({\n            preTokens: result.pre_tokens,\n            postTokens: result.post_tokens,\n            timestamp: new Date()\n          });\n        } else {\n          // Show error message\n          showErrorMessage(result.message);\n        }\n      })\n      .catch(error => {\n        showErrorMessage(error.message);\n      })\n      .finally(() => {\n        // Hide progress indicator\n        hideCompactionInProgress();\n      });\n    \n    return true; // Command handled\n  }\n  \n  return false; // Command not handled\n}\n```\n\nThe implementation should provide both a user-friendly command interface and a programmatic API for triggering compaction. It should include appropriate rate limiting to prevent abuse and provide clear feedback about the compaction results.",
        "testStrategy": "1. Unit tests:\n   - Test command parsing with different options\n   - Verify rate limiting functionality\n   - Test response formatting\n\n2. Integration tests:\n   - Test API endpoint with various request parameters\n   - Verify command handler integration with chat interface\n   - Test error handling and recovery\n\n3. Performance tests:\n   - Measure API response time\n   - Test with different conversation sizes\n\n4. User acceptance testing:\n   - Verify command works as expected in chat interface\n   - Test force option functionality\n   - Confirm metrics display is clear and informative",
        "priority": "medium",
        "dependencies": [
          "203",
          "204"
        ],
        "status": "in-progress",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement slash command parser for /compact command",
            "description": "Create a command parser that handles the /compact command and its various flags from the chat interface.",
            "dependencies": [],
            "details": "Implement the handle_compact_command function that parses the /compact command from the chat interface. This should extract flags like --force and --fast from the message string, create a CompactRequest object with the appropriate parameters, and call the compact_context API handler. The function should properly format the response for display in the chat interface.",
            "status": "pending",
            "testStrategy": "Unit test the command parser with different input combinations (no flags, with --force, with --fast, with both flags). Verify that the parser correctly identifies and extracts all command options.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement force mode and threshold logic",
            "description": "Add logic to handle the --force flag that allows compaction even when below the threshold.",
            "dependencies": [
              1
            ],
            "details": "Modify the compact_context API handler to check if the current context usage is below the compaction threshold. If it is below threshold and force=False, return an appropriate message. If force=True, proceed with compaction regardless of the current usage. Include the current usage percentage and threshold in the response message for clarity.",
            "status": "pending",
            "testStrategy": "Test compaction requests both above and below threshold with and without the force flag. Verify that compaction is only prevented when below threshold AND force is not set.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement fast mode with Haiku model option",
            "description": "Add support for a --fast flag that uses a smaller, faster model for compaction.",
            "dependencies": [
              1
            ],
            "details": "Extend the CompactRequest model to include a 'fast' boolean parameter. Update the compact_context handler to pass this parameter to the condensing_engine. Modify the condensing_engine to use a smaller, faster model (Haiku) when the fast option is enabled. This should reduce compaction time from 30+ seconds to 5-10 seconds, with a potential trade-off in quality.",
            "status": "pending",
            "testStrategy": "Compare compaction performance and quality between standard and fast modes. Measure and verify the time difference between the two approaches. Test edge cases where fast compaction might produce suboptimal results.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement rate limiting with cooldown period",
            "description": "Add rate limiting to prevent abuse of the compaction feature with a 30-second cooldown.",
            "dependencies": [
              2
            ],
            "details": "Implement rate limiting in the compact_context API handler using an in-memory store (compaction_timestamps) to track the last compaction time for each session. If a request comes in less than 30 seconds after the previous compaction, return an error message with the remaining cooldown time, unless force=True. Update the timestamp after successful compaction. For production, add a comment about using Redis or similar for distributed rate limiting.",
            "status": "pending",
            "testStrategy": "Test rapid successive compaction requests and verify that the rate limit is enforced. Test that the cooldown message correctly shows the remaining time. Verify that the force flag bypasses rate limiting.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement compaction history endpoint",
            "description": "Create a GET endpoint to retrieve the compaction history for a conversation.",
            "dependencies": [
              4
            ],
            "details": "Implement a new GET /api/context/{conversation_id}/compact/history endpoint that returns the history of compaction operations for a specific conversation. This should include timestamps, before/after token counts, reduction percentages, and whether it was automatic or manual. Store compaction records in the database after each compaction operation. The endpoint should support pagination with skip/limit parameters and sorting by timestamp.",
            "status": "pending",
            "testStrategy": "Test the endpoint with conversations that have multiple compaction events. Verify pagination works correctly. Test with conversations that have no compaction history. Verify the response format includes all required fields.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-20T21:37:02.149Z"
      },
      {
        "id": "210",
        "title": "Implement Automatic Error Recovery",
        "description": "Create a system that automatically detects and recovers from context window overflow errors",
        "details": "Implement an error recovery system that detects context window overflow errors from API responses and automatically triggers context reduction. The implementation should:\n\n1. Detect overflow errors from API responses\n2. Trigger aggressive context reduction\n3. Retry failed requests automatically\n4. Preserve conversation state during recovery\n\n```python\nclass ContextErrorRecoveryService:\n    def __init__(self, context_service, condensing_engine):\n        self.context_service = context_service\n        self.condensing_engine = condensing_engine\n        self.max_retry_attempts = 3\n    \n    async def handle_api_error(self, error, session_id: str) -> tuple[bool, str]:\n        \"\"\"Handle API errors, attempting recovery for context overflow\"\"\"\n        # Check if this is a context overflow error\n        if self._is_context_overflow_error(error):\n            return await self._recover_from_overflow(session_id)\n        else:\n            # Not a context overflow error, can't recover\n            return False, f\"API error: {str(error)}\"\n    \n    def _is_context_overflow_error(self, error) -> bool:\n        \"\"\"Determine if an error is due to context window overflow\"\"\"\n        error_str = str(error).lower()\n        \n        # Check for common overflow error patterns from different APIs\n        overflow_patterns = [\n            \"context window full\",\n            \"maximum context length\",\n            \"too many tokens\",\n            \"context overflow\",\n            \"input too long\",\n            \"exceeds token limit\"\n        ]\n        \n        return any(pattern in error_str for pattern in overflow_patterns)\n    \n    async def _recover_from_overflow(self, session_id: str) -> tuple[bool, str]:\n        \"\"\"Attempt to recover from context overflow by reducing context\"\"\"\n        # Get the current context\n        context = await self.context_service.get_context(session_id)\n        if not context:\n            return False, \"Session not found\"\n        \n        # Log the recovery attempt\n        logger.info(f\"Attempting context overflow recovery for session {session_id}\")\n        \n        # Try multiple recovery attempts with increasing aggressiveness\n        for attempt in range(1, self.max_retry_attempts + 1):\n            # Calculate reduction percentage based on attempt number\n            # First attempt: 25%, Second: 40%, Third: 60%\n            reduction_percent = 25 + (attempt - 1) * 15\n            \n            try:\n                # Create checkpoint before aggressive reduction\n                checkpoint_service = get_checkpoint_service()\n                checkpoint_id = await checkpoint_service.create_checkpoint(\n                    context=context,\n                    trigger=\"error_recovery\",\n                    label=f\"Pre-recovery checkpoint (attempt {attempt})\"\n                )\n                \n                # Perform aggressive context reduction\n                result = await self._reduce_context_aggressively(\n                    context=context,\n                    reduction_percent=reduction_percent\n                )\n                \n                # Update context in database\n                await self.context_service.update_context(context)\n                \n                # Log success\n                logger.info(\n                    f\"Context recovery successful on attempt {attempt}. \"\n                    f\"Reduced from {result.pre_tokens} to {result.post_tokens} tokens \"\n                    f\"({result.reduction_percent:.1f}% reduction)\"\n                )\n                \n                return True, (\n                    f\"Recovered from context overflow (attempt {attempt}). \"\n                    f\"Reduced context by {result.reduction_percent:.1f}%.\"\n                )\n                \n            except Exception as e:\n                logger.error(f\"Recovery attempt {attempt} failed: {str(e)}\")\n                # Continue to next attempt\n        \n        # If we get here, all recovery attempts failed\n        return False, \"Failed to recover from context overflow after multiple attempts\"\n    \n    async def _reduce_context_aggressively(self, context: ConversationContext, reduction_percent: float) -> CompactionResult:\n        \"\"\"Aggressively reduce context size by the specified percentage\"\"\"\n        # Use custom prompt for aggressive reduction\n        aggressive_prompt = \"\"\"\n        You are a context reduction specialist. Your task is to aggressively condense this conversation \n        to fit within a smaller context window. This is an emergency reduction to recover from a context overflow error.\n        \n        IMPORTANT: Preserve ONLY the most critical information:\n        1. Code snippets that were created or modified\n        2. Key decisions and their rationale\n        3. Current task or problem being solved\n        4. Critical error messages\n        \n        Be extremely concise. Remove all pleasantries, explanations, and non-essential content.\n        Prioritize recent messages over older ones.\n        \n        FORMAT YOUR RESPONSE AS A BRIEF SUMMARY FOLLOWED BY KEY POINTS.\n        \"\"\"\n        \n        # Perform compaction with aggressive prompt\n        result = await self.condensing_engine.compact_conversation(\n            context=context,\n            trigger=\"error_recovery\",\n            custom_prompt=aggressive_prompt\n        )\n        \n        # If we didn't achieve the target reduction, force more aggressive reduction\n        if result.reduction_percent < reduction_percent:\n            # Calculate how many more tokens we need to remove\n            target_tokens = int(result.pre_tokens * (1 - reduction_percent / 100))\n            current_tokens = result.post_tokens\n            \n            if current_tokens > target_tokens:\n                # We need to remove more messages\n                # Start by identifying non-essential messages (not code, not errors, etc.)\n                non_essential_messages = [\n                    msg for msg in context.messages \n                    if not msg.is_protected \n                    and not self._is_essential_message(msg)\n                ]\n                \n                # Sort by oldest first\n                non_essential_messages.sort(key=lambda x: x.created_at)\n                \n                # Remove messages until we reach target\n                for msg in non_essential_messages:\n                    if current_tokens <= target_tokens:\n                        break\n                        \n                    # Remove this message\n                    context.messages = [m for m in context.messages if m.id != msg.id]\n                    current_tokens -= msg.token_count\n                \n                # Update total tokens\n                context.total_tokens = sum(msg.token_count for msg in context.messages)\n                \n                # Update result metrics\n                result.post_tokens = context.total_tokens\n                result.reduction_percent = ((result.pre_tokens - result.post_tokens) / result.pre_tokens) * 100\n        \n        return result\n    \n    def _is_essential_message(self, message: ContextMessage) -> bool:\n        \"\"\"Determine if a message contains essential information that should be preserved\"\"\"\n        content = message.content.lower()\n        \n        # Check for code blocks\n        if \"```\" in content:\n            return True\n            \n        # Check for error messages\n        error_indicators = [\"error:\", \"exception:\", \"traceback\", \"failed:\"]\n        if any(indicator in content for indicator in error_indicators):\n            return True\n            \n        # Check for file paths\n        if re.search(r'[\\w\\-\\.]+\\.[a-zA-Z]{2,4}', content):\n            return True\n            \n        # Check for decision language\n        decision_phrases = [\"decided to\", \"will use\", \"chosen\", \"selected\"]\n        if any(phrase in content for phrase in decision_phrases):\n            return True\n            \n        return False\n```\n\n```javascript\n// Client-side error handling and recovery\nasync function sendMessageWithRecovery(message, sessionId, retryCount = 0) {\n  const maxRetries = 3;\n  \n  try {\n    const response = await sendMessage(message, sessionId);\n    return response;\n  } catch (error) {\n    // Check if this is a context overflow error\n    if (isContextOverflowError(error) && retryCount < maxRetries) {\n      // Show recovery message to user\n      showRecoveryMessage(`Context window full. Attempting recovery (${retryCount + 1}/${maxRetries})...`);\n      \n      // Call recovery API\n      try {\n        const recoveryResult = await fetch('/api/context/recover', {\n          method: 'POST',\n          headers: {\n            'Content-Type': 'application/json',\n          },\n          body: JSON.stringify({\n            session_id: sessionId\n          })\n        });\n        \n        const result = await recoveryResult.json();\n        \n        if (result.success) {\n          // Recovery successful, retry the message\n          showRecoveryMessage(`Recovery successful. Retrying message...`);\n          return await sendMessageWithRecovery(message, sessionId, retryCount + 1);\n        } else {\n          // Recovery failed\n          throw new Error(`Recovery failed: ${result.message}`);\n        }\n      } catch (recoveryError) {\n        throw new Error(`Context recovery failed: ${recoveryError.message}`);\n      }\n    } else {\n      // Not a context error or we've exceeded retry attempts\n      throw error;\n    }\n  }\n}\n\nfunction isContextOverflowError(error) {\n  const errorMessage = error.message.toLowerCase();\n  const overflowPatterns = [\n    'context window full',\n    'maximum context length',\n    'too many tokens',\n    'context overflow',\n    'input too long',\n    'exceeds token limit'\n  ];\n  \n  return overflowPatterns.some(pattern => errorMessage.includes(pattern));\n}\n\nfunction showRecoveryMessage(message) {\n  // Display a recovery message in the UI\n  const recoveryElement = document.createElement('div');\n  recoveryElement.className = 'recovery-message';\n  recoveryElement.innerHTML = `\n    <div class=\"flex items-center p-3 bg-yellow-50 border border-yellow-200 rounded-md\">\n      <svg class=\"animate-spin h-5 w-5 text-yellow-500 mr-3\" viewBox=\"0 0 24 24\">\n        <circle class=\"opacity-25\" cx=\"12\" cy=\"12\" r=\"10\" stroke=\"currentColor\" stroke-width=\"4\"></circle>\n        <path class=\"opacity-75\" fill=\"currentColor\" d=\"M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z\"></path>\n      </svg>\n      <span class=\"text-sm text-yellow-800\">${message}</span>\n    </div>\n  `;\n  \n  document.getElementById('message-container').appendChild(recoveryElement);\n  \n  // Scroll to bottom\n  const container = document.getElementById('message-container');\n  container.scrollTop = container.scrollHeight;\n  \n  return recoveryElement;\n}\n```\n\nThe error recovery system should work transparently to recover from context overflow errors without requiring user intervention. It should preserve as much critical information as possible while aggressively reducing context size to fit within limits.",
        "testStrategy": "1. Unit tests:\n   - Test error detection for different API error formats\n   - Verify essential message detection logic\n   - Test recovery strategies with different reduction percentages\n\n2. Integration tests:\n   - Simulate overflow errors and verify recovery\n   - Test multiple recovery attempts\n   - Verify checkpoint creation before recovery\n\n3. End-to-end tests:\n   - Test complete recovery flow from error to successful retry\n   - Verify client-side recovery handling\n   - Test with different conversation types\n\n4. Stress tests:\n   - Test with conversations at various token limits\n   - Verify recovery with minimal available tokens\n   - Test recovery with complex conversation structures",
        "priority": "high",
        "dependencies": [
          "203",
          "206"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement error detection for context window overflow",
            "description": "Create a robust error detection system that identifies context window overflow errors from various API providers",
            "dependencies": [],
            "details": "Enhance the _is_context_overflow_error method to detect overflow errors from different LLM APIs (OpenAI, Anthropic, etc.). Add comprehensive pattern matching for different error message formats. Implement unit tests to verify detection accuracy across different error formats. Add logging for detected errors with structured data.",
            "status": "pending",
            "testStrategy": "Unit tests for error detection with different API error formats. Test with mock API responses from OpenAI, Anthropic, and other providers. Verify both positive and negative cases (overflow vs. non-overflow errors).",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement aggressive context reduction strategy",
            "description": "Create a multi-tiered context reduction system that progressively increases reduction aggressiveness",
            "dependencies": [
              1
            ],
            "details": "Implement the _reduce_context_aggressively method with configurable reduction percentages. Create logic to identify and preserve essential messages (code, errors, decisions). Implement checkpoint creation before reduction to allow rollback. Add metrics collection for reduction effectiveness (pre/post token counts, reduction percentage).",
            "status": "pending",
            "testStrategy": "Unit tests for context reduction with different reduction percentages. Test essential message detection logic. Integration tests with simulated large contexts to verify reduction effectiveness.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement automatic retry mechanism with recovery",
            "description": "Create a system that automatically retries failed requests after context reduction",
            "dependencies": [
              2
            ],
            "details": "Implement the handle_api_error method to coordinate recovery and retry. Add progressive retry logic with increasing reduction aggressiveness. Implement client-side recovery UI components to show recovery status. Add timeout handling and maximum retry limits. Implement recovery metrics tracking for success/failure rates.",
            "status": "pending",
            "testStrategy": "Integration tests simulating overflow errors and verifying recovery. Test multiple recovery attempts with different reduction percentages. Test timeout handling and maximum retry limits.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement graceful degradation and fallback mechanisms",
            "description": "Create fallback mechanisms for when automatic recovery fails or API services are unavailable",
            "dependencies": [
              3
            ],
            "details": "Implement fallback to simpler models when primary models fail. Add graceful degradation when Anthropic API is unavailable (fallback to truncation). Implement session state preservation during recovery attempts. Add user notification system for degraded service. Implement concurrent access handling with last-write-wins and notification.",
            "status": "pending",
            "testStrategy": "Integration tests for fallback mechanisms with simulated API unavailability. Test degradation paths with different failure scenarios. Test concurrent access handling with multiple simulated users.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement monitoring and observability for error recovery",
            "description": "Add comprehensive logging, metrics, and alerting for the error recovery system",
            "dependencies": [
              4
            ],
            "details": "Implement structured logging with structlog for all recovery operations. Add Prometheus metrics for recovery attempts, success rates, and reduction percentages. Create a Prometheus dashboard configuration for context management metrics. Implement alerting for repeated recovery failures. Add session update notifications in the desktop UI for concurrent modifications.",
            "status": "pending",
            "testStrategy": "Verify metrics collection during simulated recovery scenarios. Test dashboard configurations with sample data. Test alerting mechanisms with threshold violations.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-20T22:30:26.918Z"
      },
      {
        "id": "211",
        "title": "Database Schema Setup for Chat Context Management",
        "description": "Create all database migrations and verify schema setup in Supabase for the chat context window management feature. CRITICAL BLOCKER for Tasks 201, 206, 207.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": "Create and apply the following migration files for Supabase to set up the database schema for chat context management:\n\n1. **conversation_contexts table**:\n```sql\ncreate table public.conversation_contexts (\n  id uuid primary key default uuid_generate_v4(),\n  conversation_id uuid not null references public.conversations(id) on delete cascade,\n  total_tokens integer not null default 0,\n  max_tokens integer not null default 8000,\n  retained_messages integer not null default 0,\n  compaction_enabled boolean not null default true,\n  last_compaction_time timestamp with time zone,\n  created_at timestamp with time zone not null default now(),\n  updated_at timestamp with time zone not null default now(),\n  unique(conversation_id)\n);\n\ncreate index idx_conversation_contexts_conversation_id on public.conversation_contexts(conversation_id);\n```\n\n2. **context_messages table**:\n```sql\ncreate table public.context_messages (\n  id uuid primary key default uuid_generate_v4(),\n  conversation_id uuid not null references public.conversations(id) on delete cascade,\n  message_id uuid not null references public.messages(id) on delete cascade,\n  token_count integer not null,\n  is_retained boolean not null default true,\n  embedding vector(1536),\n  created_at timestamp with time zone not null default now(),\n  unique(conversation_id, message_id)\n);\n\ncreate index idx_context_messages_conversation_id on public.context_messages(conversation_id);\ncreate index idx_context_messages_message_id on public.context_messages(message_id);\ncreate index idx_context_messages_is_retained on public.context_messages(is_retained);\n```\n\n3. **compaction_logs table**:\n```sql\ncreate table public.compaction_logs (\n  id uuid primary key default uuid_generate_v4(),\n  conversation_id uuid not null references public.conversations(id) on delete cascade,\n  tokens_before integer not null,\n  tokens_after integer not null,\n  messages_before integer not null,\n  messages_after integer not null,\n  compaction_type text not null check (compaction_type in ('auto', 'manual', 'threshold')),\n  created_at timestamp with time zone not null default now()\n);\n\ncreate index idx_compaction_logs_conversation_id on public.compaction_logs(conversation_id);\ncreate index idx_compaction_logs_created_at on public.compaction_logs(created_at);\n```\n\n4. **session_checkpoints table**:\n```sql\ncreate table public.session_checkpoints (\n  id uuid primary key default uuid_generate_v4(),\n  conversation_id uuid not null references public.conversations(id) on delete cascade,\n  summary text not null,\n  token_count integer not null,\n  checkpoint_type text not null check (checkpoint_type in ('auto', 'manual', 'milestone')),\n  messages_included integer not null,\n  created_at timestamp with time zone not null default now()\n);\n\ncreate index idx_session_checkpoints_conversation_id on public.session_checkpoints(conversation_id);\n```\n\n5. **session_memories table**:\n```sql\ncreate table public.session_memories (\n  id uuid primary key default uuid_generate_v4(),\n  conversation_id uuid not null references public.conversations(id) on delete cascade,\n  memory_type text not null check (memory_type in ('key_point', 'action_item', 'decision', 'custom')),\n  content text not null,\n  token_count integer not null,\n  is_pinned boolean not null default false,\n  source_message_id uuid references public.messages(id) on delete set null,\n  created_at timestamp with time zone not null default now(),\n  updated_at timestamp with time zone not null default now()\n);\n\ncreate index idx_session_memories_conversation_id on public.session_memories(conversation_id);\ncreate index idx_session_memories_memory_type on public.session_memories(memory_type);\ncreate index idx_session_memories_is_pinned on public.session_memories(is_pinned);\n```\n\n6. **RLS policies**:\n```sql\n-- RLS for conversation_contexts\nalter table public.conversation_contexts enable row level security;\n\ncreate policy \"Users can view their own conversation contexts\"\n  on public.conversation_contexts for select\n  using (\n    conversation_id in (\n      select id from public.conversations where user_id = auth.uid()\n    )\n  );\n\ncreate policy \"Users can update their own conversation contexts\"\n  on public.conversation_contexts for update\n  using (\n    conversation_id in (\n      select id from public.conversations where user_id = auth.uid()\n    )\n  );\n\n-- RLS for context_messages\nalter table public.context_messages enable row level security;\n\ncreate policy \"Users can view their own context messages\"\n  on public.context_messages for select\n  using (\n    conversation_id in (\n      select id from public.conversations where user_id = auth.uid()\n    )\n  );\n\n-- RLS for compaction_logs\nalter table public.compaction_logs enable row level security;\n\ncreate policy \"Users can view their own compaction logs\"\n  on public.compaction_logs for select\n  using (\n    conversation_id in (\n      select id from public.conversations where user_id = auth.uid()\n    )\n  );\n\n-- RLS for session_checkpoints\nalter table public.session_checkpoints enable row level security;\n\ncreate policy \"Users can view their own session checkpoints\"\n  on public.session_checkpoints for select\n  using (\n    conversation_id in (\n      select id from public.conversations where user_id = auth.uid()\n    )\n  );\n\ncreate policy \"Users can create their own session checkpoints\"\n  on public.session_checkpoints for insert\n  with check (\n    conversation_id in (\n      select id from public.conversations where user_id = auth.uid()\n    )\n  );\n\n-- RLS for session_memories\nalter table public.session_memories enable row level security;\n\ncreate policy \"Users can view their own session memories\"\n  on public.session_memories for select\n  using (\n    conversation_id in (\n      select id from public.conversations where user_id = auth.uid()\n    )\n  );\n\ncreate policy \"Users can create their own session memories\"\n  on public.session_memories for insert\n  with check (\n    conversation_id in (\n      select id from public.conversations where user_id = auth.uid()\n    )\n  );\n\ncreate policy \"Users can update their own session memories\"\n  on public.session_memories for update\n  using (\n    conversation_id in (\n      select id from public.conversations where user_id = auth.uid()\n    )\n  );\n```\n\nImplementation steps:\n\n1. Create the migration files in the `supabase/migrations` directory with appropriate timestamps\n2. Run the migrations using `supabase db reset` or `supabase migration up`\n3. Verify the schema has been created correctly by checking the database tables\n4. Test the RLS policies by attempting to access data from different user contexts\n5. Document the schema for reference by the UI implementation team",
        "testStrategy": "1. Database schema validation tests:\n   - Verify all tables are created with the correct columns and constraints\n   - Test primary key constraints\n   - Test foreign key relationships\n   - Verify indexes are created correctly\n\n2. RLS policy tests:\n   - Test each policy with authenticated users accessing their own data\n   - Test users attempting to access other users' data\n   - Test anonymous access attempts\n   - Verify admin access works correctly\n\n3. Data integrity tests:\n   - Test cascading deletes when a conversation is deleted\n   - Test unique constraints\n   - Test check constraints on enum-like fields\n\n4. Migration tests:\n   - Test applying migrations in sequence\n   - Test rolling back migrations\n   - Test idempotency of migrations\n\n5. Performance tests:\n   - Test query performance with indexes\n   - Test with simulated large datasets\n\n6. Integration tests:\n   - Test interaction between tables with sample data\n   - Verify triggers and functions work correctly\n\n7. Manual testing checklist:\n   - Run migrations on development database\n   - Verify all tables exist with correct structure\n   - Insert sample data and verify constraints\n   - Test RLS policies with different user contexts\n   - Verify foreign key relationships work correctly",
        "subtasks": [
          {
            "id": 1,
            "title": "Create conversation_contexts table migration",
            "description": "Create the migration file for the conversation_contexts table that tracks token usage and compaction settings",
            "dependencies": [],
            "details": "Create a migration file in the supabase/migrations directory with the SQL to create the conversation_contexts table. This table will store metadata about the context for each conversation including token counts, compaction settings, and timestamps.",
            "status": "pending",
            "testStrategy": "Verify the table is created with all required columns, constraints, and indexes. Test the foreign key relationship to the conversations table.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create context_messages table migration",
            "description": "Create the migration file for the context_messages table that tracks which messages are part of the context",
            "dependencies": [],
            "details": "Create a migration file for the context_messages table that will track which messages are included in the conversation context, their token counts, and retention status. Include vector embedding column for semantic search capabilities.",
            "status": "pending",
            "testStrategy": "Test the table creation, foreign key constraints to both conversations and messages tables, and verify indexes are created correctly.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create compaction_logs table migration",
            "description": "Create the migration file for the compaction_logs table that tracks history of context compactions",
            "dependencies": [],
            "details": "Create a migration file for the compaction_logs table that will record the history of context compactions, including metrics before and after compaction, and the type of compaction performed.",
            "status": "pending",
            "testStrategy": "Verify the table is created with all required columns and constraints. Test the foreign key relationship to the conversations table.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create session_checkpoints table migration",
            "description": "Create the migration file for the session_checkpoints table that stores conversation summaries",
            "dependencies": [],
            "details": "Create a migration file for the session_checkpoints table that will store conversation summaries and checkpoints that can be used to compress context while preserving important information.",
            "status": "pending",
            "testStrategy": "Test the table creation, foreign key constraints, and verify the checkpoint_type check constraint works correctly.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create session_memories table migration",
            "description": "Create the migration file for the session_memories table that stores important conversation points",
            "dependencies": [],
            "details": "Create a migration file for the session_memories table that will store important points from the conversation that should be preserved even during context compaction, such as key points, action items, and decisions.",
            "status": "pending",
            "testStrategy": "Verify the table is created with all required columns and constraints. Test the memory_type check constraint and the foreign key relationships.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Create RLS policies for all tables",
            "description": "Create the migration file for Row Level Security policies to secure access to context tables",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Create a migration file that implements Row Level Security policies for all the context management tables to ensure users can only access their own data. Include policies for select, insert, and update operations as appropriate for each table.",
            "status": "pending",
            "testStrategy": "Test each RLS policy by attempting to access data as different users. Verify users can only access their own data and that admin access works correctly if applicable.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Run migrations and verify schema",
            "description": "Apply all migration files and verify the database schema is created correctly",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Run the migrations using Supabase CLI commands and verify that all tables, constraints, indexes, and RLS policies are created correctly. Document any issues encountered and their resolutions.",
            "status": "pending",
            "testStrategy": "Manually verify all tables exist with the correct structure. Test inserting sample data to verify constraints. Test RLS policies with different user contexts.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-01-20T20:58:08.687Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-01-20T22:30:26.918Z",
      "taskCount": 111,
      "completedCount": 90,
      "tags": [
        "v7_3_features"
      ]
    }
  },
  "empire_desktop_v75": {
    "tasks": [
      {
        "id": 1,
        "title": "Project Setup and Tauri 2.0 Initialization",
        "description": "Initialize Tauri 2.0 project with React 18, TypeScript, TailwindCSS, and shadcn/ui for macOS native desktop app.",
        "details": "Use Tauri CLI v2.0.3: `npm create tauri-app@latest -- --template react-ts`. Install React 18.3.1, TailwindCSS 3.4.10, shadcn/ui v0.9.0. Configure `tauri.conf.json` for WKWebView, Rust backend with tokio 1.40.0 for async ops, and sqlx 0.8.1 for SQLite. Set up Rust commands for secure keychain access using keyring 2.5.0 crate. Binary target: macOS arm64/x86_64.",
        "testStrategy": "Run `tauri dev` and verify app launches <2s, WKWebView renders React UI, Rust commands invoke via `invoke('check_keychain')`. Test on macOS Sonoma/Ventura.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Local SQLite Database Setup",
        "description": "Implement local SQLite database with exact schema from PRD for projects, conversations, messages, files, and settings.",
        "details": "Use sqlx 0.8.1 with SQLite feature and rusqlite 0.32.1. Create encrypted DB using SQLCipher via tauri-plugin-sql 2.0.0. Execute PRD schema SQL on init. Add migrations with sqlx-cli 0.8.1. Rust function: `init_db(path: PathBuf) -> Result<Pool<Sqlite>>`. Store in `~/.empire/empire.db`. Enable WAL mode for concurrency.",
        "testStrategy": "Unit tests for schema creation, insert/query projects table. Verify encryption with invalid key fails. Integration test: CRUD project record.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Clerk Authentication Integration",
        "description": "Integrate Clerk auth with secure JWT storage in macOS keychain and auto-refresh.",
        "details": "Use @clerk/clerk-react 5.1.2. Rust backend: keyring 2.5.0 for Keychain access (`keyring::Entry::new('empire', 'jwt')`). Implement OAuth flow redirecting to system browser. Auto-refresh using Clerk's token cache. Expose Tauri commands: `login()`, `get_token()`, `logout()`. Biometric unlock via security-framework 2.10.0 crate.",
        "testStrategy": "Mock Clerk API, test token storage/retrieval, refresh flow. Verify keychain isolation per user. E2E: full login cycle.",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Empire Backend API Client",
        "description": "Create TypeScript API client for all specified Empire v7.3 endpoints with WebSocket streaming.",
        "details": "Use axios 1.7.7 for HTTP, @tauri-apps/api 2.0.3 for WS. Implement EmpireAPI interface exactly as PRD. Streaming: `ReadableStream` from WS `/ws/chat`. Auth: Bearer token interceptor. Endpoints: `/api/query/auto`, `/api/query/adaptive`, `/api/documents/upload`, etc. TypeScript types from PRD models. Retry logic with exponential backoff using p-retry 5.0.0.",
        "testStrategy": "Mock server with MSW 2.4.11, test all endpoints, streaming chunks, error handling, auth interceptor.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Main Chat Interface with Streaming",
        "description": "Build core chat UI with multi-line input, attachments, streaming responses, source citations.",
        "details": "React 18 + shadcn/ui Chat components. Use zustand 5.0.0-rc.2 for state. Streaming: use `useEffect` with API client's AsyncGenerator. Markdown rendering: react-markdown 9.0.1 + remark-gfm. Drag-drop files: react-dropzone 14.2.3. Citations: expandable [1][2] popover. Input: TextareaAutosize from shadcn.",
        "testStrategy": "Cypress 13.15.0 E2E: send message, verify streaming animation, expand citations, file drag-drop. Unit: message rendering.",
        "priority": "high",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Projects CRUD Operations",
        "description": "Implement project management: create, list, update, delete with local+remote sync.",
        "details": "React components: ProjectList, ProjectForm using shadcn DataTable. Local ops via sqlx Rust commands. Sync: POST to Empire API, update `remote_id` and `synced_at`. Templates: store as project with `is_template=true`. Department selector: 12 options from PRD. Zustand store: `projectsStore`.",
        "testStrategy": "Unit: CRUD local DB. Integration: create project → API sync → list verifies remote_id. UI: form validation, list search.",
        "priority": "high",
        "dependencies": [
          2,
          4
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Chat History and Global Search",
        "description": "Build sidebar navigation, conversation list, global/project search with filters.",
        "details": "SQLite FTS5 for search: `CREATE VIRTUAL TABLE messages_fts USING fts5(content, tokenize=porter)`. Rust command: `search_messages(query: &str, filters: Json)`. UI: shadcn CommandMenu for Cmd+K. Results: highlight matches with context preview. Filters: date, project, attachments via SQL WHERE.",
        "testStrategy": "Performance: <500ms search 10k messages. Accuracy: insert test data, verify FTS matches. UI: search → jump to message.",
        "priority": "high",
        "dependencies": [
          2,
          5,
          6
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Real-time Data Sync and Offline Mode",
        "description": "Implement bi-directional sync between local SQLite and Supabase with offline history viewing.",
        "details": "Use Supabase JS 2.46.6 client in Rust via tauri-plugin. Conflict resolution: last-write-wins by `updated_at`. Background sync: tokio task polling every 30s. Offline: queue unsynced changes in `pending_sync` table. Sync indicator: badge on sidebar. Queue queries for online.",
        "testStrategy": "Mock network: offline → queue → online → verify sync. Conflict test: edit same record on two devices.",
        "priority": "medium",
        "dependencies": [
          2,
          3,
          4,
          6,
          7
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Keyboard Shortcuts and Native macOS Features",
        "description": "Implement all specified keyboard shortcuts, menu bar, system tray, notifications.",
        "details": "Tauri: tauri-plugin-global-shortcut 2.0.0 for Cmd+N etc. Menu: tauri-plugin-menu 2.0.0. Tray: system_tray 0.7.0 crate. Notifications: notify-rust 4.9.0. Window: tauri-plugin-window-state 2.0.0 for restore position. Dark mode: use-os-theme.",
        "testStrategy": "Manual: test all shortcuts in app. Automated: use tauri-plugin-macos-specific for key event simulation.",
        "priority": "medium",
        "dependencies": [
          1,
          5
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Settings, Quick Actions, and Packaging",
        "description": "Build settings UI, quick action buttons, project instructions/files, DMG packaging.",
        "details": "Settings: shadcn Settings panel, sync to DB. Quick actions: buttons calling API `/api/summarizer`, etc. Project files: upload to `/api/documents/upload`, store metadata. Instructions: rich textarea → project.instructions. Packaging: `tauri build` with codesign, DMG via create-dmg 2.0.0. Auto-updater: tauri-plugin-updater 2.0.0.",
        "testStrategy": "E2E: settings persist after restart, quick actions trigger API, file upload succeeds. Packaging: build → install DMG → launch verifies.",
        "priority": "medium",
        "dependencies": [
          2,
          4,
          5,
          6
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "MCP Client Foundation",
        "description": "Implement MCP client layer for Supabase/Neo4j server management and tool integration.",
        "details": "Rust: tokio for spawning processes per PRD config `~/.empire/mcp_settings.json`. JSON-RPC 2.0 over stdin/stdout using serde_json 1.0.120. Commands: `start_mcp_server(name: String)`, `list_tools()`. UI: settings page to add/remove servers. Cache resources locally.",
        "testStrategy": "Integration: spawn mock MCP server, verify JSON-RPC tool list, invoke tool. Error: invalid config fails gracefully.",
        "priority": "medium",
        "dependencies": [
          1,
          3
        ],
        "status": "done",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2026-01-02T21:40:07.235Z",
      "updated": "2026-01-02T21:40:07.235Z",
      "description": "Tasks for empire_desktop_v75 context"
    }
  },
  "006-markdown-chunking": {
    "tasks": [
      {
        "id": 1,
        "title": "Design MarkdownChunkerStrategy interface and data structures",
        "description": "Define the MarkdownChunkerStrategy class signature, configuration options, and MarkdownSection dataclass in app/services/chunking_service.py, aligned with existing ChunkingStrategy and TextNode abstractions.",
        "details": "Implementation details:\n- Open app/services/chunking_service.py and locate the existing ChunkingStrategy interface and SentenceSplitter integration to mirror patterns (typing, logging, error handling).\n- Define MarkdownSection dataclass near other internal structures:\n  ```python\n  from dataclasses import dataclass\n\n  @dataclass\n  class MarkdownSection:\n      header: str           # e.g., \"## Methods\"\n      level: int            # 1-6\n      content: str          # section content including header\n      start_line: int       # 0-based line index in original document\n      parent_headers: list  # list of (level, header_text)\n  ```\n- Define MarkdownChunkerStrategy class skeleton implementing ChunkingStrategy:\n  ```python\n  class MarkdownChunkerStrategy(ChunkingStrategy):\n      \"\"\"Splits markdown documents by headers while preserving hierarchy.\"\"\"\n\n      def __init__(\n          self,\n          headers_to_split_on: list[tuple[str, str]] | None = None,\n          max_chunk_size: int = 1024,\n          chunk_overlap: int = 200,\n          include_header_in_chunk: bool = True,\n          preserve_hierarchy: bool = True,\n      ) -> None:\n          self.headers_to_split_on = headers_to_split_on or [\n              (\"#\", \"h1\"),\n              (\"##\", \"h2\"),\n              (\"###\", \"h3\"),\n              (\"####\", \"h4\"),\n          ]\n          self.max_chunk_size = max_chunk_size\n          self.chunk_overlap = chunk_overlap\n          self.include_header_in_chunk = include_header_in_chunk\n          self.preserve_hierarchy = preserve_hierarchy\n\n      def chunk(self, text: str, metadata: dict | None = None) -> list[TextNode]:\n          \"\"\"Split markdown text into nodes by headers.\"\"\"\n          raise NotImplementedError\n\n      def _split_by_headers(self, text: str) -> list[MarkdownSection]:\n          raise NotImplementedError\n\n      def _get_header_hierarchy(self, sections: list[MarkdownSection], index: int) -> dict:\n          raise NotImplementedError\n  ```\n- Ensure type hints match the project’s minimum Python version and style (e.g., using from __future__ import annotations if the repo does so).\n- Add constructor defaults exactly as in PRD: max_chunk_size=1024, chunk_overlap=200, include_header_in_chunk=True.\n- Add docstrings matching PRD’s description of attributes and behavior.\n- Confirm TextNode import from LlamaIndex (likely from llama_index.core.schema import TextNode) and ChunkingStrategy base type to ensure compatibility with the rest of the chunking_service module.\n",
        "testStrategy": "- Add or extend unit tests in tests/test_chunking_service.py to:\n  - Validate that MarkdownSection dataclass can be instantiated and its fields are correctly assigned.\n  - Validate that MarkdownChunkerStrategy can be instantiated with default parameters and custom headers_to_split_on.\n  - Check that attributes (max_chunk_size, chunk_overlap, include_header_in_chunk, preserve_hierarchy) are set to expected defaults.\n  - Run mypy or the project’s static type checker (if configured) to ensure no type errors for new interfaces.\n  - Run pytest to confirm no regressions in existing chunking tests.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Implement markdown header detection and section splitting",
        "description": "Implement _split_by_headers in MarkdownChunkerStrategy to detect markdown headers via regex, build MarkdownSection objects, and preserve header hierarchy and line positions.",
        "details": "Implementation details:\n- In app/services/chunking_service.py implement header detection using the regex specified in the PRD:\n  ```python\n  import re\n\n  HEADER_RE = re.compile(r\"^(#{1,6})\\s+(.+)$\")\n  ```\n- Split the incoming markdown text into lines and iterate with enumerate to track line numbers.\n- For each line, use HEADER_RE.match(line) to detect headers:\n  ```python\n  match = HEADER_RE.match(line)\n  if match:\n      hashes, title = match.groups()\n      level = len(hashes)\n  ```\n- Maintain a list of section start indices; whenever a new header is found, close the previous section by slicing lines from its start_line to current index (exclusive) and create a MarkdownSection.\n- The content of each section should include the header line if include_header_in_chunk is True; otherwise store content without the header in MarkdownSection.content but keep header string separately.\n- Implement parent header tracking to preserve hierarchy:\n  - Maintain a stack of active headers: e.g., list of tuples (level, title).\n  - When a new header of level L is seen, pop from stack until top has level < L, then push (L, title).\n  - For each section, compute parent_headers as a copy of the current stack excluding the section’s own header or including only levels < current level (depending on preference, but align with PRD’s header_hierarchy example where h1 is parent of h2).\n- At the end of the document, finalize the last section from its start_line to the end of lines.\n- Return a list[MarkdownSection] preserving original order; ensure sections without any content beyond the header still have content = header line (if include_header_in_chunk) to avoid empty nodes later.\n- Handle documents with no headers by returning a single MarkdownSection with header=\"\" and level=0 and full content.\n",
        "testStrategy": "- Add unit tests in tests/test_chunking_service.py for _split_by_headers:\n  - Case 1: Document with h1, h2, h3 headers and content lines; assert number of sections, their header strings (e.g., \"# Intro\", \"## Methods\"), levels (1,2,3), and start_line indices.\n  - Case 2: Verify parent_headers: for a section under \"# Chapter 1\", \"## Methods\" header_hierarchy should reflect h1 parent.\n  - Case 3: Document without any headers returns a single MarkdownSection with level=0, header=\"\", content equal to full text.\n  - Case 4: Edge cases: multiple consecutive headers with no body, trailing content without header.\n  - Assert that include_header_in_chunk toggling changes whether the header line appears in MarkdownSection.content.\n  - Run pytest and ensure coverage for header_regex behavior and hierarchy building logic.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Implement token counting, sentence fallback, and chunk() orchestration",
        "description": "Implement chunk() in MarkdownChunkerStrategy to split by headers first, then apply SentenceSplitter for oversized sections while preserving header metadata and overlap configuration.",
        "details": "Implementation details:\n- In app/services/chunking_service.py implement helper methods:\n  - _token_count(text: str) -> int using the same tokenization approach used elsewhere in chunking_service (e.g., via tiktoken or LlamaIndex token counter). If existing utilities exist (e.g., count_tokens), reuse them for consistency.\n  - _sentence_split(text: str) -> list[TextNode] that wraps the existing SentenceSplitter strategy used by the app:\n    ```python\n    def _sentence_split(self, text: str) -> list[TextNode]:\n        splitter = SentenceSplitter(\n            chunk_size=self.max_chunk_size,\n            chunk_overlap=self.chunk_overlap,\n        )\n        return splitter.split_text(text)\n    ```\n    Adjust to existing API of SentenceSplitter in the codebase.\n  - _create_node(section: MarkdownSection, base_metadata: dict | None = None) -> TextNode that builds a TextNode from the section content and attaches metadata.\n- Implement chunk() orchestration roughly as in PRD pseudocode:\n  ```python\n  def chunk(self, text: str, metadata: dict | None = None) -> list[TextNode]:\n      sections = self._split_by_headers(text)\n      nodes: list[TextNode] = []\n\n      for idx, section in enumerate(sections):\n          if self._token_count(section.content) > self.max_chunk_size:\n              sub_nodes = self._sentence_split(section.content)\n              for node in sub_nodes:\n                  self._attach_markdown_metadata(node, section, idx, len(sections), metadata, is_header_split=False)\n              nodes.extend(sub_nodes)\n          else:\n              node = self._create_node(section, metadata)\n              self._attach_markdown_metadata(node, section, idx, len(sections), metadata, is_header_split=True)\n              nodes.append(node)\n\n      return nodes\n  ```\n- Implement _attach_markdown_metadata to enrich each TextNode’s metadata according to the PRD schema:\n  ```python\n  def _attach_markdown_metadata(\n      self,\n      node: TextNode,\n      section: MarkdownSection,\n      index: int,\n      total_sections: int,\n      base_metadata: dict | None,\n      is_header_split: bool,\n  ) -> None:\n      md = dict(base_metadata or {})\n      hierarchy = self._get_header_hierarchy(...)\n      md.update({\n          \"section_header\": section.header,\n          \"header_level\": section.level,\n          \"header_hierarchy\": hierarchy,\n          \"chunk_index\": index,\n          \"total_chunks\": total_sections,\n          \"is_header_split\": is_header_split,\n      })\n      node.metadata.update(md)\n  ```\n- Ensure that any existing metadata (e.g., source_file) is preserved and supplemented, not overwritten.\n- Respect include_header_in_chunk when assembling node.text; for fallback sentence chunks, consider prefixing the header line or leaving text as-is while only using metadata to avoid repeated headers, depending on retrieval expectations.\n- Ensure chunk_overlap semantics are applied only via SentenceSplitter and that header-based sections are not artificially overlapped unless required by downstream consumers.\n",
        "testStrategy": "- Extend tests/test_chunking_service.py:\n  - Test chunk() on a small markdown doc where all sections are below max_chunk_size: ensure number of nodes equals number of sections and that each node.text starts with the header when include_header_in_chunk=True.\n  - Test a document with a very large section exceeding max_chunk_size: verify that chunk() uses SentenceSplitter (more than one node for that section) and that each sub-node has section_header and is_header_split=False.\n  - Verify that metadata fields section_header, header_level, header_hierarchy, chunk_index, total_chunks, is_header_split are correctly populated and stable.\n  - Test that base metadata (e.g., {\"source_file\": \"doc.pdf\"}) is preserved and present on each node.\n  - Test documents without headers: ensure chunk() falls back to a single section which may then be sentence-split if over limit.\n  - Run pytest and evaluate that new tests cover both header-first and fallback behavior.",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Integrate LlamaIndex Markdown parsers where appropriate",
        "description": "Integrate LlamaIndex MarkdownNodeParser or MarkdownElementNodeParser within MarkdownChunkerStrategy or the chunking pipeline for complex markdown documents while maintaining header-based chunk semantics.",
        "details": "Implementation details:\n- Review existing LlamaIndex integration in chunking_service or related services to identify how documents and TextNode objects are currently constructed.\n- Add optional use of LlamaIndex markdown node parsers for cases where structured parsing is beneficial:\n  ```python\n  from llama_index.core.node_parser import MarkdownNodeParser, MarkdownElementNodeParser\n  ```\n- Decide on a simple selection strategy:\n  - Default to MarkdownNodeParser for general cases.\n  - Optionally allow selecting MarkdownElementNodeParser via a constructor flag or configuration (e.g., use_element_parser: bool = False) for docs with complex elements (tables, code blocks).\n- Implement an internal helper in MarkdownChunkerStrategy or a nearby utility:\n  ```python\n  def _parse_markdown_with_llamaindex(self, text: str) -> list[TextNode]:\n      parser = MarkdownNodeParser()\n      docs = [Document(text=text)]  # use correct LlamaIndex Document class\n      return parser.get_nodes_from_documents(docs)\n  ```\n- Evaluate whether to:\n  - Use LlamaIndex only for token counting and TextNode creation while still using header-based splitting; or\n  - Defer fully to LlamaIndex node parsing for specific content types while still attaching header metadata.\n  Follow the PRD’s architecture, which prioritizes header-based MarkdownChunker plus fallback SentenceSplitter; keep LlamaIndex integration minimal and non-breaking.\n- Ensure imports and usage align with the currently installed LlamaIndex version (using llama_index.core.* namespaces as in the PRD).\n- Guard this integration behind feature flags or configuration if necessary to avoid impacting existing behavior unexpectedly.\n",
        "testStrategy": "- Add focused tests (or adjust existing integration tests) to confirm that:\n  - MarkdownNodeParser import paths are valid and do not raise ImportError in the project environment.\n  - The optional parser integration function _parse_markdown_with_llamaindex returns a non-empty list of TextNode objects for sample markdown input.\n  - When the feature flag or configuration to use MarkdownElementNodeParser is enabled, it is instantiated and invoked correctly.\n  - Overall MarkdownChunkerStrategy.chunk() output structure remains the same when LlamaIndex parsing is enabled vs disabled (aside from any additional metadata), ensuring backward compatibility.\n  - Run pytest and ensure that these tests are skipped or guarded appropriately if LlamaIndex version changes are detected (e.g., via hasattr checks).",
        "priority": "medium",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Wire MarkdownChunkerStrategy into chunking_service dispatch",
        "description": "Register MarkdownChunkerStrategy within app/services/chunking_service.py and ensure it can be selected alongside existing chunking strategies without breaking current behavior.",
        "details": "Implementation details:\n- Inspect existing strategy selection mechanism in app/services/chunking_service.py (e.g., a factory function, strategy map, or enum-to-class mapping).\n- Add an entry for \"markdown\" or MarkdownChunkerStrategy to the mapping, for example:\n  ```python\n  CHUNKING_STRATEGIES = {\n      \"sentence\": SentenceChunkerStrategy,\n      \"markdown\": MarkdownChunkerStrategy,\n      # other strategies...\n  }\n  ```\n  or adjust to match the project’s existing pattern.\n- Ensure that the default strategy remains SentenceSplitter-based to preserve backward compatibility unless explicitly configured otherwise.\n- Expose MarkdownChunkerStrategy configuration options (e.g., max_chunk_size, chunk_overlap, include_header_in_chunk) through any existing configuration objects or function parameters so callers may override defaults.\n- Confirm that the return type of the factory/dispatcher remains consistent (e.g., returns a ChunkingStrategy instance) and that MarkdownChunkerStrategy adheres to this interface.\n- Add logging at debug level when MarkdownChunkerStrategy is selected to aid in troubleshooting.\n",
        "testStrategy": "- Extend chunking_service tests to cover strategy selection:\n  - Instantiate strategy via the factory/dispatcher with a value corresponding to markdown and assert that the returned instance is MarkdownChunkerStrategy.\n  - Confirm that requesting the default or existing strategies still returns the SentenceSplitter-based implementation.\n  - For a simple markdown input, call the dispatcher-created strategy.chunk() and assert that the results are not empty and reflect header awareness (e.g., first node metadata.section_header is not None).\n  - Run pytest to ensure no regressions in existing strategy selection tests.",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Add markdown content detection in document_processor",
        "description": "Enhance app/services/document_processor.py to detect markdown content (by file extension and/or content heuristics) and expose this information to downstream processing.",
        "details": "Implementation details:\n- Open app/services/document_processor.py and locate the logic that normalizes or classifies document content, especially around LlamaParse outputs.\n- Implement markdown detection with a combination of:\n  - File extension checks for .md and .markdown.\n  - Content heuristics: presence of markdown headers (lines matching ^#{1,6}\\s+), bullet lists (-, *, + at line start), or fenced code blocks (``` or ~~~).\n- Add a function like:\n  ```python\n  def is_markdown_content(source_path: str | None, text: str) -> bool:\n      if source_path and source_path.lower().endswith((\".md\", \".markdown\")):\n          return True\n      for line in text.splitlines():\n          if HEADER_RE.match(line):\n              return True\n      return False\n  ```\n  Reuse HEADER_RE from chunking_service if possible or define a shared utility.\n- When processing LlamaParse output (already configured with result_type=\"markdown\" in app/tasks/source_processing.py), annotate the document or metadata with a flag like metadata[\"is_markdown\"] = True.\n- Ensure this flag is included in whatever data structure is passed into the chunking service (e.g., SourceDocument or similar) so that source_processing can choose MarkdownChunkerStrategy when appropriate.\n- Keep behavior for non-markdown sources unchanged, and ensure that unstructured PDF/HTML that coincidentally contains some # signs does not cause misclassification by requiring multiple signals or the explicit LlamaParse markdown flag when available.\n",
        "testStrategy": "- Create unit tests in tests/test_document_processor.py:\n  - Test is_markdown_content() for a .md file with markdown headers; assert True.\n  - Test for a .pdf or .txt file with minimal markdown-like content; adjust heuristics so classification aligns with expectations (likely False unless explicitly flagged).\n  - Test detection for LlamaParse outputs where a metadata flag like result_type=\"markdown\" is present; assert that the is_markdown flag is set accordingly.\n  - End-to-end style test: simulate processing of a markdown file and assert that the resulting document metadata contains is_markdown=True.\n  - Run pytest to validate tests and ensure existing document processing behavior remains intact.",
        "priority": "high",
        "dependencies": [
          5
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Update source_processing to use MarkdownChunkerStrategy for markdown inputs",
        "description": "Modify app/tasks/source_processing.py so that when the source is markdown (native .md or LlamaParse markdown output), it uses MarkdownChunkerStrategy with SentenceSplitter fallback for large sections, while preserving backward compatibility.",
        "details": "Implementation details:\n- In app/tasks/source_processing.py locate the path where documents are parsed with LlamaParse using result_type=\"markdown\" and then passed to the chunking_service.\n- Inject logic to select the chunking strategy based on:\n  - Source file extension (.md or .markdown).\n  - is_markdown flag set by document_processor.\n  - Explicit configuration (e.g., a per-source chunking_strategy field once Phase 4 is implemented).\n- Example selection logic (pseudocode):\n  ```python\n  if doc.metadata.get(\"is_markdown\") or source_path.endswith(\".md\"):\n      chunker = MarkdownChunkerStrategy(\n          max_chunk_size=settings.chunking.max_tokens,\n          chunk_overlap=settings.chunking.overlap,\n      )\n  else:\n      chunker = SentenceChunkerStrategy(...)\n  nodes = chunker.chunk(doc.text, metadata=doc.metadata)\n  ```\n- Ensure that existing configurations that explicitly request a non-markdown strategy still work and override automatic detection if needed.\n- Confirm that for markdown sources, header metadata produced by MarkdownChunkerStrategy (section_header, header_hierarchy, etc.) is preserved through to the vector store ingestion pipeline.\n- Maintain any existing logging and error handling semantics and add a debug log when markdown chunking is applied for traceability.\n",
        "testStrategy": "- Add tests in tests/test_source_processing.py (create file if it does not exist):\n  - Mock a markdown document (e.g., metadata.is_markdown=True) and assert that source_processing chooses MarkdownChunkerStrategy by checking type of constructed chunker or inspecting resulting nodes’ metadata.\n  - Mock a non-markdown PDF document and confirm that SentenceChunkerStrategy remains in use.\n  - For a simulated LlamaParse markdown output, run through the processing pipeline and assert that chunks include section_header metadata and that chunks do not split mid-section when section size < max_chunk_size.\n  - Run the full test suite to ensure existing behavior is unaffected for non-markdown sources.",
        "priority": "high",
        "dependencies": [
          5,
          6
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Enhance chunk metadata and update vector store schema if needed",
        "description": "Extend chunk metadata to include section header details and hierarchy and update the vector store schema or ingestion logic so these fields are stored and queryable.",
        "details": "Implementation details:\n- Align metadata filled in MarkdownChunkerStrategy with the schema from the PRD:\n  ```json\n  {\n    \"section_header\": \"## Methods\",\n    \"header_level\": 2,\n    \"header_hierarchy\": {\n      \"h1\": \"Chapter 1: Introduction\",\n      \"h2\": \"Methods\"\n    },\n    \"source_file\": \"document.pdf\",\n    \"chunk_index\": 3,\n    \"total_chunks\": 15,\n    \"is_header_split\": true\n  }\n  ```\n- Implement _get_header_hierarchy(sections, index) in MarkdownChunkerStrategy so it returns a dict keyed by header name (e.g., \"h1\", \"h2\") with values from MarkdownSection.parent_headers and current section header.\n- Review the vector store ingestion logic (where TextNode.metadata is stored) to ensure that new fields are persisted; this may be in a separate service or repository layer.\n- If the vector store schema is explicit (e.g., SQL tables or a structured metadata schema), add columns or JSON fields as appropriate to hold the new metadata keys.\n- Ensure backward compatibility:\n  - For non-markdown chunks, either omit these keys or set them to sensible defaults (e.g., section_header=None, is_header_split=False).\n- Consider indexing strategies for improved retrieval (e.g., enabling filtering by section_header or header_level if the underlying vector DB supports metadata filters), but avoid overengineering beyond what the PRD requires.\n",
        "testStrategy": "- Add or update tests in the module responsible for vector store ingestion (e.g., tests/test_vector_store_integration.py):\n  - Create a TextNode with the new metadata fields and persist it; retrieve it from the vector store and assert that metadata fields are round-tripped correctly.\n  - Test that documents without markdown headers still ingest successfully and either omit or set default values for the new fields.\n  - If metadata-based filtering is supported, add a test that queries by section_header or header_level and verify that only relevant chunks are returned.\n  - Run pytest and confirm that schema changes do not break existing tests or migrations.",
        "priority": "medium",
        "dependencies": [
          3,
          7
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Add configuration and API support for chunking strategy selection",
        "description": "Introduce configuration options and API-level controls to select chunking strategies, including a markdown option and markdown-specific settings, updating app/models/sources.py and relevant routes.",
        "details": "Implementation details:\n- In app/models/sources.py add a chunking_strategy field (enum-like) to the Source or equivalent model, with allowed values such as: \"sentence\", \"markdown\", and any existing strategies.\n  - For example, using Pydantic or dataclasses:\n    ```python\n    class ChunkingStrategyEnum(str, Enum):\n        sentence = \"sentence\"\n        markdown = \"markdown\"\n    ```\n- Add optional configuration fields for markdown-specific settings (e.g., markdown_max_chunk_size, markdown_chunk_overlap, include_header_in_chunk) in the project’s config module or settings file.\n- Update API request/response schemas (e.g., FastAPI Pydantic models) to allow clients to specify chunking_strategy and markdown-related options when creating or updating sources.\n- Modify the logic in source_processing and/or chunking_service factory to honor an explicitly configured chunking_strategy over automatic detection, while falling back to detection when the value is unset.\n- Update any OpenAPI or API documentation generation annotations so that the new field appears in API docs.\n- Keep default behavior unchanged for existing clients (e.g., default chunking_strategy=\"sentence\" if not provided).\n",
        "testStrategy": "- Add or update tests for API models and routes (e.g., tests/test_sources_api.py):\n  - Validate that creating a new source with chunking_strategy=\"markdown\" is accepted and stored.\n  - Validate that omitting chunking_strategy defaults to the existing strategy (likely sentence).\n  - Verify that invalid chunking_strategy values result in proper validation errors.\n  - Test that when a source has chunking_strategy=\"markdown\", the downstream processing pipeline uses MarkdownChunkerStrategy even if heuristic detection would not choose it.\n  - Run the API test suite to ensure backward compatibility.",
        "priority": "low",
        "dependencies": [
          5,
          7
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Create comprehensive tests and validation for markdown chunk quality",
        "description": "Add higher-level tests and validation checks to ensure markdown-aware chunking preserves section boundaries, maintains context, and falls back correctly when headers are absent or sections are oversized.",
        "details": "Implementation details:\n- Design end-to-end or integration-style tests that simulate the full pipeline:\n  - Input: sample markdown documents representing common structures (e.g., PRDs, research papers with #, ##, ### headers, and large sections).\n  - Processing: run through document_processor → source_processing → chunking_service → (optional) vector store ingestion.\n- Validate key success metrics qualitatively via tests:\n  - Chunk quality: majority of chunks start with a header or represent a complete section when section size < max_chunk_size.\n  - Context preservation: content under a given header mostly appears in contiguous chunks, especially compared to the old SentenceSplitter-only behavior.\n- Implement concrete assertions, for example:\n  - For a \"Methods\" section under a specific chapter, ensure that at least one chunk’s metadata.section_header contains \"Methods\" and its text starts near that section’s content.\n  - Ensure that chunks created by sentence fallback for large sections still carry section_header metadata and header_hierarchy.\n  - Verify fallback behavior for documents with no headers: ensure the pipeline either uses SentenceSplitter or returns a single large node that is then split by sentences.\n- Optionally, implement a regression test comparing old vs new behavior by invoking SentenceSplitter directly and confirming that MarkdownChunkerStrategy produces fewer cross-section splits on a synthetic document.\n- Ensure tests are deterministic and do not depend on external services by mocking LlamaParse and vector store interactions when necessary.\n",
        "testStrategy": "- Add a new test module (e.g., tests/test_markdown_chunking_integration.py):\n  - Test that for a multi-section markdown doc, len(set(node.metadata[\"section_header\"] for node in nodes)) matches the number of logical sections.\n  - Assert that for each section, the first chunk for that section begins with the header (or at least contains it at the top) when include_header_in_chunk=True.\n  - Verify that when max_chunk_size is artificially small, hybrid behavior triggers: sections are first separated by headers, then sub-chunked via SentenceSplitter, with proper metadata.\n  - Confirm that documents without headers are successfully processed and that is_header_split is False for all nodes.\n  - Run pytest and examine coverage for the full markdown-aware pipeline.\n  - Optionally, measure simple statistics (e.g., percentage of chunks starting with a header) inside tests and assert they exceed a threshold for the sample documents.",
        "priority": "medium",
        "dependencies": [
          2,
          3,
          6,
          7,
          8,
          9
        ],
        "status": "done",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2026-01-12T01:53:19.456Z",
      "updated": "2026-01-12T01:53:19.456Z",
      "description": "Tasks for 006-markdown-chunking context"
    }
  }
}