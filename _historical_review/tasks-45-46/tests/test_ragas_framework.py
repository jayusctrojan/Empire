"""
Tests for RAGAS Framework Integration and Test Dataset

Tests RAGAS framework setup and validation:
- RAGAS library installation and imports
- Test dataset structure and validation
- Dataset format compatibility with RAGAS
- Framework initialization
"""

import pytest
import json
from pathlib import Path
from typing import List, Dict, Any


class TestRAGASInstallation:
    """Test RAGAS library installation and imports"""

    def test_ragas_import(self):
        """Test RAGAS can be imported"""
        try:
            from ragas import evaluate
            from ragas.metrics import (
                faithfulness,
                answer_relevancy,
                context_precision,
                context_recall
            )
            assert True
        except ImportError as e:
            pytest.fail(f"Failed to import RAGAS: {e}")

    def test_ragas_metrics_available(self):
        """Test all required RAGAS metrics are available"""
        from ragas.metrics import (
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall
        )

        assert faithfulness is not None
        assert answer_relevancy is not None
        assert context_precision is not None
        assert context_recall is not None


class TestDatasetStructure:
    """Test RAGAS test dataset structure and validation"""

    @pytest.fixture
    def dataset_path(self):
        """Path to RAGAS test dataset"""
        return Path(".taskmaster/docs/ragas_test_dataset.json")

    @pytest.fixture
    def dataset(self, dataset_path):
        """Load RAGAS test dataset"""
        with open(dataset_path, 'r') as f:
            data = json.load(f)
            return data.get("test_samples", [])

    def test_dataset_file_exists(self, dataset_path):
        """Test dataset file exists"""
        assert dataset_path.exists(), f"Dataset file not found at {dataset_path}"

    def test_dataset_is_valid_json(self, dataset_path):
        """Test dataset is valid JSON"""
        try:
            with open(dataset_path, 'r') as f:
                json.load(f)
        except json.JSONDecodeError as e:
            pytest.fail(f"Dataset is not valid JSON: {e}")

    def test_dataset_has_30_samples(self, dataset):
        """Test dataset contains at least 30 samples"""
        assert len(dataset) >= 30, f"Expected at least 30 samples, got {len(dataset)}"

    def test_dataset_sample_structure(self, dataset):
        """Test each sample has required fields"""
        required_fields = ["question", "contexts", "ground_truth"]

        for i, sample in enumerate(dataset):
            for field in required_fields:
                assert field in sample, f"Sample {i} missing required field: {field}"

    def test_dataset_question_field(self, dataset):
        """Test question field is non-empty string"""
        for i, sample in enumerate(dataset):
            assert isinstance(sample["question"], str), f"Sample {i} question is not a string"
            assert len(sample["question"]) > 0, f"Sample {i} question is empty"

    def test_dataset_can_generate_answer_field(self, dataset):
        """Test answer field can be generated from ground_truth"""
        for i, sample in enumerate(dataset):
            # For testing, we use ground_truth as answer placeholder
            # In production, this will be generated by the RAG pipeline
            answer = sample.get("ground_truth", "")
            assert isinstance(answer, str), f"Sample {i} cannot generate answer"
            assert len(answer) > 0, f"Sample {i} generated answer is empty"

    def test_dataset_contexts_field(self, dataset):
        """Test contexts field is list of strings"""
        for i, sample in enumerate(dataset):
            assert isinstance(sample["contexts"], list), f"Sample {i} contexts is not a list"
            assert len(sample["contexts"]) > 0, f"Sample {i} contexts is empty"

            for j, context in enumerate(sample["contexts"]):
                assert isinstance(context, str), f"Sample {i} context {j} is not a string"
                assert len(context) > 0, f"Sample {i} context {j} is empty"

    def test_dataset_ground_truth_field(self, dataset):
        """Test ground_truth field is non-empty string"""
        for i, sample in enumerate(dataset):
            assert isinstance(sample["ground_truth"], str), f"Sample {i} ground_truth is not a string"
            assert len(sample["ground_truth"]) > 0, f"Sample {i} ground_truth is empty"


class TestRAGASDatasetIntegration:
    """Test RAGAS framework integration with dataset"""

    @pytest.fixture
    def dataset_path(self):
        """Path to RAGAS test dataset"""
        return Path(".taskmaster/docs/ragas_test_dataset.json")

    @pytest.fixture
    def dataset(self, dataset_path):
        """Load RAGAS test dataset"""
        with open(dataset_path, 'r') as f:
            data = json.load(f)
            return data.get("test_samples", [])

    def test_dataset_to_ragas_format(self, dataset):
        """Test dataset can be converted to RAGAS format"""
        from datasets import Dataset

        # Convert to RAGAS-compatible format
        # Note: For testing, we use ground_truth as answer placeholder
        # In production, answer will be generated by the RAG pipeline
        ragas_data = {
            "question": [sample["question"] for sample in dataset],
            "answer": [sample["ground_truth"] for sample in dataset],  # Placeholder
            "contexts": [sample["contexts"] for sample in dataset],
            "ground_truth": [sample["ground_truth"] for sample in dataset]
        }

        # Create HuggingFace Dataset
        ragas_dataset = Dataset.from_dict(ragas_data)

        assert len(ragas_dataset) >= 30  # At least 30 samples
        assert "question" in ragas_dataset.column_names
        assert "answer" in ragas_dataset.column_names
        assert "contexts" in ragas_dataset.column_names
        assert "ground_truth" in ragas_dataset.column_names

    def test_ragas_evaluation_initialization(self, dataset):
        """Test RAGAS evaluation can be initialized"""
        from ragas import evaluate
        from ragas.metrics import (
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall
        )
        from datasets import Dataset

        # Convert to RAGAS format
        # Note: Using ground_truth as answer placeholder for testing
        ragas_data = {
            "question": [sample["question"] for sample in dataset[:5]],  # Test with 5 samples
            "answer": [sample["ground_truth"] for sample in dataset[:5]],  # Placeholder
            "contexts": [sample["contexts"] for sample in dataset[:5]],
            "ground_truth": [sample["ground_truth"] for sample in dataset[:5]]
        }

        ragas_dataset = Dataset.from_dict(ragas_data)

        # Define metrics
        metrics = [
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall
        ]

        # This should not raise an error
        # Note: We're not actually running evaluation yet (that requires API calls)
        assert ragas_dataset is not None
        assert len(metrics) == 4

    def test_dataset_diversity(self, dataset):
        """Test dataset has diverse questions"""
        questions = [sample["question"] for sample in dataset]

        # Check for unique questions
        unique_questions = set(questions)
        assert len(unique_questions) == len(questions), "Dataset contains duplicate questions"

        # Check questions cover different topics (simple heuristic)
        keywords = ["Empire", "architecture", "search", "cost", "database", "API", "service", "feature"]
        keyword_coverage = sum(
            1 for q in questions
            if any(keyword.lower() in q.lower() for keyword in keywords)
        )

        # At least 50% of questions should contain domain keywords
        min_coverage = len(questions) // 2
        assert keyword_coverage >= min_coverage, f"Dataset should cover diverse topics, only {keyword_coverage}/{len(questions)} questions contain relevant keywords"


class TestRAGASConfiguration:
    """Test RAGAS configuration and setup"""

    def test_ragas_metrics_configuration(self):
        """Test RAGAS metrics can be configured"""
        from ragas.metrics import (
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall
        )

        # All metrics should be callable/usable
        metrics = [
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall
        ]

        assert len(metrics) == 4

        # Check each metric has expected attributes
        for metric in metrics:
            assert hasattr(metric, 'name') or hasattr(metric, '__name__')

    def test_ragas_dataset_library_available(self):
        """Test HuggingFace datasets library is available"""
        try:
            from datasets import Dataset
            assert Dataset is not None
        except ImportError as e:
            pytest.fail(f"datasets library not available: {e}")


class TestRAGASEnvironment:
    """Test RAGAS environment setup"""

    def test_required_dependencies(self):
        """Test all required dependencies are installed"""
        required_packages = [
            "ragas",
            "datasets",
            "openai",  # Required for RAGAS evaluations
            "langchain"  # Required for RAGAS
        ]

        for package in required_packages:
            try:
                __import__(package)
            except ImportError as e:
                pytest.fail(f"Required package '{package}' not installed: {e}")

    def test_environment_variables(self):
        """Test required environment variables are set or can be loaded"""
        import os
        from pathlib import Path

        # Try to load .env file if it exists
        env_file = Path(".env")
        if env_file.exists():
            from dotenv import load_dotenv
            load_dotenv()

        # RAGAS requires OpenAI API key (or other LLM API)
        api_keys = [
            "OPENAI_API_KEY",
            "ANTHROPIC_API_KEY"
        ]

        has_key = any(os.getenv(key) for key in api_keys)

        if not has_key:
            pytest.skip("No LLM API key found in environment - skipping (OPENAI_API_KEY or ANTHROPIC_API_KEY required for actual evaluation)")
