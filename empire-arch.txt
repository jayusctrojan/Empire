AI Empire Complete Architecture v7.3 (15-Agent AI System + Local Embeddings + Persistent Memory) - Production-Ready
Cloud-First AI Architecture with 15 AI Agents, Local BGE-M3 Embeddings, and Graphiti Memory System

Last Updated: 2025-12-30
Version: v7.3.0
Status: âœ… ALL 46 TASKS COMPLETE - Production Deployed & Verified

================================================================================
DEVELOPMENT ENVIRONMENT & TOOLS
================================================================================

Primary IDE: Visual Studio Code
- Claude Code (CLI) - Primary architect for complex development tasks
- Cline (VS Code extension) - Rapid feature implementation within VS Code
- Continue.dev (VS Code extension) - Code completion and refactoring

MCP Servers (Model Context Protocol) Available for Development:
1. Claude Context MCP - Conversation context and project memory across sessions
2. Chrome DevTools MCP - Browser debugging, DOM inspection, network/performance analysis
3. Ref MCP - Official documentation reference (FastAPI, Neo4j, Supabase, Anthropic, LlamaIndex, Pydantic)
4. TaskMaster MCP - AI-powered task management, project planning, complexity analysis, workflow automation
5. MCP_Docker - GitHub operations (repos, PRs, issues, code search, branch management)
6. Render MCP - Deployment and service management (web services, databases, logs, metrics, environment variables)
7. Supabase MCP - Direct PostgreSQL + pgvector operations (tables, queries, indexes, RLS policies)
8. neo4j MCP - Graph database queries via natural language â†’ Cypher translation

All MCPs integrate seamlessly with Claude Code CLI for direct access to:
- Databases (PostgreSQL, Neo4j)
- Documentation (FastAPI, Neo4j, Supabase, etc.)
- GitHub (code, issues, PRs, branches)
- Render (deployment, services, logs, metrics, environment configuration)
- Browser DevTools (frontend debugging)
- Task Management (project planning and tracking)

================================================================================
VERSION 7.3 - LOCAL EMBEDDINGS + PERSISTENT MEMORY (November 2025)
================================================================================
This version adds LOCAL embedding generation and persistent memory system:
- âœ… Local BGE-M3 embeddings via Ollama (FREE - saves $50-100/month)
- âœ… Graphiti MCP Server with temporal knowledge graphs
- âœ… Complete personal/work memory separation
- âœ… 322 personal ChatGPT conversations imported
- âœ… Project-based memory organization (unlimited scalability)
- âœ… Neo4j Graph Database with natural language queries via Claude Desktop
- âœ… Chat UI (Gradio/Streamlit) for end-user access
- âœ… Bi-directional sync between Supabase and Neo4j
- âœ… BGE-M3 Embeddings (1024-dim with built-in sparse vectors) - NOW LOCAL
- âœ… Query Expansion with Claude Haiku (15-30% better recall)
- âœ… BGE-Reranker-v2 on Mac Studio (replacing Cohere, saves $30-50/month)
- âœ… Adaptive Document-Type Chunking (15-25% better precision)
- âœ… Optimized Semantic Caching (tiered similarity thresholds)
- âœ… Hybrid Search (dense + BGE-M3 sparse + ILIKE + fuzzy with RRF)
- âœ… LightRAG Knowledge Graphs (entity relationships and traversal)
- âœ… Context Expansion (neighboring chunks with hierarchical structure)
- âœ… Multi-Modal Processing (images via Claude Vision, audio via Soniox)
- âœ… Full Observability (Prometheus, Grafana, OpenTelemetry)
- RESULT: 40-60% better retrieval + DUAL ACCESS MODES + $0 EMBEDDING COSTS

Core Architecture Decisions (v7.2 - PRODUCTION + DEVELOPMENT):

**PRODUCTION ARCHITECTURE:**
- PRIMARY AI: Claude Sonnet 4.5 API (synthesis) + Claude Haiku (expansion)
- BACKEND API: FastAPI on Render (https://jb-empire-api.onrender.com) - srv-d44o2dq4d50c73elgupg
  * Plan: Starter ($7/month)
  * Region: Oregon
  * Health Check: /health
  * API Docs: /docs
  * TASK 46 DEPLOYED: LangGraph + Arcade.dev Integration (November 2024)
    - Query Endpoints: /api/query/* (health, auto, adaptive, adaptive/async, auto/async, batch, status, tools)
    - LangGraph: 5-node adaptive workflow with iterative refinement and conditional branching
    - Workflow Router: Intelligent routing (LangGraph/CrewAI/Simple RAG based on query classification)
    - Arcade.dev: External tool integration (Google Search, Slack, GitHub, etc.)
    - Environment: ARCADE_API_KEY=<from .env>, ARCADE_ENABLED=true, LANGGRAPH_DEFAULT_MODEL=claude-3-5-haiku-20241022
- TASK PROCESSING: Celery workers on Render - srv-d44oclodl3ps73bg8rmg
  * Plan: Starter ($7/month)
  * Concurrency: 2 workers
  * Tasks: Document processing, embeddings, graph sync, CrewAI workflows
- CACHE/BROKER: Upstash Redis (Serverless with TLS) - <credentials in .env>
  * Purpose: Semantic caching + Celery message broker + Rate limiting
  * Free Tier: 10,000 commands/day, 256 MB storage
  * TLS Enabled: rediss:// protocol for encrypted connections
  * Access: Global serverless access from all Render services
- PRODUCTION MEMORY: PostgreSQL graph tables (user_memory_nodes, user_memory_edges in Supabase)
- EMBEDDINGS: BGE-M3 LOCAL via Ollama (FREE - development testing)
- RERANKING: BGE-Reranker-v2 on Mac Studio (via Tailscale - <credentials in .env>)
- STORAGE: Supabase PostgreSQL + pgvector + graph tables + RLS policies (<credentials in .env>)
- ORCHESTRATION: CrewAI on Render (https://jb-crewai.onrender.com) - srv-d2n0hh3uibrs73buafo0 - MILESTONE 8
  * Task 40: CrewAI Asset Storage - B2 folder: crewai/assets/{department}/{type}/{execution_id}/
- MULTI-MODAL: Claude Vision for images, Soniox for audio transcription
- MONITORING & ALERTING (Task 44 - PRODUCTION): Prometheus + Grafana + Alertmanager
  * Prometheus: Metrics collection (port 9090)
  * Grafana: Visualization dashboards (port 3001, admin/empiregrafana123)
  * Alertmanager: Email notifications via Gmail SMTP (port 9093)
  * Node Exporter: System metrics (port 9100)
  * 39 Alert Rules: Critical/Warning/Info severity levels
  * Email Alerts: HTML formatted with severity-based styling
  * Alert Delivery: jbajaj08@gmail.com (Gmail App Password auth)
  * Alert Timing: Critical (10s), Warning (2m), Info (5m) delays
  * Files: docker-compose.monitoring.yml, alert_rules.yml, alertmanager.yml
  * Testing: ./test-alert.sh for verification
  * Status: âœ… DEPLOYED AND TESTED - Email delivery confirmed
- SECURITY: Multi-layer security with compliance (HIPAA, GDPR, SOC 2)
  * Task 41.1: HTTP security headers (HSTS, CSP, X-Frame-Options, etc.)
  * Task 41.1: Rate limiting (tiered limits per endpoint, Redis-backed)
  * Task 41.2: Row-Level Security (RLS) on 14 tables with 14 policies
  * Task 41.3: Encryption-at-rest (Supabase AES-256, B2 SSE-B2, App AES-256-GCM)
  * Task 41.5: Audit logging (comprehensive security event tracking)
  * TLS 1.2+ enforced across all services (FastAPI, Celery, Supabase, Neo4j, Redis)
  * Security Posture: 80/100 (HIGH) - improved from 65/100

**DEVELOPMENT ENVIRONMENT (Mac Studio):**
- Graphiti MCP Server: Development/testing memory ONLY (NOT production)
- Neo4j Graph Database: FREE Docker for development testing with TLS enabled
  * Connection: bolt+ssc://localhost:7687 (TLS) or bolt+ssc://100.119.86.6:7687 (via Tailscale)
  * Certificates: Self-signed, 365-day validity, mounted at /certificates/bolt/
  * TLS Mode: OPTIONAL (supports both encrypted and unencrypted connections)
- Ollama: BGE-M3 embeddings + BGE-Reranker-v2 (local, zero cost)
- Development testing for models before production deployment

================================================================================
DUAL-INTERFACE ARCHITECTURE (NEW v7.2)
================================================================================

Knowledge Access Interfaces:
â”œâ”€â”€ Interface 1: Chat UI (End Users)
â”‚   â”œâ”€â”€ Technology: Gradio/Streamlit on Render
â”‚   â”œâ”€â”€ Access: Web-based, team/public access
â”‚   â”œâ”€â”€ Features: Simple Q&A, document search, citations
â”‚   â”œâ”€â”€ Cost: $15-20/month
â”‚   â””â”€â”€ Best for: General queries, team collaboration
â”‚
â”œâ”€â”€ Interface 2: Neo4j MCP (Power Users)
â”‚   â”œâ”€â”€ Technology: Neo4j + MCP (Docker)
â”‚   â”œâ”€â”€ Access: Claude Desktop + Claude Code
â”‚   â”œâ”€â”€ Features: Natural language â†’ Cypher, graph exploration
â”‚   â”œâ”€â”€ Cost: $0 (Neo4j Community Edition)
â”‚   â””â”€â”€ Best for: Complex queries, development, analysis
â”‚
â””â”€â”€ Interface 3: Claude Code Integration (Developers)
    â”œâ”€â”€ Technology: Neo4j MCP via Docker
    â”œâ”€â”€ Access: Claude Code (VS Code integration)
    â”œâ”€â”€ Features: Code generation with graph context
    â”œâ”€â”€ Use Cases: Generate code from graph patterns
    â””â”€â”€ Best for: Development workflows, automated code generation

Data Architecture:
â”œâ”€â”€ Supabase PostgreSQL (Primary Storage)
â”‚   â”œâ”€â”€ BGE-M3 vectors (1024-dim)
â”‚   â”œâ”€â”€ Document text and metadata
â”‚   â”œâ”€â”€ Full-text search indexes
â”‚   â””â”€â”€ $25/month
â”‚
â”œâ”€â”€ Neo4j Graph Database (Graph Layer)
â”‚   â”œâ”€â”€ Entity nodes and relationships
â”‚   â”œâ”€â”€ Document-entity connections
â”‚   â”œâ”€â”€ Graph algorithms (PageRank, community detection)
â”‚   â””â”€â”€ Free (Community Edition on Mac Studio)
â”‚
â””â”€â”€ Synchronization
    â”œâ”€â”€ FastAPI service for bi-directional sync
    â”œâ”€â”€ 5-minute intervals
    â””â”€â”€ Conflict resolution: Supabase as source of truth

================================================================================
Render Services (Already Deployed)
================================================================================

Workspace ID: tea-d1vtdtre5dus73a4rb4g

LlamaIndex Service (Active):
â”œâ”€â”€ Service ID: srv-d2nl1lre5dus73atm9u0
â”œâ”€â”€ URL: https://jb-llamaindex.onrender.com
â”œâ”€â”€ Purpose: Document parsing, indexing, and vector retrieval
â”œâ”€â”€ Integration:
â”‚   â”œâ”€â”€ Parses PDFs, Word docs, contracts, policies
â”‚   â”œâ”€â”€ Creates searchable vector indexes
â”‚   â”œâ”€â”€ Integrates with Supabase pgvector
â”‚   â”œâ”€â”€ Coordinates with Neo4j for entity storage
â”‚   â””â”€â”€ Works with CrewAI for multi-document processing
â”œâ”€â”€ API Endpoints:
â”‚   â”œâ”€â”€ POST /parse - Parse documents
â”‚   â”œâ”€â”€ POST /index/create - Create vector index
â”‚   â”œâ”€â”€ POST /index/{id}/query - Query index
â”‚   â””â”€â”€ GET /health - Health check
â””â”€â”€ Cost: $7-21/month (Starter to Standard tier)

CrewAI Service (Active):
â”œâ”€â”€ Service ID: srv-d2n0hh3uibrs73buafo0
â”œâ”€â”€ URL: https://jb-crewai.onrender.com
â”œâ”€â”€ Purpose: Multi-agent AI orchestration for complex workflows
â”œâ”€â”€ Integration:
â”‚   â”œâ”€â”€ Coordinates multiple AI agents for complex tasks
â”‚   â”œâ”€â”€ Uses LlamaIndex for document retrieval
â”‚   â”œâ”€â”€ Queries Neo4j for entity relationships
â”‚   â”œâ”€â”€ Synthesizes results with Claude Sonnet
â”‚   â””â”€â”€ Manages parallel document analysis
â”œâ”€â”€ Agent Types:
â”‚   â”œâ”€â”€ Document Parser Agent (uses LlamaIndex)
â”‚   â”œâ”€â”€ Entity Extractor Agent (uses Neo4j)
â”‚   â”œâ”€â”€ Research Agent (uses hybrid search)
â”‚   â”œâ”€â”€ Synthesizer Agent (uses Claude)
â”‚   â””â”€â”€ Quality Validator Agent
â”œâ”€â”€ API Endpoints:
â”‚   â”œâ”€â”€ POST /run-crew - Execute multi-agent task
â”‚   â”œâ”€â”€ POST /analyze-documents - Batch document analysis
â”‚   â”œâ”€â”€ GET /crew-status/{id} - Check task status
â”‚   â””â”€â”€ GET /health - Health check
â””â”€â”€ Cost: $7-21/month (Starter to Standard tier)

Workflow Integration:
1. Document Upload â†’ LlamaIndex parses and chunks
2. LlamaIndex generates embeddings via BGE-M3
3. Store in Supabase + sync entities to Neo4j
4. Complex queries â†’ CrewAI orchestrates multi-agent search
5. CrewAI coordinates: LlamaIndex (retrieval) + Neo4j (graph) + Claude (synthesis)

================================================================================
Core Infrastructure
================================================================================

Mac Studio M3 Ultra (96GB) - Development & Local Services Hub
	â€¢	28-core CPU, 60-core GPU, 32-core Neural Engine
	â€¢	800 GB/s memory bandwidth
	â€¢	PRIMARY USE: Development + BGE-Reranker-v2 + Neo4j + Ollama
	â€¢	Neo4j Graph Database: Running in Docker (~4GB)
	â€¢	Ollama (BGE-M3 + BGE-Reranker-v2): Local AI models (~8-12GB)
	â€¢	BGE-Reranker-v2: Production reranking API via Tailscale (~1.5GB)
	â€¢	Claude Desktop + Claude Code: AI interfaces with MCP integration
	â€¢	Docker Desktop: Running Neo4j and MCP servers
	â€¢	Tailscale: Secure connection for production services
	â€¢	~80GB available for development, testing, caching
	â€¢	NOT running generative LLMs (APIs are better for generation)

================================================================================
NEO4J MCP DOCKER CONFIGURATION (NEW v7.2)
================================================================================

Docker Compose Setup:
```yaml
version: '3.8'
services:
  neo4j:
    image: neo4j:5-community
    container_name: empire-neo4j
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    volumes:
      - ./neo4j/data:/data
      - ./neo4j/logs:/logs
      - ./neo4j/import:/import
      - ./neo4j/plugins:/plugins
    environment:
      - NEO4J_AUTH=neo4j/empire-secure-password
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      - NEO4J_dbms_memory_heap_initial__size=2g
      - NEO4J_dbms_memory_heap_max__size=4g
      - NEO4J_dbms_memory_pagecache_size=2g
    restart: unless-stopped

  neo4j-mcp:
    image: mcp-neo4j-server:latest
    container_name: empire-neo4j-mcp
    ports:
      - "3000:3000"
    environment:
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=empire-secure-password
      - READ_ONLY=true
    depends_on:
      - neo4j
    restart: unless-stopped
```

MCP Configuration (claude_desktop_config.json):
```json
{
  "mcpServers": {
    "empire-neo4j": {
      "command": "docker",
      "args": ["exec", "-i", "empire-neo4j-mcp", "node", "/app/index.js"],
      "env": {
        "MCP_MODE": "empire-knowledge"
      }
    }
  }
}
```

Claude Code Integration:
- Neo4j MCP automatically available in Claude Code
- Natural language queries translate to Cypher
- Graph context available for code generation
- Example: "Generate Python code to process all documents related to RAG optimization"

================================================================================
GRAPHITI MCP CONFIGURATION (ADDED v7.2 - Nov 2025)
================================================================================

Developer Memory System with Graphiti:
â”œâ”€â”€ Technology Stack:
â”‚   â”œâ”€â”€ Graphiti Core: Temporal knowledge graph framework
â”‚   â”œâ”€â”€ Neo4j Community: Graph database backend (bolt://localhost:7687)
â”‚   â”œâ”€â”€ OpenAI API: Embeddings (text-embedding-3-small)
â”‚   â”œâ”€â”€ MCP Protocol: Claude Desktop/Code integration
â”‚   â””â”€â”€ Project-Based Memory Groups

â”œâ”€â”€ Memory Architecture:
â”‚   â”œâ”€â”€ Naming Convention: project_{projectname}_{aspect}
â”‚   â”œâ”€â”€ Empire Project Groups:
â”‚   â”‚   â”œâ”€â”€ project_empire - Main project memories
â”‚   â”‚   â”œâ”€â”€ project_empire_dev - Development notes
â”‚   â”‚   â”œâ”€â”€ project_empire_docs - Documentation
â”‚   â”‚   â”œâ”€â”€ project_empire_api - API specifications
â”‚   â”‚   â””â”€â”€ project_empire_notes - Meeting notes
â”‚   â”œâ”€â”€ Personal Memory:
â”‚   â”‚   â””â”€â”€ personal - ChatGPT history (332 memories imported)
â”‚   â””â”€â”€ Other Projects:
â”‚       â”œâ”€â”€ project_healthtrack - Health industry project
â”‚       â””â”€â”€ project_crypto_dashboard - Fintech project

â”œâ”€â”€ Context Management:
â”‚   â”œâ”€â”€ Quick Switch: ./mem empire (changes active context)
â”‚   â”œâ”€â”€ Add Project: ./mem add "project-name" --client "Client" --industry "Industry"
â”‚   â”œâ”€â”€ List Projects: ./mem (shows all contexts)
â”‚   â””â”€â”€ Auto-updates GROUP_ID in .env for MCP server

â”œâ”€â”€ File Locations (/mem-agent-mcp/):
â”‚   â”œâ”€â”€ graphiti-mcp-wrapper.sh - MCP server launcher
â”‚   â”œâ”€â”€ memory_context_manager.py - Context switching tool
â”‚   â”œâ”€â”€ memory_contexts.json - Project metadata
â”‚   â”œâ”€â”€ import_chatgpt_direct.py - Bulk import tool
â”‚   â””â”€â”€ graphiti/mcp_server/.env - API keys & config

â””â”€â”€ Benefits:
    â”œâ”€â”€ Complete separation of personal vs work memories
    â”œâ”€â”€ Scalable to unlimited projects
    â”œâ”€â”€ <100ms retrieval with Neo4j indexes
    â”œâ”€â”€ Context-aware code generation in Claude Code
    â””â”€â”€ Temporal tracking of knowledge evolution

Cloud AI Infrastructure (PRIMARY)
	â€¢	Claude Sonnet 4.5 API - Document synthesis ($30-50/month)
	  - Best-in-class accuracy for business documents (97-99%)
	  - Superior instruction following for consistent tagging
	  - Batch API: 90% cost savings
	  - Prompt caching: 50% additional savings
	  - Structured outputs: Reliable JSON generation
	  - Enterprise knowledge: Finance, compliance, legal
	â€¢	Claude Haiku API - Query expansion ($1.50-9/month)
	  - Ultra-fast query variation generation
	  - 4-5 semantic expansions per query
	  - $0.25 per 1M input tokens
	  - Sub-100ms latency
	  - Optional UI toggle for "Enhanced Search"
	â€¢	LlamaIndex (Render: srv-d2nl1lre5dus73atm9u0) - Document processing ($7-21/month)
	â€¢	CrewAI (Render: srv-d2n0hh3uibrs73buafo0) - Multi-agent orchestration ($7-21/month)
	â€¢	Supabase - PostgreSQL + pgvector ($15/month SMALL tier) - UNIFIED DATABASE
	â€¢	Backblaze B2 - File storage + backups ($10-20/month)

================================================================================
ADVANCED RAG ARCHITECTURE - FULLY RESTORED
================================================================================

Hybrid Search System (v7.1 - OPTIMIZED):
â”œâ”€â”€ Dense Search (BGE-M3 Embeddings)
â”‚   â”œâ”€â”€ Supabase pgvector extension
â”‚   â”œâ”€â”€ 1024-dim vectors (upgraded from 768-dim)
â”‚   â”œâ”€â”€ HNSW index for fast similarity search
â”‚   â”œâ”€â”€ Cosine distance measurement
â”‚   â””â”€â”€ 3-5% better retrieval quality
â”‚
â”œâ”€â”€ Sparse Search (BGE-M3 Built-in)
â”‚   â”œâ”€â”€ BGE-M3 native sparse vectors
â”‚   â”œâ”€â”€ Superior to traditional BM25
â”‚   â”œâ”€â”€ Stored in JSONB column
â”‚   â””â”€â”€ No separate FTS index needed
â”‚
â”œâ”€â”€ ILIKE Search (Pattern Matching)
â”‚   â”œâ”€â”€ Case-insensitive substring matching
â”‚   â”œâ”€â”€ Keyword concentration scoring
â”‚   â””â”€â”€ Percentage-based relevance
â”‚
â””â”€â”€ Fuzzy Search (String Similarity)
    â”œâ”€â”€ pg_trgm extension
    â”œâ”€â”€ word_similarity() function
    â””â”€â”€ Trigram matching for typos

Query Expansion Pipeline (v7.1 - NEW):
â”œâ”€â”€ Claude Haiku API
â”‚   â”œâ”€â”€ Generates 4-5 query variations
â”‚   â”œâ”€â”€ Sub-100ms expansion latency
â”‚   â”œâ”€â”€ Cost: ~$1.50-9/month
â”‚   â””â”€â”€ UI toggle: "Enhanced Search" mode
â”‚
â”œâ”€â”€ Parallel Execution
â”‚   â”œâ”€â”€ All variations searched simultaneously
â”‚   â”œâ”€â”€ Results merged before reranking
â”‚   â””â”€â”€ 15-30% better recall
â”‚
â””â”€â”€ Expansion Strategies
    â”œâ”€â”€ Synonym expansion for factual queries
    â”œâ”€â”€ Step variations for how-to queries
    â””â”€â”€ Concept expansion for analytical queries

Reranking Pipeline (v7.1 - OPTIMIZED):
â”œâ”€â”€ BGE-Reranker-v2 (Mac Studio)
â”‚   â”œâ”€â”€ Replaces Cohere (saves $30-50/month)
â”‚   â”œâ”€â”€ 299M parameters, ~1.5GB RAM
â”‚   â”œâ”€â”€ Accessed via Tailscale secure connection
â”‚   â”œâ”€â”€ 10-20ms latency per batch
â”‚   â””â”€â”€ 25-35% reranking improvement
â”‚
â”œâ”€â”€ Reciprocal Rank Fusion (RRF)
â”‚   â”œâ”€â”€ Combines rankings from all search methods
â”‚   â”œâ”€â”€ Configurable weights (sum to 1.0)
â”‚   â”œâ”€â”€ Flexible K parameter (default 60)
â”‚   â””â”€â”€ Production-ready algorithm
â”‚
â””â”€â”€ Score Combination
    â”œâ”€â”€ Weighted averaging
    â”œâ”€â”€ Rank-based fusion
    â””â”€â”€ Final relevance scoring

Knowledge Graph Integration (ESSENTIAL):
â”œâ”€â”€ LightRAG API
â”‚   â”œâ”€â”€ Entity extraction from documents
â”‚   â”œâ”€â”€ Relationship mapping
â”‚   â”œâ”€â”€ Graph traversal queries
â”‚   â”œâ”€â”€ Incremental updates
â”‚   â””â”€â”€ Combined with vector search
â”‚
â”œâ”€â”€ Use Cases
â”‚   â”œâ”€â”€ "Find all documents related to X"
â”‚   â”œâ”€â”€ "Show relationships between concepts"
â”‚   â”œâ”€â”€ "What entities connect these topics?"
â”‚   â””â”€â”€ "Trace information flow"
â”‚
â””â”€â”€ Integration
    â”œâ”€â”€ Parallel to vector search
    â”œâ”€â”€ Results merged with RRF
    â””â”€â”€ Unified query interface

Adaptive Chunking Strategy (v7.1 - NEW):
â”œâ”€â”€ Document-Type Detection
â”‚   â”œâ”€â”€ Claude Vision analyzes document structure
â”‚   â”œâ”€â”€ Auto-classifies: contract, technical, narrative, mixed
â”‚   â”œâ”€â”€ Preserves tables, images, code blocks
â”‚   â””â”€â”€ Multi-modal aware chunking
â”‚
â”œâ”€â”€ Optimized Chunk Sizes
â”‚   â”œâ”€â”€ Contracts: 300 tokens, 25% overlap (precision)
â”‚   â”œâ”€â”€ Policies: 400 tokens, 20% overlap (balanced)
â”‚   â”œâ”€â”€ Technical: 512 tokens, 20% overlap (context)
â”‚   â”œâ”€â”€ Transcripts: 300 tokens, speaker-aware
â”‚   â””â”€â”€ Default: 400 tokens, semantic boundaries
â”‚
â””â”€â”€ Performance Impact
    â”œâ”€â”€ 15-25% better semantic coherence
    â”œâ”€â”€ Preserves document structure
    â””â”€â”€ One-time preprocessing cost

Context Expansion (ESSENTIAL):
â”œâ”€â”€ Neighbor Chunk Retrieval
â”‚   â”œâ”€â”€ Â±1 position lookup
â”‚   â”œâ”€â”€ <100ms retrieval time
â”‚   â””â”€â”€ Expands boundary chunks
â”‚
â”œâ”€â”€ Section-Based Expansion
â”‚   â”œâ”€â”€ Retrieve full document sections
â”‚   â”œâ”€â”€ Hierarchical structure mapping
â”‚   â”œâ”€â”€ <500ms section retrieval
â”‚   â””â”€â”€ Maintains document context
â”‚
â””â”€â”€ Smart Chunk Merging
    â”œâ”€â”€ Merge tiny chunks (<100 tokens)
    â”œâ”€â”€ Preserve semantic boundaries
    â””â”€â”€ Configurable thresholds

Memory System (ESSENTIAL - v7.2 UPDATED Nov 2025):

IMPORTANT: Empire has THREE DISTINCT memory systems:
1. Developer Memory: Graphiti MCP with Neo4j (local development & Claude Code)
2. Production User Memory: Supabase graph-based (end-user workflows)
3. Personal Memory: Separated ChatGPT history (332 conversations imported)

â”œâ”€â”€ Developer Memory: Graphiti MCP with Neo4j (UPDATED Nov 2025)
â”‚   â”œâ”€â”€ Purpose: Development memory with Claude Code integration
â”‚   â”œâ”€â”€ Technology: Zep Graphiti + Neo4j Graph Database
â”‚   â”œâ”€â”€ Integration: Claude Desktop & Claude Code via MCP
â”‚   â”œâ”€â”€ Location: Mac Studio local environment (Docker)
â”‚   â”œâ”€â”€ Storage: Neo4j Community Edition (bolt://localhost:7687)
â”‚   â”œâ”€â”€ Memory Architecture:
â”‚   â”‚   â”œâ”€â”€ Project-Based Naming: project_{projectname}_{aspect}
â”‚   â”‚   â”œâ”€â”€ Empire Groups:
â”‚   â”‚   â”‚   â”œâ”€â”€ project_empire - Main project memories
â”‚   â”‚   â”‚   â”œâ”€â”€ project_empire_dev - Development notes
â”‚   â”‚   â”‚   â”œâ”€â”€ project_empire_docs - Documentation
â”‚   â”‚   â”‚   â”œâ”€â”€ project_empire_api - API specifications
â”‚   â”‚   â”‚   â””â”€â”€ project_empire_notes - Meeting notes
â”‚   â”‚   â”œâ”€â”€ Personal Groups:
â”‚   â”‚   â”‚   â””â”€â”€ personal - ChatGPT history (332 memories)
â”‚   â”‚   â””â”€â”€ Other Projects: project_healthtrack, project_crypto_dashboard
â”‚   â”œâ”€â”€ Context Management:
â”‚   â”‚   â”œâ”€â”€ Switch Command: ./mem empire
â”‚   â”‚   â”œâ”€â”€ Manager: memory_context_manager.py
â”‚   â”‚   â”œâ”€â”€ Config: memory_contexts.json
â”‚   â”‚   â””â”€â”€ Auto-updates GROUP_ID in .env
â”‚   â”œâ”€â”€ Import Tools:
â”‚   â”‚   â”œâ”€â”€ import_chatgpt_direct.py - Direct Neo4j import
â”‚   â”‚   â”œâ”€â”€ import_chatgpt_to_graphiti.py - Via Graphiti API
â”‚   â”‚   â””â”€â”€ Bulk import capability for project docs
â”‚   â”œâ”€â”€ Performance: <100ms retrieval with Neo4j indexes
â”‚   â”œâ”€â”€ Scalability: Unlimited projects without restructuring
â”‚   â””â”€â”€ Privacy: Complete separation between personal/work
â”‚
â”œâ”€â”€ Production User Memory: Supabase Graph-Based (v7.0)
â”‚   â”œâ”€â”€ Purpose: End-user memory in production n8n workflows
â”‚   â”œâ”€â”€ Architecture: Three-layer graph system
â”‚   â”‚   â”œâ”€â”€ User Memory Graph: facts, preferences, goals, context
â”‚   â”‚   â”œâ”€â”€ Document Knowledge Graph: LightRAG entities/relationships
â”‚   â”‚   â””â”€â”€ Hybrid Graph: user memories â†” document entities
â”‚   â”œâ”€â”€ Storage: Supabase PostgreSQL + pgvector
â”‚   â”œâ”€â”€ Tables: user_memory_nodes, user_memory_edges, user_document_connections
â”‚   â”œâ”€â”€ Embeddings: 768-dim nomic-embed-text vectors
â”‚   â””â”€â”€ Privacy: Row-level security, per-user isolation
â”‚
â”œâ”€â”€ Graph-Based Memory Features (Production)
â”‚   â”œâ”€â”€ Memory Extraction
â”‚   â”‚   â”œâ”€â”€ Automatic extraction via Claude API (1-2 seconds)
â”‚   â”‚   â”œâ”€â”€ Types: fact, preference, goal, context, skill, interest
â”‚   â”‚   â”œâ”€â”€ Confidence scoring (0.0-1.0)
â”‚   â”‚   â”œâ”€â”€ Importance scoring (0.0-1.0)
â”‚   â”‚   â””â”€â”€ Source tracking (explicit, inferred, conversation)
â”‚   â”œâ”€â”€ Graph Relationships
â”‚   â”‚   â”œâ”€â”€ Relationship types: causes, relates_to, contradicts, supports
â”‚   â”‚   â”œâ”€â”€ Edge strength (0.0-1.0)
â”‚   â”‚   â”œâ”€â”€ Multi-hop traversal (2 hops default, configurable 1-3)
â”‚   â”‚   â””â”€â”€ Cycle prevention in recursive queries
â”‚   â”œâ”€â”€ Memory Retrieval
â”‚   â”‚   â”œâ”€â”€ Vector similarity search for seed nodes (threshold 0.7)
â”‚   â”‚   â”œâ”€â”€ Graph traversal via recursive CTEs (<100ms)
â”‚   â”‚   â”œâ”€â”€ Personalized entity recommendations (<50ms)
â”‚   â”‚   â”œâ”€â”€ Top 10 relevant memories with relationship paths
â”‚   â”‚   â””â”€â”€ Total retrieval: <300ms including enrichment
â”‚   â””â”€â”€ Memory Lifecycle
â”‚       â”œâ”€â”€ Temporal tracking (first/last mentioned, mention count)
â”‚       â”œâ”€â”€ Confidence decay for stale memories (30-day threshold)
â”‚       â”œâ”€â”€ Contradiction detection (>0.85 similarity)
â”‚       â””â”€â”€ Optional expiration for time-sensitive facts
â”‚
â”œâ”€â”€ LightRAG Integration (Hybrid Graph)
â”‚   â”œâ”€â”€ User-document connections table
â”‚   â”œâ”€â”€ Link user skills/interests to document entities
â”‚   â”œâ”€â”€ Connection types: expert_in, interested_in, worked_on
â”‚   â”œâ”€â”€ Enables personalized document recommendations
â”‚   â””â”€â”€ "Show me documents related to my expertise" queries
â”‚
â””â”€â”€ Performance Characteristics
    â”œâ”€â”€ Memory extraction: <3 seconds (typical 3-5 memories)
    â”œâ”€â”€ Graph traversal: <100ms (2-hop, 10 seed nodes)
    â”œâ”€â”€ Context retrieval: <300ms (complete enrichment)
    â””â”€â”€ Storage: ~3.5KB per memory node (with embedding)

Multi-Modal Processing (NEW - v7.0):
â”œâ”€â”€ Image Processing
â”‚   â”œâ”€â”€ Claude Vision API integration
â”‚   â”œâ”€â”€ Formats: JPG, PNG, GIF, BMP, TIFF, WEBP
â”‚   â”œâ”€â”€ Text extraction (OCR)
â”‚   â”œâ”€â”€ Object detection and description
â”‚   â”œâ”€â”€ Caption generation
â”‚   â”œâ”€â”€ Descriptive embeddings via nomic-embed-text
â”‚   â””â”€â”€ Cross-modal search (text queries â†’ images)
â”‚
â”œâ”€â”€ Audio Processing
â”‚   â”œâ”€â”€ Soniox API for transcription
â”‚   â”œâ”€â”€ Formats: MP3, WAV, M4A, FLAC
â”‚   â”œâ”€â”€ Speaker diarization
â”‚   â”œâ”€â”€ Timestamp alignment
â”‚   â”œâ”€â”€ Cost: $0.005 per minute
â”‚   â””â”€â”€ Processed as enriched text documents
â”‚
â””â”€â”€ Video Processing (Future - v8.0)
    â”œâ”€â”€ Keyframe extraction (1 per 10 seconds)
    â”œâ”€â”€ Audio track transcription
    â””â”€â”€ Synchronized transcript with visual context

Structured Data Support (NEW - v7.0):
â”œâ”€â”€ CSV/Excel Processing
â”‚   â”œâ”€â”€ Formats: CSV, TSV, XLSX, XLS
â”‚   â”œâ”€â”€ Automatic schema inference
â”‚   â”œâ”€â”€ Column type detection
â”‚   â”œâ”€â”€ Relationship discovery
â”‚   â””â”€â”€ Foreign key pattern matching
â”‚
â”œâ”€â”€ Storage Architecture
â”‚   â”œâ”€â”€ tabular_document_rows table
â”‚   â”œâ”€â”€ JSONB format for flexibility
â”‚   â”œâ”€â”€ GIN indexing for fast queries
â”‚   â””â”€â”€ Linked to record_manager_v2
â”‚
â””â”€â”€ Query Interface
    â”œâ”€â”€ Natural language to SQL translation
    â”œâ”€â”€ Filter, aggregate, join operations
    â”œâ”€â”€ Claude API for query generation
    â””â”€â”€ Result set integration with RAG

Semantic Caching (v7.1 - OPTIMIZED):
â”œâ”€â”€ Redis-Based Cache
â”‚   â”œâ”€â”€ Upstash Redis ($10-15/month)
â”‚   â”œâ”€â”€ Query embedding storage
â”‚   â”œâ”€â”€ Result caching with TTL
â”‚   â””â”€â”€ Automatic invalidation
â”‚
â”œâ”€â”€ Tiered Similarity Thresholds
â”‚   â”œâ”€â”€ 0.98+: Direct cache hit (identical query)
â”‚   â”œâ”€â”€ 0.93-0.97: Return with "similar answer" note
â”‚   â”œâ”€â”€ 0.88-0.92: Show as suggestion
â”‚   â””â”€â”€ <0.88: Full pipeline execution
â”‚
â”œâ”€â”€ Adaptive Cache Strategy
â”‚   â”œâ”€â”€ Policy documents: 24 hour TTL
â”‚   â”œâ”€â”€ Product info: 1 hour TTL
â”‚   â”œâ”€â”€ Real-time data: 5 minute TTL
â”‚   â”œâ”€â”€ Document-based invalidation
â”‚   â””â”€â”€ UI "Refresh" button bypass
â”‚
â””â”€â”€ Performance Impact
    â”œâ”€â”€ Cache hit: <50ms total latency
    â”œâ”€â”€ 60-80% hit rate for common queries
    â”œâ”€â”€ 50x faster on cache hits
    â””â”€â”€ Significant cost reduction

Observability Stack (Task 44 - PRODUCTION DEPLOYED):
â”œâ”€â”€ Metrics Collection (Prometheus)
â”‚   â”œâ”€â”€ Prometheus for metrics scraping (port 9090)
â”‚   â”œâ”€â”€ Custom metrics: query latency, search quality, token usage
â”‚   â”œâ”€â”€ System metrics: CPU, memory, disk I/O (via Node Exporter port 9100)
â”‚   â”œâ”€â”€ Cost tracking: per-query and aggregate
â”‚   â”œâ”€â”€ Self-hosted via Docker Compose
â”‚   â””â”€â”€ 30-day retention with TSDB storage
â”‚
â”œâ”€â”€ Visualization (Grafana)
â”‚   â”œâ”€â”€ Grafana dashboards (port 3001)
â”‚   â”œâ”€â”€ Credentials: admin/empiregrafana123
â”‚   â”œâ”€â”€ Real-time query performance graphs
â”‚   â”œâ”€â”€ Search quality trend analysis
â”‚   â”œâ”€â”€ Cost analytics and budget tracking
â”‚   â”œâ”€â”€ Error rate monitoring by component
â”‚   â””â”€â”€ Custom Empire dashboard pre-configured
â”‚
â”œâ”€â”€ Distributed Tracing (Future - OpenTelemetry)
â”‚   â”œâ”€â”€ OpenTelemetry framework integration planned
â”‚   â”œâ”€â”€ Jaeger for trace storage
â”‚   â”œâ”€â”€ End-to-end request tracking
â”‚   â”œâ”€â”€ Component-level latency breakdown
â”‚   â””â”€â”€ Debugging production issues
â”‚
â”œâ”€â”€ Structured Logging
â”‚   â”œâ”€â”€ JSON format for all logs
â”‚   â”œâ”€â”€ Docker log aggregation
â”‚   â”œâ”€â”€ File-based logging (default)
â”‚   â”œâ”€â”€ 90-day retention
â”‚   â””â”€â”€ Full audit trail for compliance
â”‚
â””â”€â”€ Alerting & Notifications (Alertmanager - Task 44 âœ…)
    â”œâ”€â”€ Alertmanager for alert routing (port 9093)
    â”œâ”€â”€ Email notifications via Gmail SMTP
    â”‚   â”œâ”€â”€ Recipient: jbajaj08@gmail.com
    â”‚   â”œâ”€â”€ Gmail App Password authentication
    â”‚   â”œâ”€â”€ TLS required (smtp.gmail.com:587)
    â”‚   â””â”€â”€ HTML formatted emails with severity styling
    â”œâ”€â”€ 39 Alert Rules across 3 severity levels:
    â”‚   â”œâ”€â”€ Critical (ðŸš¨): 10s delay, 1hr repeat
    â”‚   â”‚   â”œâ”€â”€ Service downtime (API, Redis, Neo4j) - >5 min
    â”‚   â”‚   â”œâ”€â”€ Critical error rates (>5 errors/sec) - 2 min
    â”‚   â”‚   â”œâ”€â”€ Very slow processing (>60s P95)
    â”‚   â”‚   â”œâ”€â”€ Resource exhaustion (>95% CPU/Memory)
    â”‚   â”‚   â””â”€â”€ High queue backlog (>500 tasks)
    â”‚   â”œâ”€â”€ Warning (âš ï¸): 2m delay, 12hr repeat
    â”‚   â”‚   â”œâ”€â”€ High error rates (>1/sec)
    â”‚   â”‚   â”œâ”€â”€ Slow processing (>30s P95)
    â”‚   â”‚   â”œâ”€â”€ High resource usage (>80%)
    â”‚   â”‚   â”œâ”€â”€ Queue backlogs (>100 tasks)
    â”‚   â”‚   â””â”€â”€ Low cache hit rates (<40%)
    â”‚   â””â”€â”€ Info (â„¹ï¸): 5m delay, 24hr repeat
    â”‚       â”œâ”€â”€ System health summaries
    â”‚       â”œâ”€â”€ Usage statistics and trends
    â”‚       â””â”€â”€ Performance benchmarks
    â”œâ”€â”€ Alert Inhibition Rules:
    â”‚   â”œâ”€â”€ APIDown suppresses APISlowResponse
    â”‚   â”œâ”€â”€ CacheServiceDown suppresses LowCacheHitRate
    â”‚   â””â”€â”€ NoQueriesProcessed suppresses business metric alerts
    â”œâ”€â”€ Testing: ./test-alert.sh script for verification
    â”œâ”€â”€ Deployment Status: âœ… PRODUCTION - Email delivery confirmed
    â””â”€â”€ Configuration Files:
        â”œâ”€â”€ monitoring/alertmanager.yml - Email config
        â”œâ”€â”€ monitoring/alert_rules.yml - 39 alert definitions
        â”œâ”€â”€ monitoring/prometheus.yml - Scrape targets
        â””â”€â”€ docker-compose.monitoring.yml - Service orchestration

================================================================================
Data Architecture - Advanced Implementation
================================================================================

Supabase PostgreSQL + pgvector (UNIFIED DATABASE):

Key Extensions:
â”œâ”€â”€ pgvector - Vector similarity search
â”œâ”€â”€ pg_trgm - Fuzzy text matching
â””â”€â”€ PostgreSQL FTS - Full-text search

Database Schema:
â”œâ”€â”€ documents_v2
â”‚   â”œâ”€â”€ content (TEXT)
â”‚   â”œâ”€â”€ metadata (JSONB - unlimited rich metadata)
â”‚   â”œâ”€â”€ embedding (VECTOR(1536))
â”‚   â”œâ”€â”€ fts (TSVECTOR - full-text search)
â”‚   â””â”€â”€ hierarchical_index (JSONB - section mapping)
â”‚
â”œâ”€â”€ record_manager_v2
â”‚   â”œâ”€â”€ document tracking
â”‚   â”œâ”€â”€ hash-based deduplication
â”‚   â”œâ”€â”€ graph_id mapping (LightRAG)
â”‚   â””â”€â”€ version history
â”‚
â”œâ”€â”€ tabular_document_rows
â”‚   â”œâ”€â”€ Extracted table data
â”‚   â”œâ”€â”€ SQL queryable
â”‚   â””â”€â”€ Linked to source documents
â”‚
â””â”€â”€ metadata_fields
    â”œâ”€â”€ Controlled vocabularies
    â”œâ”€â”€ Dynamic field definitions
    â””â”€â”€ Query optimization

================================================================================
NEO4J GRAPH DATABASE SCHEMA (NEW v7.2)
================================================================================

Node Types:
â”œâ”€â”€ (:Document)
â”‚   â”œâ”€â”€ id: UUID (unique)
â”‚   â”œâ”€â”€ title: String
â”‚   â”œâ”€â”€ content: Text
â”‚   â”œâ”€â”€ embedding: List[Float] (1024-dim BGE-M3)
â”‚   â”œâ”€â”€ doc_type: String (contract|policy|technical|transcript)
â”‚   â”œâ”€â”€ chunk_size: Integer (adaptive 300-512)
â”‚   â”œâ”€â”€ created_at: DateTime
â”‚   â”œâ”€â”€ hash: String (SHA-256)
â”‚   â”œâ”€â”€ confidence: Float (0.0-1.0)
â”‚   â””â”€â”€ source_url: String
â”‚
â”œâ”€â”€ (:Entity)
â”‚   â”œâ”€â”€ id: UUID (unique)
â”‚   â”œâ”€â”€ name: String (indexed)
â”‚   â”œâ”€â”€ type: String (person|org|concept|technology)
â”‚   â”œâ”€â”€ description: Text
â”‚   â”œâ”€â”€ embedding: List[Float] (1024-dim)
â”‚   â”œâ”€â”€ importance: Float (0.0-1.0)
â”‚   â””â”€â”€ first_seen: DateTime
â”‚
â”œâ”€â”€ (:Memory)
â”‚   â”œâ”€â”€ id: UUID
â”‚   â”œâ”€â”€ content: String
â”‚   â”œâ”€â”€ type: String (fact|preference|goal|context)
â”‚   â”œâ”€â”€ user_id: String
â”‚   â”œâ”€â”€ confidence: Float
â”‚   â””â”€â”€ created_at: DateTime
â”‚
â”œâ”€â”€ (:Query)
â”‚   â”œâ”€â”€ id: UUID
â”‚   â”œâ”€â”€ text: String
â”‚   â”œâ”€â”€ expanded_queries: List[String] (Claude Haiku)
â”‚   â”œâ”€â”€ timestamp: DateTime
â”‚   â”œâ”€â”€ cache_hit: Boolean
â”‚   â””â”€â”€ response_time_ms: Integer
â”‚
â””â”€â”€ (:User)
    â”œâ”€â”€ id: UUID
    â”œâ”€â”€ email: String
    â””â”€â”€ created_at: DateTime

Relationship Types:
â”œâ”€â”€ (:Document)-[:CONTAINS {position: Int, confidence: Float}]->(:Entity)
â”œâ”€â”€ (:Entity)-[:RELATES_TO {strength: Float, type: String}]->(:Entity)
â”œâ”€â”€ (:Document)-[:CITES {page: Int, section: String}]->(:Document)
â”œâ”€â”€ (:Query)-[:RETRIEVED {relevance: Float, rank: Int}]->(:Document)
â”œâ”€â”€ (:Memory)-[:ABOUT]->(:Entity)
â”œâ”€â”€ (:User)-[:INTERESTED_IN {weight: Float}]->(:Entity)
â”œâ”€â”€ (:Document)-[:SIMILAR_TO {score: Float}]->(:Document)
â”œâ”€â”€ (:Query)-[:EXPANDED_TO]->(:Query)
â””â”€â”€ (:User)-[:ASKED]->(:Query)

Indexes:
â”œâ”€â”€ CREATE INDEX doc_title FOR (d:Document) ON (d.title)
â”œâ”€â”€ CREATE INDEX entity_name FOR (e:Entity) ON (e.name)
â”œâ”€â”€ CREATE INDEX entity_type FOR (e:Entity) ON (e.type)
â”œâ”€â”€ CREATE VECTOR INDEX doc_embedding FOR (d:Document) ON (d.embedding)
â””â”€â”€ CREATE FULLTEXT INDEX doc_content FOR (d:Document) ON (d.content)

Graph Algorithms (via Graph Data Science):
â”œâ”€â”€ PageRank: Identify important documents/entities
â”œâ”€â”€ Community Detection: Find related concept clusters
â”œâ”€â”€ Path Finding: Shortest path between concepts
â”œâ”€â”€ Similarity: Find similar documents via embeddings
â””â”€â”€ Centrality: Identify key knowledge hubs

Advanced Database Functions:

1. dynamic_hybrid_search_db (438 lines)
   - Combines all 4 search methods
   - Dynamic filter handling
   - RRF score combination
   - Metadata filtering with $or/$and
   - Type detection (numeric, timestamp, text)

2. get_chunks_by_ranges() (NEW - v7.0)
   - Batch context expansion for multiple ranges
   - Input: JSON array [{doc_id, start, end}]
   - Returns: chunks with hierarchical_context and graph_entities
   - Builds parent/child relationships automatically
   - Links to knowledge graph entities from LightRAG
   - Performance: <300ms for â‰¤10 ranges

3. hierarchical_structure_extraction
   - Document outline extraction
   - Chunk-to-section mapping
   - H1-H6 heading relationships

Supabase Edge Functions (HTTP API - NEW v7.0):

1. hybrid-search (TypeScript/Deno)
   - HTTP wrapper for dynamic_hybrid_search_db
   - Endpoint: /functions/v1/hybrid-search
   - CORS support for web clients
   - JWT authentication via Supabase Auth
   - Returns: JSON with success/error/results

2. context-expansion (TypeScript/Deno)
   - HTTP wrapper for get_chunks_by_ranges()
   - Endpoint: /functions/v1/context-expansion
   - Batch processing of chunk ranges
   - Returns: chunks with hierarchical context

3. graph-query (TypeScript/Deno)
   - Knowledge graph entity and relationship queries
   - Endpoint: /functions/v1/graph-query
   - Queries local knowledge_entities/relationships tables
   - Returns: entity + related relationships

Authentication Levels:
â”œâ”€â”€ Anon Key: Rate-limited, RLS enforced, client-safe
â”œâ”€â”€ Service Role Key: Full access, Edge Functions only (server-side)
â””â”€â”€ User JWT: Per-user auth, automatic RLS filtering

Deployment:
- supabase functions deploy [name]
- supabase secrets set for API keys
- Local testing: supabase functions serve

Query Optimization Features (NEW - v7.0):

1. Dynamic Weight Adjustment
   - Analyzes query characteristics before search
   - Automatically tunes search method weights
   - Query type detection:
     * Exact match (contains "exactly", quotes) â†’ boost ILIKE (40%)
     * Short queries (<3 words) â†’ boost fuzzy (30%)
     * Semantic (contains "similar", "like") â†’ boost dense vector (60%)
     * Long queries (>8 words) â†’ boost sparse BM25 (40%)
     * Contains numbers/years â†’ boost pattern matching (35%)
   - Performance: <10ms overhead, 10-15% better results
   - Tracked in query_performance_log with query_type

2. Natural Language to SQL Translation
   - LLM-powered SQL generation for tabular data
   - Enables natural language queries on CSV/Excel uploads
   - Example queries:
     * "Show customers from CA with revenue > $100k"
     * "What's the average revenue by state?"
     * "List top 10 products by sales"
   - Implementation:
     * Detects structured query keywords (show, list, filter, count, avg)
     * Retrieves table schemas from record_manager_v2
     * Claude generates PostgreSQL with JSONB operators
     * Executes read-only SELECT queries
   - Security: 100-row limit, 30s timeout, no DROP/DELETE/UPDATE
   - Performance: <3 seconds total (schema + LLM + execution)
   - Fallback: Reverts to semantic search on SQL errors
   - Combines: Can merge structured results with semantic search

Performance:
- Dense search: <50ms (HNSW index)
- Sparse search: <100ms (GIN index)
- ILIKE search: <200ms (pattern matching)
- Fuzzy search: <150ms (trigram)
- Reranking: <1 second (Cohere API)
- Context expansion: <500ms (PostgreSQL)

================================================================================
Document Processing Pipeline - Complete Flow
================================================================================

Input Sources:
â”œâ”€â”€ Web UI Upload (Gradio/Streamlit) â†’ FastAPI â†’ Immediate processing
â”œâ”€â”€ Mountain Duck Upload â†’ B2 pending/courses/ â†’ B2 Monitor (30s poll) â†’ Processing
â”œâ”€â”€ Backblaze B2 monitoring (30-second polling for Mountain Duck auto-detection)
â”œâ”€â”€ YouTube URLs
â”œâ”€â”€ Web Scraping (Firecrawl)
â””â”€â”€ Direct file uploads (n8n webhook)

Dual Upload Architecture (NEW v7.2):

Method 1: Mountain Duck (Direct B2 Upload)
â”œâ”€â”€ User drags file to /Volumes/Backblaze B2/pending/courses/
â”œâ”€â”€ B2 Folder Monitor polls every 30 seconds (FastAPI background task)
â”œâ”€â”€ New file detected â†’ triggers processing workflow automatically
â”œâ”€â”€ Store metadata in Supabase with upload_source='mountain_duck'
â””â”€â”€ AI auto-classification and intelligent filename generation

Method 2: Web UI (Gradio/Streamlit)
â”œâ”€â”€ User uploads via browser (drag-drop or file picker)
â”œâ”€â”€ FastAPI receives file via POST /api/upload/single or /batch
â”œâ”€â”€ Upload to B2 pending/courses/
â”œâ”€â”€ Store metadata in Supabase with upload_source='web_ui'
â”œâ”€â”€ Immediate processing trigger (no 30-second polling delay)
â””â”€â”€ AI auto-classification and intelligent filename generation

Both Methods Converge at Processing Pipeline:
â”œâ”€â”€ B2 pending/courses/ â†’ AI Classification
â”œâ”€â”€ Department classification (1 of 10 departments)
â”œâ”€â”€ Course structure extraction (instructor, modules, lessons)
â”œâ”€â”€ Intelligent filename generation (proper sorting with M01, L01 format)
â”œâ”€â”€ Document processing (content extraction, embeddings, storage)
â”œâ”€â”€ CrewAI analysis (PDF summaries + suggestions)
â””â”€â”€ Move to processed/courses/{department}/

B2 Folder Structure (10 Departments):
```
JB-Course-KB/
â”œâ”€â”€ pending/courses/              # Drop ANY course here (via UI or Mountain Duck)
â”œâ”€â”€ processing/                   # Temporary processing
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ courses/                  # 10 department folders (actual course files)
â”‚   â”œâ”€â”€ crewai-summaries/        # 10 department folders (PDF summaries with images)
â”‚   â””â”€â”€ crewai-suggestions/      # 10 department folders (YAML skills + MD commands)
â””â”€â”€ failed/                       # Failed processing attempts
```

10 Business Departments:
1. it-engineering - Technology, software, DevOps, data, security, cloud
2. sales-marketing - Sales, marketing, product strategy, R&D, customer success
3. customer-support - Technical support, help desk, SLA management
4. operations-hr-supply - HR, operations, supply chain, procurement, legal & compliance
5. finance-accounting - FP&A, accounting, tax, audits, financial risk
6. project-management - Agile, Scrum, PMP, project tools, stakeholder management
7. real-estate - Property management, CRE, investment, development, leasing
8. private-equity-ma - M&A, due diligence, PE, valuation, exit strategies
9. consulting - Management consulting, strategy frameworks, client engagement
10. personal-continuing-ed - Psychology, NLP, life coaching, mindfulness, wellness

Intelligent File Naming (AI-Powered):
- Format: {Instructor/Company}-{Course}-M{##}-{Module}-L{##}-{Lesson}.ext
- Examples:
  * Grant_Cardone-10X_Sales_System-M01-Prospecting-L01-Cold_Calling.pdf
  * McKinsey-Strategy_Frameworks-M01-BCG_Matrix.pdf
  * VirtualCoach-NLP_Practitioner-M05-Anchoring_Techniques.pdf
- Proper alphabetical sorting (M01, M02... L01, L02...)
- AI extracts structure using Claude Haiku

CrewAI Outputs:
1. crewai-summaries/ - PDF with embedded images, frameworks, module tables
2. crewai-suggestions/ - YAML Claude skills + Markdown slash commands

Mountain Duck Configuration:
- Protocol: Backblaze B2
- Credentials: Stored in .env file (NOT in git)
- Bucket: JB-Course-KB
- Mount: /Volumes/Backblaze B2/

Processing Services:
â”œâ”€â”€ MarkItDown MCP â†’ 40+ format conversion to Markdown
â”œâ”€â”€ LlamaIndex (Render: srv-d2nl1lre5dus73atm9u0) â†’ Document processing & indexing ($7-21/month)
â”œâ”€â”€ CrewAI (Render: srv-d2n0hh3uibrs73buafo0) â†’ Multi-agent orchestration ($7-21/month)
â”œâ”€â”€ LangExtract â†’ Gemini-powered extraction for precise grounding ($10-20/month)
â”œâ”€â”€ Mistral OCR â†’ Complex PDFs only ($20/month)
â”œâ”€â”€ Soniox â†’ Audio/video transcription ($10-20/month)
â””â”€â”€ Claude Sonnet 4.5 â†’ ALL intelligent processing

Claude Sonnet 4.5 Handles:
âœ… Data extraction from documents
âœ… Entity recognition and tagging
âœ… Document categorization
âœ… Summary generation
âœ… Quality validation
âœ… Structured JSON output
âœ… Context for embedding generation

Advanced Processing Steps:

1. Document Upload
   â†“
2. MarkItDown MCP (extract text)
   â†“
3. Hash Check (skip if unchanged)
   â†“
4. Claude Sonnet 4.5 API
   - Extract structured data
   - Identify entities
   - Generate metadata
   - Create summaries
   â†“
5. Semantic Chunking
   - Context-aware segmentation
   - Configurable size/overlap
   - Preserve boundaries
   â†“
6. LightRAG Entity Extraction
   - Extract entities and relationships
   - Build knowledge graph
   - Store graph mappings
   â†“
7. Hierarchical Structure Extraction
   - Document outline (H1-H6)
   - Section ranges
   - Chunk-to-section mapping
   â†“
8. Embedding Generation
   - Generate vectors for chunks
   - Include contextual descriptions
   - Batch processing
   â†“
9. Store in Supabase
   - Content + vectors
   - Metadata (rich JSONB)
   - FTS index
   - Graph IDs
   - Hierarchical structure
   â†“
10. CrewAI Content Analysis
    - Extract insights
    - Identify frameworks
    - Map to departments
    - Generate documentation
    â†“
11. Upload original to B2
    â†“
12. Done! âœ…

================================================================================
Query Workflow - Advanced RAG
================================================================================

User Question â†’ Chat UI
  â†“
mem-agent retrieves context (<100ms local)
  â†“
Query Enhancement
  - Identify query type (keyword vs semantic)
  - Extract key terms
  - Determine search strategy
  â†“
Dynamic Hybrid Search (Supabase)
  â”œâ”€â”€ Dense Search (vector similarity)
  â”œâ”€â”€ Sparse Search (BM25/FTS)
  â”œâ”€â”€ ILIKE Search (pattern matching)
  â””â”€â”€ Fuzzy Search (typo-tolerant)
  â†“
Reciprocal Rank Fusion
  - Combine results from all 4 methods
  - Apply configurable weights
  - Generate unified ranking
  â†“
LightRAG Graph Query (if relevant)
  - Find related entities
  - Traverse relationships
  - Retrieve connected nodes
  â†“
Merge Vector + Graph Results
  â†“
Cohere Rerank v3.5
  - Re-score all results
  - Optimize for relevance
  - Return top-N (default 10)
  â†“
Context Expansion
  - Retrieve neighbor chunks
  - Expand to full sections
  - Include parent context
  â†“
Claude Sonnet 4.5 Synthesis
  - Read expanded context
  - Generate comprehensive answer
  - Include citations
  â†“
Response in 1-3 seconds total âœ…

================================================================================
Storage Architecture
================================================================================

Storage Type          Location       Purpose                    Backup
================================================================================
Memory Store          Mac Studio     mem-agent MCP access       B2 (encrypted)
Vector Embeddings     Supabase       pgvector semantic search   Built-in + B2
Sparse Index          Supabase       Full-text search (GIN)     Built-in + B2
Structured Data       Supabase       PostgreSQL queries         Built-in + B2
Graph Data           LightRAG API    Entity relationships       API managed
Raw Files            Backblaze B2    Primary storage            Cross-region
Cache Layer          Mac Studio      Fast dev access (88GB)     Temporary
Configurations       GitHub          System settings            Private repo

================================================================================
Privacy & Security Architecture
================================================================================

Mac Studio Security:
- FileVault encryption (always on)
- mem-agent data encrypted locally
- Development environment isolation
- API key vault for cloud services
- Tailscale VPN for remote access
- No production data on local machine

Cloud Security:
- TLS encryption in transit
- AES-256 encryption at rest
- Claude API: SOC 2 compliant
- Supabase: Private PostgreSQL + pgvector
- Backblaze B2: Client-side encryption
- Cohere: Enterprise security standards
- LightRAG: Secure API access

File Upload Security (v7.3):
- Layer 1: Basic validation (extension whitelist, size limits)
- Layer 2: Advanced validation (MIME type detection, file header verification)
- Layer 3: Malware scanning (VirusTotal API with hash-first approach)
  â€¢ Smart hash-first strategy: Hash check (FREE) â†’ Upload only if unknown
  â€¢ 70+ antivirus engines for comprehensive detection
  â€¢ Free tier: 500 uploads/day â†’ ~10,000+ effective scans/day
  â€¢ Graceful degradation if API unavailable
  â€¢ API key: <from .env VIRUSTOTAL_API_KEY>

================================================================================
AI Model Distribution
================================================================================

PRIMARY: Claude Sonnet 4.5 API
	â€¢	Document processing and extraction (97-99% accuracy)
	â€¢	Entity recognition and tagging
	â€¢	Summary generation
	â€¢	Quality validation
	â€¢	Batch API: 90% cost reduction
	â€¢	Prompt caching: 50% additional savings
	â€¢	Cost: $30-50/month for 200 docs/day

ESSENTIAL: Cohere Rerank v3.5
	â€¢	Search result optimization
	â€¢	Multi-lingual support
	â€¢	Dramatically improves relevance
	â€¢	$1 per 1000 rerank operations
	â€¢	Cost: ~$20/month for heavy use

ESSENTIAL: LightRAG API
	â€¢	Knowledge graph construction
	â€¢	Entity extraction
	â€¢	Relationship mapping
	â€¢	Graph traversal queries
	â€¢	Cost: ~$15/month

ESSENTIAL: CrewAI Content Analyzer (Render)
	â€¢	Service ID: srv-d2n0hh3uibrs73buafo0
	â€¢	URL: https://jb-crewai.onrender.com
	â€¢	Workspace: tea-d1vtdtre5dus73a4rb4g
	â€¢	Multi-agent AI orchestration for complex workflows
	â€¢	Analyzes ALL ingested content
	â€¢	Generates course documentation
	â€¢	Extracts frameworks and workflows
	â€¢	Maps content to departments
	â€¢	Creates implementation guides
	â€¢	Cost: ~$7-21/month (Starter to Standard tier)

BACKUP: Other APIs as needed
	â€¢	Mistral OCR for complex PDFs
	â€¢	Soniox for transcription
	â€¢	Minimal usage, task-specific

PRECISION EXTRACTION: LlamaIndex + LangExtract
	â€¢	LlamaIndex (Render): Document processing, indexing & retrieval
	  - Service ID: srv-d2nl1lre5dus73atm9u0
	  - URL: https://jb-llamaindex.onrender.com
	  - Workspace: tea-d1vtdtre5dus73a4rb4g
	  - Document ingestion and parsing
	  - Vector index creation and management
	  - Query interface and retrieval
	  - Integration with pgvector and Neo4j
	  - Coordinates with CrewAI for multi-agent processing
	  - Cost: ~$7-21/month (Starter to Standard tier)
	â€¢	LangExtract: Gemini-powered extraction ($10-20/month)
	  - Precise information extraction with schemas
	  - Entity and relationship extraction
	  - Cross-validation with LlamaIndex for grounding
	  - Structured field extraction (dates, IDs, amounts)
	  - >95% extraction accuracy with confidence scores

LOCAL: Development Only
	â€¢	Claude Desktop for development
	â€¢	Testing and experimentation

================================================================================
Complete n8n Workflow - All Features Integrated
================================================================================

Milestone 2: Universal Document Processing

Node 1: Document Upload/Trigger
Node 2: Hash Check (Supabase)
Node 3: MarkItDown Conversion
Node 4: Claude Extraction
Node 5: Semantic Chunking

Milestone 3: Advanced RAG Features

Node 6: LightRAG Entity Extraction
  - Extract entities from document
  - Build knowledge graph
  - Store entity-relationship mappings
  - Link to document chunks

Node 7: Hierarchical Structure Extraction
  - Parse document outline (H1-H6)
  - Map chunks to sections
  - Store hierarchical index
  - Enable section-based retrieval

Node 8: Embedding Generation
  - Generate vectors for all chunks
  - Include contextual descriptions
  - Batch processing for efficiency
  - Store in Supabase pgvector

Node 9: Supabase Storage
  - Store content + vectors
  - Store metadata (JSONB)
  - Create FTS index
  - Store hierarchical mappings
  - Link to LightRAG graph IDs

Node 10: CrewAI Analysis
Node 11: B2 Upload
Node 12: Done

Milestone 4: Query Processing with Advanced RAG

Node 1: Query Input (Chat UI)
Node 2: mem-agent Context Retrieval (<100ms)

Node 3: Query Analysis
  - Determine query type
  - Extract key terms
  - Select search strategy weights

Node 4: Dynamic Hybrid Search (Supabase)
  - Execute 4-method search:
    * Dense (vector)
    * Sparse (BM25)
    * ILIKE (pattern)
    * Fuzzy (trigram)
  - Apply RRF to combine results
  - Return top-30 candidates

Node 5: LightRAG Graph Query
  - Search knowledge graph
  - Find related entities
  - Traverse relationships
  - Return connected nodes

Node 6: Merge Results
  - Combine vector + graph results
  - Deduplicate by document ID
  - Preserve source attribution

Node 7: Cohere Reranking
  - Send all results to Cohere
  - Apply relevance scoring
  - Return top-10 results

Node 8: Context Expansion
  - Retrieve neighbor chunks (Â±1)
  - Expand to full sections
  - Include parent context
  - Smart merging

Node 9: Claude Synthesis
  - Read expanded context
  - Generate answer
  - Include citations
  - Format response

Node 10: Response to User

Milestone 8: Document Lifecycle Management (NEW - v7.0)

Node 1: Delete Document Webhook (/document/:document_id)
  - Accept DELETE requests with document_id
  - Extract deletion metadata (user, reason)
  - Validate document exists

Node 2-8: Cascade Deletion Sequence
  - Delete from documents_v2 (vectors)
  - Delete from tabular_document_rows (structured data)
  - Delete from knowledge_entities (graph nodes)
  - Delete from record_manager_v2 (tracking)
  - Delete from documents (main record)
  - Delete from Backblaze B2 (source file)
  - Delete from LightRAG API (knowledge graph)

Node 9: Audit Logging
  - Record deletion event
  - Preserve metadata for compliance
  - Track deletion reason and user

Node 10: Soft Delete Alternative
  - Mark as deleted (status = 'deleted')
  - Preserve data for retention period
  - Auto-cleanup after 90 days

Milestone 9: Batch Operations (NEW - v7.0)

Node 1: Scheduled Trigger (2 AM Daily)
  - Cron expression: 0 2 * * *
  - Off-peak processing hours

Node 2: Get Pending Documents
  - Query processing_status = 'pending'
  - Include failed docs with retry_count < 3
  - Limit 100 documents per batch

Node 3: Split Into Batches
  - Batch size: 10 concurrent documents
  - Parallel execution with resource limits
  - Loop until all processed

Node 4: Mark as Processing
  - Update status to 'processing'
  - Increment retry_count
  - Record processing_started_at

Node 5: Execute Document Processing
  - Call main processing workflow
  - Pass document metadata
  - Handle success/failure

Node 6-7: Status Updates
  - Mark as 'complete' on success
  - Mark as 'error' on failure
  - Track processing duration

Node 8: Aggregate Batch Results
  - Count successful/failed
  - Calculate processing time
  - Generate batch summary

Node 9: Log Batch Summary
  - Store in batch_processing_log table
  - Track metrics over time
  - Enable performance analysis

================================================================================
Performance & Capacity
================================================================================

API Performance:
	â€¢	Claude Sonnet 4.5: 1-3 second responses
	â€¢	Document capacity: 200-500 per day
	â€¢	Batch processing: Overnight for large volumes
	â€¢	Accuracy: 97-99% for business documents
	â€¢	Reliability: 99.9% uptime (Claude API)

Supabase pgvector Performance:
	â€¢	28x lower latency vs traditional vector DBs
	â€¢	16x higher throughput
	â€¢	HNSW indexing for fast similarity search
	â€¢	Dense search: <50ms
	â€¢	Sparse search: <100ms
	â€¢	ILIKE search: <200ms
	â€¢	Fuzzy search: <150ms
	â€¢	Combined hybrid: <300ms

Cohere Reranking:
	â€¢	Reranking latency: <1 second
	â€¢	Batch size: up to 1000 documents
	â€¢	Dramatically improves precision
	â€¢	Essential for production quality

LightRAG Performance:
	â€¢	Entity extraction: 1-2 seconds
	â€¢	Graph query: <500ms
	â€¢	Incremental updates: ~50% faster
	â€¢	Essential for knowledge discovery

Mac Studio Usage:
	â€¢	mem-agent: 8GB always running
	â€¢	Development: VS Code, Docker, testing
	â€¢	Cache: 88GB available for hot data
	â€¢	Not for production LLM inference

================================================================================
Cost Structure - CORRECTED
================================================================================

One-Time Costs (Already Delivered)
	â€¢	Mac Studio M3 Ultra (96GB): $3,999
	â€¢	UPS Battery Backup: $150-200
	â€¢	Ethernet/accessories: $50
	â€¢	Total: ~$4,200

Monthly Recurring (v7.2 DUAL-INTERFACE with ALL FEATURES)

Core Infrastructure ($165-220/month):
	â€¢	Claude Sonnet 4.5 API: $50-80 (with batch + caching + vision)
	â€¢	Claude Haiku API: $1.50-9 (query expansion)
	â€¢	Render (n8n): $30 (workflow orchestration)
	â€¢	CrewAI: $20 (content analysis agent)
	â€¢	Chat UI (Gradio/Streamlit): $15-20 (end-user interface)
	â€¢	Supabase: $25 (PostgreSQL + pgvector + FTS)
	â€¢	Neo4j Community: $0 (graph database on Mac Studio)
	â€¢	Backblaze B2: $15-25 (file storage)

Advanced Features ($85-130/month):
	â€¢	LightRAG API: $30-50 (knowledge graph)
	â€¢	BGE-Reranker-v2: $0 (Mac Studio, was $30-50 Cohere)
	â€¢	Redis Cache (Upstash): $10-15 (tiered semantic caching)
	â€¢	LlamaIndex (Render): $15-20 (indexing framework)
	â€¢	LlamaCloud Free Tier: $0 (LlamaParse - 10K pages/month OCR)
	â€¢	LangExtract: $10-20 (Gemini-powered extraction)
	â€¢	Soniox: $10-20 (audio transcription)
	â€¢	Monitoring Stack: $20-30 (Prometheus/Grafana)

Total v7.2 Production: $350-500/month (includes BOTH interfaces)

Cost Breakdown by Usage (under 10K pages/month, 1000 queries/day):
	â€¢	Claude Sonnet API: ~$60-80/month (synthesis)
	â€¢	Claude Haiku API: ~$1.50-9/month (query expansion)
	â€¢	BGE-Reranker-v2: $0 (Mac Studio, was $30-50 Cohere)
	â€¢	Core Infrastructure: ~$85-110/month
	â€¢	Document Processing: ~$25-40/month
	  - LlamaIndex (Render): $15-20 (indexing framework)
	  - LlamaCloud Free: $0 (10K pages OCR)
	  - LangExtract: $10-20 (structured extraction)
	â€¢	Monitoring & Observability: ~$20-30/month
	â€¢	Audio Processing: ~$10-20/month (Soniox, usage-based)
	â€¢	Semantic Caching: ~$10-15/month (Upstash Redis)
	â€¢	Total: $335-480/month (saved $40-70/month from v7.0)

Value Proposition (v7.1 - STATE-OF-THE-ART):
	â€¢	40-60% better retrieval quality (BGE-M3 + expansion + reranking)
	â€¢	BGE-M3 with built-in sparse vectors (better than BM25)
	â€¢	Query expansion: 15-30% better recall
	â€¢	<100ms query latency with tiered caching
	â€¢	Adaptive chunking: 15-25% better precision
	â€¢	BGE-Reranker-v2: Same quality, 10x faster than Cohere
	â€¢	Knowledge graph for entity relationships
	â€¢	Multi-modal support (text, images, audio, structured data)
	â€¢	Persistent memory via mem-agent MCP
	â€¢	Full observability stack (metrics, tracing, logging, alerts)
	â€¢	Production-ready with 99.9% uptime SLA
	â€¢	Scalable to 1000+ docs/day, 5000+ queries/day
	â€¢	Lower costs with better performance

Cost Optimization Notes:
	â€¢	Batch API: 90% savings on document processing
	â€¢	Prompt caching: 50% additional savings on repeated content
	â€¢	Semantic cache: 60-80% hit rate reduces API calls
	â€¢	Combined savings: 70-85% vs. non-optimized approach
	â€¢	Actual cost: ~$0.35-0.50 per document processed
	â€¢	Query cost: ~$0.01-0.03 per query (with caching)

================================================================================
Implementation Status - CORRECTED
================================================================================

âœ… COMPLETED:
	â€¢	n8n deployed on Render
	â€¢	CrewAI deployed (ESSENTIAL)
	â€¢	Supabase database configured with pgvector
	â€¢	Backblaze B2 integrated
	â€¢	MarkItDown MCP working
	â€¢	YouTube processing active
	â€¢	Article conversion working
	â€¢	Mac Studio delivered and setup
	â€¢	mem-agent MCP configured

ðŸ”„ IN PROGRESS:
	â€¢	Claude Sonnet 4.5 API integration in n8n
	â€¢	Batch processing workflow
	â€¢	Prompt caching optimization

âŒ MISSING (URGENT) - ADVANCED RAG:
	â€¢	Cohere Reranking integration (ESSENTIAL)
	â€¢	LightRAG API integration (ESSENTIAL)
	â€¢	Hybrid search implementation in Supabase
	â€¢	Context expansion functions
	â€¢	Hierarchical structure extraction
	â€¢	Chat UI for knowledge base queries
	â€¢	RRF score combination logic

â³ PLANNED:
	â€¢	Deploy Chat UI (Gradio) - 1-2 days
	â€¢	Implement dynamic_hybrid_search_db function
	â€¢	Integrate Cohere Rerank v3.5
	â€¢	Connect LightRAG API
	â€¢	Build context expansion functions
	â€¢	Complete milestone workflows 4-8
	â€¢	Automated quality checks
	â€¢	Performance optimization

================================================================================
Key Architecture Principles (v6.0 CORRECTED)
================================================================================
	1	Best of Both Worlds - Simple AI (Claude API) + Sophisticated RAG
	2	No Compromises - Keep ALL advanced search/reranking features
	3	Unified Database - Supabase handles data + vectors + FTS
	4	Knowledge Graphs - LightRAG ESSENTIAL for discovery
	5	Search Quality - Hybrid search + Cohere reranking NOT optional
	6	Context Expansion - Essential for quality retrieval
	7	Cost Effective - $167-240/month for COMPLETE system
	8	Maintainable - API for processing, advanced DB for retrieval
	9	Scalability - All components scale independently
	10	Quality First - Never sacrifice search quality for simplicity

================================================================================
Why Advanced RAG Features Are ESSENTIAL
================================================================================

Without Hybrid Search:
âŒ Miss 40% of relevant results (keyword-only queries fail)
âŒ Typos cause complete search failure
âŒ No pattern matching for codes/IDs
âŒ Poor recall on technical terminology

With Hybrid Search:
âœ… 30-50% better search quality
âœ… Handles semantic AND keyword queries
âœ… Typo-tolerant with fuzzy matching
âœ… Pattern matching for structured data
âœ… Multiple strategies cover all query types

Without Cohere Reranking:
âŒ Irrelevant results in top 10
âŒ Good results buried on page 3
âŒ Poor user experience
âŒ Wasted Claude API tokens on bad context

With Cohere Reranking:
âœ… Best results always in top 10
âœ… 40% improvement in precision
âœ… Better user experience
âœ… Efficient use of Claude context

Without LightRAG:
âŒ No relationship discovery
âŒ Miss connected information
âŒ Can't traverse knowledge
âŒ Limited to direct matches

With LightRAG:
âœ… Discover related concepts
âœ… Trace information flows
âœ… Find indirect connections
âœ… Comprehensive knowledge exploration

Without Context Expansion:
âŒ Truncated information
âŒ Missing important context
âŒ Fragmented responses
âŒ Poor answer quality

With Context Expansion:
âœ… Full context maintained
âœ… Coherent information
âœ… Complete answers
âœ… Better synthesis

================================================================================
Critical Missing Component: Chat UI
================================================================================

WITHOUT CHAT UI:
âŒ Cannot query ingested documents
âŒ Cannot test RAG pipeline
âŒ Advanced RAG value not demonstrable
âŒ Users cannot interact with knowledge base

WITH CHAT UI (Deploy This Week):
âœ… Query interface for all documents
âœ… Hybrid search + reranking visible
âœ… Knowledge graph exploration
âœ… Source citations
âœ… Cost tracking per query
âœ… Complete system functionality

Deployment: Gradio on Render
Cost: $7-15/month
Time: 1-2 days
Priority: URGENT

================================================================================
n8n Workflow Architecture Patterns (NEW - v7.0)
================================================================================

Sub-Workflow Organization:
â”œâ”€â”€ Main Workflows (Entry Points)
â”‚   â”œâ”€â”€ Document Ingestion Workflow
â”‚   â”œâ”€â”€ RAG Query Pipeline
â”‚   â”œâ”€â”€ Chat Interface Workflow
â”‚   â””â”€â”€ Admin/Management Workflows
â”‚
â”œâ”€â”€ Sub-Workflows (Modular Components - NEW v7.0)
â”‚   â”œâ”€â”€ Multimodal Processing Sub-Workflow
â”‚   â”‚   â”œâ”€â”€ Trigger: HTTP POST /multimodal-process
â”‚   â”‚   â”œâ”€â”€ Image processing via Claude Vision
â”‚   â”‚   â”œâ”€â”€ Audio transcription via Soniox
â”‚   â”‚   â”œâ”€â”€ Result format standardization
â”‚   â”‚   â””â”€â”€ Returns: {status, data, metadata}
â”‚   â”‚
â”‚   â”œâ”€â”€ Knowledge Graph Sub-Workflow
â”‚   â”‚   â”œâ”€â”€ Trigger: HTTP POST /kg-process
â”‚   â”‚   â”œâ”€â”€ LightRAG API integration
â”‚   â”‚   â”œâ”€â”€ Async processing with wait/poll pattern
â”‚   â”‚   â”œâ”€â”€ Status checking (max 10 retries)
â”‚   â”‚   â”œâ”€â”€ Graph ID mapping to documents
â”‚   â”‚   â””â”€â”€ Returns: {graph_id, status, error}
â”‚   â”‚
â”‚   â””â”€â”€ Memory Management Sub-Workflow (Future)
â”‚       â”œâ”€â”€ Scheduled execution (daily)
â”‚       â”œâ”€â”€ Memory pruning logic
â”‚       â”œâ”€â”€ Export/backup to B2
â”‚       â””â”€â”€ Analytics generation
â”‚
â””â”€â”€ Benefits
    â”œâ”€â”€ Improved maintainability (isolated concerns)
    â”œâ”€â”€ Independent testing
    â”œâ”€â”€ Parallel execution
    â”œâ”€â”€ Reusability across workflows
    â””â”€â”€ Better error isolation

Asynchronous Processing Patterns (NEW - v7.0):

Wait/Poll Pattern (for long-running operations):
â”œâ”€â”€ Use Cases
â”‚   â”œâ”€â”€ LightRAG knowledge graph processing (30s-2min)
â”‚   â”œâ”€â”€ Mistral OCR jobs (10s-60s)
â”‚   â”œâ”€â”€ Batch processing operations
â”‚   â””â”€â”€ External API async workflows
â”‚
â”œâ”€â”€ Implementation
â”‚   â”œâ”€â”€ Initialize polling state (job_id, status, poll_count)
â”‚   â”œâ”€â”€ Wait node with configurable interval (5s default)
â”‚   â”œâ”€â”€ Status check via HTTP GET
â”‚   â”œâ”€â”€ Switch node for status routing (complete/error/pending)
â”‚   â”œâ”€â”€ Exponential backoff (1.5x factor, max 30s)
â”‚   â”œâ”€â”€ Max retries: 20 (configurable)
â”‚   â””â”€â”€ Timeout handling with graceful degradation
â”‚
â””â”€â”€ Status States
    â”œâ”€â”€ pending (initial)
    â”œâ”€â”€ processing (in progress)
    â”œâ”€â”€ complete (success)
    â”œâ”€â”€ error (failed with details)
    â””â”€â”€ timeout (exceeded max wait time)

Error Handling & Retry Patterns (NEW - v7.0):

Retry Configuration (all critical nodes):
â”œâ”€â”€ retryOnFail: true
â”œâ”€â”€ maxTries: 3
â”œâ”€â”€ waitBetweenTries: 2000ms
â”œâ”€â”€ backoffFactor: 2 (exponential)
â””â”€â”€ continueOnFail: false (fail explicitly)

Error Classification:
â”œâ”€â”€ Retryable Errors (auto-retry)
â”‚   â”œâ”€â”€ ETIMEDOUT (connection timeout)
â”‚   â”œâ”€â”€ ECONNRESET (connection reset)
â”‚   â”œâ”€â”€ ENOTFOUND (DNS failure)
â”‚   â”œâ”€â”€ 502 Bad Gateway
â”‚   â”œâ”€â”€ 503 Service Unavailable
â”‚   â””â”€â”€ 504 Gateway Timeout
â”‚
â””â”€â”€ Non-Retryable Errors (immediate fail)
    â”œâ”€â”€ 400 Bad Request
    â”œâ”€â”€ 401 Unauthorized
    â”œâ”€â”€ 403 Forbidden
    â”œâ”€â”€ 404 Not Found
    â””â”€â”€ 422 Unprocessable Entity

Empty Result Handling:
â”œâ”€â”€ alwaysOutputData: true (for nodes that may return empty)
â”œâ”€â”€ Prevents workflow halts on empty queries
â”œâ”€â”€ Applies to: database lookups, searches, filters
â””â”€â”€ Graceful handling of zero-result scenarios

Document Lifecycle Management (NEW - v7.0):

Document Deletion Workflow:
â”œâ”€â”€ Trigger: DELETE /document/:id webhook
â”œâ”€â”€ Authentication: Required (user or admin)
â”œâ”€â”€ Cascade Operations (in order):
â”‚   â”œâ”€â”€ Delete from documents_v2 (vector embeddings)
â”‚   â”œâ”€â”€ Delete from tabular_document_rows (structured data)
â”‚   â”œâ”€â”€ Delete from LightRAG (knowledge graph)
â”‚   â”œâ”€â”€ Delete from Backblaze B2 (object storage)
â”‚   â””â”€â”€ Delete from documents (main record)
â”œâ”€â”€ Audit Trail
â”‚   â”œâ”€â”€ Log to audit_log table
â”‚   â”œâ”€â”€ Store: deleted_by, deleted_at, metadata
â”‚   â”œâ”€â”€ Retention: 90 days (configurable)
â”‚   â””â”€â”€ Compliance: GDPR right to be forgotten
â””â”€â”€ Response: {success, deleted_id, deleted_at, components_deleted}

Document Versioning:
â”œâ”€â”€ Version Fields
â”‚   â”œâ”€â”€ version_number (integer, auto-increment)
â”‚   â”œâ”€â”€ previous_version_id (UUID reference)
â”‚   â”œâ”€â”€ is_current_version (boolean)
â”‚   â””â”€â”€ content_hash (SHA-256 for change detection)
â”‚
â”œâ”€â”€ Update Detection
â”‚   â”œâ”€â”€ Hash comparison on re-ingestion
â”‚   â”œâ”€â”€ Automatic versioning when hash changes
â”‚   â”œâ”€â”€ Archive old version (is_current_version = false)
â”‚   â””â”€â”€ Create new version with incremented number
â”‚
â””â”€â”€ Version Management
    â”œâ”€â”€ All versions preserved (unless manually purged)
    â”œâ”€â”€ Query: current version by default
    â”œâ”€â”€ Historical access: query by version_number
    â””â”€â”€ Rollback: set is_current_version on old version

Hash-Based Deduplication (NEW - v7.0):
â”œâ”€â”€ Implementation
â”‚   â”œâ”€â”€ Generate SHA-256 hash of document content
â”‚   â”œâ”€â”€ Include: content + filename + file_size + modified_date
â”‚   â”œâ”€â”€ Check hash in documents table before processing
â”‚   â”œâ”€â”€ Skip if duplicate found
â”‚   â””â”€â”€ Save processing time and storage
â”‚
â”œâ”€â”€ Database Fields
â”‚   â”œâ”€â”€ content_hash TEXT (unique index)
â”‚   â”œâ”€â”€ processing_status VARCHAR(50)
â”‚   â”œâ”€â”€ processing_started_at TIMESTAMPTZ
â”‚   â”œâ”€â”€ processing_completed_at TIMESTAMPTZ
â”‚   â””â”€â”€ processing_error TEXT
â”‚
â””â”€â”€ Status Values
    â”œâ”€â”€ pending (queued for processing)
    â”œâ”€â”€ processing (in progress)
    â”œâ”€â”€ complete (successfully processed)
    â”œâ”€â”€ error (failed with details)
    â””â”€â”€ duplicate (skipped due to hash match)

Batch Processing Patterns (NEW - v7.0):
â”œâ”€â”€ splitInBatches node usage
â”‚   â”œâ”€â”€ Batch size: 10-100 (configurable)
â”‚   â”œâ”€â”€ Reset: false (continue from last position)
â”‚   â””â”€â”€ Progress tracking per batch
â”‚
â”œâ”€â”€ Aggregate node after batches
â”‚   â”œâ”€â”€ Collect all results
â”‚   â”œâ”€â”€ Combine statistics
â”‚   â””â”€â”€ Generate summary report
â”‚
â””â”€â”€ Use Cases
    â”œâ”€â”€ Bulk document uploads
    â”œâ”€â”€ Mass reprocessing
    â”œâ”€â”€ Periodic maintenance tasks
    â””â”€â”€ Large-scale data migrations

================================================================================
n8n Node Implementation Patterns (NEW - v7.0 Complete)
================================================================================

Essential Node Patterns for Production Workflows:

1. Extract From File Node
â”œâ”€â”€ Purpose: Direct text extraction without external dependencies
â”œâ”€â”€ Use Cases: PDFs, HTML, simple documents
â””â”€â”€ Fallback: When MarkItDown fails

2. Mistral OCR Upload Pattern
â”œâ”€â”€ Upload â†’ Wait â†’ Poll â†’ Retrieve
â”œâ”€â”€ Handles complex PDFs with tables/scans
â”œâ”€â”€ Async processing with status checking
â””â”€â”€ 10-60 second processing time

3. Cohere Reranking Integration
â”œâ”€â”€ Hybrid Search (20 results) â†’ Prepare â†’ Rerank API â†’ Map Results
â”œâ”€â”€ Model: rerank-english-v3.5
â”œâ”€â”€ Returns top 10 with relevance scores
â””â”€â”€ 20-30% improvement in result ordering

4. Document ID Generation
â”œâ”€â”€ UUID v4 for unique identifiers
â”œâ”€â”€ Alternative: SHA-256 content hash
â””â”€â”€ Ensures deduplication and tracking

5. Loop Node Pattern
â”œâ”€â”€ Process arrays iteratively
â”œâ”€â”€ Add delays for rate limiting
â”œâ”€â”€ Check completion conditions
â””â”€â”€ Handle large datasets efficiently

6. Set Node Pattern
â”œâ”€â”€ Transform and standardize data
â”œâ”€â”€ Set default values
â”œâ”€â”€ Type conversion (string/number/boolean)
â””â”€â”€ Dot notation for nested properties

7. Merge Node Pattern
â”œâ”€â”€ Combine: Merge by key or multiplex
â”œâ”€â”€ Append: Add results sequentially
â”œâ”€â”€ Join: Link related data
â””â”€â”€ Handle multiple data sources

Complete Workflow Integration:
Webhook â†’ Generate ID â†’ Extract/OCR â†’ Set Status â†’ Loop Processing
â†’ Hybrid Search â†’ Cohere Rerank â†’ Merge Results â†’ Response

================================================================================
Migration Path from Simplified to Advanced
================================================================================

Current State: Simplified v6.0 (no advanced RAG)
Target State: Complete v6.0 (all advanced RAG features)

Phase 1: Database Functions (Week 1)
â”œâ”€â”€ Implement dynamic_hybrid_search_db function
â”œâ”€â”€ Add context expansion functions
â”œâ”€â”€ Create hierarchical structure extraction
â””â”€â”€ Test all search methods independently

Phase 2: API Integrations (Week 1-2)
â”œâ”€â”€ Integrate Cohere Rerank v3.5
â”œâ”€â”€ Connect LightRAG API
â”œâ”€â”€ Test reranking pipeline
â””â”€â”€ Validate graph queries

Phase 3: n8n Workflows (Week 2)
â”œâ”€â”€ Update ingestion to extract hierarchy
â”œâ”€â”€ Add LightRAG entity extraction step
â”œâ”€â”€ Store all required metadata
â””â”€â”€ Test end-to-end ingestion

Phase 4: Query Pipeline (Week 2-3)
â”œâ”€â”€ Implement dynamic hybrid search
â”œâ”€â”€ Add Cohere reranking step
â”œâ”€â”€ Integrate LightRAG graph queries
â”œâ”€â”€ Add context expansion
â””â”€â”€ Test complete query flow

Phase 5: Chat UI Deployment (Week 3)
â”œâ”€â”€ Deploy Gradio interface
â”œâ”€â”€ Connect to query pipeline
â”œâ”€â”€ Add cost tracking
â”œâ”€â”€ Test user experience
â””â”€â”€ Launch to production

Total Time: 2-3 weeks
Additional Cost: $35-45/month
Value: Best-in-class RAG system

================================================================================
Gap Resolution Summary (v7.0 Complete)
================================================================================

Analysis completed for two comprehensive gap documents:
1. EMPIRE_v7_GAP_ANALYSIS_WORKING.md (34 gaps)
2. EMPIRE_v7_vs_TOTAL_RAG_GAP_ANALYSIS.md (32 actionable gaps)

All critical and high-priority gaps have been resolved:

Critical Gaps (14/14 Complete):
âœ… Gap 1.1: Context expansion function with hierarchical context
âœ… Gap 1.2: Supabase Edge Functions for HTTP API access
âœ… Gap 1.3: Tabular data storage with schema inference
âœ… Gap 1.4: Chat history persistence with session tracking
âœ… Gap 1.5: Dynamic metadata fields management
âœ… Gap 1.6: LlamaIndex + LangExtract precision extraction
âœ… Gap 1.7: Multimodal sub-workflow pattern
âœ… Gap 1.8: Knowledge graph sub-workflow pattern
âœ… Gap 1.9: Memory management workflow
âœ… Gap 1.10: Hash-based deduplication
âœ… Gap 1.11: Document status tracking
âœ… Gap 1.12: File update triggers
âœ… Gap 1.13: Document deletion workflow
âœ… Gap 1.14: Batch processing patterns

High-Priority Gaps (8/8 Complete):
âœ… Gap 2.1: Advanced metadata filtering
âœ… Gap 2.2: Wait/poll async patterns
âœ… Gap 2.3: Structured output parsing
âœ… Gap 2.4: Aggregate node patterns
âœ… Gap 2.5: Retry configuration
âœ… Gap 2.6: AlwaysOutputData pattern
âœ… Gap 2.7: Execute workflow triggers
âœ… Gap 2.8: Switch node routing

Medium-Priority Gaps (12/12 Complete):
âœ… All n8n node patterns documented
âœ… Complete SQL schemas provided
âœ… Edge function implementations
âœ… Workflow JSON definitions

Total Implementation Coverage: 100%
Documentation: 10,000+ lines across all sections
Production Readiness: Complete

Empire Advantages Over Total RAG:
âœ… Superior AI Stack: Claude Sonnet 4.5 > GPT-4
âœ… Better Memory: Graphiti MCP with Neo4j (temporal knowledge graphs)
âœ… More Efficient: 768-dim embeddings vs 1536-dim
âœ… Advanced Extraction: LlamaIndex + LangExtract (Total RAG lacks)
âœ… Full Observability: Prometheus + Grafana (Total RAG lacks)
âœ… Better Schema: error_logs, processing_queue, audit_log tables
âœ… Cost Tracking: Built-in cost optimization (Total RAG lacks)
âœ… IEEE 830-1998 Compliant SRS: 340+ requirements documented

================================================================================
MEMORY SYSTEM UPDATES (November 2025) - WITH LOCAL EMBEDDINGS
================================================================================

Developer Memory System Upgraded to Graphiti with Local Embeddings:
âœ… Replaced mem-agent MCP with Graphiti + Neo4j for developer memory
âœ… LOCAL BGE-M3 EMBEDDINGS via Ollama - $0 cost (was OpenAI API)
âœ… Implemented project-based memory architecture (project_{name}_{aspect})
âœ… Complete separation of personal (322 ChatGPT memories) vs work memories
âœ… ALL 322 personal ChatGPT conversations imported with local embeddings
âœ… Context switching via ./mem command (instant project switching)
âœ… Scalable to unlimited projects without restructuring
âœ… Temporal knowledge graphs tracking information evolution
âœ… Full integration with Claude Code for context-aware development
âœ… MCP Server configured with Ollama embedder integration

Technical Implementation:
â”œâ”€â”€ Graphiti MCP Server: /mem-agent-mcp/graphiti/mcp_server/
â”œâ”€â”€ Ollama BGE-M3: 1024-dim embeddings (local, <10ms generation)
â”œâ”€â”€ Neo4j Backend: bolt://localhost:7687 (<from .env>)
â”œâ”€â”€ Configuration: config/config.yaml with embedder: "ollama"
â”œâ”€â”€ Custom Integration: OllamaEmbedder class for Graphiti compatibility
â””â”€â”€ Import Scripts: import_chatgpt_ollama_direct.py for batch processing

Empire Project Memory Structure:
â”œâ”€â”€ project_empire - Main project memories
â”œâ”€â”€ project_empire_dev - Development notes and decisions
â”œâ”€â”€ project_empire_docs - Documentation and specifications
â”œâ”€â”€ project_empire_api - API endpoints and contracts
â””â”€â”€ project_empire_notes - Meeting notes and discussions

Personal Memory Structure (322 conversations):
â”œâ”€â”€ Topics: Flight booking, health questions, coding help, etc.
â”œâ”€â”€ Format: Episodic nodes with content_embedding vectors
â”œâ”€â”€ Storage: Neo4j with group_id='personal'
â”œâ”€â”€ Access: Via Graphiti MCP or direct Cypher queries
â””â”€â”€ Privacy: Completely isolated from work memories

Cost Savings Achieved:
â€¢ Embedding Costs: $0 (was ~$0.002 per embedding with OpenAI)
â€¢ Import Savings: $0.64 for 322 conversations
â€¢ Monthly Savings: $50-100 on embedding API costs
â€¢ Performance: <10ms local vs 50-100ms API latency
â€¢ Quality: BGE-M3 often superior for technical content

Key Benefits:
â€¢ Privacy: Personal memories completely isolated from work
â€¢ Cost: ZERO embedding costs with local BGE-M3
â€¢ Speed: <10ms local embeddings vs 50-100ms API
â€¢ Scalability: Add new projects with ./mem add "project-name"
â€¢ Performance: <100ms retrieval with Neo4j indexing
â€¢ Integration: Seamless Claude Code MCP integration
â€¢ Flexibility: Switch contexts instantly without restart
â€¢ Ownership: All data processed locally on Mac Studio

================================================================================
Last Updated: November 2, 2025
Version: 7.3 - Local Embeddings + Persistent Memory with Graphiti
Classification: Confidential - Internal Use
Implementation Status: Memory System Deployed, RAG Pipeline in Development
Priority: HIGH - Ready for production implementation
================================================================================