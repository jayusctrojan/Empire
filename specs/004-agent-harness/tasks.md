# Implementation Tasks: Research Projects (Agent Harness)

**Generated from**: TaskMaster PRD parsing
**Date**: 2025-01-10
**Total Tasks**: 10
**Tag**: v7_3_features

## Task Overview

| ID | Title | Priority | Status | Dependencies |
|----|-------|----------|--------|--------------|
| 91 | Create Database Schema for Research Projects | high | pending | None |
| 92 | Implement Research Projects API Router | high | pending | 91 | âœ“ 5 subtasks |
| 93 | Develop Research Initializer Service | high | pending | 91 |
| 94 | Implement Task Harness Service (Sequential Ex... | high | pending | 91, 93 |
| 95 | Develop Basic Retrieval Executor | high | pending | 91, 94 |
| 96 | Implement Concurrent Task Execution Engine | high | pending | 94 |
| 97 | Develop Synthesis Executor for Research Findi... | medium | pending | 95 |
| 98 | Implement Report Generation Executor | medium | pending | 97 |
| 99 | Implement Email Notification Service | medium | pending | 92 |
| 100 | Implement Performance Monitoring and Metrics | medium | pending | 96 |

---

## Task 91: Create Database Schema for Research Projects

**Priority**: high
**Status**: pending
**Dependencies**: None

### Description

Implement the database schema for research jobs, plan tasks, and research artifacts tables as specified in the PRD.

### Implementation Details

Create the following tables in Supabase:

1. `research_jobs` table:
```sql
CREATE TABLE research_jobs (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  user_id UUID REFERENCES auth.users(id),
  customer_id TEXT NOT NULL,
  query TEXT NOT NULL,
  context TEXT,
  status TEXT DEFAULT 'initializing',
  notify_email TEXT NOT NULL,
  locked_at TIMESTAMP,
  report_content TEXT,
  report_url TEXT,
  summary TEXT,
  key_findings TEXT,
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW(),
  completed_at TIMESTAMP,
  execution_id INTEGER,
  retries SMALLINT DEFAULT 0,
  error_message TEXT
);
```

2. `plan_tasks` table:
```sql
CREATE TABLE plan_tasks (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  job_id BIGINT REFERENCES research_jobs(id) ON DELETE CASCADE,
  task_key TEXT NOT NULL,
  sequence_order INTEGER NOT NULL,
  task_type TEXT NOT NULL,
  query TEXT,
  config JSONB,
  status TEXT DEFAULT 'pending',
  result_summary TEXT,
  started_at TIMESTAMP,
  completed_at TIMESTAMP,
  error_message TEXT,
  created_at TIMESTAMP DEFAULT NOW(),
  UNIQUE(job_id, task_key)
);
```

3. `research_artifacts` table:
```sql
CREATE TABLE research_artifacts (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  job_id BIGINT REFERENCES research_jobs(id) ON DELETE CASCADE,
  task_id BIGINT REFERENCES plan_tasks(id) ON DELETE SET NULL,
  artifact_type TEXT NOT NULL,
  source TEXT,
  query_used TEXT,
  raw_response JSONB,
  processed_content TEXT,
  confidence_score FLOAT,
  created_at TIMESTAMP DEFAULT NOW()
);
```

Also create appropriate indexes for query performance and implement Row Level Security (RLS) policies to ensure user isolation.

### Test Strategy

1. Verify all tables are created with correct columns, types, and constraints
2. Test foreign key constraints by attempting to delete parent records
3. Verify identity columns auto-increment correctly
4. Test RLS policies by attempting to access data from different user contexts
5. Benchmark query performance with sample data to ensure indexes are effective
6. Validate cascade delete functionality works as expected

---

## Task 92: Implement Research Projects API Router

**Priority**: high
**Status**: pending
**Dependencies**: 91

### Description

Create a FastAPI router with endpoints for creating, retrieving, and managing research projects.

### Implementation Details

Create a new FastAPI router in `app/api/research_projects.py` with the following endpoints:

1. `POST /` - Create new research project
   - Accept query, context, and notify_email
   - Create record in research_jobs table
   - Trigger initializer Celery task
   - Return job ID and status

2. `GET /` - List user's projects
   - Return paginated list of user's research jobs
   - Include basic metadata and status

3. `GET /{id}` - Get project details
   - Return full job details including tasks
   - Include progress information

4. `GET /{id}/status` - Get current status
   - Lightweight endpoint for polling
   - Include progress percentage

5. `DELETE /{id}` - Cancel project
   - Mark job as cancelled
   - Cancel any pending tasks

6. `GET /{id}/report` - Get final report
   - Return report content or URL

7. `WebSocket /ws/{id}` - Real-time updates
   - Stream status changes to client

Implement proper authentication middleware to ensure users can only access their own projects. Use Pydantic models for request/response validation.

### Test Strategy

1. Unit tests for each endpoint with mocked database calls
2. Integration tests verifying database interactions
3. Authentication tests ensuring proper access control
4. WebSocket connection tests with simulated updates
5. Load testing endpoints with concurrent requests
6. Error handling tests with invalid inputs and error conditions

### Subtasks

| ID | Title | Status | Dependencies |
|----|-------|--------|--------------|
| 92.1 | Implement Core API Router and Basic Endpoints | pending | - |
| 92.2 | Implement Project Detail and Status Endpoints | pending | 92.1 |
| 92.3 | Implement Project Management and WebSocket Endpoints | pending | 92.2 |
| 92.4 | Implement Partial Findings and Share Creation Endpoints (FR-009, FR-005) | pending | 92.2 |
| 92.5 | Implement Share Management and Public Access Endpoints (FR-005a) | pending | 92.4 |

**Subtask 92.4 Details** (NEW - Coverage Gap Fix):
- `GET /{id}/findings` - Returns artifacts from completed tasks before full report
- `POST /{id}/share` - Creates shareable public link with secure token (FR-005)

**Subtask 92.5 Details** (NEW - Coverage Gap Fix):
- `GET /{id}/shares` - Lists all shares (active and revoked)
- `DELETE /{id}/share/{share_id}` - Revokes a share link (FR-005a)
- `GET /shared/{token}` - Public endpoint, no auth, returns report content

---

## Task 93: Develop Research Initializer Service

**Priority**: high
**Status**: pending
**Dependencies**: 91

### Description

Create the AI-powered planning service that analyzes research queries and creates structured task plans.

### Implementation Details

Implement `app/services/research_initializer.py` with the following components:

1. `ResearchInitializer` class with methods:
   - `analyze_query(query: str, context: str) -> dict` - Uses Claude Sonnet to analyze the research query and determine required tasks
   - `create_task_plan(job_id: int, analysis: dict) -> list[dict]` - Creates task records in the database based on analysis
   - `initialize_job(job_id: int) -> None` - Main entry point that orchestrates the initialization process

2. Implement structured output format for Claude:
```python
def get_task_plan_prompt(query: str, context: str) -> str:
    return f"""Analyze this research query and break it down into a sequence of tasks.
    Query: {query}
    Context: {context}
    
    Return a JSON array of tasks with the following structure:
    [{{
      "task_key": "unique_key",
      "task_type": "retrieval_rag|retrieval_nlq|retrieval_graph|synthesis|write_report",
      "sequence_order": 1,
      "query": "specific query for this task",
      "dependencies": ["dependent_task_keys"]
    }}]
    """
```

3. Implement dependency graph creation:
```python
def build_dependency_graph(tasks: list[dict]) -> dict:
    """Convert task list with dependencies into a DAG structure."""
    graph = {}
    for task in tasks:
        graph[task['task_key']] = {
            'task': task,
            'dependencies': task.get('dependencies', []),
            'dependents': []
        }
    
    # Populate dependents
    for task_key, node in graph.items():
        for dep in node['dependencies']:
            graph[dep]['dependents'].append(task_key)
    
    return graph
```

4. Integrate with Celery task for async execution

### Test Strategy

1. Unit test query analysis with various research queries
2. Test task plan creation with different complexity levels
3. Verify dependency graph creation with complex dependency chains
4. Integration test with Claude API using recorded responses
5. Test error handling for malformed Claude responses
6. Benchmark initialization time for different query complexities

---

## Task 94: Implement Task Harness Service (Sequential Execution)

**Priority**: high
**Status**: pending
**Dependencies**: 91, 93

### Description

Create the core task execution engine that processes research tasks sequentially according to the plan.

### Implementation Details

Implement `app/services/task_harness.py` with the following components:

1. `TaskHarness` class with methods:
   - `get_next_tasks(job_id: int) -> list[dict]` - Get tasks ready for execution (dependencies satisfied)
   - `execute_task(task_id: int) -> dict` - Execute a single task based on its type
   - `update_task_status(task_id: int, status: str, result: dict = None) -> None` - Update task status and trigger dependents
   - `check_job_completion(job_id: int) -> bool` - Check if all tasks are complete

2. Task type router:
```python
def execute_task_by_type(task: dict) -> dict:
    """Route task to appropriate executor based on task_type."""
    task_type = task['task_type']
    if task_type.startswith('retrieval_'):
        return retrieval_executor.execute(task)
    elif task_type == 'synthesis':
        return synthesis_executor.execute(task)
    elif task_type == 'write_report':
        return report_executor.execute(task)
    else:
        raise ValueError(f"Unknown task type: {task_type}")
```

3. Sequential execution flow:
```python
def process_job(job_id: int) -> None:
    """Process all tasks for a job in sequence order."""
    while True:
        next_tasks = get_next_tasks(job_id)
        if not next_tasks:
            if check_job_completion(job_id):
                update_job_status(job_id, 'complete')
                return
            else:
                # No tasks ready but job not complete - possible error
                check_for_errors(job_id)
                return
        
        # Execute next task
        task = next_tasks[0]  # Sequential - just take first task
        try:
            update_task_status(task['id'], 'running')
            result = execute_task_by_type(task)
            update_task_status(task['id'], 'complete', result)
        except Exception as e:
            update_task_status(task['id'], 'failed', {'error': str(e)})
```

4. Integrate with Celery for background execution

### Test Strategy

1. Unit test task selection logic with various dependency scenarios
2. Test task execution routing to correct executors
3. Verify task status updates trigger dependent tasks
4. Test job completion detection
5. Integration test with mock executors
6. Error handling tests for task failures

---

## Task 95: Develop Basic Retrieval Executor

**Priority**: high
**Status**: pending
**Dependencies**: 91, 94

### Description

Implement the retrieval executor for RAG-based document retrieval tasks.

### Implementation Details

Create `app/services/task_executors/retrieval_executor.py` with the following components:

1. `RetrievalExecutor` class with methods:
   - `execute_rag(task: dict) -> dict` - Execute RAG retrieval using existing Empire vector search
   - `store_artifacts(job_id: int, task_id: int, results: list) -> list[int]` - Store retrieved chunks as artifacts
   - `apply_quality_gate(results: list) -> (bool, list)` - Apply quality threshold to results

2. RAG implementation using existing Empire infrastructure:
```python
def execute_rag(task: dict) -> dict:
    """Execute RAG retrieval using Supabase pgvector."""
    query = task['query']
    config = task.get('config', {})
    
    # Use existing RAG pipeline
    from app.services.rag import perform_vector_search
    
    results = perform_vector_search(
        query=query,
        limit=config.get('limit', 10),
        min_score=config.get('min_score', 0.7)
    )
    
    # Apply quality gate
    passed, filtered_results = apply_quality_gate(results)
    
    if not passed:
        # Implement retry with expanded query
        expanded_query = expand_query(query)
        results = perform_vector_search(
            query=expanded_query,
            limit=config.get('limit', 15),  # Increase limit for retry
            min_score=config.get('min_score', 0.65)  # Lower threshold slightly
        )
        passed, filtered_results = apply_quality_gate(results)
        
        if not passed:
            raise QualityGateFailedError("Failed to retrieve sufficient quality results")
    
    # Store artifacts
    artifact_ids = store_artifacts(
        job_id=task['job_id'],
        task_id=task['id'],
        results=filtered_results
    )
    
    return {
        'artifact_ids': artifact_ids,
        'result_count': len(filtered_results),
        'average_score': sum(r['score'] for r in filtered_results) / len(filtered_results)
    }
```

3. Quality gate implementation:
```python
def apply_quality_gate(results: list) -> (bool, list):
    """Apply quality threshold to results."""
    # Filter by minimum score
    filtered = [r for r in results if r['score'] >= 0.7]
    
    # Check minimum number of results
    if len(filtered) >= 5:
        return True, filtered
    else:
        return False, filtered
```

### Test Strategy

1. Unit test RAG retrieval with various queries
2. Test quality gate with different result sets
3. Verify artifact storage correctly persists chunks
4. Test retry mechanism with expanded queries
5. Integration test with actual vector database
6. Performance testing with various query complexities

---

## Task 96: Implement Concurrent Task Execution Engine

**Priority**: high
**Status**: pending
**Dependencies**: 94

### Description

Enhance the Task Harness to support concurrent execution of independent tasks for maximum efficiency.

### Implementation Details

Extend `app/services/task_harness.py` to implement concurrent execution:

1. Dependency Graph Analysis:
```python
def build_execution_plan(job_id: int) -> dict:
    """Build a dependency graph for all tasks in the job."""
    tasks = get_all_tasks(job_id)
    graph = {}
    
    # Build initial graph
    for task in tasks:
        task_key = task['task_key']
        graph[task_key] = {
            'task': task,
            'dependencies': [],
            'dependents': [],
            'status': task['status']
        }
    
    # Populate dependencies and dependents
    for task in tasks:
        task_key = task['task_key']
        dependencies = get_task_dependencies(task['id'])
        graph[task_key]['dependencies'] = dependencies
        
        for dep in dependencies:
            if dep in graph:
                graph[dep]['dependents'].append(task_key)
    
    return graph
```

2. Concurrent Dispatch Logic:
```python
def get_ready_tasks(graph: dict) -> list[str]:
    """Get all tasks that are ready to execute (dependencies satisfied)."""
    ready_tasks = []
    
    for task_key, node in graph.items():
        if node['status'] == 'pending':
            deps_satisfied = all(
                graph[dep]['status'] == 'complete'
                for dep in node['dependencies']
            )
            if deps_satisfied:
                ready_tasks.append(task_key)
    
    return ready_tasks

def process_job_concurrent(job_id: int, max_concurrent: int = 5) -> None:
    """Process job tasks with maximum concurrency."""
    graph = build_execution_plan(job_id)
    running_tasks = set()
    completed_count = 0
    total_tasks = len(graph)
    
    while completed_count < total_tasks:
        # Get tasks ready to execute
        ready_tasks = get_ready_tasks(graph)
        
        # Launch tasks up to max_concurrent limit
        available_slots = max_concurrent - len(running_tasks)
        for i in range(min(available_slots, len(ready_tasks))):
            task_key = ready_tasks[i]
            task = graph[task_key]['task']
            
            # Launch task asynchronously
            task_id = task['id']
            execute_research_task.delay(task_id)
            
            # Update status in graph and track running
            graph[task_key]['status'] = 'running'
            running_tasks.add(task_key)
        
        # Check for completed tasks
        for task_key in list(running_tasks):
            task = graph[task_key]['task']
            current_status = get_task_status(task['id'])
            
            if current_status in ['complete', 'failed']:
                # Update graph
                graph[task_key]['status'] = current_status
                running_tasks.remove(task_key)
                completed_count += 1
                
                # If complete, check dependents
                if current_status == 'complete':
                    for dependent in graph[task_key]['dependents']:
                        # Check if dependent is ready
                        deps = graph[dependent]['dependencies']
                        if all(graph[dep]['status'] == 'complete' for dep in deps):
                            # Dependent is ready - queue immediately
                            if len(running_tasks) < max_concurrent:
                                dependent_task = graph[dependent]['task']
                                execute_research_task.delay(dependent_task['id'])
                                graph[dependent]['status'] = 'running'
                                running_tasks.add(dependent)
        
        # Short sleep to prevent CPU spinning
        time.sleep(0.1)
    
    # All tasks completed
    update_job_status(job_id, 'complete')
```

3. Wave-Based Execution Tracking:
```python
def identify_execution_waves(graph: dict) -> list[list[str]]:
    """Group tasks into execution waves based on dependencies."""
    waves = []
    remaining = set(graph.keys())
    
    while remaining:
        # Find all tasks with no remaining dependencies
        wave = []
        for task_key in list(remaining):
            deps = [d for d in graph[task_key]['dependencies'] if d in remaining]
            if not deps:
                wave.append(task_key)
        
        if not wave:
            # Circular dependency or other error
            break
        
        waves.append(wave)
        remaining -= set(wave)
    
    return waves
```

4. Performance Instrumentation:
```python
def record_execution_metrics(job_id: int, metrics: dict) -> None:
    """Record execution metrics for analysis."""
    # Store metrics in database or send to monitoring system
    db.execute(
        """INSERT INTO research_job_metrics 
           (job_id, total_duration, parallelism_ratio, max_concurrent, idle_time)
           VALUES (:job_id, :total_duration, :parallelism_ratio, :max_concurrent, :idle_time)
        """,
        {
            'job_id': job_id,
            'total_duration': metrics['total_duration'],
            'parallelism_ratio': metrics['parallelism_ratio'],
            'max_concurrent': metrics['max_concurrent'],
            'idle_time': metrics['idle_time']
        }
    )
```

### Test Strategy

1. Unit test dependency graph creation with various task configurations
2. Test ready task identification with complex dependency chains
3. Verify concurrent execution respects max_concurrent limit
4. Test wave identification with multi-level dependencies
5. Measure execution time compared to sequential for the same tasks
6. Verify metrics recording captures accurate performance data
7. Load test with multiple concurrent jobs
8. Test handling of failed tasks within concurrent execution

---

## Task 97: Develop Synthesis Executor for Research Findings

**Priority**: medium
**Status**: pending
**Dependencies**: 95

### Description

Implement the synthesis executor that combines retrieved artifacts into coherent research findings.

### Implementation Details

Create `app/services/task_executors/synthesis_executor.py` with the following components:

1. `SynthesisExecutor` class with methods:
   - `execute(task: dict) -> dict` - Main entry point for synthesis tasks
   - `get_input_artifacts(job_id: int, dependency_ids: list[int]) -> list[dict]` - Retrieve artifacts from dependent tasks
   - `synthesize_findings(query: str, artifacts: list[dict]) -> dict` - Use AI to synthesize findings
   - `apply_quality_gate(synthesis: dict) -> bool` - Validate synthesis quality

2. Synthesis implementation using Claude:
```python
def synthesize_findings(query: str, artifacts: list[dict]) -> dict:
    """Use Claude to synthesize findings from artifacts."""
    # Prepare context from artifacts
    context = "\n\n".join([f"Source {i+1}: {a['processed_content']}" 
                         for i, a in enumerate(artifacts)])
    
    # Create prompt for synthesis
    prompt = f"""You are synthesizing research findings based on the following sources.
    
    Research Query: {query}
    
    {context}
    
    Synthesize these sources into a coherent set of findings. Identify key themes, 
    insights, and conclusions. Maintain factual accuracy and cite sources.
    
    Structure your response as JSON with the following format:
    {{
      "key_findings": [list of main findings],
      "detailed_analysis": "comprehensive analysis",
      "source_citations": [list of citations],
      "confidence_score": float between 0-1
    }}
    """
    
    # Call Claude API
    from app.services.ai import get_claude_response
    
    response = get_claude_response(
        prompt=prompt,
        model="claude-3-sonnet-20240229",
        temperature=0.2,
        max_tokens=4000
    )
    
    # Parse JSON response
    try:
        synthesis = json.loads(response)
        return synthesis
    except json.JSONDecodeError:
        # Fallback parsing if not valid JSON
        return {
            "key_findings": ["Error: Could not parse AI response"],
            "detailed_analysis": response,
            "source_citations": [],
            "confidence_score": 0.5
        }
```

3. Quality gate implementation:
```python
def apply_quality_gate(synthesis: dict) -> bool:
    """Validate synthesis quality."""
    # Check confidence score
    if synthesis.get('confidence_score', 0) < 0.8:
        return False
    
    # Check key findings
    if not synthesis.get('key_findings') or len(synthesis['key_findings']) < 3:
        return False
    
    # Check detailed analysis length
    detailed = synthesis.get('detailed_analysis', '')
    if len(detailed) < 500:  # Minimum character count
        return False
    
    # Check source citations
    if not synthesis.get('source_citations') or len(synthesis['source_citations']) < 2:
        return False
    
    return True
```

4. Store synthesis as artifact:
```python
def store_synthesis_artifact(job_id: int, task_id: int, synthesis: dict) -> int:
    """Store synthesis result as an artifact."""
    artifact_id = db.execute(
        """INSERT INTO research_artifacts 
           (job_id, task_id, artifact_type, processed_content, confidence_score, raw_response)
           VALUES (:job_id, :task_id, 'synthesis_finding', :content, :score, :raw)
           RETURNING id
        """,
        {
            'job_id': job_id,
            'task_id': task_id,
            'content': synthesis['detailed_analysis'],
            'score': synthesis['confidence_score'],
            'raw': json.dumps(synthesis)
        }
    ).fetchone()[0]
    
    return artifact_id
```

### Test Strategy

1. Unit test synthesis with various artifact combinations
2. Test quality gate with different synthesis outputs
3. Verify artifact storage correctly persists synthesis
4. Integration test with Claude API using recorded responses
5. Test error handling for malformed AI responses
6. Benchmark synthesis time for different input sizes
7. Test source citation extraction and validation

---

## Task 98: Implement Report Generation Executor

**Priority**: medium
**Status**: pending
**Dependencies**: 97

### Description

Create the report executor that generates comprehensive research reports from all findings.

### Implementation Details

Create `app/services/task_executors/report_executor.py` with the following components:

1. `ReportExecutor` class with methods:
   - `execute(task: dict) -> dict` - Main entry point for report generation
   - `get_all_artifacts(job_id: int) -> dict` - Retrieve all artifacts for the job
   - `generate_report(query: str, context: str, artifacts: dict) -> dict` - Use AI to generate report
   - `format_report(report: dict) -> dict` - Format report in multiple formats (Markdown, PDF)
   - `store_report(job_id: int, report: dict) -> str` - Store report and get URL

2. Report generation using Empire's writing agent:
```python
def generate_report(query: str, context: str, artifacts: dict) -> dict:
    """Use AGENT-014 (Writing Agent) to generate comprehensive report."""
    # Prepare input for writing agent
    from app.agents.writing_agent import generate_content
    
    # Extract synthesis findings
    syntheses = artifacts.get('synthesis_finding', [])
    findings = "\n\n".join([s['processed_content'] for s in syntheses])
    
    # Extract raw chunks for reference
    chunks = artifacts.get('retrieved_chunk', [])
    references = "\n\n".join([f"Source {i+1}: {c['processed_content']}" 
                           for i, c in enumerate(chunks[:10])])
    
    # Generate report
    report_content = generate_content(
        content_type="research_report",
        topic=query,
        context=context,
        findings=findings,
        references=references,
        max_length=10000
    )
    
    # Extract sections
    sections = extract_report_sections(report_content)
    
    # Quality check with AGENT-015
    from app.agents.review_agent import review_content
    
    quality_review = review_content(
        content=report_content,
        content_type="research_report",
        criteria=["accuracy", "completeness", "clarity"]
    )
    
    # If quality below threshold, revise
    if quality_review['overall_score'] < 0.85:
        report_content = generate_content(
            content_type="research_report",
            topic=query,
            context=context,
            findings=findings,
            references=references,
            feedback=quality_review['improvement_suggestions'],
            max_length=10000
        )
        sections = extract_report_sections(report_content)
    
    return {
        'full_content': report_content,
        'sections': sections,
        'quality_score': quality_review['overall_score'],
        'review_feedback': quality_review['improvement_suggestions']
    }
```

3. Report formatting and storage:
```python
def format_report(report: dict) -> dict:
    """Format report in multiple formats."""
    # Markdown is the base format
    markdown = report['full_content']
    
    # Generate PDF
    from app.utils.pdf_generator import markdown_to_pdf
    
    pdf_bytes = markdown_to_pdf(markdown)
    
    return {
        'markdown': markdown,
        'pdf': pdf_bytes
    }

def store_report(job_id: int, report: dict) -> dict:
    """Store report in B2 and update job record."""
    from app.services.storage import upload_to_b2
    
    # Generate filenames
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    md_filename = f"research-reports/{job_id}/report_{timestamp}.md"
    pdf_filename = f"research-reports/{job_id}/report_{timestamp}.pdf"
    
    # Upload to B2
    md_url = upload_to_b2(report['markdown'].encode('utf-8'), md_filename, 'text/markdown')
    pdf_url = upload_to_b2(report['pdf'], pdf_filename, 'application/pdf')
    
    # Update job record
    db.execute(
        """UPDATE research_jobs
           SET report_content = :content,
               report_url = :url,
               summary = :summary,
               key_findings = :findings,
               status = 'complete',
               completed_at = NOW()
           WHERE id = :job_id
        """,
        {
            'job_id': job_id,
            'content': report['markdown'],
            'url': pdf_url,
            'summary': report['sections'].get('executive_summary', ''),
            'findings': json.dumps(report['sections'].get('key_findings', []))
        }
    )
    
    return {
        'markdown_url': md_url,
        'pdf_url': pdf_url
    }
```

### Test Strategy

1. Unit test report generation with various artifact combinations
2. Test quality review and revision process
3. Verify PDF generation produces valid documents
4. Test B2 storage and URL generation
5. Integration test with writing and review agents
6. Verify report sections extraction
7. Test handling of large reports
8. Benchmark report generation time for different input sizes

---

## Task 99: Implement Email Notification Service

**Priority**: medium
**Status**: pending
**Dependencies**: 92

### Description

Create a notification service that sends emails to users at key points in the research process.

### Implementation Details

Create `app/services/notification_service.py` with the following components:

1. `NotificationService` class with methods:
   - `send_project_started(job_id: int) -> None` - Notify user that project has started
   - `send_progress_update(job_id: int, progress: float) -> None` - Send progress milestone updates
   - `send_project_completed(job_id: int, report_url: str) -> None` - Notify user of completion with report link
   - `send_project_failed(job_id: int, error: str) -> None` - Notify user of failure

2. Email template implementation:
```python
def get_project_started_template(job: dict) -> dict:
    """Get email template for project started notification."""
    return {
        'subject': f"Research Project Started: {job['query'][:50]}...",
        'body': f"""<p>Hello,</p>
        
        <p>Your research project has been started. Our AI is now working on your query:</p>
        
        <blockquote>{job['query']}</blockquote>
        
        <p>We've broken this down into {job['estimated_tasks']} tasks and will notify you when it's complete.</p>
        
        <p>You can check progress anytime by visiting <a href='{get_project_url(job['id'])}'>your project page</a>.</p>
        
        <p>Estimated completion time: {get_estimated_completion_time(job)}</p>
        
        <p>Thank you for using Empire!</p>
        """
    }

def get_project_completed_template(job: dict, report_url: str) -> dict:
    """Get email template for project completed notification."""
    return {
        'subject': f"Research Complete: {job['query'][:50]}...",
        'body': f"""<p>Hello,</p>
        
        <p>Great news! Your research project is complete:</p>
        
        <blockquote>{job['query']}</blockquote>
        
        <p><strong>Key Findings:</strong></p>
        <ul>
        {get_key_findings_html(job)}
        </ul>
        
        <p>Your full report is ready to view:</p>
        
        <p><a href='{report_url}' class='button'>View Full Report</a></p>
        
        <p>You can also access this and all your research projects from <a href='{get_projects_url()}'>your dashboard</a>.</p>
        
        <p>Thank you for using Empire!</p>
        """
    }
```

3. Email sending implementation:
```python
def send_email(to_email: str, template: dict) -> None:
    """Send email using SendGrid/Postmark."""
    # Use existing email service
    from app.services.email import send_email_template
    
    send_email_template(
        to_email=to_email,
        subject=template['subject'],
        html_content=template['body'],
        text_content=html_to_text(template['body'])
    )
```

4. Notification triggers:
```python
def register_notification_hooks() -> None:
    """Register hooks for notification events."""
    # Project started - after initialization
    @app.task_success.connect(sender=initialize_research_job)
    def on_project_initialized(sender=None, **kwargs):
        job_id = kwargs['args'][0]
        notification_service = NotificationService()
        notification_service.send_project_started(job_id)
    
    # Project completed - after report generation
    @app.task_success.connect(sender=generate_research_report)
    def on_report_generated(sender=None, **kwargs):
        job_id = kwargs['args'][0]
        notification_service = NotificationService()
        notification_service.send_project_completed(job_id)
    
    # Project failed - after any critical error
    @app.task_failure.connect
    def on_task_failure(sender=None, task_id=None, exception=None, **kwargs):
        if sender in [initialize_research_job, execute_research_task, generate_research_report]:
            job_id = kwargs['args'][0]
            notification_service = NotificationService()
            notification_service.send_project_failed(job_id, str(exception))
```

### Test Strategy

1. Unit test email template generation with various job states
2. Test notification triggers with simulated task events
3. Verify email sending integration with SendGrid/Postmark
4. Test HTML to text conversion for plain text emails
5. Integration test with actual email delivery (to test accounts)
6. Test error handling for email sending failures
7. Verify all notification types (start, progress, complete, fail)

---

## Task 100: Implement Performance Monitoring and Metrics

**Priority**: medium
**Status**: pending
**Dependencies**: 96

### Description

Create a comprehensive performance monitoring system to track efficiency metrics and ensure quality standards.

### Implementation Details

Create `app/services/performance_monitor.py` with the following components:

1. `PerformanceMonitor` class with methods:
   - `record_task_timing(task_id: int, start_time: float, end_time: float) -> None` - Record task execution time
   - `calculate_parallelism_ratio(job_id: int) -> float` - Calculate ratio of concurrent execution
   - `record_quality_score(artifact_id: int, score: float) -> None` - Record quality metrics
   - `generate_job_metrics(job_id: int) -> dict` - Calculate comprehensive job metrics
   - `check_sla_compliance(job_id: int) -> bool` - Check if job meets SLA targets

2. Prometheus metrics implementation:
```python
from prometheus_client import Histogram, Gauge, Counter

# Task execution metrics
research_task_duration_seconds = Histogram(
    'research_task_duration_seconds',
    'Task execution duration',
    ['task_type', 'job_id']
)

research_task_queue_wait_seconds = Histogram(
    'research_task_queue_wait_seconds',
    'Time task spent waiting in queue',
    ['task_type']
)

research_concurrent_tasks = Gauge(
    'research_concurrent_tasks',
    'Current number of concurrent tasks',
    ['job_id']
)

research_parallelism_ratio = Gauge(
    'research_parallelism_ratio',
    'Ratio of concurrent execution achieved',
    ['job_id']
)

research_wave_transition_latency_seconds = Histogram(
    'research_wave_transition_latency_seconds',
    'Time between wave completion and next wave start'
)

research_quality_score = Gauge(
    'research_quality_score',
    'Quality score for artifacts',
    ['artifact_type', 'job_id']
)

research_quality_gate_failures_total = Counter(
    'research_quality_gate_failures_total',
    'Number of quality gate failures',
    ['gate_type', 'job_id']
)
```

3. Performance data collection:
```python
def instrument_task_execution(task_id: int, func):
    """Decorator to instrument task execution."""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        # Get task info
        task = get_task(task_id)
        job_id = task['job_id']
        task_type = task['task_type']
        
        # Record queue wait time
        queue_wait = time.time() - task['created_at'].timestamp()
        research_task_queue_wait_seconds.labels(task_type).observe(queue_wait)
        
        # Update concurrent tasks gauge
        current_running = get_running_task_count(job_id)
        research_concurrent_tasks.labels(job_id).set(current_running)
        
        # Execute task and time it
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            
            # Record execution time
            duration = time.time() - start_time
            research_task_duration_seconds.labels(task_type, job_id).observe(duration)
            
            # Record task timing in database
            record_task_timing(task_id, start_time, time.time())
            
            return result
        except Exception as e:
            # Record failure
            duration = time.time() - start_time
            research_task_duration_seconds.labels(task_type, job_id).observe(duration)
            raise e
    
    return wrapper
```

4. SLA compliance checking:
```python
def check_sla_compliance(job_id: int) -> dict:
    """Check if job meets SLA targets."""
    job = get_job(job_id)
    task_count = get_task_count(job_id)
    duration = (job['completed_at'] - job['created_at']).total_seconds()
    
    # Determine SLA based on task count
    if task_count <= 5:
        sla_target = 120  # 2 minutes
        complexity = 'simple'
    elif task_count <= 10:
        sla_target = 300  # 5 minutes
        complexity = 'medium'
    else:
        sla_target = 900  # 15 minutes
        complexity = 'complex'
    
    # Check compliance
    compliant = duration <= sla_target
    
    return {
        'compliant': compliant,
        'duration': duration,
        'sla_target': sla_target,
        'complexity': complexity,
        'margin': sla_target - duration
    }
```

5. Quality gate tracking:
```python
def record_quality_gate_result(job_id: int, gate_type: str, passed: bool) -> None:
    """Record quality gate result."""
    if not passed:
        research_quality_gate_failures_total.labels(gate_type, job_id).inc()
    
    # Store in database
    db.execute(
        """INSERT INTO research_quality_gates 
           (job_id, gate_type, passed, recorded_at)
           VALUES (:job_id, :gate_type, :passed, NOW())
        """,
        {
            'job_id': job_id,
            'gate_type': gate_type,
            'passed': passed
        }
    )
```

### Test Strategy

1. Unit test metric recording with simulated task executions
2. Test parallelism ratio calculation with various execution patterns
3. Verify SLA compliance checking with different job durations
4. Test quality gate tracking for pass/fail scenarios
5. Integration test with Prometheus metrics endpoint
6. Benchmark performance overhead of instrumentation
7. Test dashboard data collection with sample jobs
8. Verify alert conditions trigger correctly

---

